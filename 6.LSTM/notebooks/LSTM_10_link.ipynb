{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/link/anaconda3/lib/python3.6/site-packages/h5py/__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n",
      "Using TensorFlow backend.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using Keras version: 2.2.2 backend: tensorflow\n",
      "BTC_10_25_1_0.1_param.pickle FILE ALREADY EXIST.\n",
      "ETH_10_25_1_0.1_param.pickle FILE ALREADY EXIST.\n",
      "[INFO] X_train.shape : (42027, 8, 25, 4)\n",
      "[INFO] y_train.shape : (42027, 2)\n",
      "[INFO] X_test.shape : (10507, 8, 25, 4)\n",
      "[INFO] y_test.shape : (10507, 2)\n",
      "\n",
      "[INFO] X_train_2.shape: (42027, 25, 8, 4)\n",
      "[INFO] X_test_2.shape: (10507, 25, 8, 4)\n",
      "\n",
      "[INFO] X_train_3.shape: (42027, 25, 32)\n",
      "[INFO] X_test_3.shape: (10507, 25, 32)\n",
      "\n",
      "[INFO] X_train_reshape.shape: (42027, 800)\n",
      "[INFO] X_test_reshape.shape: (10507, 800)\n",
      "\n",
      "\n",
      "\n",
      "----------------------\n",
      "__XRP__time unit: 10  |  window_size :25  |  gap :1  |  margin_rate :0.1  started.\n",
      "Fitting 1000 folds for each of 30 candidates, totalling 30000 fits\n",
      "Train on 41984 samples, validate on 10507 samples\n",
      "Epoch 1/50\n"
     ]
    },
    {
     "ename": "InternalError",
     "evalue": "Blas GEMM launch failed : a.shape=(256, 32), b.shape=(32, 64), m=256, n=64, k=32\n\t [[Node: lstm_1/while/MatMul_3 = MatMul[T=DT_FLOAT, _class=[\"loc:@training/SGD/gradients/lstm_1/while/MatMul_3_grad/MatMul_1\"], transpose_a=false, transpose_b=false, _device=\"/job:localhost/replica:0/task:0/device:GPU:0\"](lstm_1/while/TensorArrayReadV3, lstm_1/while/MatMul_3/Enter)]]\n\t [[Node: metrics/acc/Mean/_81 = _Recv[client_terminated=false, recv_device=\"/job:localhost/replica:0/task:0/device:CPU:0\", send_device=\"/job:localhost/replica:0/task:0/device:GPU:0\", send_device_incarnation=1, tensor_name=\"edge_1789_metrics/acc/Mean\", tensor_type=DT_FLOAT, _device=\"/job:localhost/replica:0/task:0/device:CPU:0\"]()]]",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mInternalError\u001b[0m                             Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-1-ec82199daca6>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m    711\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    712\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0m__name__\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m\"__main__\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 713\u001b[0;31m     \u001b[0mStart_Model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    714\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    715\u001b[0m     \u001b[0;31m# dir = \"./evaluate_result/\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-1-ec82199daca6>\u001b[0m in \u001b[0;36mStart_Model\u001b[0;34m()\u001b[0m\n\u001b[1;32m    669\u001b[0m                                        validation_data=(X_test_scaled,\n\u001b[1;32m    670\u001b[0m                                                         y_test),\n\u001b[0;32m--> 671\u001b[0;31m                                        callbacks=[early_stop])\n\u001b[0m\u001b[1;32m    672\u001b[0m             \u001b[0;31m# grid_result = grid_result.reset_states()\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    673\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/sklearn/model_selection/_search.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, X, y, groups, **fit_params)\u001b[0m\n\u001b[1;32m    637\u001b[0m                                   error_score=self.error_score)\n\u001b[1;32m    638\u001b[0m           for parameters, (train, test) in product(candidate_params,\n\u001b[0;32m--> 639\u001b[0;31m                                                    cv.split(X, y, groups)))\n\u001b[0m\u001b[1;32m    640\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    641\u001b[0m         \u001b[0;31m# if one choose to see train score, \"out\" will contain train score info\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/sklearn/externals/joblib/parallel.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, iterable)\u001b[0m\n\u001b[1;32m    777\u001b[0m             \u001b[0;31m# was dispatched. In particular this covers the edge\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    778\u001b[0m             \u001b[0;31m# case of Parallel used with an exhausted iterator.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 779\u001b[0;31m             \u001b[0;32mwhile\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdispatch_one_batch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0miterator\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    780\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_iterating\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    781\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/sklearn/externals/joblib/parallel.py\u001b[0m in \u001b[0;36mdispatch_one_batch\u001b[0;34m(self, iterator)\u001b[0m\n\u001b[1;32m    623\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    624\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 625\u001b[0;31m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_dispatch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtasks\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    626\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    627\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/sklearn/externals/joblib/parallel.py\u001b[0m in \u001b[0;36m_dispatch\u001b[0;34m(self, batch)\u001b[0m\n\u001b[1;32m    586\u001b[0m         \u001b[0mdispatch_timestamp\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtime\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    587\u001b[0m         \u001b[0mcb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mBatchCompletionCallBack\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdispatch_timestamp\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 588\u001b[0;31m         \u001b[0mjob\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_backend\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mapply_async\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcallback\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcb\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    589\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jobs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mjob\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    590\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/sklearn/externals/joblib/_parallel_backends.py\u001b[0m in \u001b[0;36mapply_async\u001b[0;34m(self, func, callback)\u001b[0m\n\u001b[1;32m    109\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mapply_async\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcallback\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    110\u001b[0m         \u001b[0;34m\"\"\"Schedule a func to be run\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 111\u001b[0;31m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mImmediateResult\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfunc\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    112\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mcallback\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    113\u001b[0m             \u001b[0mcallback\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mresult\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/sklearn/externals/joblib/_parallel_backends.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, batch)\u001b[0m\n\u001b[1;32m    330\u001b[0m         \u001b[0;31m# Don't delay the application, to avoid keeping the input\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    331\u001b[0m         \u001b[0;31m# arguments in memory\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 332\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mresults\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbatch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    333\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    334\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/sklearn/externals/joblib/parallel.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    129\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    130\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__call__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 131\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitems\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    132\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    133\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__len__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/sklearn/externals/joblib/parallel.py\u001b[0m in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m    129\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    130\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__call__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 131\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitems\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    132\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    133\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__len__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/sklearn/model_selection/_validation.py\u001b[0m in \u001b[0;36m_fit_and_score\u001b[0;34m(estimator, X, y, scorer, train, test, verbose, parameters, fit_params, return_train_score, return_parameters, return_n_test_samples, return_times, error_score)\u001b[0m\n\u001b[1;32m    456\u001b[0m             \u001b[0mestimator\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mfit_params\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    457\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 458\u001b[0;31m             \u001b[0mestimator\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mfit_params\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    459\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    460\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/keras/wrappers/scikit_learn.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, x, y, sample_weight, **kwargs)\u001b[0m\n\u001b[1;32m    208\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0msample_weight\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    209\u001b[0m             \u001b[0mkwargs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'sample_weight'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msample_weight\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 210\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0msuper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mKerasClassifier\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    211\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    212\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/keras/wrappers/scikit_learn.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, x, y, **kwargs)\u001b[0m\n\u001b[1;32m    150\u001b[0m         \u001b[0mfit_args\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    151\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 152\u001b[0;31m         \u001b[0mhistory\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mfit_args\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    153\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    154\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mhistory\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/keras/engine/training.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, **kwargs)\u001b[0m\n\u001b[1;32m   1035\u001b[0m                                         \u001b[0minitial_epoch\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minitial_epoch\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1036\u001b[0m                                         \u001b[0msteps_per_epoch\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msteps_per_epoch\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1037\u001b[0;31m                                         validation_steps=validation_steps)\n\u001b[0m\u001b[1;32m   1038\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1039\u001b[0m     def evaluate(self, x=None, y=None,\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/keras/engine/training_arrays.py\u001b[0m in \u001b[0;36mfit_loop\u001b[0;34m(model, f, ins, out_labels, batch_size, epochs, verbose, callbacks, val_f, val_ins, shuffle, callback_metrics, initial_epoch, steps_per_epoch, validation_steps)\u001b[0m\n\u001b[1;32m    197\u001b[0m                     \u001b[0mins_batch\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mins_batch\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtoarray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    198\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 199\u001b[0;31m                 \u001b[0mouts\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mins_batch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    200\u001b[0m                 \u001b[0mouts\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mto_list\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mouts\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    201\u001b[0m                 \u001b[0;32mfor\u001b[0m \u001b[0ml\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mo\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mzip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mout_labels\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mouts\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/keras/backend/tensorflow_backend.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, inputs)\u001b[0m\n\u001b[1;32m   2664\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_legacy_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2665\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2666\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2667\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2668\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mpy_any\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mis_tensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/keras/backend/tensorflow_backend.py\u001b[0m in \u001b[0;36m_call\u001b[0;34m(self, inputs)\u001b[0m\n\u001b[1;32m   2634\u001b[0m                                 \u001b[0msymbol_vals\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2635\u001b[0m                                 session)\n\u001b[0;32m-> 2636\u001b[0;31m         \u001b[0mfetched\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_callable_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0marray_vals\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2637\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mfetched\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moutputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2638\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m   1452\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1453\u001b[0m           return tf_session.TF_DeprecatedSessionRunCallable(\n\u001b[0;32m-> 1454\u001b[0;31m               self._session._session, self._handle, args, status, None)\n\u001b[0m\u001b[1;32m   1455\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1456\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__del__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/tensorflow/python/framework/errors_impl.py\u001b[0m in \u001b[0;36m__exit__\u001b[0;34m(self, type_arg, value_arg, traceback_arg)\u001b[0m\n\u001b[1;32m    517\u001b[0m             \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    518\u001b[0m             \u001b[0mcompat\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mas_text\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mc_api\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTF_Message\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstatus\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstatus\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 519\u001b[0;31m             c_api.TF_GetCode(self.status.status))\n\u001b[0m\u001b[1;32m    520\u001b[0m     \u001b[0;31m# Delete the underlying status object from memory otherwise it stays alive\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    521\u001b[0m     \u001b[0;31m# as there is a reference to status from this from the traceback due to\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mInternalError\u001b[0m: Blas GEMM launch failed : a.shape=(256, 32), b.shape=(32, 64), m=256, n=64, k=32\n\t [[Node: lstm_1/while/MatMul_3 = MatMul[T=DT_FLOAT, _class=[\"loc:@training/SGD/gradients/lstm_1/while/MatMul_3_grad/MatMul_1\"], transpose_a=false, transpose_b=false, _device=\"/job:localhost/replica:0/task:0/device:GPU:0\"](lstm_1/while/TensorArrayReadV3, lstm_1/while/MatMul_3/Enter)]]\n\t [[Node: metrics/acc/Mean/_81 = _Recv[client_terminated=false, recv_device=\"/job:localhost/replica:0/task:0/device:CPU:0\", send_device=\"/job:localhost/replica:0/task:0/device:GPU:0\", send_device_incarnation=1, tensor_name=\"edge_1789_metrics/acc/Mean\", tensor_type=DT_FLOAT, _device=\"/job:localhost/replica:0/task:0/device:CPU:0\"]()]]"
     ],
     "output_type": "error"
    }
   ],
   "source": [
    "\"\"\"\n",
    "LSTM\n",
    "순환 신경망\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "# import library\n",
    "import tensorflow as tf\n",
    "import keras\n",
    "from keras import __version__\n",
    "from keras import backend as K\n",
    "from keras.constraints import maxnorm\n",
    "\n",
    "print('Using Keras version:', __version__, 'backend:', K.backend())\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, LSTM, Bidirectional, BatchNormalization, Embedding, CuDNNLSTM, Dropout\n",
    "from keras.wrappers.scikit_learn import KerasClassifier\n",
    "from keras.callbacks import EarlyStopping\n",
    "from sklearn.preprocessing import MinMaxScaler, StandardScaler\n",
    "from sklearn.model_selection import train_test_split, RandomizedSearchCV, GridSearchCV\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from datetime import datetime\n",
    "import sys\n",
    "import os.path\n",
    "import os\n",
    "os.environ['KMP_DUPLICATE_LIB_OK']='True'\n",
    "from os.path import isfile, join\n",
    "import boto3\n",
    "import pickle\n",
    "import time\n",
    "import warnings\n",
    "import smtplib\n",
    "from email.mime.text import MIMEText\n",
    "from keras.utils import multi_gpu_model\n",
    "# import email_info\n",
    "\n",
    "# stopper = EarlyStopping(monitor='val_loss', patience=3, verbose=1, min_delta=0.001)\n",
    "early_stop = keras.callbacks.EarlyStopping(monitor='f1_metric', mode='max', patience = 2, verbose=1)\n",
    "\n",
    "sys.path.append(os.getcwd())\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "'''\n",
    "slave05> [30, 60] BTC, ETH, XRP\n",
    "slave04> [30, 60] BCH, LTC, DASH\n",
    "link> [10, 30, 60] ETC\n",
    "'''\n",
    "coins = {\n",
    "    0: 'KRW',\n",
    "#     1: 'BTC',\n",
    "#     2: 'ETH',\n",
    "#     3: 'XRP',\n",
    "#     4: 'BCH',\n",
    "#     5: 'LTC',\n",
    "#     6: 'DASH',\n",
    "    7: 'ETC'\n",
    "}\n",
    "# aws_client = boto3.client(\n",
    "#     's3',\n",
    "#     aws_access_key_id=LINK_AWSAccessKeyId,\n",
    "#     aws_secret_access_key=LINK_AWSSecretKey\n",
    "# )\n",
    "bucket = \"bithumb10\"\n",
    "cleanup_file_name = \"coin_{0}_{1}_cleanup.csv\"\n",
    "#######################################################\n",
    "def Load_Dataset_X(dir_path, time_unit, window_size, gap, margin_rate):\n",
    "    key_name_X = \"X_\" + \\\n",
    "                 str(time_unit) + \"_\" + \\\n",
    "                 str(window_size) + \"_\" + \\\n",
    "                 str(gap) + \"_\" + \\\n",
    "                 str(margin_rate)\n",
    "\n",
    "    with open(dir_path + key_name_X + \".pickle\", 'rb') as handle:\n",
    "        b_x = pickle.load(handle)\n",
    "    return b_x\n",
    "def Load_Dataset_y(dir_path, time_unit, window_size, gap, margin_rate):\n",
    "    key_name_y = \"y_\" + \\\n",
    "                 str(time_unit) + \"_\" + \\\n",
    "                 str(window_size) + \"_\" + \\\n",
    "                 str(gap) + \"_\" + \\\n",
    "                 str(margin_rate)\n",
    "\n",
    "    with open(dir_path + key_name_y + \".pickle\", 'rb') as handle:\n",
    "        b_y = pickle.load(handle)\n",
    "    return b_y\n",
    "def recall(y_true, y_pred):\n",
    "    K.set_epsilon(1e-05)\n",
    "    true_positives = K.sum(K.round(K.clip(y_true * y_pred, 0, 1)))\n",
    "    possible_positives = K.sum(K.round(K.clip(y_true, 0, 1)))\n",
    "    recall = true_positives / (possible_positives + K.epsilon())\n",
    "    return recall\n",
    "def precision(y_true, y_pred):\n",
    "    K.set_epsilon(1e-05)\n",
    "    true_positives = K.sum(K.round(K.clip(y_true * y_pred, 0, 1)))\n",
    "    predicted_positives = K.sum(K.round(K.clip(y_pred, 0, 1)))\n",
    "    precision = true_positives / (predicted_positives + K.epsilon())\n",
    "    return precision\n",
    "def f1_metric(y_true, y_pred):\n",
    "    def recall(y_true, y_pred):\n",
    "        true_positives = K.sum(K.round(K.clip(y_true * y_pred, 0, 1)))\n",
    "        possible_positives = K.sum(K.round(K.clip(y_true, 0, 1)))\n",
    "        recall = true_positives / (possible_positives + K.epsilon())\n",
    "        return recall\n",
    "\n",
    "    def precision(y_true, y_pred):\n",
    "        true_positives = K.sum(K.round(K.clip(y_true * y_pred, 0, 1)))\n",
    "        predicted_positives = K.sum(K.round(K.clip(y_pred, 0, 1)))\n",
    "        precision = true_positives / (predicted_positives + K.epsilon())\n",
    "        return precision\n",
    "    precision = precision(y_true, y_pred)\n",
    "    recall = recall(y_true, y_pred)\n",
    "    return 2*((precision*recall)/(precision+recall+K.epsilon()))\n",
    "def _f1_score(y_true, y_pred):\n",
    "    def recall(y_true, y_pred):\n",
    "        K.set_epsilon(1e-05)\n",
    "        true_positives = K.sum(K.round(K.clip(y_true * y_pred, 0, 1)))\n",
    "        possible_positives = K.sum(K.round(K.clip(y_true, 0, 1)))\n",
    "        recall = true_positives / (possible_positives + K.epsilon())\n",
    "        return recall\n",
    "\n",
    "    def precision(y_true, y_pred):\n",
    "        K.set_epsilon(1e-05)\n",
    "        true_positives = K.sum(K.round(K.clip(y_true * y_pred, 0, 1)))\n",
    "        predicted_positives = K.sum(K.round(K.clip(y_pred, 0, 1)))\n",
    "        precision = true_positives / (predicted_positives + K.epsilon())\n",
    "        return precision\n",
    "\n",
    "    precision = precision(y_true, y_pred)\n",
    "    recall = recall(y_true, y_pred)\n",
    "    return 2 * ((precision * recall) / (precision + recall + K.epsilon()))\n",
    "def input_reshape(X_train_data, X_test_data, n_steps, n_coins, n_price):\n",
    "    X_train_reshape = X_train_data.reshape(\n",
    "        -1,\n",
    "        n_steps,\n",
    "        n_coins * n_price\n",
    "    )\n",
    "    X_test_reshape = X_test_data.reshape(\n",
    "        -1,\n",
    "        n_steps,\n",
    "        n_coins * n_price\n",
    "    )\n",
    "    return X_train_reshape, X_test_reshape\n",
    "def onehottify(x, n=None, dtype=np.int):\n",
    "    \"\"\"1-hot encode x with the max value n (computed from data if n is None).\"\"\"\n",
    "    x = np.asarray(x)\n",
    "    n = np.max(x) + 1 if n is None else n\n",
    "    return np.eye(n, dtype=dtype)[x]\n",
    "def sending_eamil(message):\n",
    "    f = open('../../../email_info.bin', 'rb')\n",
    "    a = pickle.load(f)\n",
    "\n",
    "    smtp = smtplib.SMTP_SSL('smtp.naver.com', 465)\n",
    "    smtp.ehlo()  # say Hello\n",
    "    # smtp.starttls()  # TLS 사용시 필요\n",
    "    smtp.login(a['email'], a['pw'])\n",
    "\n",
    "    msg = MIMEText(str(message)) \n",
    "    msg['Subject'] = '실험 완료'\n",
    "    msg['To'] = 'ulujo_dohk@naver.com'\n",
    "    smtp.sendmail(a['email'], 'ulujo_dohk@naver.com', msg.as_string())\n",
    "\n",
    "    smtp.quit()\n",
    "\n",
    "class SortedDisplayDict(dict):\n",
    "    def __str__(self):\n",
    "        return \"{\" + \", \".join(\"%r: %r\" % (key, self[key]) for key in sorted(self)) + \"}\"\n",
    "\n",
    "    def ordered_keys(self):\n",
    "        return sorted(self.keys())\n",
    "def search(dirname):\n",
    "    filenames = os.listdir(dirname)\n",
    "    fileList = []\n",
    "    for filename in filenames:\n",
    "        full_filename = os.path.join(dirname, filename)\n",
    "        fileList.append(full_filename)\n",
    "    return fileList\n",
    "def drawGraph(dir):\n",
    "    fileList = search(dir)\n",
    "    temp_list = []\n",
    "    cluster_coef_value_list_lstm = []\n",
    "    for file in fileList:\n",
    "        temp_list.append(pd.read_pickle(file))\n",
    "\n",
    "        cluster_coef_value_list_lstm = []\n",
    "    for i in range(len(temp_list)):\n",
    "        # print(temp_list[i])\n",
    "        # print()\n",
    "        # print(temp_list[i][list(temp_list[i].keys())[0]])\n",
    "        # print()\n",
    "        cluster_coef_value_list_lstm.append(temp_list[i][list(temp_list[i].keys())[0]]['Score'][0])\n",
    "        # print(score)\n",
    "        # print()\n",
    "\n",
    "    cluster_coef_value_list_gradientBoosting = [0.671, 0.616, 0.622, 0.672, 0.7, 0.69, 0.69]\n",
    "\n",
    "    first_legend_label = 'xgboost'\n",
    "    second_legend_label = 'lstm'\n",
    "    x_label = 'Cryptocurrency'\n",
    "    y_label = 'f1-score'\n",
    "    filename = str(datetime.now())\n",
    "\n",
    "    graph(cluster_coef_value_list_gradientBoosting,\n",
    "          cluster_coef_value_list_lstm[:7],\n",
    "          first_legend_label,\n",
    "          second_legend_label,\n",
    "          x_label,\n",
    "          y_label,\n",
    "          filename)\n",
    "def graph(cluster_coef_value_list_xgboost,\n",
    "          cluster_coef_value_list_lstm,\n",
    "          first_legend_label,\n",
    "          second_legend_label,\n",
    "          x_label,\n",
    "          y_label,\n",
    "          filename):\n",
    "    '''\n",
    "    < EXAMPLE >\n",
    "    f1_score = [0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9] # 9개\n",
    "    first_legend_label = 'xgboost'\n",
    "    second_legend_label = 'lstm'\n",
    "    x_label = 'Cryptocurrency'\n",
    "    y_label = 'f1-score'\n",
    "    filename = '_clustering_and_diameter'\n",
    "    '''\n",
    "    link_addition_ratio = f1_score\n",
    "    data_profile = {\n",
    "        \"Infocom05\": {\n",
    "            \"file_name\": \"data_infocom05.csv\",\n",
    "            \"num_nodes\": 41,\n",
    "            \"median\": 2684,\n",
    "            \"mean\": 9961,\n",
    "            \"std\": 26513,\n",
    "            \"contact_weight_map\": [((6, 9), 2187), ((12, 1), 4943), ((7, 12), 40849), ((1, 6), 768)],\n",
    "\n",
    "            \"graph\": {},\n",
    "            \"numberOfNodes\": {},\n",
    "            \"numberOfEdges\": {},\n",
    "            \"durationThreshold\": {},\n",
    "\n",
    "            \"density\": {},\n",
    "            \"clustering_coefficient\": {},\n",
    "            \"diameter_cc\": {},\n",
    "\n",
    "            \"global_bet\": {},\n",
    "            \"Brandes_ego_bet\": {},\n",
    "            \"Brandes_ego_elapsed_time\": {},\n",
    "            \"Brandes_xego_bet\": {},\n",
    "            \"Brandes_xego_elapsed_time\": {},\n",
    "\n",
    "            \"Proposed_ego_bet\": {},\n",
    "            \"Proposed_ego_elapsed_time\": {},\n",
    "            \"Proposed_xego_bet\": {},\n",
    "            \"Proposed_xego_elapsed_time\": {},\n",
    "\n",
    "            \"ego_global_pearson_corr\": {},\n",
    "            \"xego_global_pearson_corr\": {},\n",
    "\n",
    "            \"ego_global_spearman_corr\": {},\n",
    "            \"xego_global_spearman_corr\": {},\n",
    "\n",
    "            \"ego_node_coverage_in_connected_component\": {},\n",
    "            \"ego_edge_coverage_in_connected_component\": {},\n",
    "\n",
    "            \"xego_node_coverage_in_connected_component\": {},\n",
    "            \"xego_edge_coverage_in_connected_component\": {}\n",
    "        }\n",
    "    }\n",
    "    xticklabels = [r'$BTC$', r'$ETH$', r'$XRP$', r'$BCH$', r'$LTC$', r'$DASH$', r'$ETC$']\n",
    "    yticklabels = [r'$0.40$', r'$0.45$', r'$0.50$', r'$0.55$', r'$0.60$', r'$0.65$', r'$0.70$', r'$0.75$', r'$0.80$']\n",
    "    # yticklabels2 = [r'$0$', r'$2$', r'$4$', r'$6$', r'$8$', r'$10$', r'$12$']\n",
    "\n",
    "    ind = np.arange(len(xticklabels))\n",
    "\n",
    "    fig, axes = plt.subplots(1, 1, figsize=(8.719, 6.07))\n",
    "    subfigures = {}\n",
    "    data_name = 'Infocom05'\n",
    "    subfigures[data_name] = axes\n",
    "    subfigures[data_name].set_xticks(ind)\n",
    "    subfigures[data_name].set_xticklabels(xticklabels, fontsize=21)\n",
    "    subfigures[data_name].set_ylim([0.4, 0.8])\n",
    "    subfigures[data_name].set_yticklabels(yticklabels, fontsize=21)\n",
    "\n",
    "    cluster_coef_dic = SortedDisplayDict(data_profile[data_name]['clustering_coefficient'])\n",
    "    cluster_coef_value_list = [cluster_coef_dic[x] for x in cluster_coef_dic.ordered_keys()]\n",
    "\n",
    "    # cluster_coef_value_list_xgboost = [0.56, 0.67, 0.66, 0.78, 0.45, 0.47, 0.65]\n",
    "    # cluster_coef_value_list_lstm = [0.66, 0.77, 0.68, 0.68, 0.55, 0.57, 0.66]\n",
    "    cluster_coef_value_list_xgboost = cluster_coef_value_list_xgboost\n",
    "    cluster_coef_value_list_lstm = cluster_coef_value_list_lstm\n",
    "    subfigures[data_name].plot(ind, cluster_coef_value_list_xgboost,\n",
    "                               color='k', linestyle='-', marker='s', markersize=8,\n",
    "                               label=first_legend_label)\n",
    "    subfigures[data_name].plot(ind, cluster_coef_value_list_lstm,\n",
    "                               color='k', linestyle='--', marker='^', markersize=8,\n",
    "                               label=second_legend_label)\n",
    "    subfigures[data_name].set_xlabel(x_label, fontsize=21)\n",
    "    subfigures[data_name].set_ylabel(y_label, fontsize=21)\n",
    "\n",
    "    # subfigures[data_name].set_title(data_name, fontsize=21)\n",
    "    subfigures[data_name].grid(True)\n",
    "    if data_name == 'Infocom05':\n",
    "        subfigures[data_name].legend(loc=30, fontsize=18)\n",
    "\n",
    "    # subfigures[data_name] = subfigures[data_name].twinx()\n",
    "    # subfigures[data_name].set_ylim([0, 12])\n",
    "    # subfigures[data_name].set_yticklabels(yticklabels2, fontsize=21)\n",
    "\n",
    "    diameter_dic = SortedDisplayDict(data_profile[data_name]['diameter_cc'])\n",
    "    diameter_list = [diameter_dic[x] for x in diameter_dic.ordered_keys()]\n",
    "    # subfigures[data_name].bar(ind, diameter_list, barWidth, color='k', alpha=0.3, label='Diameter of Connected Component')\n",
    "    # subfigures[data_name].set_ylabel('Diameter of Connected Component', fontsize=21)\n",
    "    if data_name == 'Infocom05':\n",
    "        subfigures[data_name].legend(loc=4, fontsize=18)\n",
    "    subfigures[data_name].grid(True)\n",
    "\n",
    "    fig.savefig('./img/' + filename + '.pdf', format='pdf', bbox_inches='tight')\n",
    "    plt.show()\n",
    "\n",
    "def create_model(n_state_units, \n",
    "                 neurons,\n",
    "                 init,\n",
    "                 activation,\n",
    "                 activation_1,\n",
    "                 window_size, \n",
    "                 optimizer,\n",
    "                 weight_constraint,\n",
    "                 dropout_rate):\n",
    "\n",
    "    model = Sequential()\n",
    "    model.add(\n",
    "        LSTM(units=n_state_units,\n",
    "             activation=activation,\n",
    "             input_shape=(window_size, 32)))\n",
    "    model.add(Dense(neurons, input_dim=8, \n",
    "                    init=init, \n",
    "                    activation=activation_1,\n",
    "                    kernel_constraint=maxnorm(weight_constraint)))\n",
    "    model.add(Dense(8, init=init, activation=activation_1))\n",
    "    model.add(Dense(2, init=init, activation=activation_1))\n",
    "    # Compile model\n",
    "    model.compile(loss=keras.losses.categorical_crossentropy, \n",
    "                  optimizer=optimizer, \n",
    "                  metrics=[f1_metric, 'accuracy', recall, precision])\n",
    "\n",
    "    return model\n",
    "\n",
    "def vanilla_LSTM(window_size, units_1, units_2, n_state_units=32, activation_1='softmax', activation_2='relu', optimizer='adam'):\n",
    "    #     global metrics\n",
    "    K.clear_session()\n",
    "    config = tf.ConfigProto()\n",
    "    config.gpu_options.per_process_gpu_memory_fraction = 0.4  # what portion of gpu to use\n",
    "    session = tf.Session(config=config)\n",
    "    K.set_session(session)\n",
    "\n",
    "    model = Sequential()\n",
    "    model.add(\n",
    "        LSTM(units=n_state_units,\n",
    "             activation=activation_1,\n",
    "             input_shape=(window_size, 32)))\n",
    "    model.add(Dense(2))\n",
    "    # model = multi_gpu_model(model, gpus=2)\n",
    "    model.compile(loss=keras.losses.categorical_crossentropy,\n",
    "                  optimizer=optimizer,\n",
    "                  metrics=[_f1_score, 'accuracy', recall, precision])\n",
    "\n",
    "    return model\n",
    "def stacked_LSTM(window_size, n_state_units=32, activation_1='softmax', activation_2='relu', optimizer='adam'):\n",
    "    #     global metrics\n",
    "    K.clear_session()\n",
    "    model = Sequential()\n",
    "    model.add(\n",
    "        LSTM(units=n_state_units,\n",
    "             activation=activation_1,\n",
    "             return_sequences=True,\n",
    "             input_shape=(window_size, 32)))\n",
    "    model.add(\n",
    "        LSTM(units=n_state_units,\n",
    "             activation=activation_2))\n",
    "    model.add(Dense(2))\n",
    "    model.compile(loss=keras.losses.categorical_crossentropy,\n",
    "                  optimizer=optimizer,\n",
    "                  metrics=[_f1_score, 'accuracy', recall, precision])\n",
    "\n",
    "    return model\n",
    "def bidirectioanl_LSTM(window_size, units_1, units_2, n_state_units=32, activation_1='softmax', activation_2='relu', optimizer='adam'):\n",
    "    #     global metrics\n",
    "    K.clear_session()\n",
    "    model = Sequential()\n",
    "    model.add(\n",
    "        Bidirectional(\n",
    "            LSTM(units=n_state_units,\n",
    "                 activation=activation_1),\n",
    "            input_shape=(window_size, 32)))\n",
    "    model.add(Dense(2))\n",
    "    model.compile(loss=keras.losses.categorical_crossentropy,\n",
    "                  optimizer=optimizer,\n",
    "                  metrics=[_f1_score, 'accuracy', recall, precision])\n",
    "\n",
    "    return model\n",
    "def bidirectioanl_LSTM_with_BN(window_size, units_1, units_2, n_state_units=32, activation_1='softmax', activation_2='relu', optimizer='adam'):\n",
    "    # https://keras.io/layers/normalization/\n",
    "    #     global metrics\n",
    "    K.clear_session()\n",
    "    model = Sequential()\n",
    "    model.add(\n",
    "        Bidirectional(\n",
    "            LSTM(units=n_state_units,\n",
    "                 activation=activation_1),\n",
    "            input_shape=(window_size, 32)))\n",
    "    model.add(BatchNormalization())\n",
    "    model.add(Dense(2))\n",
    "    model.compile(loss=keras.losses.categorical_crossentropy,\n",
    "                  optimizer=optimizer,\n",
    "                  metrics=[_f1_score, 'accuracy', recall, precision])\n",
    "\n",
    "    return model\n",
    "def stacked_bidirectioanl_LSTM(window_size, units_1, units_2, n_state_units=32, activation_1='softmax', activation_2='relu', optimizer='adam'):\n",
    "    #     global metrics\n",
    "    K.clear_session()\n",
    "    model = Sequential()\n",
    "    model.add(\n",
    "        Bidirectional(\n",
    "            LSTM(units=n_state_units,\n",
    "                 activation=activation_1,\n",
    "                 return_sequences=True),\n",
    "            input_shape=(window_size, 32)))\n",
    "    model.add(\n",
    "        Bidirectional(\n",
    "            LSTM(units=n_state_units,\n",
    "                 activation=activation_2)))\n",
    "    model.add(Dense(2))\n",
    "    model.compile(loss=keras.losses.categorical_crossentropy,\n",
    "                  optimizer=optimizer,\n",
    "                  metrics=[_f1_score, 'accuracy', recall, precision])\n",
    "\n",
    "    return model\n",
    "def cuDNN_LSTM(window_size, units_1, units_2, n_state_units=32, activation_1='softmax', activation_2='relu', optimizer='adam'):\n",
    "    #     global metrics\n",
    "    K.clear_session()\n",
    "    model = Sequential()\n",
    "    model.add(\n",
    "        Bidirectional(\n",
    "            LSTM(units=n_state_units,\n",
    "                 activation=activation_1,\n",
    "                 return_sequences=True),\n",
    "            input_shape=(window_size, 32)))\n",
    "    model.add(\n",
    "        Bidirectional(\n",
    "            LSTM(units=n_state_units,\n",
    "                 activation=activation_2)))\n",
    "    model.add(Dense(2))\n",
    "    model.compile(loss=keras.losses.categorical_crossentropy,\n",
    "                  optimizer=optimizer,\n",
    "                  metrics=[_f1_score, 'accuracy', recall, precision])\n",
    "\n",
    "    return model\n",
    "def advanced_LSTM(window_size, activation, optimizer='adam', n_state_units=32):\n",
    "    # https://keras.io/layers/normalization/\n",
    "    #     global metrics\n",
    "    K.clear_session()\n",
    "    model = Sequential([\n",
    "        Bidirectional(LSTM(n_state_units, activation=activation, recurrent_activation='sigmoid')),\n",
    "        # BatchNormalization(), # BatchNormailization을 적용하면, nan값이 뜬다.\n",
    "        Dropout(0.2),\n",
    "        Dense(2)\n",
    "    ])\n",
    "    model.compile(loss=keras.losses.categorical_crossentropy,\n",
    "                  optimizer=optimizer,\n",
    "                  metrics=[_f1_score, 'accuracy', recall, precision])\n",
    "\n",
    "    return model\n",
    "\n",
    "\n",
    "def Start_Model():\n",
    "\n",
    "    pickle_load_dir_path = './data/RNN_coin/'\n",
    "    pickle_result_dir_path = './evaluate_result/'\n",
    "\n",
    "    idx_time_unit = 10 # 10, 30, 60\n",
    "    idx_window_size = 25 # 25, 50, 100\n",
    "    idx_gap = 1\n",
    "    idx_margin_rate = 0.1\n",
    "    epochs = 1\n",
    "    # _GPU = True\n",
    "    n_jobs = 1\n",
    "    cv = 1000\n",
    "    n_iter = 30 # maximum 30\n",
    "    dataset_scale = -1 # [:10000] for test\n",
    "\n",
    "    # scaler = MinMaxScaler(feature_range=(-1, 1))\n",
    "    scaler = StandardScaler()\n",
    "    \n",
    "    param_grid_create_model = {'batch_size' : [64, 256],\n",
    "                               'epochs' : [50],\n",
    "                               'neurons': [5, 30],\n",
    "                               'window_size': [idx_window_size],\n",
    "                               'init':['normal', 'zero', 'glorot_normal', 'he_normal'],\n",
    "                               'n_state_units': [32, 64, 128],\n",
    "                               'activation': ['softmax', 'relu', 'tanh', 'sigmoid'],\n",
    "                               'activation_1': ['tanh', 'sigmoid', 'relu'],\n",
    "                               'optimizer': ['SGD', 'RMSprop', 'Adagrad', 'Adam'],\n",
    "                               'weight_constraint':[1],\n",
    "                               'dropout_rate':[0.0]\n",
    "                              }\n",
    "    \n",
    "    param_grid_vanilla = {\n",
    "                  'window_size': [idx_window_size],\n",
    "                  'units_1': [32, 64],\n",
    "                  'units_2': [32, 64],\n",
    "                  'n_state_units': [64, 128],\n",
    "                  'activation_1': ['tanh', 'sigmoid', 'relu'],\n",
    "                  'activation_2': ['tanh', 'sigmoid', 'relu'],\n",
    "                  'optimizer': ['rmsprop', 'Adam', 'SGD']}\n",
    "\n",
    "    param_grid_test = {'window_size': [idx_window_size],\n",
    "                  'units_1': [16],\n",
    "                  'n_state_units': [32],\n",
    "                  'activation_1': ['relu'],\n",
    "                  'activation_2': ['relu'],\n",
    "                  'optimizer': ['Adam']}\n",
    "\n",
    "    param_grid_advanced_LSTM = {'window_size': [idx_window_size],\n",
    "                  'n_state_units': [32, 128],\n",
    "                  'activation': ['tanh', 'sigmoid', 'relu'],\n",
    "                  'optimizer': ['rmsprop', 'Adam', 'SGD']}\n",
    "\n",
    "    param_grid_stacked_LSTM = {'window_size': [idx_window_size],\n",
    "                                'n_state_units': [32],\n",
    "                                'activation_1': ['sigmoid', 'relu'],\n",
    "                                'activation_2': ['tanh', 'sigmoid', 'relu'],\n",
    "                                'optimizer': ['rmsprop', 'Adam', 'SGD']}\n",
    "    model = KerasClassifier(build_fn=create_model, verbose=True)\n",
    "\n",
    "#     model = KerasClassifier(build_fn=vanilla_LSTM,\n",
    "#                             epochs=epochs,\n",
    "#                             batch_size=300,\n",
    "#                             verbose=True)\n",
    "\n",
    "    grid = RandomizedSearchCV(estimator=model,\n",
    "                              param_distributions=param_grid_create_model,\n",
    "                              n_iter=n_iter,\n",
    "                              cv=cv,\n",
    "                              random_state=42,\n",
    "                              n_jobs=n_jobs,\n",
    "                              verbose=1)\n",
    "\n",
    "    key_name_X = \"X_\"\n",
    "    key_name_y = \"y_\"\n",
    "\n",
    "    key_name_X += str(idx_time_unit) + \"_\" + str(idx_window_size) + \"_\" + str(idx_gap) + \"_\" + str(idx_margin_rate)\n",
    "    key_name_y += str(idx_time_unit) + \"_\" + str(idx_window_size) + \"_\" + str(idx_gap) + \"_\" + str(idx_margin_rate)\n",
    "\n",
    "    # remove [:10000], when real training\n",
    "    X = Load_Dataset_X(pickle_load_dir_path, idx_time_unit, idx_window_size, idx_gap, idx_margin_rate)[0][\n",
    "        :dataset_scale]\n",
    "    y = Load_Dataset_y(pickle_load_dir_path, idx_time_unit, idx_window_size, idx_gap, idx_margin_rate)[1][\n",
    "        :dataset_scale]\n",
    "\n",
    "    y_single = {}\n",
    "    #     print(\"[INFO] y : {}\".format(y))\n",
    "    #     y = np.asarray(y[0])\n",
    "    #     print(\"[INFO] y.shape : {}\".format(y.shape))\n",
    "    #     print(\"[INFO] y : {}\".format(y))\n",
    "    y_single['BTC'] = y[:, 1]\n",
    "    y_single['ETH'] = y[:, 2]\n",
    "    y_single['XRP'] = y[:, 3]\n",
    "    y_single['BCH'] = y[:, 4]\n",
    "    y_single['LTC'] = y[:, 5]\n",
    "    y_single['DASH'] = y[:, 6]\n",
    "    y_single['ETC'] = y[:, 7]\n",
    "\n",
    "    coin_list2 = [\"BTC\", \"ETH\", \"XRP\", \"BCH\", \"LTC\", \"DASH\", \"ETC\"]\n",
    "\n",
    "    for coin in coin_list2:\n",
    "        if (os.path.isfile(pickle_result_dir_path + \\\n",
    "                           coin + \"_\" + \\\n",
    "                           str(idx_time_unit) + \"_\" + \\\n",
    "                           str(idx_window_size) + \"_\" + \\\n",
    "                           str(idx_gap) + \"_\" + \\\n",
    "                           str(idx_margin_rate) + \\\n",
    "                           \"_param.pickle\")) is True:\n",
    "            print(coin + \"_\" + \\\n",
    "                  str(idx_time_unit) + \"_\" + \\\n",
    "                  str(idx_window_size) + \"_\" + \\\n",
    "                  str(idx_gap) + \"_\" + \\\n",
    "                  str(idx_margin_rate) + \\\n",
    "                  \"_param.pickle FILE ALREADY EXIST.\")\n",
    "            continue\n",
    "        elif (os.path.isfile(pickle_result_dir_path + \\\n",
    "                             coin + \"_\" + \\\n",
    "                             str(idx_time_unit) + \"_\" + \\\n",
    "                             str(idx_window_size) + \"_\" + \\\n",
    "                             str(idx_gap) + \"_\" + \\\n",
    "                             str(idx_margin_rate) + \\\n",
    "                             \"_result.pickle\")) is True:\n",
    "            print(coin + \"_\" + \\\n",
    "                  str(idx_time_unit) + \"_\" + \\\n",
    "                  str(idx_window_size) + \"_\" + \\\n",
    "                  str(idx_gap) + \"_\" + \\\n",
    "                  str(idx_margin_rate) + \\\n",
    "                  \"_result.pickle FILE ALREADY EXIST.\")\n",
    "            continue\n",
    "        else:\n",
    "            y2 = onehottify(y_single[coin], n=2)\n",
    "\n",
    "            X_train, X_test, y_train, y_test = train_test_split(X,\n",
    "                                                                y2,\n",
    "                                                                test_size=0.2,\n",
    "                                                                random_state=42,\n",
    "                                                                shuffle=True)\n",
    "\n",
    "            print(\"[INFO] X_train.shape : {}\".format(X_train.shape))\n",
    "            print(\"[INFO] y_train.shape : {}\".format(y_train.shape))\n",
    "            print(\"[INFO] X_test.shape : {}\".format(X_test.shape))\n",
    "            print(\"[INFO] y_test.shape : {}\".format(y_test.shape))\n",
    "            print()\n",
    "\n",
    "            n_coins = 8\n",
    "            n_price = 4\n",
    "            n_steps = idx_window_size\n",
    "\n",
    "            X_train_2 = X_train.transpose([0, 2, 1, 3])\n",
    "            X_test_2 = X_test.transpose([0, 2, 1, 3])\n",
    "            print(\"[INFO] X_train_2.shape: {}\".format(X_train_2.shape))\n",
    "            print(\"[INFO] X_test_2.shape: {}\".format(X_test_2.shape))\n",
    "            print()\n",
    "\n",
    "            X_train_3 = X_train_2.reshape([X_train.shape[0], n_steps, n_coins * n_price])\n",
    "            X_test_3 = X_test_2.reshape([X_test.shape[0], n_steps, n_coins * n_price])\n",
    "            print(\"[INFO] X_train_3.shape: {}\".format(X_train_3.shape))\n",
    "            print(\"[INFO] X_test_3.shape: {}\".format(X_test_3.shape))\n",
    "            print()\n",
    "\n",
    "            X_train_reshape = X_train_2.reshape([X_train.shape[0], n_steps * n_coins * n_price])\n",
    "            X_test_reshape = X_test_2.reshape([X_test.shape[0], n_steps * n_coins * n_price])\n",
    "            print(\"[INFO] X_train_reshape.shape: {}\".format(X_train_reshape.shape))\n",
    "            print(\"[INFO] X_test_reshape.shape: {}\".format(X_test_reshape.shape))\n",
    "            print()\n",
    "\n",
    "            scaler.fit(X_train_reshape)\n",
    "\n",
    "            # X_train_scaled = scaler.transform(X_train_reshape)\n",
    "            # X_test_scaled = scaler.transform(X_test_reshape)\n",
    "            X_train_scaled = scaler.fit_transform(X_train_reshape)\n",
    "            X_test_scaled = scaler.fit_transform(X_test_reshape)\n",
    "\n",
    "            X_train_scaled = X_train_scaled.reshape(-1,\n",
    "                                                    n_steps,\n",
    "                                                    n_coins * n_price)\n",
    "            X_test_scaled = X_test_scaled.reshape(-1,\n",
    "                                                  n_steps,\n",
    "                                                  n_coins * n_price)\n",
    "\n",
    "\n",
    "            # model = KerasClassifier(build_fn=stacked_LSTM,\n",
    "            #                         epochs=epochs,\n",
    "            #                         batch_size=300,\n",
    "            #                         verbose=True)\n",
    "\n",
    "\n",
    "            #             grid = GridSearchCV(estimator=model,\n",
    "            #                                 cv=cv,\n",
    "            #                                 n_jobs=n_jobs, # test\n",
    "            #                                 param_grid=param_grid,\n",
    "            #                                 verbose=1)\n",
    "\n",
    "            # grid = RandomizedSearchCV(estimator=model,\n",
    "            #                           param_distributions=param_grid,\n",
    "            #                           n_iter=n_iter,\n",
    "            #                           cv=cv,\n",
    "            #                           random_state=42,\n",
    "            #                           n_jobs=n_jobs,\n",
    "            #                           verbose=1)\n",
    "\n",
    "            X_train_scaled, X_test_scaled = input_reshape(X_train_scaled,\n",
    "                                                          X_test_scaled,\n",
    "                                                          n_steps,\n",
    "                                                          n_coins,\n",
    "                                                          n_price)\n",
    "\n",
    "            print()\n",
    "            print()\n",
    "            print(\"----------------------\")\n",
    "            print(\"__\" + coin + \"__\" + \\\n",
    "                  \"time unit: \" + str(idx_time_unit) + \"  |  \" + \\\n",
    "                  \"window_size :\" + str(idx_window_size) + \"  |  \" + \\\n",
    "                  \"gap :\" + str(idx_gap) + \"  |  \" + \\\n",
    "                  \"margin_rate :\" + str(idx_margin_rate) + \\\n",
    "                  \"  started.\")\n",
    "\n",
    "            #             fit_params = dict(callbacks=[stopper])\n",
    "\n",
    "            with K.tf.device('/device:XLA_CPU:0'):\n",
    "#                 grid_result = grid.fit(X_train_scaled,\n",
    "#                                        y_train,\n",
    "#                                        validation_data=(X_test_scaled,\n",
    "#                                                         y_test),\n",
    "#                                        batch_size=512,\n",
    "#                                        callbacks=[stopper])\n",
    "                grid_result = grid.fit(X_train_scaled,\n",
    "                                       y_train,\n",
    "                                       validation_data=(X_test_scaled,\n",
    "                                                        y_test),\n",
    "                                       callbacks=[early_stop])\n",
    "            # grid_result = grid_result.reset_states()\n",
    "\n",
    "            print(\"----------------------\")\n",
    "            print(\"grid_result.score(X_test_scaled, y_test): \", grid_result.score(X_test_scaled, y_test))\n",
    "\n",
    "            evaluate_result = {}\n",
    "            test_score = grid_result.score(X_test_scaled, y_test)\n",
    "            evaluate_result[coin + \"_\" + \\\n",
    "                            str(idx_time_unit) + \"_\" + \\\n",
    "                            str(idx_window_size) + \"_\" + \\\n",
    "                            str(idx_gap) + \"_\" + \\\n",
    "                            str(idx_margin_rate)] = {\"Cryptocurrency\": coin, \\\n",
    "                                                     \"Score\": grid_result.cv_results_['mean_test_score'], \\\n",
    "                                                     \"Params\": grid_result.cv_results_['params'], \\\n",
    "                                                     \"test_score\": test_score}\n",
    "            #     print()\n",
    "            #     print(\"evaluate result dict: \", evaluate_result)\n",
    "            #     print()\n",
    "\n",
    "            # summarize results\n",
    "            print()\n",
    "            print(\"Best: %f using %s\" % (grid_result.best_score_, grid_result.best_params_))\n",
    "            print()\n",
    "            # for checking pickle file exist\n",
    "            means = grid_result.cv_results_['mean_test_score']\n",
    "            stds = grid_result.cv_results_['std_test_score']\n",
    "            params = grid_result.cv_results_['params']\n",
    "            for mean, stdev, param in zip(means, stds, params):\n",
    "                print(\"%f (%f) with: %r\" % (mean, stdev, param))\n",
    "            print(\"---pickle saving..\")\n",
    "\n",
    "            X = {}\n",
    "            y = {}\n",
    "            key_name_X = \"X_\"\n",
    "            key_name_y = \"y_\"\n",
    "\n",
    "            key_name_X += str(idx_time_unit) + \"_\" + str(idx_window_size) + \"_\" + str(idx_gap) + \"_\" + str(\n",
    "                idx_margin_rate)\n",
    "            key_name_y += str(idx_time_unit) + \"_\" + str(idx_window_size) + \"_\" + str(idx_gap) + \"_\" + str(\n",
    "                idx_margin_rate)\n",
    "            if (os.path.isfile(pickle_result_dir_path + \\\n",
    "                               coin + \"_\" + \\\n",
    "                               str(idx_time_unit) + \"_\" + \\\n",
    "                               str(idx_window_size) + \"_\" + \\\n",
    "                               str(idx_gap) + \"_\" + \\\n",
    "                               str(idx_margin_rate) + \\\n",
    "                               \"_param.pickle\")) is not True:\n",
    "                with open(pickle_result_dir_path + \\\n",
    "                          coin + \"_\" + \\\n",
    "                          str(idx_time_unit) + \"_\" + \\\n",
    "                          str(idx_window_size) + \"_\" + \\\n",
    "                          str(idx_gap) + \"_\" + \\\n",
    "                          str(idx_margin_rate) + \\\n",
    "                          \"_param.pickle\", 'wb') as handle:\n",
    "                    pickle.dump(evaluate_result, handle, protocol=pickle.HIGHEST_PROTOCOL)\n",
    "                    sending_eamil(evaluate_result)\n",
    "\n",
    "            return grid_result\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    Start_Model()\n",
    "\n",
    "    # dir = \"./evaluate_result/\"\n",
    "    # dir = dir + model_info_2 + '/'\n",
    "    # time_unit_list = [10]\n",
    "    # window_size_list = [10, 50, 75, 100]\n",
    "    #\n",
    "    # for time_unit in time_unit_list:\n",
    "    #     # dir = dir + str(time_unit) + '/'\n",
    "    #     dir = \"./evaluate_result/\"\n",
    "    #     print(dir)\n",
    "    #     drawGraph(dir)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
