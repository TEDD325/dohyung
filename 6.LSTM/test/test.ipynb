{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/link/anaconda3/lib/python3.6/site-packages/h5py/__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n",
      "Using TensorFlow backend.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using Keras version: 2.2.2 backend: tensorflow\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"]=\"1\"\n",
    "import tensorflow as tf\n",
    "import keras\n",
    "from keras import __version__\n",
    "from keras import backend as K\n",
    "\n",
    "print('Using Keras version:', __version__, 'backend:', K.backend())\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, LSTM, Bidirectional, BatchNormalization, Embedding, CuDNNLSTM, Dropout\n",
    "from keras.wrappers.scikit_learn import KerasClassifier\n",
    "from keras.callbacks import EarlyStopping\n",
    "from sklearn.preprocessing import MinMaxScaler, StandardScaler\n",
    "from sklearn.model_selection import train_test_split, RandomizedSearchCV, GridSearchCV\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from datetime import datetime\n",
    "import sys\n",
    "import os.path\n",
    "import os\n",
    "os.environ['KMP_DUPLICATE_LIB_OK']='True'\n",
    "from os.path import isfile, join\n",
    "import boto3\n",
    "import pickle\n",
    "import time\n",
    "import warnings\n",
    "import smtplib\n",
    "from email.mime.text import MIMEText\n",
    "from keras.utils import multi_gpu_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "sys.path.append(os.getcwd())\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "'''\n",
    "slave05> [30, 60] BTC, ETH, XRP\n",
    "slave04> [30, 60] BCH, LTC, DASH\n",
    "link> [10, 30, 60] ETC\n",
    "'''\n",
    "coins = {\n",
    "    0: 'KRW',\n",
    "    1: 'BTC',\n",
    "    2: 'ETH',\n",
    "    3: 'XRP',\n",
    "    # 4: 'BCH',\n",
    "    # 5: 'LTC',\n",
    "    # 6: 'DASH',\n",
    "    # 7: 'ETC'\n",
    "}\n",
    "# aws_client = boto3.client(\n",
    "#     's3',\n",
    "#     aws_access_key_id=LINK_AWSAccessKeyId,\n",
    "#     aws_secret_access_key=LINK_AWSSecretKey\n",
    "# )\n",
    "bucket = \"bithumb10\"\n",
    "cleanup_file_name = \"coin_{0}_{1}_cleanup.csv\"\n",
    "#######################################################\n",
    "def Load_Dataset_X(dir_path, time_unit, window_size, gap, margin_rate):\n",
    "    key_name_X = \"X_\" + \\\n",
    "                 str(time_unit) + \"_\" + \\\n",
    "                 str(window_size) + \"_\" + \\\n",
    "                 str(gap) + \"_\" + \\\n",
    "                 str(margin_rate)\n",
    "\n",
    "    with open(dir_path + key_name_X + \".pickle\", 'rb') as handle:\n",
    "        b_x = pickle.load(handle)\n",
    "    return b_x\n",
    "def Load_Dataset_y(dir_path, time_unit, window_size, gap, margin_rate):\n",
    "    key_name_y = \"y_\" + \\\n",
    "                 str(time_unit) + \"_\" + \\\n",
    "                 str(window_size) + \"_\" + \\\n",
    "                 str(gap) + \"_\" + \\\n",
    "                 str(margin_rate)\n",
    "\n",
    "    with open(dir_path + key_name_y + \".pickle\", 'rb') as handle:\n",
    "        b_y = pickle.load(handle)\n",
    "    return b_y\n",
    "def input_reshape(X_train_data, X_test_data, n_steps, n_coins, n_price):\n",
    "    X_train_reshape = X_train_data.reshape(\n",
    "        -1,\n",
    "        n_steps,\n",
    "        n_coins * n_price\n",
    "    )\n",
    "    X_test_reshape = X_test_data.reshape(\n",
    "        -1,\n",
    "        n_steps,\n",
    "        n_coins * n_price\n",
    "    )\n",
    "    return X_train_reshape, X_test_reshape\n",
    "def onehottify(x, n=None, dtype=np.int):\n",
    "    \"\"\"1-hot encode x with the max value n (computed from data if n is None).\"\"\"\n",
    "    x = np.asarray(x)\n",
    "    n = np.max(x) + 1 if n is None else n\n",
    "    return np.eye(n, dtype=dtype)[x]\n",
    "def sending_eamil(message):\n",
    "    f = open('../../../email_info.bin', 'rb')\n",
    "    a = pickle.load(f)\n",
    "\n",
    "    smtp = smtplib.SMTP_SSL('smtp.naver.com', 465)\n",
    "    smtp.ehlo()  # say Hello\n",
    "    # smtp.starttls()  # TLS 사용시 필요\n",
    "    smtp.login(a['email'], a['pw'])\n",
    "\n",
    "    msg = MIMEText(str(message)) \n",
    "    msg['Subject'] = '실험 완료'\n",
    "    msg['To'] = 'ulujo_dohk@naver.com'\n",
    "    smtp.sendmail(a['email'], 'ulujo_dohk@naver.com', msg.as_string())\n",
    "\n",
    "    smtp.quit()\n",
    "\n",
    "class SortedDisplayDict(dict):\n",
    "    def __str__(self):\n",
    "        return \"{\" + \", \".join(\"%r: %r\" % (key, self[key]) for key in sorted(self)) + \"}\"\n",
    "\n",
    "    def ordered_keys(self):\n",
    "        return sorted(self.keys())\n",
    "def search(dirname):\n",
    "    filenames = os.listdir(dirname)\n",
    "    fileList = []\n",
    "    for filename in filenames:\n",
    "        full_filename = os.path.join(dirname, filename)\n",
    "        fileList.append(full_filename)\n",
    "    return fileList\n",
    "def drawGraph(dir):\n",
    "    fileList = search(dir)\n",
    "    temp_list = []\n",
    "    cluster_coef_value_list_lstm = []\n",
    "    for file in fileList:\n",
    "        temp_list.append(pd.read_pickle(file))\n",
    "\n",
    "        cluster_coef_value_list_lstm = []\n",
    "    for i in range(len(temp_list)):\n",
    "        # print(temp_list[i])\n",
    "        # print()\n",
    "        # print(temp_list[i][list(temp_list[i].keys())[0]])\n",
    "        # print()\n",
    "        cluster_coef_value_list_lstm.append(temp_list[i][list(temp_list[i].keys())[0]]['Score'][0])\n",
    "        # print(score)\n",
    "        # print()\n",
    "\n",
    "    cluster_coef_value_list_gradientBoosting = [0.671, 0.616, 0.622, 0.672, 0.7, 0.69, 0.69]\n",
    "\n",
    "    first_legend_label = 'xgboost'\n",
    "    second_legend_label = 'lstm'\n",
    "    x_label = 'Cryptocurrency'\n",
    "    y_label = 'f1-score'\n",
    "    filename = str(datetime.now())\n",
    "\n",
    "    graph(cluster_coef_value_list_gradientBoosting,\n",
    "          cluster_coef_value_list_lstm[:7],\n",
    "          first_legend_label,\n",
    "          second_legend_label,\n",
    "          x_label,\n",
    "          y_label,\n",
    "          filename)\n",
    "def graph(cluster_coef_value_list_xgboost,\n",
    "          cluster_coef_value_list_lstm,\n",
    "          first_legend_label,\n",
    "          second_legend_label,\n",
    "          x_label,\n",
    "          y_label,\n",
    "          filename):\n",
    "    '''\n",
    "    < EXAMPLE >\n",
    "    f1_score = [0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9] # 9개\n",
    "    first_legend_label = 'xgboost'\n",
    "    second_legend_label = 'lstm'\n",
    "    x_label = 'Cryptocurrency'\n",
    "    y_label = 'f1-score'\n",
    "    filename = '_clustering_and_diameter'\n",
    "    '''\n",
    "    link_addition_ratio = f1_score\n",
    "    data_profile = {\n",
    "        \"Infocom05\": {\n",
    "            \"file_name\": \"data_infocom05.csv\",\n",
    "            \"num_nodes\": 41,\n",
    "            \"median\": 2684,\n",
    "            \"mean\": 9961,\n",
    "            \"std\": 26513,\n",
    "            \"contact_weight_map\": [((6, 9), 2187), ((12, 1), 4943), ((7, 12), 40849), ((1, 6), 768)],\n",
    "\n",
    "            \"graph\": {},\n",
    "            \"numberOfNodes\": {},\n",
    "            \"numberOfEdges\": {},\n",
    "            \"durationThreshold\": {},\n",
    "\n",
    "            \"density\": {},\n",
    "            \"clustering_coefficient\": {},\n",
    "            \"diameter_cc\": {},\n",
    "\n",
    "            \"global_bet\": {},\n",
    "            \"Brandes_ego_bet\": {},\n",
    "            \"Brandes_ego_elapsed_time\": {},\n",
    "            \"Brandes_xego_bet\": {},\n",
    "            \"Brandes_xego_elapsed_time\": {},\n",
    "\n",
    "            \"Proposed_ego_bet\": {},\n",
    "            \"Proposed_ego_elapsed_time\": {},\n",
    "            \"Proposed_xego_bet\": {},\n",
    "            \"Proposed_xego_elapsed_time\": {},\n",
    "\n",
    "            \"ego_global_pearson_corr\": {},\n",
    "            \"xego_global_pearson_corr\": {},\n",
    "\n",
    "            \"ego_global_spearman_corr\": {},\n",
    "            \"xego_global_spearman_corr\": {},\n",
    "\n",
    "            \"ego_node_coverage_in_connected_component\": {},\n",
    "            \"ego_edge_coverage_in_connected_component\": {},\n",
    "\n",
    "            \"xego_node_coverage_in_connected_component\": {},\n",
    "            \"xego_edge_coverage_in_connected_component\": {}\n",
    "        }\n",
    "    }\n",
    "    xticklabels = [r'$BTC$', r'$ETH$', r'$XRP$', r'$BCH$', r'$LTC$', r'$DASH$', r'$ETC$']\n",
    "    yticklabels = [r'$0.40$', r'$0.45$', r'$0.50$', r'$0.55$', r'$0.60$', r'$0.65$', r'$0.70$', r'$0.75$', r'$0.80$']\n",
    "    # yticklabels2 = [r'$0$', r'$2$', r'$4$', r'$6$', r'$8$', r'$10$', r'$12$']\n",
    "\n",
    "    ind = np.arange(len(xticklabels))\n",
    "\n",
    "    fig, axes = plt.subplots(1, 1, figsize=(8.719, 6.07))\n",
    "    subfigures = {}\n",
    "    data_name = 'Infocom05'\n",
    "    subfigures[data_name] = axes\n",
    "    subfigures[data_name].set_xticks(ind)\n",
    "    subfigures[data_name].set_xticklabels(xticklabels, fontsize=21)\n",
    "    subfigures[data_name].set_ylim([0.4, 0.8])\n",
    "    subfigures[data_name].set_yticklabels(yticklabels, fontsize=21)\n",
    "\n",
    "    cluster_coef_dic = SortedDisplayDict(data_profile[data_name]['clustering_coefficient'])\n",
    "    cluster_coef_value_list = [cluster_coef_dic[x] for x in cluster_coef_dic.ordered_keys()]\n",
    "\n",
    "    # cluster_coef_value_list_xgboost = [0.56, 0.67, 0.66, 0.78, 0.45, 0.47, 0.65]\n",
    "    # cluster_coef_value_list_lstm = [0.66, 0.77, 0.68, 0.68, 0.55, 0.57, 0.66]\n",
    "    cluster_coef_value_list_xgboost = cluster_coef_value_list_xgboost\n",
    "    cluster_coef_value_list_lstm = cluster_coef_value_list_lstm\n",
    "    subfigures[data_name].plot(ind, cluster_coef_value_list_xgboost,\n",
    "                               color='k', linestyle='-', marker='s', markersize=8,\n",
    "                               label=first_legend_label)\n",
    "    subfigures[data_name].plot(ind, cluster_coef_value_list_lstm,\n",
    "                               color='k', linestyle='--', marker='^', markersize=8,\n",
    "                               label=second_legend_label)\n",
    "    subfigures[data_name].set_xlabel(x_label, fontsize=21)\n",
    "    subfigures[data_name].set_ylabel(y_label, fontsize=21)\n",
    "\n",
    "    # subfigures[data_name].set_title(data_name, fontsize=21)\n",
    "    subfigures[data_name].grid(True)\n",
    "    if data_name == 'Infocom05':\n",
    "        subfigures[data_name].legend(loc=30, fontsize=18)\n",
    "\n",
    "    # subfigures[data_name] = subfigures[data_name].twinx()\n",
    "    # subfigures[data_name].set_ylim([0, 12])\n",
    "    # subfigures[data_name].set_yticklabels(yticklabels2, fontsize=21)\n",
    "\n",
    "    diameter_dic = SortedDisplayDict(data_profile[data_name]['diameter_cc'])\n",
    "    diameter_list = [diameter_dic[x] for x in diameter_dic.ordered_keys()]\n",
    "    # subfigures[data_name].bar(ind, diameter_list, barWidth, color='k', alpha=0.3, label='Diameter of Connected Component')\n",
    "    # subfigures[data_name].set_ylabel('Diameter of Connected Component', fontsize=21)\n",
    "    if data_name == 'Infocom05':\n",
    "        subfigures[data_name].legend(loc=4, fontsize=18)\n",
    "    subfigures[data_name].grid(True)\n",
    "\n",
    "    fig.savefig('./img/' + filename + '.pdf', format='pdf', bbox_inches='tight')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/link/anaconda3/lib/python3.6/site-packages/h5py/__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n",
      "Using TensorFlow backend.\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/IPython/core/interactiveshell.py\u001b[0m in \u001b[0;36mrun_code\u001b[0;34m(self, code_obj, result)\u001b[0m\n\u001b[1;32m   2962\u001b[0m                 \u001b[0;31m#rprint('Running code', repr(code_obj)) # dbg\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2963\u001b[0;31m                 \u001b[0mexec\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcode_obj\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0muser_global_ns\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0muser_ns\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2964\u001b[0m             \u001b[0;32mfinally\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-1-8f64844af662>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m    132\u001b[0m \u001b[0mgrid\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mGridSearchCV\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mestimator\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mparam_grid\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mparam_grid\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mverbose\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 133\u001b[0;31m \u001b[0mgrid_result\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgrid\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mY\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mverbose\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcallbacks\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mearly_stop\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    134\u001b[0m \u001b[0;31m# summarize results\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/sklearn/model_selection/_search.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, X, y, groups, **fit_params)\u001b[0m\n\u001b[1;32m    617\u001b[0m         \u001b[0;31m# Regenerate parameter iterable for each fit\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 618\u001b[0;31m         \u001b[0mcandidate_params\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_get_param_iterator\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    619\u001b[0m         \u001b[0mn_candidates\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcandidate_params\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/sklearn/model_selection/_search.py\u001b[0m in \u001b[0;36m__iter__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    115\u001b[0m                 \u001b[0;32mfor\u001b[0m \u001b[0mv\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mproduct\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 116\u001b[0;31m                     \u001b[0mparams\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mzip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkeys\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mv\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    117\u001b[0m                     \u001b[0;32myield\u001b[0m \u001b[0mparams\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: ",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import keras\n",
    "from keras import backend as K\n",
    "def recall(y_true, y_pred):\n",
    "    K.set_epsilon(1e-05)\n",
    "    true_positives = K.sum(K.round(K.clip(y_true * y_pred, 0, 1)))\n",
    "    possible_positives = K.sum(K.round(K.clip(y_true, 0, 1)))\n",
    "    recall = true_positives / (possible_positives + K.epsilon())\n",
    "    return recall\n",
    "def precision(y_true, y_pred):\n",
    "    K.set_epsilon(1e-05)\n",
    "    true_positives = K.sum(K.round(K.clip(y_true * y_pred, 0, 1)))\n",
    "    predicted_positives = K.sum(K.round(K.clip(y_pred, 0, 1)))\n",
    "    precision = true_positives / (predicted_positives + K.epsilon())\n",
    "    return precision\n",
    "def f1_metric(y_true, y_pred):\n",
    "    def recall(y_true, y_pred):\n",
    "        true_positives = K.sum(K.round(K.clip(y_true * y_pred, 0, 1)))\n",
    "        possible_positives = K.sum(K.round(K.clip(y_true, 0, 1)))\n",
    "        recall = true_positives / (possible_positives + K.epsilon())\n",
    "        return recall\n",
    "\n",
    "    def precision(y_true, y_pred):\n",
    "        true_positives = K.sum(K.round(K.clip(y_true * y_pred, 0, 1)))\n",
    "        predicted_positives = K.sum(K.round(K.clip(y_pred, 0, 1)))\n",
    "        precision = true_positives / (predicted_positives + K.epsilon())\n",
    "        return precision\n",
    "    precision = precision(y_true, y_pred)\n",
    "    recall = recall(y_true, y_pred)\n",
    "    return 2*((precision*recall)/(precision+recall+K.epsilon()))\n",
    "\n",
    "# Create first network with Keras\n",
    "import numpy\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from keras.wrappers.scikit_learn import KerasClassifier\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "import numpy\n",
    "# fix random seed for reproducibility\n",
    "seed = 7\n",
    "numpy.random.seed(seed)\n",
    "# load pima indians dataset\n",
    "dataset = numpy.loadtxt(\"./notebooks/pima-indians-diabetes.data.csv\", delimiter=\",\")\n",
    "# split into input (X) and output (Y) variables\n",
    "X = dataset[:,0:8]\n",
    "Y = dataset[:,8]\n",
    "\n",
    "pickle_load_dir_path = './data/RNN_coin/'\n",
    "pickle_result_dir_path = './evaluate_result/'\n",
    "\n",
    "idx_time_unit = 60 # 10, 30, 60\n",
    "idx_window_size = 25 # 25, 50, 100\n",
    "idx_gap = 1\n",
    "idx_margin_rate = 0.1\n",
    "epochs = 1\n",
    "# _GPU = True\n",
    "n_jobs = 1\n",
    "cv = 2\n",
    "n_iter = 30 # maximum 30\n",
    "dataset_scale = -1 # [:10000] for test\n",
    "\n",
    "# scaler = MinMaxScaler(feature_range=(-1, 1))\n",
    "# scaler = StandardScaler()\n",
    "\n",
    "# create model\n",
    "def create_model(n_state_units, \n",
    "                 neurons,\n",
    "                 init,\n",
    "                 activation,\n",
    "                 activation_1,\n",
    "                 activation_2, \n",
    "                 window_size, \n",
    "                 optimizer,\n",
    "                 weight_constraint,\n",
    "                 dropout_rate):\n",
    "    \n",
    "    model = Sequential()\n",
    "    model.add(\n",
    "        LSTM(units=n_state_units,\n",
    "             activation=activation,\n",
    "             input_shape=(window_size, 32)))\n",
    "    model.add(Dense(neurons, input_dim=8, \n",
    "                    init=init, \n",
    "                    activation=activation_1,\n",
    "                    kernel_constraint=maxnorm(weight_constraint)))\n",
    "    model.add(Dense(8, init=init, activation_1=activation_1))\n",
    "    model.add(Dense(2, init=init, activation_2=activation_2))\n",
    "    # Compile model\n",
    "    model.compile(loss=keras.losses.categorical_crossentropy, \n",
    "                  optimizer=optimizer, \n",
    "                  metrics=[f1_metric, 'accuracy', recall, precision])\n",
    "    \n",
    "    return model\n",
    "\n",
    "seed = 7\n",
    "numpy.random.seed(seed)\n",
    "\n",
    "# calculate predictions\n",
    "from keras.callbacks import EarlyStopping\n",
    "early_stop = keras.callbacks.EarlyStopping(monitor='f1_metric', mode='max', patience = 5, verbose=1)\n",
    "# stopper = EarlyStopping(monitor='f1_metric', \n",
    "#                         verbose=1,\n",
    "#                         mode='min')\n",
    "#                         patience=3, \n",
    "#                         verbose=1, \n",
    "#                         min_delta=0.001)\n",
    "\n",
    "model = KerasClassifier(build_fn=create_model, verbose=1)\n",
    "\n",
    "# Fit the model\n",
    "# model.fit(X, Y, \n",
    "#           epochs=150, \n",
    "#           batch_size=10,  \n",
    "#           verbose=2, \n",
    "#           callbacks=[stopper])\n",
    "\n",
    "# define the grid search parameters\n",
    "# batch_size = [10, 20, 40, 60, 80, 100]\n",
    "# epochs = [10, 50, 100]\n",
    "param_grid = {'batch_size' : [64, 128,256, 512, 1024, 2048],\n",
    "              'epochs' : [10, 50, 100],\n",
    "              'neurons': [1, 5, 10, 15, 20, 25, 30],\n",
    "              'window_size': [idx_window_size],\n",
    "              'init':['uniform', 'lecun_uniform', 'normal', 'zero', 'glorot_normal', 'glorot_uniform', 'he_normal', 'he_uniform'],\n",
    "              'n_state_units': [32, 64, 128],\n",
    "              'activation': ['softmax', 'softplus', 'softsign', 'relu', 'tanh', 'sigmoid', 'hard_sigmoid', 'linear'],\n",
    "              'activation_2': ['tanh', 'sigmoid', 'relu'],\n",
    "              'activation_1': ['tanh', 'sigmoid', 'relu'],\n",
    "              'optimizer': ['SGD', 'RMSprop', 'Adagrad', 'Adadelta', 'Adam', 'Adamax', 'Nadam'],\n",
    "              'weight_constraint':[1, 2, 3, 4, 5],\n",
    "              'dropout_rate':[0.0, 0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9]}\n",
    "# param_grid = dict(batch_size=batch_size, epochs=epochs)\n",
    "grid = GridSearchCV(estimator=model, param_grid=param_grid, verbose=1)\n",
    "grid_result = grid.fit(X, Y, verbose=2, callbacks=[early_stop])\n",
    "# summarize results\n",
    "print(\"Best: %f using %s\" % (grid_result.best_score_, grid_result.best_params_))\n",
    "means = grid_result.cv_results_['mean_test_score']\n",
    "stds = grid_result.cv_results_['std_test_score']\n",
    "params = grid_result.cv_results_['params']\n",
    "for mean, stdev, param in zip(means, stds, params):\n",
    "    print(\"%f (%f) with: %r\" % (mean, stdev, param))\n",
    "\n",
    "predictions = model.predict(X)\n",
    "# round predictions\n",
    "rounded = [round(x[0]) for x in predictions]\n",
    "print(rounded)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "{'batch_size' : [64, 128, 256, 512],\n",
    "  'epochs' : [10, 50, 100],\n",
    "  'neurons': [5, 15, 20, 30],\n",
    "  'window_size': [idx_window_size],\n",
    "  'init':['uniform', 'normal', 'zero', \n",
    "          'glorot_normal', 'glorot_uniform', \n",
    "          'he_normal', 'he_uniform'],\n",
    "  'n_state_units': [32, 64, 128],\n",
    "  'activation': ['softmax', 'relu', 'tanh', 'sigmoid'],\n",
    "  'activation_2': ['tanh', 'sigmoid', 'relu'],\n",
    "  'activation_1': ['tanh', 'sigmoid', 'relu'],\n",
    "  'optimizer': ['SGD', 'RMSprop', 'Adam'],\n",
    "  'weight_constraint':[1, 3, 5],\n",
    "  'dropout_rate':[0.0, 0.2, 0.4]}\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
