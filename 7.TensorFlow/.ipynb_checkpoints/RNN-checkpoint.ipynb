{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/anaconda3/lib/python3.6/importlib/_bootstrap.py:219: RuntimeWarning: compiletime version 3.5 of module 'tensorflow.python.framework.fast_tensor_util' does not match runtime version 3.6\n",
      "  return f(*args, **kwds)\n",
      "/anaconda3/lib/python3.6/site-packages/h5py/__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "char_arr:\n",
      "['a', 'b', 'c', 'd', 'e', 'f', 'g', 'h', 'i', 'j', 'k', 'l', 'm', 'n', 'o', 'p', 'q', 'r', 's', 't', 'u', 'v', 'w', 'x', 'y', 'z']\n",
      "\n",
      "num_dic:\n",
      "{'a': 0, 'b': 1, 'c': 2, 'd': 3, 'e': 4, 'f': 5, 'g': 6, 'h': 7, 'i': 8, 'j': 9, 'k': 10, 'l': 11, 'm': 12, 'n': 13, 'o': 14, 'p': 15, 'q': 16, 'r': 17, 's': 18, 't': 19, 'u': 20, 'v': 21, 'w': 22, 'x': 23, 'y': 24, 'z': 25}\n",
      "\n",
      "num_dic encoding of \"word\":\n",
      "[22, 14, 17]\n",
      "3\n",
      "\n",
      "input_batch\n",
      "[array([[0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 1., 0., 0., 0.],\n",
      "       [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "       [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 1., 0., 0., 0., 0., 0., 0., 0., 0.]])]\n",
      "\n",
      "target_batch\n",
      "[3]\n"
     ]
    }
   ],
   "source": [
    "# Word Prediction with Tensorflow RNN\n",
    "# written by Sung Kyu Lim\n",
    "# limsk@ece.gatech.edu\n",
    "# 1/7/2019\n",
    "\n",
    "\n",
    "# import tensorflow and numpy\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import os\n",
    "os.environ['TF_CPP_MIN_LOG_LEVEL']='2'\n",
    "\n",
    "\n",
    "# array for alphabet\n",
    "char_arr = [c for c in 'abcdefghijklmnopqrstuvwxyz']\n",
    "print('char_arr:')\n",
    "print(char_arr)\n",
    "\n",
    "\n",
    "# assign array index to each alphabet\n",
    "# ex: 'a': 0, 'b': 1, 'c': 2  ...\n",
    "num_dic = {n: i for i, n in enumerate(char_arr)}\n",
    "print('\\nnum_dic:')\n",
    "print(num_dic)\n",
    "\n",
    "\n",
    "# num_dic testing \n",
    "seq = 'word'\n",
    "input = [num_dic[n] for n in seq[0:-1]]\n",
    "print('\\nnum_dic encoding of \"word\":')\n",
    "print(input)\n",
    "target = num_dic[seq[3]]\n",
    "print(target)\n",
    "\n",
    "\n",
    "# batch testing\n",
    "input_batch = []\n",
    "input_batch.append(np.eye(26)[input])\n",
    "print('\\ninput_batch')\n",
    "print(input_batch)\n",
    "\n",
    "target_batch = []\n",
    "target_batch.append(target)\n",
    "print('\\ntarget_batch')\n",
    "print(target_batch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/anaconda3/lib/python3.6/importlib/_bootstrap.py:219: RuntimeWarning: compiletime version 3.5 of module 'tensorflow.python.framework.fast_tensor_util' does not match runtime version 3.6\n",
      "  return f(*args, **kwds)\n",
      "/anaconda3/lib/python3.6/site-packages/h5py/__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(4, ?, 128)\n",
      "(?, 128)\n",
      "eage l\n",
      "enem l\n",
      "frui l\n",
      "grac l\n",
      "laug l\n",
      "mone l\n",
      "peac l\n",
      "part l\n",
      "read l\n",
      "stag l\n",
      "\n",
      "epoch: 0000 error = 3.6324\n",
      "epoch: 0001 error = 3.0854\n",
      "epoch: 0002 error = 1.9044\n",
      "epoch: 0003 error = 6.2722\n",
      "epoch: 0004 error = 1.3935\n",
      "epoch: 0005 error = 1.9103\n",
      "epoch: 0006 error = 1.7403\n",
      "epoch: 0007 error = 1.4842\n",
      "epoch: 0008 error = 1.1420\n",
      "epoch: 0009 error = 0.8397\n",
      "epoch: 0010 error = 0.6927\n",
      "epoch: 0011 error = 0.6378\n",
      "epoch: 0012 error = 0.4560\n",
      "epoch: 0013 error = 0.4587\n",
      "epoch: 0014 error = 0.1886\n",
      "epoch: 0015 error = 0.1454\n",
      "epoch: 0016 error = 0.1056\n",
      "epoch: 0017 error = 0.0470\n",
      "epoch: 0018 error = 0.0121\n",
      "epoch: 0019 error = 0.5094\n",
      "epoch: 0020 error = 0.0488\n",
      "epoch: 0021 error = 0.7401\n",
      "epoch: 0022 error = 0.0125\n",
      "epoch: 0023 error = 0.0127\n",
      "epoch: 0024 error = 0.0034\n",
      "epoch: 0025 error = 0.0035\n",
      "epoch: 0026 error = 0.0483\n",
      "epoch: 0027 error = 0.0003\n",
      "epoch: 0028 error = 0.0026\n",
      "epoch: 0029 error = 0.0016\n",
      "epoch: 0030 error = 0.0005\n",
      "epoch: 0031 error = 0.0244\n",
      "epoch: 0032 error = 0.0244\n",
      "epoch: 0033 error = 0.0000\n",
      "epoch: 0034 error = 0.0000\n",
      "epoch: 0035 error = 0.0000\n",
      "epoch: 0036 error = 0.0000\n",
      "epoch: 0037 error = 0.5234\n",
      "epoch: 0038 error = 0.5791\n",
      "epoch: 0039 error = 3.4432\n",
      "epoch: 0040 error = 3.7798\n",
      "epoch: 0041 error = 2.6827\n",
      "epoch: 0042 error = 1.7580\n",
      "epoch: 0043 error = 2.6510\n",
      "epoch: 0044 error = 0.9494\n",
      "epoch: 0045 error = 0.2783\n",
      "epoch: 0046 error = 2.9991\n",
      "epoch: 0047 error = 0.5550\n",
      "epoch: 0048 error = 0.2327\n",
      "epoch: 0049 error = 0.2427\n",
      "epoch: 0050 error = 0.5897\n",
      "epoch: 0051 error = 0.1283\n",
      "epoch: 0052 error = 0.0006\n",
      "epoch: 0053 error = 0.9740\n",
      "epoch: 0054 error = 0.0524\n",
      "epoch: 0055 error = 0.0010\n",
      "epoch: 0056 error = 0.0394\n",
      "epoch: 0057 error = 0.0176\n",
      "epoch: 0058 error = 0.3066\n",
      "epoch: 0059 error = 0.2481\n",
      "epoch: 0060 error = 0.0024\n",
      "epoch: 0061 error = 0.0038\n",
      "epoch: 0062 error = 0.0004\n",
      "epoch: 0063 error = 0.5143\n",
      "epoch: 0064 error = 0.1892\n",
      "epoch: 0065 error = 0.1213\n",
      "epoch: 0066 error = 0.0015\n",
      "epoch: 0067 error = 0.0000\n",
      "epoch: 0068 error = 0.0000\n",
      "epoch: 0069 error = 0.0002\n",
      "epoch: 0070 error = 0.0000\n",
      "epoch: 0071 error = 0.0000\n",
      "epoch: 0072 error = 0.4125\n",
      "epoch: 0073 error = 0.0001\n",
      "epoch: 0074 error = 0.4840\n",
      "epoch: 0075 error = 0.5687\n",
      "epoch: 0076 error = 0.0312\n",
      "epoch: 0077 error = 0.0000\n",
      "epoch: 0078 error = 0.0001\n",
      "epoch: 0079 error = 0.0002\n",
      "epoch: 0080 error = 0.0000\n",
      "epoch: 0081 error = 0.0001\n",
      "epoch: 0082 error = 0.9893\n",
      "epoch: 0083 error = 0.0657\n",
      "epoch: 0084 error = 0.0006\n",
      "epoch: 0085 error = 2.1029\n",
      "epoch: 0086 error = 0.0067\n",
      "epoch: 0087 error = 0.0131\n",
      "epoch: 0088 error = 0.3971\n",
      "epoch: 0089 error = 0.0008\n",
      "epoch: 0090 error = 0.0000\n",
      "epoch: 0091 error = 0.0108\n",
      "epoch: 0092 error = 0.1155\n",
      "epoch: 0093 error = 0.0001\n",
      "epoch: 0094 error = 0.0000\n",
      "epoch: 0095 error = 0.0006\n",
      "epoch: 0096 error = 0.0002\n",
      "epoch: 0097 error = 0.0001\n",
      "epoch: 0098 error = 0.0097\n",
      "epoch: 0099 error = 0.0013\n",
      "\n",
      "word: ['eager']\n",
      "model: [[ 1.4702070e-01 -7.7149272e+00 -1.7407179e-02  1.1863549e+00\n",
      "  -4.2551627e+00  5.8803213e-01 -7.0966077e+00 -8.3041840e+00\n",
      "  -2.1235971e+00  4.4301786e+00 -4.8600712e+00 -1.0190289e+01\n",
      "  -6.3859940e+00 -5.7118349e+00 -9.0893507e+00 -2.9124975e+00\n",
      "  -1.1030840e+01  2.9645739e+01 -5.5313826e-01 -1.4375789e+00\n",
      "  -6.2246079e+00 -1.1983337e+00 -3.8384404e+00 -5.4543252e+00\n",
      "   8.9085999e+00  3.8238690e+00]]\n",
      "prediction: [17]\n",
      "predicted character r\n",
      "\n",
      "eage r\n",
      "enem y\n",
      "frui t\n",
      "grac e\n",
      "laug e\n",
      "mone y\n",
      "peac e\n",
      "part y\n",
      "read y\n",
      "stag e\n",
      "\n",
      "topi y\n",
      "kore y\n",
      "japa y\n",
      "cros e\n",
      "enem y\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Word Prediction with Tensorflow RNN\n",
    "# written by Sung Kyu Lim\n",
    "# limsk@ece.gatech.edu\n",
    "# 1/7/2019\n",
    "\n",
    "\n",
    "# import tensorflow and numpy\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import os\n",
    "os.environ['TF_CPP_MIN_LOG_LEVEL']='2'\n",
    "\n",
    "\n",
    "# array for alphabet\n",
    "char_arr = [c for c in 'abcdefghijklmnopqrstuvwxyz']\n",
    "\n",
    "\n",
    "# assign array index to each alphabet\n",
    "# ex: 'a': 0, 'b': 1, 'c': 2  ...\n",
    "num_dic = {n: i for i, n in enumerate(char_arr)}\n",
    "\n",
    "\n",
    "# training words and constants\n",
    "seq_data = ['eager', 'enemy', 'fruit', 'grace', 'laugh', 'money', 'peace', 'party', 'ready', 'stage']\n",
    "n_input = n_class = 26\n",
    "n_stage = 4\n",
    "\n",
    "\n",
    "# input encoder\n",
    "def make_batch(seq_data):\n",
    "    input_batch = []\n",
    "    target_batch = []\n",
    "\n",
    "    for seq in seq_data:\n",
    "        input = [num_dic[n] for n in seq[0:n_stage]]\n",
    "        target = num_dic[seq[-1]]\n",
    "        input_batch.append(np.eye(26)[input])\n",
    "        target_batch.append(target)\n",
    "    return input_batch, target_batch\n",
    "\n",
    "\n",
    "# global parameters\n",
    "learning_rate = 0.01\n",
    "n_hidden = 128\n",
    "total_epoch = 100\n",
    "\n",
    "\n",
    "# placeholders and variables\n",
    "# the input placeolder should be 3-dimensional \n",
    "# in order to use tensorflow RNN cells\n",
    "# note that Y, output label, is 1-dimensional\n",
    "X = tf.placeholder(tf.float32, [None, n_stage, n_input])\n",
    "Y = tf.placeholder(tf.int32, [None])\n",
    "W = tf.Variable(tf.random_normal([n_hidden, n_class]))\n",
    "b = tf.Variable(tf.random_normal([n_class]))\n",
    "\n",
    "\n",
    "# two RNN cells and their deep RNN network\n",
    "# we use dropout in cell 1\n",
    "cell1 = tf.nn.rnn_cell.LSTMCell(n_hidden)\n",
    "cell1 = tf.nn.rnn_cell.DropoutWrapper(cell1, output_keep_prob=0.5)\n",
    "cell2 = tf.nn.rnn_cell.LSTMCell(n_hidden)\n",
    "cell2 = tf.nn.rnn_cell.DropoutWrapper(cell2, output_keep_prob=0.5)\n",
    "cell3 = tf.nn.rnn_cell.LSTMCell(n_hidden)\n",
    "cell3 = tf.nn.rnn_cell.DropoutWrapper(cell3, output_keep_prob=0.5)\n",
    "cell4 = tf.nn.rnn_cell.LSTMCell(n_hidden)\n",
    "\n",
    "multi_cell = tf.nn.rnn_cell.MultiRNNCell([cell1, cell2, cell3, cell4])\n",
    "outputs, states = tf.nn.dynamic_rnn(multi_cell, X, dtype=tf.float32)\n",
    "\n",
    "\n",
    "# RNN output re-ordering and trimming in 3 steps\n",
    "# [batch_size, n_stage, n_hidden] ->\n",
    "# [n_stage, batch_size, n_hidden] ->\n",
    "# [batch_size, n_hidden] \n",
    "# this cases the last stage output to be used\n",
    "# model produces 26 floating point values\n",
    "# prediction uses argmax to find which index model computes\n",
    "outputs = tf.transpose(outputs, [1, 0, 2])\n",
    "print(outputs.shape)\n",
    "outputs = outputs[-1]\n",
    "print(outputs.shape)\n",
    "# outputs = outputs[]\n",
    "# tmp = tf.reshape(outputs, [-1, 1, 2])\n",
    "# print(tf.reshape(outputs, [-1, 1, 2]))\n",
    "# print(tmp.shape)\n",
    "model = tf.matmul(outputs, W) + b\n",
    "# prediction = tf.cast(tf.argmax(model, 1), tf.int32)\n",
    "prediction = tf.cast(tf.argmax(model, 1), tf.int32)\n",
    "              \n",
    "\n",
    "# prints model and prediction information\n",
    "def info():\n",
    "    sample = ['eager']\n",
    "    input_batch, target_batch = make_batch(sample)\n",
    "    m_info, p_info = sess.run([model, prediction], feed_dict={X: input_batch, Y: target_batch})\n",
    "\n",
    "    print('word:', sample)\n",
    "    print('model:', m_info)\n",
    "    print('prediction:', p_info)\n",
    "    print('predicted character', char_arr[p_info[0]])\n",
    "    print()\n",
    "\n",
    "\n",
    "# loss function and optimizer\n",
    "# this new tool better handles 1-dimensional label\n",
    "# note the difference in size between logits and labels\n",
    "loss = tf.reduce_mean(tf.nn.sparse_softmax_cross_entropy_with_logits(logits=model, labels=Y))\n",
    "optimizer = tf.train.AdamOptimizer(learning_rate).minimize(loss)\n",
    "\n",
    "\n",
    "# testing the given set of words\n",
    "def test(words):\n",
    "    input_batch, target_batch = make_batch(words)\n",
    "    guess = sess.run(prediction, feed_dict={X: input_batch, Y: target_batch})\n",
    "\n",
    "    for i, seq in enumerate(words):\n",
    "        print(seq[0:n_stage], char_arr[guess[i]])\n",
    "    print()\n",
    "\n",
    "\n",
    "# session and main flow\n",
    "sess = tf.Session()\n",
    "sess.run(tf.global_variables_initializer())\n",
    "\n",
    "test(seq_data)\n",
    "input_batch, target_batch = make_batch(seq_data)\n",
    "for epoch in range(total_epoch):\n",
    "    _, error = sess.run([optimizer, loss], feed_dict={X: input_batch, Y: target_batch})\n",
    "    print('epoch: %04d' % epoch, 'error = %.4f' % error)\n",
    "\n",
    "print()\n",
    "info()\n",
    "test(seq_data)\n",
    "seq_data = ['topi', 'kore', 'japa', 'cros', 'enem']\n",
    "# seq_data = ['topic', 'korea', 'japan', 'cross', 'enemy'] # if seq[0:len(seq_data[0])-1]\n",
    "\n",
    "test(seq_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
