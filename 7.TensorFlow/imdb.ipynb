{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_4 (Embedding)      (None, 80, 64)            640000    \n",
      "_________________________________________________________________\n",
      "lstm_4 (LSTM)                (None, 50)                23000     \n",
      "_________________________________________________________________\n",
      "dense_4 (Dense)              (None, 1)                 51        \n",
      "=================================================================\n",
      "Total params: 663,051\n",
      "Trainable params: 663,051\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "\n",
      "Sample review:\n",
      "[1, 14, 47, 8, 30, 31, 7, 4, 249, 108, 7, 4, 5974, 54, 61, 369, 13, 71, 149, 14, 22, 112, 4, 2401, 311, 12, 16, 3711, 33, 75, 43, 1829, 296, 4, 86, 320, 35, 534, 19, 263, 4821, 1301, 4, 1873, 33, 89, 78, 12, 66, 16, 4, 360, 7, 4, 58, 316, 334, 11, 4, 1716, 43, 645, 662, 8, 257, 85, 1200, 42, 1228, 2578, 83, 68, 3912, 15, 36, 165, 1539, 278, 36, 69, 2, 780, 8, 106, 14, 6905, 1338, 18, 6, 22, 12, 215, 28, 610, 40, 6, 87, 326, 23, 2300, 21, 23, 22, 12, 272, 40, 57, 31, 11, 4, 22, 47, 6, 2307, 51, 9, 170, 23, 595, 116, 595, 1352, 13, 191, 79, 638, 89, 2, 14, 9, 8, 106, 607, 624, 35, 534, 6, 227, 7, 129, 113]\n",
      "Number of words: 141\n",
      "Sentiment: 0\n",
      "\n",
      "The index of \"the\" is 1\n",
      "The index of \"the\" after the dictionary update is 4\n",
      "The word at index 4 is: the\n",
      "\n",
      "Word translation:\n",
      " this has to be one of the worst films of the 1990s when my friends i were watching this film being the target audience it was aimed at we just sat watched the first half an hour with our jaws touching the floor at how bad it really was the rest of the time everyone else in the theatre just started talking to each other leaving or generally crying into their popcorn that they actually paid money they had ??? working to watch this feeble excuse for a film it must have looked like a great idea on paper but on film it looks like no one in the film has a clue what is going on crap acting crap costumes i can't get across how ??? this is to watch save yourself an hour a bit of your life\n",
      "\n",
      "Translation after truncating:\n",
      " this has to be one of the worst films of the 1990s when my friends i were watching this film being the target audience it was aimed at we just sat watched the first half an hour with our jaws touching the floor at how bad it really was the rest of the time everyone else in the theatre just started talking to each other leaving or generally crying into their popcorn that they actually paid money they had\n",
      "Number of words: 80 \n",
      "\n",
      "Train on 20000 samples, validate on 5000 samples\n",
      "Epoch 1/3\n",
      "20000/20000 [==============================] - 65s 3ms/step - loss: 0.5742 - acc: 0.7036 - val_loss: 0.5246 - val_acc: 0.7618\n",
      "Epoch 2/3\n",
      "20000/20000 [==============================] - 64s 3ms/step - loss: 0.4222 - acc: 0.8234 - val_loss: 0.4982 - val_acc: 0.7580\n",
      "Epoch 3/3\n",
      "20000/20000 [==============================] - 64s 3ms/step - loss: 0.3468 - acc: 0.8630 - val_loss: 0.4383 - val_acc: 0.8060\n",
      "25000/25000 [==============================] - 5s 216us/step\n",
      "Test Loss and Accuracy: [0.48930457887649537, 0.7835199999618531]\n",
      "[INFO]result:  [[0.05622699]\n",
      " [0.88948476]\n",
      " [0.87109   ]\n",
      " ...\n",
      " [0.08121972]\n",
      " [0.16735554]\n",
      " [0.9030558 ]]\n"
     ]
    }
   ],
   "source": [
    "# IMDB Movie Review Sentiment Classification with Keras RNN\n",
    "# written by Sung Kyu Lim\n",
    "# limsk@ece.gatech.edu\n",
    "# 1/3/2019\n",
    "\n",
    "\n",
    "# imports \n",
    "from keras import models, layers\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from keras.datasets import imdb\n",
    "\n",
    "\n",
    "# remove annoying warning\n",
    "import os\n",
    "os.environ['TF_CPP_MIN_LOG_LEVEL'] = '2'\n",
    "\n",
    "\n",
    "# IMDB dataset processing:\n",
    "# (1) read the dataset\n",
    "# (2) truncate each review based on vocab_size and maxlen setting\n",
    "# under debug mode, we show a sample review\n",
    "# in its raw data as well as its translated format\n",
    "def data_func(vocab_size, maxlen):\n",
    "    # True: show database details, False: do not show\n",
    "    DEBUG = True\n",
    "\n",
    "    # vocab_size = number of most popular words used\n",
    "    (x_train, y_train), (x_test, y_test) = imdb.load_data(num_words = vocab_size)\n",
    "\n",
    "    if DEBUG:\n",
    "        # sample review before converting to words\n",
    "        print('\\nSample review:')\n",
    "        print(x_train[2])\n",
    "        print('Number of words:', len(x_train[2]))\n",
    "        print('Sentiment:', y_train[2])\n",
    "\n",
    "        # original python dictionary: word -> index\n",
    "        # index 1 is the most popular word\n",
    "        # zero index is not used\n",
    "        word_to_id = imdb.get_word_index()\n",
    "        print('\\nThe index of \"the\" is', word_to_id['the'])\n",
    "\n",
    "        # insert 3 special words in the dictionary\n",
    "        # index 0 is for padding (= filling empry space)\n",
    "        # index 1 is for indicating the beginning of a review\n",
    "        # index 2 is for dropped word (= out of bound)\n",
    "        for key, val in word_to_id.items():\n",
    "            word_to_id[key] = val + 3\n",
    "        word_to_id[\"-\"] = 0\n",
    "        word_to_id[\"\"] = 1\n",
    "        word_to_id[\"???\"] = 2\n",
    "        print('The index of \"the\" after the dictionary update is', word_to_id['the'])\n",
    "\n",
    "        # reversing the dictionary: index -> word\n",
    "        id_to_word = {}\n",
    "        for key, val in word_to_id.items():\n",
    "            id_to_word[val] = key\n",
    "        print('The word at index 4 is:', id_to_word[4])\n",
    "\n",
    "        # translate the sample after adding special characters\n",
    "        print('\\nWord translation:')\n",
    "        print(' '.join(id_to_word[id] for id in x_train[2]))\n",
    "\n",
    "    x_train = pad_sequences(x_train, truncating = 'post', padding = 'post', maxlen = maxlen)\n",
    "    x_test = pad_sequences(x_test, truncating = 'post', padding = 'post', maxlen = maxlen)\n",
    "\n",
    "    if DEBUG:\n",
    "        print('\\nTranslation after truncating:')\n",
    "        print(' '.join(id_to_word[id] for id in x_train[2])) \n",
    "        print('Number of words:', len(x_train[2]), '\\n')\n",
    "\n",
    "    return (x_train, y_train), (x_test, y_test)\n",
    "\n",
    "\n",
    "# keras sequential model for RNN\n",
    "# optimizer: adam\n",
    "# loss: binary cross-entropy\n",
    "# objective: accuracy\n",
    "# activation: sigmoid\n",
    "class RNN(models.Sequential):\n",
    "    def __init__(self, vocab_size, maxlen):\n",
    "        super().__init__()\n",
    "\n",
    "        # RNN is unrolled 80 times to accept 80 words input\n",
    "        self.add(layers.InputLayer(input_shape = (maxlen,)))\n",
    "\n",
    "        # word embedding is key in natual language process (NLP)\n",
    "        # it simplfies vector representation of words\n",
    "        # each word is reduced from 10000 (one-hot) down to 64\n",
    "        # we can visualize word relations on x/y plane \n",
    "        self.add(layers.Embedding(vocab_size, 64))\n",
    "\n",
    "        # size of hidden layer in LSTM cell is 50\n",
    "        # dropout: filters input/output synapses\n",
    "        # recurrent dropout: filters synapses between stages        \n",
    "        self.add(layers.LSTM(50, dropout = 0.2, recurrent_dropout = 0.2))\n",
    "\n",
    "        # form a 50 x 1 fully connected layer for output \n",
    "        self.add(layers.Dense(1, activation = 'sigmoid'))\n",
    "        \n",
    "#         print(\"[INFO]output: \", self.output)\n",
    "        \n",
    "        self.compile(loss = 'binary_crossentropy', optimizer = 'adam', metrics = ['accuracy'])\n",
    "        self.summary()\n",
    "\n",
    "\n",
    "# main function\n",
    "def main():\n",
    "    # hyper parameters\n",
    "    vocab_size = 10000\n",
    "    maxlen = 80\n",
    "\n",
    "    # create an RNN and the IMDB database\n",
    "    model = RNN(vocab_size, maxlen)\n",
    "    (x_train, y_train), (x_test, y_test) = data_func(vocab_size, maxlen)\n",
    "\n",
    "    # conduct learning\n",
    "    model.fit(x_train, y_train, batch_size = 128, epochs = 3, validation_split = 0.2)\n",
    "\n",
    "    # conduct evaluation\n",
    "    result = model.predict(x_test)\n",
    "    \n",
    "    test = model.evaluate(x_test, y_test, batch_size = 128)\n",
    "    print('Test Loss and Accuracy:', test)\n",
    "    print(\"[INFO]result: \", result)\n",
    "\n",
    "\n",
    "# this is how we call main function\n",
    "if __name__ == '__main__':\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_5 (Embedding)      (None, 80, 64)            640000    \n",
      "_________________________________________________________________\n",
      "lstm_5 (LSTM)                (None, 50)                23000     \n",
      "_________________________________________________________________\n",
      "dense_5 (Dense)              (None, 1)                 51        \n",
      "=================================================================\n",
      "Total params: 663,051\n",
      "Trainable params: 663,051\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "\n",
      "Sample review:\n",
      "[1, 14, 47, 8, 30, 31, 7, 4, 249, 108, 7, 4, 5974, 54, 61, 369, 13, 71, 149, 14, 22, 112, 4, 2401, 311, 12, 16, 3711, 33, 75, 43, 1829, 296, 4, 86, 320, 35, 534, 19, 263, 4821, 1301, 4, 1873, 33, 89, 78, 12, 66, 16, 4, 360, 7, 4, 58, 316, 334, 11, 4, 1716, 43, 645, 662, 8, 257, 85, 1200, 42, 1228, 2578, 83, 68, 3912, 15, 36, 165, 1539, 278, 36, 69, 2, 780, 8, 106, 14, 6905, 1338, 18, 6, 22, 12, 215, 28, 610, 40, 6, 87, 326, 23, 2300, 21, 23, 22, 12, 272, 40, 57, 31, 11, 4, 22, 47, 6, 2307, 51, 9, 170, 23, 595, 116, 595, 1352, 13, 191, 79, 638, 89, 2, 14, 9, 8, 106, 607, 624, 35, 534, 6, 227, 7, 129, 113]\n",
      "Number of words: 141\n",
      "Sentiment: 0\n",
      "\n",
      "The index of \"the\" is 1\n",
      "The index of \"the\" after the dictionary update is 4\n",
      "The word at index 4 is: the\n",
      "\n",
      "Word translation:\n",
      " this has to be one of the worst films of the 1990s when my friends i were watching this film being the target audience it was aimed at we just sat watched the first half an hour with our jaws touching the floor at how bad it really was the rest of the time everyone else in the theatre just started talking to each other leaving or generally crying into their popcorn that they actually paid money they had ??? working to watch this feeble excuse for a film it must have looked like a great idea on paper but on film it looks like no one in the film has a clue what is going on crap acting crap costumes i can't get across how ??? this is to watch save yourself an hour a bit of your life\n",
      "\n",
      "Translation after truncating:\n",
      " this has to be one of the worst films of the 1990s when my friends i were watching this film being the target audience it was aimed at we just sat watched the first half an hour with our jaws touching the floor at how bad it really was the rest of the time everyone else in the theatre just started talking to each other leaving or generally crying into their popcorn that they actually paid money they had\n",
      "Number of words: 80 \n",
      "\n",
      "Train on 20000 samples, validate on 5000 samples\n",
      "Epoch 1/3\n",
      "20000/20000 [==============================] - 22s 1ms/step - loss: 0.6107 - acc: 0.6690 - val_loss: 0.4652 - val_acc: 0.7938\n",
      "Epoch 2/3\n",
      "20000/20000 [==============================] - 21s 1ms/step - loss: 0.4301 - acc: 0.8201 - val_loss: 0.4548 - val_acc: 0.7926\n",
      "Epoch 3/3\n",
      "20000/20000 [==============================] - 21s 1ms/step - loss: 0.3637 - acc: 0.8559 - val_loss: 0.4478 - val_acc: 0.8094\n",
      "25000/25000 [==============================] - 6s 220us/step\n",
      "Test Loss and Accuracy: [0.5052015677642823, 0.7819599999618531]\n",
      "[INFO]result:  [[0.06434944]\n",
      " [0.88503194]\n",
      " [0.6927412 ]\n",
      " ...\n",
      " [0.04943266]\n",
      " [0.14737396]\n",
      " [0.2949893 ]]\n"
     ]
    }
   ],
   "source": [
    "# hyper parameters\n",
    "vocab_size = 10000\n",
    "maxlen = 80\n",
    "\n",
    "# create an RNN and the IMDB database\n",
    "model = RNN(vocab_size, maxlen)\n",
    "(x_train, y_train), (x_test, y_test) = data_func(vocab_size, maxlen)\n",
    "\n",
    "# conduct learning\n",
    "model.fit(x_train, y_train, batch_size = 128, epochs = 3, validation_split = 0.2)\n",
    "\n",
    "# conduct evaluation\n",
    "result = model.predict(x_test)\n",
    "\n",
    "test = model.evaluate(x_test, y_test, batch_size = 128)\n",
    "print('Test Loss and Accuracy:',test)\n",
    "print(\"[INFO]result: \", result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "numpy.ndarray"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(type(result))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(25000, 1)"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(25000, 80)"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(25000, 80)"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0, 1, 1, ..., 0, 0, 0])"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "tmp = np.round(result) == y_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(25000, 25000)"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tmp.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ True, False, False, ...,  True,  True,  True],\n",
       "       [False,  True,  True, ..., False, False, False],\n",
       "       [False,  True,  True, ..., False, False, False],\n",
       "       ...,\n",
       "       [ True, False, False, ...,  True,  True,  True],\n",
       "       [ True, False, False, ...,  True,  True,  True],\n",
       "       [ True, False, False, ...,  True,  True,  True]])"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tmp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "number = 0\n",
    "for i in tmp[2]:\n",
    "    if i == True:\n",
    "        number = number+1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "12500"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "number"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.5"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "number / list(tmp.shape)[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
