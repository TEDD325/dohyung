{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['../data/RNN_coin/X_30_50_1_0.1.pickle', '../data/RNN_coin/y_30_25_1_0.1.pickle', '../data/RNN_coin/y_10_100_1_0.1.pickle', '../data/RNN_coin/y_30_10_1_0.1.pickle', '../data/RNN_coin/X_10_25_1_0.1.pickle', '../data/RNN_coin/y_60_25_1_0.1.pickle', '../data/RNN_coin/X_60_100_1_0.1.pickle', '../data/RNN_coin/y_60_10_1_0.1.pickle', '../data/RNN_coin/y_10_25_1_0.1.pickle', '../data/RNN_coin/X_30_100_1_0.1.pickle', '../data/RNN_coin/y_30_75_1_0.1.pickle', '../data/RNN_coin/y_10_75_1_0.1.pickle', '../data/RNN_coin/y_10_50_1_0.1.pickle', '../data/RNN_coin/X_60_50_1_0.1.pickle', '../data/RNN_coin/y_30_100_1_0.1.pickle', '../data/RNN_coin/X_30_25_1_0.1.pickle', '../data/RNN_coin/X_10_75_1_0.1.pickle', '../data/RNN_coin/X_60_75_1_0.1.pickle', '../data/RNN_coin/X_30_10_1_0.1.pickle', '../data/RNN_coin/X_10_100_1_0.1.pickle', '../data/RNN_coin/y_30_50_1_0.1.pickle', '../data/RNN_coin/y_10_10_1_0.1.pickle', '../data/RNN_coin/X_10_50_1_0.1.pickle', '../data/RNN_coin/y_60_75_1_0.1.pickle', '../data/RNN_coin/X_60_10_1_0.1.pickle', '../data/RNN_coin/y_60_100_1_0.1.pickle', '../data/RNN_coin/X_30_75_1_0.1.pickle', '../data/RNN_coin/.DS_Store', '../data/RNN_coin/X_10_10_1_0.1.pickle', '../data/RNN_coin/X_60_25_1_0.1.pickle', '../data/RNN_coin/y_60_50_1_0.1.pickle']\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from sklearn.preprocessing import scale, robust_scale, minmax_scale, maxabs_scale\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "# import mglearn\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "\n",
    "def search(dirname):\n",
    "    filenames = os.listdir(dirname)\n",
    "    fileList = []\n",
    "    for filename in filenames:\n",
    "        full_filename = os.path.join(dirname, filename)\n",
    "        fileList.append(full_filename)\n",
    "    return fileList\n",
    "\n",
    "fileList = search(\"./data/RNN_coin/\")\n",
    "# print(fileList)\n",
    "fileList.remove(\"./data/RNN_coin/.DS_Store\")\n",
    "\n",
    "coin_data_dict = {}\n",
    "coin_data_x_np = {}\n",
    "coin_data_y_np = {}\n",
    "coin_data_x_np_reshaped = {}\n",
    "coin_data_y_np_reshaped = {}\n",
    "coin_data_x_pd = {}\n",
    "coin_data_y_pd = {}\n",
    "X_train = {}\n",
    "X_test = {}\n",
    "y_train = {}\n",
    "y_test = {}\n",
    "\n",
    "for file in fileList:\n",
    "    print(file)\n",
    "    data = pd.read_pickle(file)\n",
    "    X = np.array(data[0])\n",
    "    y = np.array(data[1])\n",
    "    X_reshaped = X.reshape(len(y) * 8, -1)\n",
    "    y_reshaped = y.reshape(-1)\n",
    "    X_pd = pd.DataFrame(X_reshaped)\n",
    "    y_pd = pd.DataFrame(y_reshaped)\n",
    "\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X_pd,\n",
    "                                                        y_pd,\n",
    "                                                        test_size=0.1,\n",
    "                                                        random_state=42,\n",
    "                                                        shuffle=False)\n",
    "\n",
    "    X_train = np.array(X_train)\n",
    "    y_train = np.array(y_train).reshape(-1)\n",
    "    X_test = np.array(X_test)\n",
    "    y_test = np.array(y_test).reshape(-1)\n",
    "\n",
    "    # X_train = maxabs_scale(X_train)\n",
    "    # X_test = maxabs_scale(X_test)\n",
    "    # y_train = maxabs_scale(y_train)\n",
    "    # y_test = maxabs_scale(y_test)\n",
    "    #\n",
    "    # GaussianNB의 경우 scaling 적용 시 오히려 정확도가 떨어지는 문제\n",
    "\n",
    "    Gaussian = GaussianNB()\n",
    "\n",
    "    # C=1.0, max_iter=10\n",
    "    train_score_sum = []\n",
    "    test_score_sum = []\n",
    "\n",
    "    clf_2 = Gaussian.fit(X_train, y_train)\n",
    "    param_info = clf_2.get_params()\n",
    "    train_score = clf_2.score(X_train, y_train)\n",
    "    test_score = clf_2.score(X_test, y_test)\n",
    "    train_score_sum.append(train_score)\n",
    "    test_score_sum.append(test_score)\n",
    "    print(\"train_score : {}\".format(train_score))\n",
    "    print(\"test_score : {}\".format(test_score))\n",
    "    print()\n",
    "    print()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "나이브 베이즈 분류기는 LinearSVC보다 훈련 속도가 빠른 편이지만, 일반화 성능이 뒤쳐진다.<br>\n",
    "GaussianNB는 연속적인 어떤 데이터에도 적용할 수 있다.<br>\n",
    "GaussianNBsms 클래스별로 각 특성의 표준편차와 평균을 저장한다. 예측시엔 데이터 포인트를 클래스의 통계 값과 비교해서 가장 잘 맞는 클래스를 예측값으로 한다.<br>\n",
    "GaussianNB는 대부분 매우 고차원인 데이터셋에 사용한다.<br>\n",
    "선형 모델로는 학습 시간이 너무 오래 걸리는 매우 큰 데이터셋에는 나이브 베이즈 모델을 시도해볼 만하며 종종 사용된다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from sklearn.preprocessing import scale, robust_scale, minmax_scale, maxabs_scale\n",
    "# X_train = maxabs_scale(X_train)\n",
    "# X_test = maxabs_scale(X_test)\n",
    "# y_train = maxabs_scale(y_train)\n",
    "# y_test = maxabs_scale(y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "GaussianNB의 경우 scaling 적용 시 오히려 정확도가 떨어지는 문제"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
