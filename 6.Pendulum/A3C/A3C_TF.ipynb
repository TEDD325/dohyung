{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "usage: ipykernel_launcher.py [-h] [--algorithm ALGORITHM] [--train] [--lr LR]\n",
      "                             [--update-freq UPDATE_FREQ] [--max-eps MAX_EPS]\n",
      "                             [--gamma GAMMA] [--save-dir SAVE_DIR]\n",
      "ipykernel_launcher.py: error: unrecognized arguments: -f /Users/dohyungkwon/Library/Jupyter/runtime/kernel-db9cf4c1-c8f8-4ee4-8030-67bbd820306a.json\n"
     ]
    },
    {
     "ename": "SystemExit",
     "evalue": "2",
     "output_type": "error",
     "traceback": [
      "An exception has occurred, use %tb to see the full traceback.\n",
      "\u001b[0;31mSystemExit\u001b[0m\u001b[0;31m:\u001b[0m 2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/dohyungkwon/anaconda3/lib/python3.7/site-packages/IPython/core/interactiveshell.py:3275: UserWarning: To exit: use 'exit', 'quit', or Ctrl-D.\n",
      "  warn(\"To exit: use 'exit', 'quit', or Ctrl-D.\", stacklevel=1)\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"\"\n",
    "\n",
    "import threading\n",
    "import gym\n",
    "import multiprocessing\n",
    "import numpy as np\n",
    "from queue import Queue\n",
    "import argparse\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow.python import keras\n",
    "from tensorflow.python.keras import layers\n",
    "\n",
    "tf.enable_eager_execution()\n",
    "\n",
    "parser = argparse.ArgumentParser(description='Run A3C algorithm on the game '\n",
    "                                             'Cartpole.')\n",
    "parser.add_argument('--algorithm', default='a3c', type=str,\n",
    "                    help='Choose between \\'a3c\\' and \\'random\\'.')\n",
    "parser.add_argument('--train', dest='train', action='store_true',\n",
    "                    help='Train our model.')\n",
    "parser.add_argument('--lr', default=0.001,\n",
    "                    help='Learning rate for the shared optimizer.')\n",
    "parser.add_argument('--update-freq', default=20, type=int,\n",
    "                    help='How often to update the global model.')\n",
    "parser.add_argument('--max-eps', default=1000, type=int,\n",
    "                    help='Global maximum number of episodes to run.')\n",
    "parser.add_argument('--gamma', default=0.99,\n",
    "                    help='Discount factor of rewards.')\n",
    "parser.add_argument('--save-dir', default='/tmp/', type=str,\n",
    "                    help='Directory in which you desire to save the model.')\n",
    "args = parser.parse_args()\n",
    "\n",
    "class ActorCriticModel(keras.Model):\n",
    "  def __init__(self, state_size, action_size):\n",
    "    super(ActorCriticModel, self).__init__()\n",
    "    self.state_size = state_size\n",
    "    self.action_size = action_size\n",
    "    self.dense1 = layers.Dense(100, activation='relu')\n",
    "    self.policy_logits = layers.Dense(action_size)\n",
    "    self.dense2 = layers.Dense(100, activation='relu')\n",
    "    self.values = layers.Dense(1)\n",
    "\n",
    "  def call(self, inputs):\n",
    "    # Forward pass\n",
    "    x = self.dense1(inputs)\n",
    "    logits = self.policy_logits(x)\n",
    "    v1 = self.dense2(inputs)\n",
    "    values = self.values(v1)\n",
    "    return logits, values\n",
    "\n",
    "def record(episode,\n",
    "           episode_reward,\n",
    "           worker_idx,\n",
    "           global_ep_reward,\n",
    "           result_queue,\n",
    "           total_loss,\n",
    "           num_steps):\n",
    "  \"\"\"Helper function to store score and print statistics.\n",
    "\n",
    "  Arguments:\n",
    "    episode: Current episode\n",
    "    episode_reward: Reward accumulated over the current episode\n",
    "    worker_idx: Which thread (worker)\n",
    "    global_ep_reward: The moving average of the global reward\n",
    "    result_queue: Queue storing the moving average of the scores\n",
    "    total_loss: The total loss accumualted over the current episode\n",
    "    num_steps: The number of steps the episode took to complete\n",
    "  \"\"\"\n",
    "  if global_ep_reward == 0:\n",
    "    global_ep_reward = episode_reward\n",
    "  else:\n",
    "    global_ep_reward = global_ep_reward * 0.99 + episode_reward * 0.01\n",
    "  print(\n",
    "      f\"Episode: {episode} | \"\n",
    "      f\"Moving Average Reward: {int(global_ep_reward)} | \"\n",
    "      f\"Episode Reward: {int(episode_reward)} | \"\n",
    "      f\"Loss: {int(total_loss / float(num_steps) * 1000) / 1000} | \"\n",
    "      f\"Steps: {num_steps} | \"\n",
    "      f\"Worker: {worker_idx}\"\n",
    "  )\n",
    "  result_queue.put(global_ep_reward)\n",
    "  return global_ep_reward\n",
    "\n",
    "\n",
    "class RandomAgent:\n",
    "  \"\"\"Random Agent that will play the specified game\n",
    "\n",
    "    Arguments:\n",
    "      env_name: Name of the environment to be played\n",
    "      max_eps: Maximum number of episodes to run agent for.\n",
    "  \"\"\"\n",
    "  def __init__(self, env_name, max_eps):\n",
    "    self.env = gym.make(env_name)\n",
    "    self.max_episodes = max_eps\n",
    "    self.global_moving_average_reward = 0\n",
    "    self.res_queue = Queue()\n",
    "\n",
    "  def run(self):\n",
    "    reward_avg = 0\n",
    "    for episode in range(self.max_episodes):\n",
    "      done = False\n",
    "      self.env.reset()\n",
    "      reward_sum = 0.0\n",
    "      steps = 0\n",
    "      while not done:\n",
    "        # Sample randomly from the action space and step\n",
    "        _, reward, done, _ = self.env.step(self.env.action_space.sample())\n",
    "        steps += 1\n",
    "        reward_sum += reward\n",
    "      # Record statistics\n",
    "      self.global_moving_average_reward = record(episode,\n",
    "                                                 reward_sum,\n",
    "                                                 0,\n",
    "                                                 self.global_moving_average_reward,\n",
    "                                                 self.res_queue, 0, steps)\n",
    "\n",
    "      reward_avg += reward_sum\n",
    "    final_avg = reward_avg / float(self.max_episodes)\n",
    "    print(\"Average score across {} episodes: {}\".format(self.max_episodes, final_avg))\n",
    "    return final_avg\n",
    "\n",
    "\n",
    "class MasterAgent():\n",
    "  def __init__(self):\n",
    "    self.game_name = 'CartPole-v0'\n",
    "    save_dir = args.save_dir\n",
    "    self.save_dir = save_dir\n",
    "    if not os.path.exists(save_dir):\n",
    "      os.makedirs(save_dir)\n",
    "\n",
    "    env = gym.make(self.game_name)\n",
    "    self.state_size = env.observation_space.shape[0]\n",
    "    self.action_size = env.action_space.n\n",
    "    self.opt = tf.train.AdamOptimizer(args.lr, use_locking=True)\n",
    "    print(self.state_size, self.action_size)\n",
    "\n",
    "    self.global_model = ActorCriticModel(self.state_size, self.action_size)  # global network\n",
    "    self.global_model(tf.convert_to_tensor(np.random.random((1, self.state_size)), dtype=tf.float32))\n",
    "\n",
    "  def train(self):\n",
    "    if args.algorithm == 'random':\n",
    "      random_agent = RandomAgent(self.game_name, args.max_eps)\n",
    "      random_agent.run()\n",
    "      return\n",
    "\n",
    "    res_queue = Queue()\n",
    "\n",
    "    workers = [Worker(self.state_size,\n",
    "                      self.action_size,\n",
    "                      self.global_model,\n",
    "                      self.opt, res_queue,\n",
    "                      i, game_name=self.game_name,\n",
    "                      save_dir=self.save_dir) for i in range(multiprocessing.cpu_count())]\n",
    "\n",
    "    for i, worker in enumerate(workers):\n",
    "      print(\"Starting worker {}\".format(i))\n",
    "      worker.start()\n",
    "\n",
    "    moving_average_rewards = []  # record episode reward to plot\n",
    "    while True:\n",
    "      reward = res_queue.get()\n",
    "      if reward is not None:\n",
    "        moving_average_rewards.append(reward)\n",
    "      else:\n",
    "        break\n",
    "    [w.join() for w in workers]\n",
    "\n",
    "    plt.plot(moving_average_rewards)\n",
    "    plt.ylabel('Moving average ep reward')\n",
    "    plt.xlabel('Step')\n",
    "    plt.savefig(os.path.join(self.save_dir,\n",
    "                             '{} Moving Average.png'.format(self.game_name)))\n",
    "    plt.show()\n",
    "\n",
    "  def play(self):\n",
    "    env = gym.make(self.game_name).unwrapped\n",
    "    state = env.reset()\n",
    "    model = self.global_model\n",
    "    model_path = os.path.join(self.save_dir, 'model_{}.h5'.format(self.game_name))\n",
    "    print('Loading model from: {}'.format(model_path))\n",
    "    model.load_weights(model_path)\n",
    "    done = False\n",
    "    step_counter = 0\n",
    "    reward_sum = 0\n",
    "\n",
    "    try:\n",
    "      while not done:\n",
    "        env.render(mode='rgb_array')\n",
    "        policy, value = model(tf.convert_to_tensor(state[None, :], dtype=tf.float32))\n",
    "        policy = tf.nn.softmax(policy)\n",
    "        action = np.argmax(policy)\n",
    "        state, reward, done, _ = env.step(action)\n",
    "        reward_sum += reward\n",
    "        print(\"{}. Reward: {}, action: {}\".format(step_counter, reward_sum, action))\n",
    "        step_counter += 1\n",
    "    except KeyboardInterrupt:\n",
    "      print(\"Received Keyboard Interrupt. Shutting down.\")\n",
    "    finally:\n",
    "      env.close()\n",
    "\n",
    "\n",
    "class Memory:\n",
    "  def __init__(self):\n",
    "    self.states = []\n",
    "    self.actions = []\n",
    "    self.rewards = []\n",
    "\n",
    "  def store(self, state, action, reward):\n",
    "    self.states.append(state)\n",
    "    self.actions.append(action)\n",
    "    self.rewards.append(reward)\n",
    "\n",
    "  def clear(self):\n",
    "    self.states = []\n",
    "    self.actions = []\n",
    "    self.rewards = []\n",
    "\n",
    "\n",
    "class Worker(threading.Thread):\n",
    "  # Set up global variables across different threads\n",
    "  global_episode = 0\n",
    "  # Moving average reward\n",
    "  global_moving_average_reward = 0\n",
    "  best_score = 0\n",
    "  save_lock = threading.Lock()\n",
    "\n",
    "  def __init__(self,\n",
    "               state_size,\n",
    "               action_size,\n",
    "               global_model,\n",
    "               opt,\n",
    "               result_queue,\n",
    "               idx,\n",
    "               game_name='CartPole-v0',\n",
    "               save_dir='/tmp'):\n",
    "    super(Worker, self).__init__()\n",
    "    self.state_size = state_size\n",
    "    self.action_size = action_size\n",
    "    self.result_queue = result_queue\n",
    "    self.global_model = global_model\n",
    "    self.opt = opt\n",
    "    self.local_model = ActorCriticModel(self.state_size, self.action_size)\n",
    "    self.worker_idx = idx\n",
    "    self.game_name = game_name\n",
    "    self.env = gym.make(self.game_name).unwrapped\n",
    "    self.save_dir = save_dir\n",
    "    self.ep_loss = 0.0\n",
    "\n",
    "  def run(self):\n",
    "    total_step = 1\n",
    "    mem = Memory()\n",
    "    while Worker.global_episode < args.max_eps:\n",
    "      current_state = self.env.reset()\n",
    "      mem.clear()\n",
    "      ep_reward = 0.\n",
    "      ep_steps = 0\n",
    "      self.ep_loss = 0\n",
    "\n",
    "      time_count = 0\n",
    "      done = False\n",
    "      while not done:\n",
    "        logits, _ = self.local_model(\n",
    "            tf.convert_to_tensor(current_state[None, :],\n",
    "                                 dtype=tf.float32))\n",
    "        probs = tf.nn.softmax(logits)\n",
    "\n",
    "        action = np.random.choice(self.action_size, p=probs.numpy()[0])\n",
    "        new_state, reward, done, _ = self.env.step(action)\n",
    "        if done:\n",
    "          reward = -1\n",
    "        ep_reward += reward\n",
    "        mem.store(current_state, action, reward)\n",
    "\n",
    "        if time_count == args.update_freq or done:\n",
    "          # Calculate gradient wrt to local model. We do so by tracking the\n",
    "          # variables involved in computing the loss by using tf.GradientTape\n",
    "          with tf.GradientTape() as tape:\n",
    "            total_loss = self.compute_loss(done,\n",
    "                                           new_state,\n",
    "                                           mem,\n",
    "                                           args.gamma)\n",
    "          self.ep_loss += total_loss\n",
    "          # Calculate local gradients\n",
    "          grads = tape.gradient(total_loss, self.local_model.trainable_weights)\n",
    "          # Push local gradients to global model\n",
    "          self.opt.apply_gradients(zip(grads,\n",
    "                                       self.global_model.trainable_weights))\n",
    "          # Update local model with new weights\n",
    "          self.local_model.set_weights(self.global_model.get_weights())\n",
    "\n",
    "          mem.clear()\n",
    "          time_count = 0\n",
    "\n",
    "          if done:  # done and print information\n",
    "            Worker.global_moving_average_reward = \\\n",
    "              record(Worker.global_episode, ep_reward, self.worker_idx,\n",
    "                     Worker.global_moving_average_reward, self.result_queue,\n",
    "                     self.ep_loss, ep_steps)\n",
    "            # We must use a lock to save our model and to print to prevent data races.\n",
    "            if ep_reward > Worker.best_score:\n",
    "              with Worker.save_lock:\n",
    "                print(\"Saving best model to {}, \"\n",
    "                      \"episode score: {}\".format(self.save_dir, ep_reward))\n",
    "                self.global_model.save_weights(\n",
    "                    os.path.join(self.save_dir,\n",
    "                                 'model_{}.h5'.format(self.game_name))\n",
    "                )\n",
    "                Worker.best_score = ep_reward\n",
    "            Worker.global_episode += 1\n",
    "        ep_steps += 1\n",
    "\n",
    "        time_count += 1\n",
    "        current_state = new_state\n",
    "        total_step += 1\n",
    "    self.result_queue.put(None)\n",
    "\n",
    "  def compute_loss(self,\n",
    "                   done,\n",
    "                   new_state,\n",
    "                   memory,\n",
    "                   gamma=0.99):\n",
    "    if done:\n",
    "      reward_sum = 0.  # terminal\n",
    "    else:\n",
    "      reward_sum = self.local_model(\n",
    "          tf.convert_to_tensor(new_state[None, :],\n",
    "                               dtype=tf.float32))[-1].numpy()[0]\n",
    "\n",
    "    # Get discounted rewards\n",
    "    discounted_rewards = []\n",
    "    for reward in memory.rewards[::-1]:  # reverse buffer r\n",
    "      reward_sum = reward + gamma * reward_sum\n",
    "      discounted_rewards.append(reward_sum)\n",
    "    discounted_rewards.reverse()\n",
    "\n",
    "    logits, values = self.local_model(\n",
    "        tf.convert_to_tensor(np.vstack(memory.states),\n",
    "                             dtype=tf.float32))\n",
    "    # Get our advantages\n",
    "    advantage = tf.convert_to_tensor(np.array(discounted_rewards)[:, None],\n",
    "                            dtype=tf.float32) - values\n",
    "    # Value loss\n",
    "    value_loss = advantage ** 2\n",
    "\n",
    "    # Calculate our policy loss\n",
    "    policy = tf.nn.softmax(logits)\n",
    "    entropy = tf.nn.softmax_cross_entropy_with_logits_v2(labels=policy, logits=logits)\n",
    "\n",
    "    policy_loss = tf.nn.sparse_softmax_cross_entropy_with_logits(labels=memory.actions,\n",
    "                                                                 logits=logits)\n",
    "    policy_loss *= tf.stop_gradient(advantage)\n",
    "    policy_loss -= 0.01 * entropy\n",
    "    total_loss = tf.reduce_mean((0.5 * value_loss + policy_loss))\n",
    "    return total_loss\n",
    "\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    print(args)\n",
    "    master = MasterAgent()\n",
    "    master.train()\n",
    "#     if args.train:\n",
    "#         master.train()\n",
    "#     else:\n",
    "#         master.play()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
