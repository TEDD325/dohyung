{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](./img/6A673C9E-61C7-4956-B4A2-D236E93B2181.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "우리가 수집한 코인 데이터는 오를 것인지, 내릴 것인지에 label 정보가 있는 supervised learning이라고 할 수 있겠다. 따라서, 논문에 supervised learning에 대한 언급을 먼저 하는 것이 좋을 것 같다.\n",
    "그 다음, supervised learning에서 가장 강력하다고 알려진 XGBoost에 대해 설명하는 섹션을 만든다. XGBoost에 대한 개념을 설명하는 섹션이 필요하고 왜 강력한지에 대한 설명도 필요하다.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "import xgboost as xgb\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Q1.\n",
    "## XGBoost: Fit/Predict\n",
    "It's time to create your first XGBoost model! As Sergey showed you in the video, you can use the scikit-learn <code>.fit()</code> / <code>.predict()</code> paradigm that you are already familiar to build your XGBoost models, as the <code>xgboost</code> library has a scikit-learn compatible API!\n",
    "<br><br>\n",
    "Here, you'll be working with churn data. This dataset contains imaginary data from a ride-sharing app with user behaviors over their first month of app usage in a set of imaginary cities as well as whether they used the service 5 months after sign-up. It has been pre-loaded for you into a DataFrame called <code>churn_data</code> - explore it in the Shell!\n",
    "<br><br>\n",
    "Your goal is to use the first month's worth of data to predict whether the app's users will remain users of the service at the 5 month mark. This is a typical setup for a churn prediction problem. To do this, you'll split the data into training and test sets, fit a small <code>xgboost</code> model on the training set, and evaluate its performance on the test set by computing its accuracy.\n",
    "<br><br>\n",
    "<code>pandas</code> and <code>numpy</code> have been imported as <code>pd</code> and <code>np</code>, and <code>train_test_split</code> has been imported from <code>sklearn.model_selection</code>. Additionally, the arrays for the features and the target have been created as <code>X</code> and <code>y</code>.\n",
    "<br><br>\n",
    "- Import <code>xgboost</code> as <code>xgb</code>.\n",
    "- Create training and test sets such that 20% of the data is used for testing. Use a <code>random_state</code> of <code>123</code>.<br>\n",
    "- Instantiate an <code>XGBoostClassifier</code> as <code>xg_cl</code> using <code>xgb.XGBClassifier()</code>. Specify <code>n_estimators</code> to be <code>10</code> estimators and an <code>objective</code> of <code>'binary:logistic'</code>. Do not worry about what this means just yet, you will learn about these parameters later in this course.<br>\n",
    "- Fit <code>xg_cl</code> to the training set (<code>X_train, y_train</code>) using the <code>.fit()</code> method.\n",
    "- Predict the labels of the test set (X_test) using the <code>.predict()</code> method and hit 'Submit Answer' to print the accuracy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import xgboost\n",
    "____\n",
    "\n",
    "# Create arrays for the features and the target: X, y\n",
    "X, y = churn_data.iloc[:,:-1], churn_data.iloc[:,-1]\n",
    "\n",
    "# Create the training and test sets\n",
    "X_train, X_test, y_train, y_test= ____(____, ____, test_size=____, random_state=123)\n",
    "\n",
    "# Instantiate the XGBClassifier: xg_cl\n",
    "xg_cl = ____.____(____='____', ____=____, seed=123)\n",
    "\n",
    "# Fit the classifier to the training set\n",
    "____\n",
    "\n",
    "# Predict the labels of the test set: preds\n",
    "preds = ____\n",
    "\n",
    "# Compute the accuracy: accuracy\n",
    "accuracy = float(np.sum(preds==y_test))/y_test.shape[0]\n",
    "print(\"accuracy: %f\" % (accuracy))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# A1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import xgboost\n",
    "import xgboost as xgb\n",
    "\n",
    "# Create arrays for the features and the target: X, y\n",
    "X, y = churn_data.iloc[:,:-1], churn_data.iloc[:,-1]\n",
    "\n",
    "# Create the training and test sets\n",
    "X_train, X_test, y_train, y_test= train_test_split(X, y, test_size=0.2, random_state=123)\n",
    "\n",
    "# Instantiate the XGBClassifier: xg_cl\n",
    "xg_cl = xgb.XGBClassifier(objective='binary:logistic', n_estimators=10, seed=123)\n",
    "\n",
    "# Fit the classifier to the training set\n",
    "xg_cl.fit(X_train, y_train)\n",
    "\n",
    "# Predict the labels of the test set: preds\n",
    "preds = xg_cl.predict(X_test)\n",
    "\n",
    "# Compute the accuracy: accuracy\n",
    "accuracy = float(np.sum(preds==y_test))/y_test.shape[0]\n",
    "print(\"accuracy: %f\" % (accuracy))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Q2.\n",
    "## Decision trees\n",
    "Your task in this exercise is to make a simple decision tree using scikit-learn's <code>DecisionTreeClassifier</code> on the <code>breast cancer</code> dataset that comes pre-loaded with scikit-learn.\n",
    "<br><br>\n",
    "This dataset contains numeric measurements of various dimensions of individual tumors (such as perimeter and texture) from breast biopsies and a single outcome value (the tumor is either malignant, or benign).\n",
    "<br><br>\n",
    "We've preloaded the dataset of samples (measurements) into <code>X</code> and the target values per tumor into <code>y</code>. Now, you have to split the complete dataset into training and testing sets, and then train a <code>DecisionTreeClassifier</code>. You'll specify a parameter called <code>max_depth</code>. Many other parameters can be modified within this model, and you can check all of them out here.\n",
    "<br><br><br>\n",
    "\n",
    "- Import:\n",
    "  - <code>train_test_split from</code> <code>sklearn.model_selection</code>.\n",
    "  - <code>DecisionTreeClassifier</code> from <code>sklearn.tree</code>.\n",
    "- Create training and test sets such that 20% of the data is used for testing. Use a <code>random_state</code> of <code>123</code>.\n",
    "- Instantiate a <code>DecisionTreeClassifier</code> called <code>dt_clf_4</code> with a <code>max_depth</code> of <code>4</code>. This parameter specifies the maximum number of successive split points you can have before reaching a leaf node.\n",
    "- Fit the classifier to the training set and predict the labels of the test set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import the necessary modules\n",
    "____\n",
    "____\n",
    "\n",
    "# Create the training and test sets\n",
    "X_train, X_test, y_train, y_test = ____(____, ____, test_size=____, random_state=____)\n",
    "\n",
    "# Instantiate the classifier: dt_clf_4\n",
    "dt_clf_4 = ____\n",
    "\n",
    "# Fit the classifier to the training set\n",
    "____\n",
    "\n",
    "# Predict the labels of the test set: y_pred_4\n",
    "y_pred_4 = ____\n",
    "\n",
    "# Compute the accuracy of the predictions: accuracy\n",
    "accuracy = float(np.sum(y_pred_4==y_test))/y_test.shape[0]\n",
    "print(\"accuracy:\", accuracy)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# A2."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import the necessary modules\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "\n",
    "# Create the training and test sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=123)\n",
    "\n",
    "# Instantiate the classifier: dt_clf_4\n",
    "dt_clf_4 = DecisionTreeClassifier(max_depth=4)\n",
    "\n",
    "# Fit the classifier to the training set\n",
    "dt_clf_4.fit(X_train, y_train)\n",
    "\n",
    "# Predict the labels of the test set: y_pred_4\n",
    "y_pred_4 = dt_clf_4.predict(X_test)\n",
    "\n",
    "# Compute the accuracy of the predictions: accuracy\n",
    "accuracy = float(np.sum(y_pred_4==y_test))/y_test.shape[0]\n",
    "print(\"accuracy:\", accuracy)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Q3.\n",
    "## Measuring accuracy\n",
    "You'll now practice using XGBoost's learning API through its baked in cross-validation capabilities. As Sergey discussed in the previous video, XGBoost gets its lauded performance and efficiency gains by utilizing its own optimized data structure for datasets called a <code>DMatrix</code>.\n",
    "<br><br>\n",
    "In the previous exercise, the input datasets were converted into <code>DMatrix</code> data on the fly, but when you use the <code>xgboost</code> <code>cv</code> object, you have to first explicitly convert your data into a <code>DMatrix</code>. So, that's what you will do here before running cross-validation on <code>churn_data</code>.\n",
    "<br><br><br>\n",
    "- Create a <code>DMatrix</code> called <code>churn_dmatrix</code> from <code>churn_data</code> using <code>xgb.DMatrix()</code>. The features are available in <code>X</code> and the labels in <code>y</code>.\n",
    "- Perform 3-fold cross-validation by calling <code>xgb.cv()</code>. <code>dtrain</code> is your <code>churn_dmatrix</code>, <code>params</code> is your parameter dictionary, <code>folds</code> is the number of cross-validation folds (3), <code>num_boosting_rounds</code> is the number of trees we want to build (5), <code>metrics</code> is the metric you want to compute (this will be \"<code>error</code>\", which we will convert to an accuracy)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the DMatrix: churn_dmatrix\n",
    "churn_dmatrix = ____(data=____, label=____)\n",
    "\n",
    "# Create the parameter dictionary: params\n",
    "params = {\"objective\":\"reg:logistic\", \"max_depth\":3}\n",
    "\n",
    "# Perform cross-validation: cv_results\n",
    "cv_results = ____(dtrain=____, params=____, nfold=____, num_boost_round=____, metrics=\"____\", as_pandas=____, seed=123)\n",
    "\n",
    "# Print cv_results\n",
    "print(cv_results)\n",
    "\n",
    "# Print the accuracy\n",
    "print(((1-cv_results[\"test-error-mean\"]).iloc[-1]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# A3."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the DMatrix: churn_dmatrix\n",
    "churn_dmatrix = xgb.DMatrix(data=X, label=y)\n",
    "\n",
    "# Create the parameter dictionary: params\n",
    "params = {\"objective\":\"reg:logistic\", \"max_depth\":3}\n",
    "\n",
    "# Perform cross-validation: cv_results\n",
    "cv_results = xgb.cv(dtrain=churn_dmatrix, params=params, nfold=3, num_boost_round=5, metrics=\"error\", as_pandas=True, seed=123)\n",
    "\n",
    "# Print cv_results\n",
    "print(cv_results)\n",
    "\n",
    "# Print the accuracy\n",
    "print(((1-cv_results[\"test-error-mean\"]).iloc[-1]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<code>cv_results</code> stores the training and test mean and standard deviation of the error per boosting round (tree built) as a DataFrame. From <code>cv_results</code>, the final round '<code>test-error-mean</code>' is extracted and converted into an accuracy, where accuracy is <code>1-error</code>."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Q4.\n",
    "## Measuring AUC\n",
    "Now that you've used cross-validation to compute average out-of-sample accuracy (after converting from an error), it's very easy to compute any other metric you might be interested in. All you have to do is pass it (or a list of metrics) in as an argument to the <code>metrics</code> parameter of <code>xgb.cv()</code>.\n",
    "<br><br>\n",
    "Your job in this exercise is to compute another common metric used in binary classification - the area under the curve (\"<code>auc</code>\"). As before, <code>churn_data</code> is available in your workspace, along with the DMatrix <code>churn_dmatrix</code> and parameter dictionary <code>params</code>.\n",
    "<br><br><br>\n",
    "- Perform 3-fold cross-validation with <code>5</code> boosting rounds and \"<code>auc</code>\" as your metric.\n",
    "- Print the \"<code>test-auc-mean</code>\" column of <code>cv_results</code>."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Perform cross_validation: cv_results\n",
    "cv_results = ____(dtrain=____, params=____, nfold=____, num_boost_round=____, metrics=\"____\", as_pandas=True, seed=123)\n",
    "\n",
    "# Print cv_results\n",
    "print(cv_results)\n",
    "\n",
    "# Print the AUC\n",
    "print((cv_results[\"____\"]).iloc[-1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# A4."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Perform cross_validation: cv_results\n",
    "cv_results = xgb.cv(dtrain=churn_dmatrix, params=params, nfold=3, num_boost_round=5, metrics=\"auc\", as_pandas=True, seed=123)\n",
    "\n",
    "# Print cv_results\n",
    "print(cv_results)\n",
    "\n",
    "# Print the AUC\n",
    "print((cv_results[\"test-auc-mean\"]).iloc[-1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- XGBoost는 모든 supervised learning에서 강력하다.\n",
    "- feature 개수가 sample 개수보다 적기만 하다면 사용하기 적합할 것이다.\n",
    "  - 그러나 sample set 자체가 적을 경우에는 적합하지 않다.\n",
    "- XGBoost는 image recognition, computer vision, natural language processing등에는 적합하지 않다. 해당 도메인들은 deep learning approach가 적합하다. (왜? 논문에 이걸 언급하면 좋을 듯.)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Q5.\n",
    "## Using XGBoost\n",
    "XGBoost is a powerful library that scales very well to many samples and works for a variety of supervised learning problems. That said, as Sergey described in the video, you shouldn't always pick it as your default machine learning library when starting a new project, since there are some situations in which it is not the best option. In this exercise, your job is to consider the below examples and select the one which would be the best use of XGBoost."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Possible Answers<br>\n",
    "<u><b>[1]</b></u> Visualizing the similarity between stocks by comparing the time series of their historical prices relative to each other.<br>\n",
    "<u><b>[2]</b></u> Predicting whether a person will develop cancer using genetic data with millions of genes, 23 examples of genomes of people that didn't develop cancer, 3 genomes of people who wound up getting cancer.<br>\n",
    "<u><b>[3]</b></u> Clustering documents into topics based on the terms used in them.<br>\n",
    "<u><b>[4]</b></u> Predicting the likelihood that a given user will click an ad from a very large clickstream log with millions of users and their web interactions."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# A5.\n",
    "[4]번이 정답"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Q6.\n",
    "## When is tuning your model a bad idea?\n",
    "Now that you've seen the effect that tuning has on the overall performance of your XGBoost model, let's turn the question on its head and see if you can figure out when tuning your model might not be the best idea. <b>Given that model tuning can be time-intensive and complicated, which of the following scenarios would NOT call for careful tuning of your model</b>?\n",
    "<br><br>\n",
    "### Possible Answers\n",
    "<b><u>[1]</u></b> You have lots of examples from some dataset and very many features at your disposal.<br>\n",
    "<b><u>[2]</u></b> You are very short on time before you must push an initial model to production and have little data to train your model on.<br>\n",
    "<b><u>[3]</u></b> You have access to a multi-core (64 cores) server with lots of memory (200GB RAM) and no time constraints.<br>\n",
    "<b><u>[4]</u></b> You must squeeze out every last bit of performance out of your xgboost model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# A6.\n",
    "[2]번이 정답<br>\n",
    "You cannot tune if you do not have time!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Q7.\n",
    "## Tuning the number of boosting rounds\n",
    "Let's start with parameter tuning by seeing how the number of boosting rounds (number of trees you build) impacts the out-of-sample performance of your XGBoost model. You'll use <code>xgb.cv()</code> inside a <code>for</code> loop and build one model per <code>num_boost_round</code> parameter.\n",
    "<br><br>\n",
    "Here, you'll continue working with the Ames housing dataset. The features are available in the array <code>X</code>, and the target vector is contained in <code>y</code>.\n",
    "<br><br><br>\n",
    "- Create a <code>DMatrix</code> called <code>housing_dmatrix</code> from <code>X</code> and <code>y</code>.\n",
    "- Create a parameter dictionary called <code>params</code>, passing in the appropriate \"<code>objective</code>\" (\"<code>reg:linear</code>\") and \"<code>max_depth</code>\" (set it to <code>3</code>).\n",
    "- Iterate over <code>num_rounds</code> inside a <code>for</code> loop and perform 3-fold cross-validation. In each iteration of the loop, pass in the current number of boosting rounds (<code>curr_num_rounds</code>) to <code>xgb.cv()</code> as the argument to <code>num_boost_round</code>.\n",
    "- Append the final boosting round RMSE for each cross-validated XGBoost model to the <code>final_rmse_per_round</code> list.\n",
    "- <code>num_rounds</code> and <code>final_rmse_per_round</code> have been zipped and converted into a DataFrame so you can easily see how the model performs with each boosting round. Hit 'Submit Answer' to see the results!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the DMatrix: housing_dmatrix\n",
    "housing_dmatrix = ____\n",
    "\n",
    "# Create the parameter dictionary for each tree: params \n",
    "params = {\"____\":\"____\", \"____\":____}\n",
    "\n",
    "# Create list of number of boosting rounds\n",
    "num_rounds = [5, 10, 15]\n",
    "\n",
    "# Empty list to store final round rmse per XGBoost model\n",
    "final_rmse_per_round = []\n",
    "\n",
    "# Iterate over num_rounds and build one model per num_boost_round parameter\n",
    "for curr_num_rounds in num_rounds:\n",
    "\n",
    "    # Perform cross-validation: cv_results\n",
    "    cv_results = ____(dtrain=____, params=____, nfold=3, num_boost_round=____, metrics=\"rmse\", as_pandas=True, seed=123)\n",
    "    \n",
    "    # Append final round RMSE\n",
    "    ____.____(cv_results[\"test-rmse-mean\"].tail().values[-1])\n",
    "\n",
    "# Print the resultant DataFrame\n",
    "num_rounds_rmses = list(zip(num_rounds, final_rmse_per_round))\n",
    "print(pd.DataFrame(num_rounds_rmses,columns=[\"num_boosting_rounds\",\"rmse\"]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# A7."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the DMatrix: housing_dmatrix\n",
    "housing_dmatrix = xgb.DMatrix(data=X, label=y)\n",
    "\n",
    "# Create the parameter dictionary for each tree: params \n",
    "params = {\"objective\":\"reg:linear\", \"max_depth\":3}\n",
    "\n",
    "# Create list of number of boosting rounds\n",
    "num_rounds = [5, 10, 15]\n",
    "\n",
    "# Empty list to store final round rmse per XGBoost model\n",
    "final_rmse_per_round = []\n",
    "\n",
    "# Iterate over num_rounds and build one model per num_boost_round parameter\n",
    "for curr_num_rounds in num_rounds:\n",
    "\n",
    "    # Perform cross-validation: cv_results\n",
    "    cv_results = xgb.cv(dtrain=housing_dmatrix, params=params, nfold=3, num_boost_round=curr_num_rounds, metrics=\"rmse\", as_pandas=True, seed=123)\n",
    "    \n",
    "    # Append final round RMSE\n",
    "    final_rmse_per_round.append(cv_results[\"test-rmse-mean\"].tail().values[-1])\n",
    "\n",
    "# Print the resultant DataFrame\n",
    "num_rounds_rmses = list(zip(num_rounds, final_rmse_per_round))\n",
    "print(pd.DataFrame(num_rounds_rmses,columns=[\"num_boosting_rounds\",\"rmse\"]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As you can see, increasing the number of boosting rounds decreases the RMSE."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Q8.\n",
    "## Automated boosting round selection using early_stopping\n",
    "Now, instead of attempting to cherry pick the best possible number of boosting rounds, you can very easily have XGBoost automatically select the number of boosting rounds for you within <code>xgb.cv()</code>. This is done using a technique called <b>early stopping</b>.\n",
    "<br><br>\n",
    "<b>Early stopping</b> works by testing the XGBoost model after every boosting round against a hold-out dataset and stopping the creation of additional boosting rounds (thereby finishing training of the model early) if the hold-out metric (\"<code>rmse</code>\" in our case) does not improve for a given number of rounds. Here you will use the <code>early_stopping_rounds</code> parameter in <code>xgb.cv()</code> with a large possible number of boosting rounds (50). Bear in mind that if the holdout metric continuously improves up through when <code>num_boosting_rounds</code> is reached, then early stopping does not occur.\n",
    "<br><br>\n",
    "Here, the <code>DMatrix</code> and parameter dictionary have been created for you. Your task is to use cross-validation with early stopping. Go for it!\n",
    "<br><br><br>\n",
    "- Perform 3-fold cross-validation with early stopping and \"<code>rmse</code>\" as your metric. Use <code>10</code> early stopping rounds and <code>50</code> boosting rounds. Specify a <code>seed</code> of <code>123</code> and make sure the output is a <code>pandas</code> DataFrame. Remember to specify the other parameters such as <code>dtrain</code>, <code>params</code>, and <code>metrics</code>.\n",
    "- Print <code>cv_results</code>."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create your housing DMatrix: housing_dmatrix\n",
    "housing_dmatrix = xgb.DMatrix(data=X, label=y)\n",
    "\n",
    "# Create the parameter dictionary for each tree: params\n",
    "params = {\"objective\":\"reg:linear\", \"max_depth\":4}\n",
    "\n",
    "# Perform cross-validation with early stopping: cv_results\n",
    "cv_results = ____\n",
    "\n",
    "# Print cv_results\n",
    "print(____)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# A8."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create your housing DMatrix: housing_dmatrix\n",
    "housing_dmatrix = xgb.DMatrix(data=X, label=y)\n",
    "\n",
    "# Create the parameter dictionary for each tree: params\n",
    "params = {\"objective\":\"reg:linear\", \"max_depth\":4}\n",
    "\n",
    "# Perform cross-validation with early stopping: cv_results\n",
    "cv_results = xgb.cv(metrics=[\"rmse\"],\n",
    "                    params=params,\n",
    "                    dtrain=housing_dmatrix,\n",
    "                    nfold=3,\n",
    "                    early_stopping_rounds=50,\n",
    "                    seed=123,\n",
    "                    as_pandas=True)\n",
    "\n",
    "# Print cv_results\n",
    "print(cv_results)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Q9.\n",
    "## Tuning eta\n",
    "It's time to practice tuning other XGBoost hyperparameters in earnest and observing their effect on model performance! You'll begin by tuning the \"<code>eta</code>\", also known as the learning rate.\n",
    "<br><br>\n",
    "The learning rate in XGBoost is a parameter that can range between <code>0</code> and <code>1</code>, with higher values of \"<code>eta</code>\" penalizing feature weights more strongly, causing much stronger regularization.\n",
    "<br><br>\n",
    "\n",
    "- Create a list called <code>eta_vals</code> to store the following \"<code>eta</code>\" values: <code>0.001</code>, <code>0.01</code>, and <code>0.1</code>.\n",
    "- Iterate over your <code>eta_vals</code> list using a <code>for</code> loop.\n",
    "- In each iteration of the <code>for</code> loop, set the \"<code>eta</code>\" key of <code>params</code> to be equal to <code>curr_val</code>. Then, perform 3-fold cross-validation with early stopping (<code>5</code> rounds), <code>10</code> boosting rounds, a metric of \"<code>rmse</code>\", and a <code>seed</code> of <code>123</code>. Ensure the output is a DataFrame.\n",
    "- Append the final round RMSE to the <code>best_rmse</code> list."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create your housing DMatrix: housing_dmatrix\n",
    "housing_dmatrix = xgb.DMatrix(data=X, label=y)\n",
    "\n",
    "# Create the parameter dictionary for each tree (boosting round)\n",
    "params = {\"objective\":\"reg:linear\", \"max_depth\":3}\n",
    "\n",
    "# Create list of eta values and empty list to store final round rmse per xgboost model\n",
    "____ = [____, ____, ____]\n",
    "best_rmse = []\n",
    "\n",
    "# Systematically vary the eta \n",
    "for curr_val in ____:\n",
    "\n",
    "    params[\"___\"] = curr_val\n",
    "    \n",
    "    # Perform cross-validation: cv_results\n",
    "    cv_results = ____\n",
    "    \n",
    "    \n",
    "    \n",
    "    # Append the final round rmse to best_rmse\n",
    "    ____.____(____[\"____\"].tail().values[-1])\n",
    "\n",
    "# Print the resultant DataFrame\n",
    "print(pd.DataFrame(list(zip(eta_vals, best_rmse)), columns=[\"eta\",\"best_rmse\"]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# A9."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
