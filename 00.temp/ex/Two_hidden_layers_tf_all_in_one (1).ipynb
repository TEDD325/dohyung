{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# MNIST-Neural Network-Two Hidden Layers with Tensorflow - All-in-One"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.image as mpimg\n",
    "from scipy.misc import imread\n",
    "import numpy as np\n",
    "import os\n",
    "import tensorflow as tf\n",
    "import glob\n",
    "import random\n",
    "import shutil\n",
    "\n",
    "IMG_width = 320\n",
    "IMG_height = 240\n",
    "IMG_channel = 1\n",
    "DATASET_PATH = \"./dataset_jpg_gray\"\n",
    "kinds = [\"train\", \"validation\", \"test\"]\n",
    "train_img = np.array([], dtype=\"float32\")\n",
    "train_label = np.array([], dtype=\"int8\")\n",
    "validation_img = np.array([], dtype=\"float32\")\n",
    "validation_label = np.array([], dtype=\"int8\")\n",
    "test_img = np.array([], dtype=\"float32\")\n",
    "test_label = np.array([], dtype=\"int8\")\n",
    "(label_train, filename_train) = (0, 0)\n",
    "(label_validation, filename_validation) = (0, 0)\n",
    "(label_test, filename_test) = (0, 0)\n",
    "\n",
    "\n",
    "def file_info(category_name, dataset_path=DATASET_PATH):\n",
    "    # 디렉토리 상의 파일경로와 파일의 제일 앞에 매겨진 숫자정보(라벨 정보)를 긁어서 반환\n",
    "    full_path =  dataset_path + '/' + category_name + '/' + '*.jpg'\n",
    "    image_filenames = glob.glob(full_path)\n",
    "    filename = []\n",
    "    label = []\n",
    "    for image_filename in image_filenames:\n",
    "        filename.append(image_filename.split(\"/\")[3])\n",
    "        label.append(image_filename.split(\"/\")[3].split(\"-\")[0])\n",
    "    return (label, filename)\n",
    "\n",
    "\n",
    "def display_image(image, label):\n",
    "    %matplotlib\n",
    "    inline\n",
    "\n",
    "    fig = plt.figure(figsize=(20, 5))\n",
    "    for i in range(5):\n",
    "        print(label[i])\n",
    "        img = image[i]\n",
    "        img = img.reshape(240, 320, 1)\n",
    "        img.shape = (240, 320, 1)\n",
    "        plt.subplot(150 + (i + 1))\n",
    "        plt.imshow(img)\n",
    "\n",
    "\n",
    "def data_processing_about_train(idx, kind):\n",
    "    for i in range(idx):\n",
    "        global train_img\n",
    "        global train_label\n",
    "\n",
    "        (labels, filenames) = file_info(kind)\n",
    "        random_value = random.randrange(0, len(filenames))\n",
    "        filename = filenames[random_value]\n",
    "        label = labels[random_value]\n",
    "        #         label = _change_one_hot_label(train_label, label)\n",
    "        #         label = labels[random_value]\n",
    "        #         image_display(kinds, filename)\n",
    "        full_path = ( DATASET_PATH + '/' + kind + '/' + filename)\n",
    "        image = imread(full_path)\n",
    "        image = image.reshape(IMG_width * IMG_height * IMG_channel) / 255.0\n",
    "        train_img = np.append(train_img, image)\n",
    "        label = _change_one_hot_label(label)\n",
    "        train_label = np.append(train_label, label)\n",
    "        # file move\n",
    "        # 학습한 데이터는 이동시킴\n",
    "        print(\"---------\")\n",
    "        print(len(filenames))\n",
    "        print(filename)\n",
    "        src =  DATASET_PATH + '/' + kind + '/'\n",
    "        dir =  DATASET_PATH + '/' + 'tmp' + '/' + kind + '/'\n",
    "        shutil.move(src + filename, dir + filename)\n",
    "\n",
    "\n",
    "def data_processing_about_validation(idx, kind):\n",
    "    for i in range(idx):\n",
    "        global validation_img\n",
    "        global validation_label\n",
    "\n",
    "        (labels, filenames) = file_info(kind)\n",
    "        random_value = random.randrange(0, len(filenames))\n",
    "        filename = filenames[random_value]\n",
    "        label = labels[random_value]\n",
    "        #         label = _change_one_hot_label(train_label, label)\n",
    "        #         label = labels[random_value]\n",
    "        #         image_display(kinds, filename)\n",
    "        full_path = ( DATASET_PATH + '/' + kind + '/' + filename)\n",
    "        image = imread(full_path)\n",
    "        image = image.reshape(IMG_width * IMG_height * IMG_channel) / 255.0\n",
    "        validation_img = np.append(validation_img, image)\n",
    "        label = _change_one_hot_label(label)\n",
    "        validation_label = np.append(validation_label, label)\n",
    "        # file move\n",
    "        print(\"---------\")\n",
    "        print(len(filenames))\n",
    "        print(filename)\n",
    "        src =  DATASET_PATH + '/' + kind + '/'\n",
    "        dir =  DATASET_PATH + '/' + 'tmp' + '/' + kind + '/'\n",
    "        shutil.move(src + filename, dir + filename)\n",
    "\n",
    "\n",
    "def data_processing_about_test(idx, kind):\n",
    "    for i in range(idx):\n",
    "        global test_img\n",
    "        global test_label\n",
    "\n",
    "        (labels, filenames) = file_info(kind)\n",
    "        random_value = random.randrange(0, len(filenames))\n",
    "        filename = filenames[random_value]\n",
    "        label = labels[random_value]\n",
    "        #         label = _change_one_hot_label(train_label, label)\n",
    "        #         label = labels[random_value]\n",
    "        #         image_display(kinds, filename)\n",
    "        full_path = ( DATASET_PATH + '/' + kind + '/' + filename)\n",
    "        image = imread(full_path)\n",
    "        image = image.reshape(IMG_width * IMG_height * IMG_channel) / 255.0\n",
    "        test_img = np.append(test_img, image)\n",
    "        label = _change_one_hot_label(label)\n",
    "        test_label = np.append(test_label, label)\n",
    "        # file move\n",
    "        print(\"---------\")\n",
    "        print(len(filenames))\n",
    "        print(filename)\n",
    "        src =  DATASET_PATH + '/' + kind + '/'\n",
    "        dir =  DATASET_PATH + '/' + 'tmp' + '/' + kind + '/'\n",
    "        shutil.move(src + filename, dir + filename)\n",
    "\n",
    "\n",
    "def _change_one_hot_label(target_label):\n",
    "    target_label = int(target_label)\n",
    "    T = np.zeros((1, 3))\n",
    "    T[0][target_label] = 1\n",
    "\n",
    "    return T\n",
    "\n",
    "\n",
    "def store_dataset(dataset_path=DATASET_PATH):\n",
    "    global train_img\n",
    "    global train_label\n",
    "    global validation_img\n",
    "    global validation_label\n",
    "    global test_img\n",
    "    global test_label\n",
    "\n",
    "    (_, idx_train) = file_info(\"train\")\n",
    "    (_, idx_validation) = file_info(\"validation\")\n",
    "    (_, idx_test) = file_info(\"test\")\n",
    "    if len(idx_train) != 0 or len(idx_validation) != 0 or len(idx_test) != 0:\n",
    "        data_processing_about_train(len(idx_train), \"train\")\n",
    "        data_processing_about_validation(len(idx_validation), \"validation\")\n",
    "        data_processing_about_test(len(idx_test), \"test\")\n",
    "\n",
    "        # file로 쓰기\n",
    "        train_img.tofile(dataset_path + '/' + 'train_img_dataset.txt')\n",
    "        train_label.tofile(dataset_path + '/' + 'train_label.txt')\n",
    "        validation_img.tofile(dataset_path + '/' + 'validation_img_dataset.txt')\n",
    "        validation_label.tofile(dataset_path + '/' + 'validation_label.txt')\n",
    "        test_img.tofile(dataset_path + '/' + 'test_img_dataset.txt')\n",
    "        test_label.tofile(dataset_path + '/' + 'test_label.txt')\n",
    "\n",
    "    else:\n",
    "        print(\"[!] Already Finished Generateing Dataset. Please check directory.\")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def load_dataset(dataset_path):\n",
    "    (img_train, label_train) = (0,0)\n",
    "    (img_validation, label_validation) = (0,0)\n",
    "    (img_test, label_test) = (0,0)\n",
    "    filenames = glob.glob(dataset_path + \"/\" + \"*.txt\")\n",
    "    filename = []\n",
    "    for filename in filenames:\n",
    "        print(filename, )\n",
    "        if filename.split(\"/\")[6][-4:] == \".txt\": # CAUTION : filename.split(\"/\")[directory level][-4:]\n",
    "            img_train = np.fromfile(open(dataset_path + \"/\" + \"train_img_dataset.txt\", 'rb')).reshape(451,IMG_width * IMG_height * IMG_channel)\n",
    "            label_train = np.fromfile(open(dataset_path + \"/\" + \"train_label.txt\", 'rb')).reshape(451, 3)\n",
    "            img_validation = np.fromfile(open(dataset_path + \"/\" + \"validation_img_dataset.txt\", 'rb')).reshape(65, IMG_width * IMG_height * IMG_channel)\n",
    "            label_validation = np.fromfile(open(dataset_path + \"/\" + \"validation_label.txt\", 'rb')).reshape(65,3)\n",
    "            img_test = np.fromfile(open(dataset_path + \"/\" + \"test_img_dataset.txt\", 'rb')).reshape(130,IMG_width * IMG_height * IMG_channel)\n",
    "            label_test = np.fromfile(open(dataset_path + \"/\" + \"test_label.txt\", 'rb')).reshape(130, 3)\n",
    "    return ((img_train, label_train), (img_validation, label_validation), (img_test, label_test))\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/Users/do-hyungkwon/GoogleDrive/jupyter_notebook/imdata/test_img_dataset.txt\n",
      "/Users/do-hyungkwon/GoogleDrive/jupyter_notebook/imdata/test_label.txt\n",
      "/Users/do-hyungkwon/GoogleDrive/jupyter_notebook/imdata/train_img_dataset.txt\n",
      "/Users/do-hyungkwon/GoogleDrive/jupyter_notebook/imdata/train_label.txt\n",
      "/Users/do-hyungkwon/GoogleDrive/jupyter_notebook/imdata/validation_img_dataset.txt\n",
      "/Users/do-hyungkwon/GoogleDrive/jupyter_notebook/imdata/validation_label.txt\n",
      "Total batch: 46\n",
      "Epoch:  0, Train Error: 18910.04492, Validation Error: 19500.00195, Test Accuracy: 0.27692\n",
      "Epoch:  1, Train Error: 4867.32910, Validation Error: 4431.22021, Test Accuracy: 0.46154\n",
      "Epoch:  2, Train Error: 3452.83594, Validation Error: 3516.75000, Test Accuracy: 0.51538\n",
      "Epoch:  3, Train Error: 3570.69409, Validation Error: 4010.55273, Test Accuracy: 0.46923\n",
      "Epoch:  4, Train Error: 2113.99805, Validation Error: 2034.10620, Test Accuracy: 0.49231\n",
      "Epoch:  5, Train Error: 1327.21509, Validation Error: 1549.68762, Test Accuracy: 0.47692\n",
      "Epoch:  6, Train Error: 2123.46143, Validation Error: 2505.52661, Test Accuracy: 0.51538\n",
      "Epoch:  7, Train Error: 1362.50623, Validation Error: 1952.72888, Test Accuracy: 0.60769\n",
      "Epoch:  8, Train Error: 1871.86865, Validation Error: 2184.51489, Test Accuracy: 0.54615\n",
      "Epoch:  9, Train Error: 1381.97205, Validation Error: 1697.32422, Test Accuracy: 0.57692\n",
      "Epoch: 10, Train Error: 992.33185, Validation Error: 1404.59741, Test Accuracy: 0.63846\n",
      "Epoch: 11, Train Error: 982.48639, Validation Error: 1240.99500, Test Accuracy: 0.46923\n",
      "Epoch: 12, Train Error: 1078.87402, Validation Error: 1305.91248, Test Accuracy: 0.49231\n",
      "Epoch: 13, Train Error: 2298.13354, Validation Error: 2430.63257, Test Accuracy: 0.42308\n",
      "Epoch: 14, Train Error: 2343.11304, Validation Error: 2614.33203, Test Accuracy: 0.39231\n",
      "Epoch: 15, Train Error: 1547.14783, Validation Error: 2167.27368, Test Accuracy: 0.40769\n",
      "Epoch: 16, Train Error: 641.93665, Validation Error: 1043.94214, Test Accuracy: 0.50000\n",
      "Epoch: 17, Train Error: 568.32031, Validation Error: 828.32092, Test Accuracy: 0.50000\n",
      "Epoch: 18, Train Error: 766.20380, Validation Error: 1030.10645, Test Accuracy: 0.48462\n",
      "Epoch: 19, Train Error: 1607.99976, Validation Error: 2119.97803, Test Accuracy: 0.39231\n",
      "Epoch: 20, Train Error: 1108.58398, Validation Error: 1456.79602, Test Accuracy: 0.38462\n",
      "Epoch: 21, Train Error: 764.97644, Validation Error: 1064.64233, Test Accuracy: 0.57692\n",
      "Epoch: 22, Train Error: 1064.78113, Validation Error: 1559.56311, Test Accuracy: 0.42308\n",
      "Epoch: 23, Train Error: 501.25818, Validation Error: 1124.42163, Test Accuracy: 0.46923\n",
      "Epoch: 24, Train Error: 937.34814, Validation Error: 1400.27051, Test Accuracy: 0.42308\n",
      "Epoch: 25, Train Error: 328.29416, Validation Error: 619.39014, Test Accuracy: 0.56154\n",
      "Epoch: 26, Train Error: 264.01361, Validation Error: 623.97778, Test Accuracy: 0.47692\n",
      "Epoch: 27, Train Error: 663.90063, Validation Error: 1085.18286, Test Accuracy: 0.41538\n",
      "Epoch: 28, Train Error: 766.62457, Validation Error: 990.93445, Test Accuracy: 0.42308\n",
      "Epoch: 29, Train Error: 443.38770, Validation Error: 875.70813, Test Accuracy: 0.42308\n",
      "Epoch: 30, Train Error: 1398.15051, Validation Error: 1789.96240, Test Accuracy: 0.36154\n",
      "Epoch: 31, Train Error: 506.82965, Validation Error: 994.12738, Test Accuracy: 0.48462\n",
      "Epoch: 32, Train Error: 485.41989, Validation Error: 928.67413, Test Accuracy: 0.42308\n",
      "Epoch: 33, Train Error: 416.87149, Validation Error: 861.99243, Test Accuracy: 0.43846\n",
      "Epoch: 34, Train Error: 339.41501, Validation Error: 806.35529, Test Accuracy: 0.49231\n",
      "Epoch: 35, Train Error: 1003.77057, Validation Error: 1524.86987, Test Accuracy: 0.36154\n",
      "Epoch: 36, Train Error: 533.93115, Validation Error: 972.61523, Test Accuracy: 0.46154\n",
      "Epoch: 37, Train Error: 421.39517, Validation Error: 689.61224, Test Accuracy: 0.47692\n",
      "Epoch: 38, Train Error: 309.05478, Validation Error: 623.86957, Test Accuracy: 0.51538\n",
      "Epoch: 39, Train Error: 1042.50146, Validation Error: 1437.82935, Test Accuracy: 0.45385\n",
      "Epoch: 40, Train Error: 975.72144, Validation Error: 1280.18359, Test Accuracy: 0.47692\n",
      "Epoch: 41, Train Error: 693.79504, Validation Error: 1125.80957, Test Accuracy: 0.53846\n",
      "Epoch: 42, Train Error: 844.48712, Validation Error: 1449.54309, Test Accuracy: 0.49231\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import math\n",
    "import tensorflow as tf\n",
    "\n",
    "(img_train, label_train), (img_validation, label_validation), (img_test, label_test) = load_dataset(\"/Users/do-hyungkwon/GoogleDrive/jupyter_notebook/imdata\")\n",
    "\n",
    "batch_size = 10\n",
    "training_epochs = 50\n",
    "learning_rate = 0.001\n",
    "\n",
    "epoch_list = []\n",
    "train_error_list = []\n",
    "validation_error_list = []\n",
    "test_accuracy_list = []\n",
    "diff_index_list = []\n",
    "\n",
    "# Network Parameters\n",
    "n_input = 76800 # MNIST data input (img shape: 28*28)\n",
    "n_hidden_1 = 256 # 1st layer number of features\n",
    "n_hidden_2 = 256 # 2nd layer number of features\n",
    "n_classes = 3 # MNIST total classes (0-9 digits)\n",
    "    \n",
    "# Data Preparation\n",
    "x = tf.placeholder(tf.float32, [None, n_input])\n",
    "y_target = tf.placeholder(tf.float32, [None, n_classes])\n",
    "\n",
    "# Model Construction\n",
    "# Store layers weight & bias\n",
    "weights = {\n",
    "    'W1': tf.Variable(tf.random_normal([n_input, n_hidden_1])),\n",
    "    'W2': tf.Variable(tf.random_normal([n_hidden_1, n_hidden_2])),\n",
    "    'out': tf.Variable(tf.random_normal([n_hidden_2, n_classes]))\n",
    "}\n",
    "\n",
    "biases = {\n",
    "    'b1': tf.Variable(tf.random_normal([n_hidden_1])),\n",
    "    'b2': tf.Variable(tf.random_normal([n_hidden_2])),\n",
    "    'out': tf.Variable(tf.random_normal([n_classes]))\n",
    "}\n",
    "\n",
    "# 1st Hidden layer with RELU activation\n",
    "u2 = tf.matmul(x, weights['W1']) + biases['b1']\n",
    "z2 = tf.nn.relu(u2)\n",
    "\n",
    "# 2ndHidden layer with RELU activation\n",
    "u3 = tf.matmul(z2, weights['W2']) + biases['b2']\n",
    "z3 = tf.nn.relu(u3)\n",
    "\n",
    "# Output layer with linear activation\n",
    "u_out = tf.matmul(z3, weights['out']) + biases['out']\n",
    "\n",
    "# Target Setup\n",
    "error = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(logits=u_out, labels=y_target))\n",
    "optimizer = tf.train.AdamOptimizer(learning_rate).minimize(error)\n",
    "\n",
    "# Accuracy   \n",
    "prediction_and_ground_truth = tf.equal(tf.argmax(u_out, 1), tf.argmax(y_target, 1))\n",
    "accuracy = tf.reduce_mean(tf.cast(prediction_and_ground_truth, tf.float32))\n",
    "\n",
    "def draw_error_values_and_accuracy():\n",
    "    # Draw Error Values and Accuracy\n",
    "    fig = plt.figure(figsize=(20, 5))\n",
    "    plt.subplot(121)\n",
    "    plt.plot(epoch_list[1:], train_error_list[1:], 'r', label='Train')\n",
    "    plt.plot(epoch_list[1:], validation_error_list[1:], 'g', label='Validation')\n",
    "    plt.ylabel('Total Error')\n",
    "    plt.xlabel('Epochs')\n",
    "    plt.grid(True)\n",
    "    plt.legend(loc='upper right')\n",
    "\n",
    "    plt.subplot(122)\n",
    "    plt.plot(epoch_list[1:], test_accuracy_list[1:], 'b', label='Test')\n",
    "    plt.ylabel('Accuracy')\n",
    "    plt.xlabel('Epochs')\n",
    "    plt.yticks(np.arange(0.0, 1.0, 0.05))\n",
    "    plt.grid(True)\n",
    "    plt.legend(loc='lower right')\n",
    "    plt.show()\n",
    "    \n",
    "def draw_false_prediction():\n",
    "    fig = plt.figure(figsize=(20, 5))\n",
    "    for i in range(5):\n",
    "        j = diff_index_list[i]\n",
    "        print(\"False Prediction Index: %s, Prediction: %s, Ground Truth: %s\" % (j, prediction[j], ground_truth[j]))\n",
    "        img = np.array(img_test[j])\n",
    "        img.shape = (240, 320)\n",
    "        plt.subplot(150 + (i+1))\n",
    "        plt.imshow(img, cmap='gray')\n",
    "    \n",
    "with tf.Session() as sess:\n",
    "    init = tf.global_variables_initializer()\n",
    "    sess.run(init)\n",
    "    \n",
    "    total_batch = int(math.ceil(len(img_train)/float(batch_size)))\n",
    "    print(\"Total batch: %d\" % total_batch)    \n",
    "\n",
    "    for epoch in range(training_epochs):\n",
    "        epoch_list.append(epoch)\n",
    "        # Train Error Value\n",
    "        train_error_value = sess.run(error, feed_dict={x: img_train, y_target: label_train})\n",
    "        train_error_list.append(train_error_value)\n",
    "        \n",
    "        validation_error_value = sess.run(error, feed_dict={x: img_validation, y_target: label_validation})\n",
    "        validation_error_list.append(validation_error_value)\n",
    "        \n",
    "        test_accuracy_value = sess.run(accuracy, feed_dict={x: img_test, y_target: label_test})\n",
    "        test_accuracy_list.append(test_accuracy_value) \n",
    "        print(\"Epoch: {0:2d}, Train Error: {1:0.5f}, Validation Error: {2:0.5f}, Test Accuracy: {3:0.5f}\".format(epoch, train_error_value, validation_error_value, test_accuracy_value))\n",
    "        \n",
    "        \n",
    "        \n",
    "        for i in range(total_batch):\n",
    "            batch_images= img_train[i * batch_size : i * batch_size + batch_size]\n",
    "            batch_labels = label_train[i * batch_size : i * batch_size + batch_size]\n",
    "            sess.run(optimizer, feed_dict={x: batch_images, y_target: batch_labels})\n",
    "    \n",
    "\n",
    "    # Draw Graph about Error Values & Accuracy Values\n",
    "    draw_error_values_and_accuracy()\n",
    "    \n",
    "    # False Prediction Profile\n",
    "    prediction = sess.run(tf.argmax(u_out, 1), feed_dict={x:img_test})\n",
    "    ground_truth = sess.run(tf.argmax(y_target, 1), feed_dict={y_target:label_test})\n",
    "\n",
    "    print(prediction)\n",
    "    print(ground_truth)\n",
    "\n",
    "    for i in range(len(img_test)):\n",
    "        if (prediction[i] != ground_truth[i]):\n",
    "            diff_index_list.append(i)\n",
    "            \n",
    "    print(\"Number of False Prediction:\", len(diff_index_list))\n",
    "    draw_false_prediction()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
