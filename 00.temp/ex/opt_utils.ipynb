{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# %load opt_utils.py\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import h5py\n",
    "import scipy.io\n",
    "import sklearn\n",
    "import sklearn.datasets\n",
    "\n",
    "def sigmoid(x):\n",
    "    \"\"\"\n",
    "    Compute the sigmoid of x\n",
    "\n",
    "    Arguments:\n",
    "    x -- A scalar or numpy array of any size.\n",
    "\n",
    "    Return:\n",
    "    s -- sigmoid(x)\n",
    "    \"\"\"\n",
    "    s = 1/(1+np.exp(-x))\n",
    "    return s\n",
    "\n",
    "def relu(x):\n",
    "    \"\"\"\n",
    "    Compute the relu of x\n",
    "\n",
    "    Arguments:\n",
    "    x -- A scalar or numpy array of any size.\n",
    "\n",
    "    Return:\n",
    "    s -- relu(x)\n",
    "    \"\"\"\n",
    "    s = np.maximum(0,x)\n",
    "    \n",
    "    return s\n",
    "\n",
    "def load_params_and_grads(seed=1):\n",
    "    np.random.seed(seed)\n",
    "    W1 = np.random.randn(2,3)\n",
    "    b1 = np.random.randn(2,1)\n",
    "    W2 = np.random.randn(3,3)\n",
    "    b2 = np.random.randn(3,1)\n",
    "\n",
    "    dW1 = np.random.randn(2,3)\n",
    "    db1 = np.random.randn(2,1)\n",
    "    dW2 = np.random.randn(3,3)\n",
    "    db2 = np.random.randn(3,1)\n",
    "    \n",
    "    return W1, b1, W2, b2, dW1, db1, dW2, db2\n",
    "\n",
    "\n",
    "def initialize_parameters(layer_dims):\n",
    "    \"\"\"\n",
    "    Arguments:\n",
    "    layer_dims -- python array (list) containing the dimensions of each layer in our network\n",
    "    \n",
    "    Returns:\n",
    "    parameters -- python dictionary containing your parameters \"W1\", \"b1\", ..., \"WL\", \"bL\":\n",
    "                    W1 -- weight matrix of shape (layer_dims[l], layer_dims[l-1])\n",
    "                    b1 -- bias vector of shape (layer_dims[l], 1)\n",
    "                    Wl -- weight matrix of shape (layer_dims[l-1], layer_dims[l])\n",
    "                    bl -- bias vector of shape (1, layer_dims[l])\n",
    "                    \n",
    "    Tips:\n",
    "    - For example: the layer_dims for the \"Planar Data classification model\" would have been [2,2,1]. \n",
    "    This means W1's shape was (2,2), b1 was (1,2), W2 was (2,1) and b2 was (1,1). Now you have to generalize it!\n",
    "    - In the for loop, use parameters['W' + str(l)] to access Wl, where l is the iterative integer.\n",
    "    \"\"\"\n",
    "    \n",
    "    np.random.seed(3)\n",
    "    parameters = {}\n",
    "    L = len(layer_dims) # number of layers in the network\n",
    "\n",
    "    for l in range(1, L):\n",
    "        parameters['W' + str(l)] = np.random.randn(layer_dims[l], layer_dims[l-1])*  np.sqrt(2 / layer_dims[l-1])\n",
    "        parameters['b' + str(l)] = np.zeros((layer_dims[l], 1))\n",
    "        \n",
    "        assert(parameters['W' + str(l)].shape == layer_dims[l], layer_dims[l-1])\n",
    "        assert(parameters['W' + str(l)].shape == layer_dims[l], 1)\n",
    "        \n",
    "    return parameters\n",
    "\n",
    "\n",
    "def compute_cost(a3, Y):\n",
    "    \n",
    "    \"\"\"\n",
    "    Implement the cost function\n",
    "    \n",
    "    Arguments:\n",
    "    a3 -- post-activation, output of forward propagation\n",
    "    Y -- \"true\" labels vector, same shape as a3\n",
    "    \n",
    "    Returns:\n",
    "    cost - value of the cost function\n",
    "    \"\"\"\n",
    "    m = Y.shape[1]\n",
    "    \n",
    "    logprobs = np.multiply(-np.log(a3),Y) + np.multiply(-np.log(1 - a3), 1 - Y)\n",
    "    cost = 1./m * np.sum(logprobs)\n",
    "    \n",
    "    return cost\n",
    "\n",
    "def forward_propagation(X, parameters):\n",
    "    \"\"\"\n",
    "    Implements the forward propagation (and computes the loss) presented in Figure 2.\n",
    "    \n",
    "    Arguments:\n",
    "    X -- input dataset, of shape (input size, number of examples)\n",
    "    parameters -- python dictionary containing your parameters \"W1\", \"b1\", \"W2\", \"b2\", \"W3\", \"b3\":\n",
    "                    W1 -- weight matrix of shape ()\n",
    "                    b1 -- bias vector of shape ()\n",
    "                    W2 -- weight matrix of shape ()\n",
    "                    b2 -- bias vector of shape ()\n",
    "                    W3 -- weight matrix of shape ()\n",
    "                    b3 -- bias vector of shape ()\n",
    "    \n",
    "    Returns:\n",
    "    loss -- the loss function (vanilla logistic loss)\n",
    "    \"\"\"\n",
    "    \n",
    "    # retrieve parameters\n",
    "    W1 = parameters[\"W1\"]\n",
    "    b1 = parameters[\"b1\"]\n",
    "    W2 = parameters[\"W2\"]\n",
    "    b2 = parameters[\"b2\"]\n",
    "    W3 = parameters[\"W3\"]\n",
    "    b3 = parameters[\"b3\"]\n",
    "    \n",
    "    # LINEAR -> RELU -> LINEAR -> RELU -> LINEAR -> SIGMOID\n",
    "    z1 = np.dot(W1, X) + b1\n",
    "    a1 = relu(z1)\n",
    "    z2 = np.dot(W2, a1) + b2\n",
    "    a2 = relu(z2)\n",
    "    z3 = np.dot(W3, a2) + b3\n",
    "    a3 = sigmoid(z3)\n",
    "    \n",
    "    cache = (z1, a1, W1, b1, z2, a2, W2, b2, z3, a3, W3, b3)\n",
    "    \n",
    "    return a3, cache\n",
    "\n",
    "def backward_propagation(X, Y, cache):\n",
    "    \"\"\"\n",
    "    Implement the backward propagation presented in figure 2.\n",
    "    \n",
    "    Arguments:\n",
    "    X -- input dataset, of shape (input size, number of examples)\n",
    "    Y -- true \"label\" vector (containing 0 if cat, 1 if non-cat)\n",
    "    cache -- cache output from forward_propagation()\n",
    "    \n",
    "    Returns:\n",
    "    gradients -- A dictionary with the gradients with respect to each parameter, activation and pre-activation variables\n",
    "    \"\"\"\n",
    "    m = X.shape[1]\n",
    "    (z1, a1, W1, b1, z2, a2, W2, b2, z3, a3, W3, b3) = cache\n",
    "    \n",
    "    dz3 = 1./m * (a3 - Y)\n",
    "    dW3 = np.dot(dz3, a2.T)\n",
    "    db3 = np.sum(dz3, axis=1, keepdims = True)\n",
    "    \n",
    "    da2 = np.dot(W3.T, dz3)\n",
    "    dz2 = np.multiply(da2, np.int64(a2 > 0))\n",
    "    dW2 = np.dot(dz2, a1.T)\n",
    "    db2 = np.sum(dz2, axis=1, keepdims = True)\n",
    "    \n",
    "    da1 = np.dot(W2.T, dz2)\n",
    "    dz1 = np.multiply(da1, np.int64(a1 > 0))\n",
    "    dW1 = np.dot(dz1, X.T)\n",
    "    db1 = np.sum(dz1, axis=1, keepdims = True)\n",
    "    \n",
    "    gradients = {\"dz3\": dz3, \"dW3\": dW3, \"db3\": db3,\n",
    "                 \"da2\": da2, \"dz2\": dz2, \"dW2\": dW2, \"db2\": db2,\n",
    "                 \"da1\": da1, \"dz1\": dz1, \"dW1\": dW1, \"db1\": db1}\n",
    "    \n",
    "    return gradients\n",
    "\n",
    "def predict(X, y, parameters):\n",
    "    \"\"\"\n",
    "    This function is used to predict the results of a  n-layer neural network.\n",
    "    \n",
    "    Arguments:\n",
    "    X -- data set of examples you would like to label\n",
    "    parameters -- parameters of the trained model\n",
    "    \n",
    "    Returns:\n",
    "    p -- predictions for the given dataset X\n",
    "    \"\"\"\n",
    "    \n",
    "    m = X.shape[1]\n",
    "    p = np.zeros((1,m), dtype = np.int)\n",
    "    \n",
    "    # Forward propagation\n",
    "    a3, caches = forward_propagation(X, parameters)\n",
    "    \n",
    "    # convert probas to 0/1 predictions\n",
    "    for i in range(0, a3.shape[1]):\n",
    "        if a3[0,i] > 0.5:\n",
    "            p[0,i] = 1\n",
    "        else:\n",
    "            p[0,i] = 0\n",
    "\n",
    "    # print results\n",
    "\n",
    "    #print (\"predictions: \" + str(p[0,:]))\n",
    "    #print (\"true labels: \" + str(y[0,:]))\n",
    "    print(\"Accuracy: \"  + str(np.mean((p[0,:] == y[0,:]))))\n",
    "    \n",
    "    return p\n",
    "\n",
    "def load_2D_dataset():\n",
    "    data = scipy.io.loadmat('datasets/data.mat')\n",
    "    train_X = data['X'].T\n",
    "    train_Y = data['y'].T\n",
    "    test_X = data['Xval'].T\n",
    "    test_Y = data['yval'].T\n",
    "\n",
    "    plt.scatter(train_X[0, :], train_X[1, :], c=train_Y, s=40, cmap=plt.cm.Spectral);\n",
    "    \n",
    "    return train_X, train_Y, test_X, test_Y\n",
    "\n",
    "def plot_decision_boundary(model, X, y):\n",
    "    # Set min and max values and give it some padding\n",
    "    x_min, x_max = X[0, :].min() - 1, X[0, :].max() + 1\n",
    "    y_min, y_max = X[1, :].min() - 1, X[1, :].max() + 1\n",
    "    h = 0.01\n",
    "    # Generate a grid of points with distance h between them\n",
    "    xx, yy = np.meshgrid(np.arange(x_min, x_max, h), np.arange(y_min, y_max, h))\n",
    "    # Predict the function value for the whole grid\n",
    "    Z = model(np.c_[xx.ravel(), yy.ravel()])\n",
    "    Z = Z.reshape(xx.shape)\n",
    "    # Plot the contour and training examples\n",
    "    plt.contourf(xx, yy, Z, cmap=plt.cm.Spectral)\n",
    "    plt.ylabel('x2')\n",
    "    plt.xlabel('x1')\n",
    "    plt.scatter(X[0, :], X[1, :], c=y, cmap=plt.cm.Spectral)\n",
    "    plt.show()\n",
    "    \n",
    "def predict_dec(parameters, X):\n",
    "    \"\"\"\n",
    "    Used for plotting decision boundary.\n",
    "    \n",
    "    Arguments:\n",
    "    parameters -- python dictionary containing your parameters \n",
    "    X -- input data of size (m, K)\n",
    "    \n",
    "    Returns\n",
    "    predictions -- vector of predictions of our model (red: 0 / blue: 1)\n",
    "    \"\"\"\n",
    "    \n",
    "    # Predict using forward propagation and a classification threshold of 0.5\n",
    "    a3, cache = forward_propagation(X, parameters)\n",
    "    predictions = (a3 > 0.5)\n",
    "    return predictions\n",
    "\n",
    "def load_dataset():\n",
    "    np.random.seed(3)\n",
    "    train_X, train_Y = sklearn.datasets.make_moons(n_samples=300, noise=.2) #300 #0.2 \n",
    "    # Visualize the data\n",
    "    plt.scatter(train_X[:, 0], train_X[:, 1], c=train_Y, s=40, cmap=plt.cm.Spectral);\n",
    "    train_X = train_X.T\n",
    "    train_Y = train_Y.reshape((1, train_Y.shape[0]))\n",
    "    \n",
    "    return train_X, train_Y"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
