{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/link/anaconda3/lib/python3.6/site-packages/h5py/__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n",
      "Using TensorFlow backend.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using Keras version: 2.2.2 backend: tensorflow\n"
     ]
    }
   ],
   "source": [
    "# import library\n",
    "from tensorflow.python.ops import math_ops\n",
    "\n",
    "from __future__ import print_function\n",
    "import keras\n",
    "from keras.datasets import mnist\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Dropout, Flatten, Activation\n",
    "from keras.layers import SimpleRNN, LSTM, GRU\n",
    "from keras.wrappers.scikit_learn import KerasClassifier\n",
    "from keras import backend as K\n",
    "from sklearn.metrics import confusion_matrix, f1_score, precision_score, recall_score\n",
    "from sklearn.metrics import classification_report # https://stackoverflow.com/questions/50065484/getting-precision-recall-and-f1-score-per-class-in-keras\n",
    "from keras.utils import multi_gpu_model\n",
    "from keras import metrics\n",
    "from keras import losses\n",
    "from keras import __version__\n",
    "print('Using Keras version:', __version__, 'backend:', K.backend())\n",
    "# assert(LV(__version__) >= LV(\"2.0.0\"))\n",
    "\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.preprocessing import MinMaxScaler, StandardScaler, PolynomialFeatures\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "from sklearn.metrics import precision_recall_fscore_support\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.metrics import classification_report\n",
    "\n",
    "import numpy as np\n",
    "from distutils.version import LooseVersion as LV\n",
    "from datetime import datetime\n",
    "import csv\n",
    "import sys\n",
    "import os\n",
    "from os import listdir\n",
    "from os.path import isfile, join\n",
    "import pprint\n",
    "#import boto3\n",
    "import pickle\n",
    "import time\n",
    "import os.path\n",
    "import pickle\n",
    "sys.path.append(os.getcwd())\n",
    "from matplotlib import pyplot\n",
    "\n",
    "from link_aws_key import *\n",
    "pp = pprint.PrettyPrinter(indent=4)\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "coins = {\n",
    "    0: 'KRW',\n",
    "    1: 'BTC',\n",
    "    2: 'ETH',\n",
    "    3: 'XRP',\n",
    "    4: 'BCH',\n",
    "    5: 'LTC',\n",
    "    6: 'DASH',\n",
    "    7: 'ETC'\n",
    "}\n",
    "\n",
    "# aws_client = boto3.client(\n",
    "#     's3',\n",
    "#     aws_access_key_id=LINK_AWSAccessKeyId,\n",
    "#     aws_secret_access_key=LINK_AWSSecretKey\n",
    "# )\n",
    "\n",
    "bucket = \"bithumb10\"\n",
    "cleanup_file_name = \"coin_{0}_{1}_cleanup.csv\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "def get_all_raw_data_from_aws(coin_name_list, start_date, end_date):\n",
    "    start_ms_time = datetime.strptime(start_date + \" +0900\", \"%Y-%m-%d %H:%M:%S %z\").timestamp() * 1000\n",
    "    end_ms_time = datetime.strptime(end_date + \" +0900\", \"%Y-%m-%d %H:%M:%S %z\").timestamp() * 1000\n",
    "    \n",
    "    year_temp = start_date[:4]\n",
    "    years = [year_temp]\n",
    "    while year_temp < end_date[:4]:\n",
    "        year_temp = str(int(start_date[:4]) + 1)\n",
    "        years.append(year_temp)\n",
    "    raw_data = {}  # 전체 CSV Raw 데이터\n",
    "    for coin_name in coin_name_list:\n",
    "        raw_data[coin_name] = []\n",
    "\n",
    "    # KRW 제외한 나머지 CSV Raw 데이터 수집\n",
    "    for coin_name in coin_name_list:\n",
    "        if coin_name == 'KRW':\n",
    "            continue\n",
    "        lines = []\n",
    "        for year in years:\n",
    "            obj = aws_client.get_object(\n",
    "                Bucket=bucket,\n",
    "                Key='cleanup/' + year + '/' + cleanup_file_name.format(coin_name, year)\n",
    "            )\n",
    "            if lines != []:\n",
    "                lines += obj.get('Body')._raw_stream.readlines()\n",
    "            else:\n",
    "                lines = obj.get('Body')._raw_stream.readlines()\n",
    "\n",
    "        for line in lines:\n",
    "            line = str(line.strip())[2:-1]\n",
    "            line = line.split(',')\n",
    "            if start_ms_time <= int(line[0]) and int(line[0]) <= end_ms_time:\n",
    "                raw_data[coin_name].append(line)\n",
    "\n",
    "    raw_data['KRW'] = list()\n",
    "    for line in raw_data['BTC']:\n",
    "        raw_data['KRW'].append([line[0], line[1], 1, 1, 1, 1, 1.0, 'normal'])\n",
    "\n",
    "    return raw_data\n",
    "\n",
    "def get_all_raw_data(coin_name_list, start_date, end_date, data_files_dir):\n",
    "    trading_files = []\n",
    "    for coin_name in coin_name_list:\n",
    "        for data_file_name in [f for f in listdir(data_files_dir) if isfile(join(data_files_dir, f))]:\n",
    "            if coin_name in data_file_name:\n",
    "                trading_files.append(data_file_name)\n",
    "\n",
    "    start_ms_time = datetime.strptime(start_date, \"%Y-%m-%d %H:%M:%S\").timestamp() * 1000\n",
    "    end_ms_time = datetime.strptime(end_date, \"%Y-%m-%d %H:%M:%S\").timestamp() * 1000\n",
    "    \n",
    "    raw_data = {} #전체 CSV Raw 데이터\n",
    "    for coin_name in coin_name_list:\n",
    "        raw_data[coin_name] = []\n",
    "    \n",
    "    #KRW 제외한 나머지 CSV Raw 데이터 수집\n",
    "    for coin_name in coin_name_list:\n",
    "        for data_file_name in trading_files:\n",
    "            if coin_name in data_file_name:\n",
    "                file = open(data_files_dir + data_file_name, 'r', encoding='utf-8')\n",
    "                rdr = csv.reader(file)\n",
    "                for line in rdr:\n",
    "                    if start_ms_time <= int(line[0]) and int(line[0]) <= end_ms_time:\n",
    "                        raw_data[coin_name].append(line)\n",
    "                file.close()\n",
    "    \n",
    "    for line in raw_data['BTC']:\n",
    "        raw_data['KRW'].append([line[0], line[1], 1, 1, 1, 1, 1.0, 'normal'])\n",
    "#     print(\"test\")\n",
    "    return raw_data\n",
    "\n",
    "def Make_Dataset(dir_path, coin_list, start_date, end_date, time_unit, window_size, gap, margin_rate):\n",
    "    print(\"Make_Dataset is Started.\")\n",
    "    a = time.time()\n",
    "\n",
    "    X = {}\n",
    "    y = {}\n",
    "    idx = []\n",
    "\n",
    "    key_name_X = \"X_\"\n",
    "    key_name_y = \"y_\"\n",
    "    for idx_time_unit in time_unit:\n",
    "        for idx_window_size in window_size:\n",
    "            for idx_gap in gap:\n",
    "                for idx_margix_rate in margin_rate:\n",
    "                    key_name_X += str(idx_time_unit) + \"_\" + str(idx_window_size) + \"_\" + str(idx_gap) + \"_\" + str(idx_margix_rate)\n",
    "                    key_name_y += str(idx_time_unit) + \"_\" + str(idx_window_size) + \"_\" + str(idx_gap) + \"_\" + str(idx_margix_rate)\n",
    "                    if (os.path.isfile(dir_path+key_name_X+\".pickle\")) is not True:\n",
    "                        print(key_name_X)\n",
    "                        X[key_name_X] = \\\n",
    "                        make_cryptocurrency_dataset(\n",
    "                                                    coin_list, \n",
    "                                                    start_date, \n",
    "                                                    end_date,\n",
    "                                                    idx_time_unit,\n",
    "                                                    idx_window_size,\n",
    "                                                    idx_gap,\n",
    "                                                    idx_margix_rate,\n",
    "                                                    data_files_dir)\n",
    "                        with open(dir_path + key_name_X+\".pickle\", 'wb') as handle:\n",
    "                            pickle.dump(X[key_name_X], handle, protocol=pickle.HIGHEST_PROTOCOL)\n",
    "                        X = {}\n",
    "                        print(\"X_success.\")\n",
    "                    if (os.path.isfile(dir_path + key_name_y+\".pickle\")) is not True:\n",
    "                        print(key_name_y)\n",
    "                        y[key_name_y] = \\\n",
    "                        make_cryptocurrency_dataset(\n",
    "                                                    coin_list, \n",
    "                                                    start_date, \n",
    "                                                    end_date,\n",
    "                                                    idx_time_unit,\n",
    "                                                    idx_window_size,\n",
    "                                                    idx_gap,\n",
    "                                                    idx_margix_rate,\n",
    "                                                    data_files_dir)\n",
    "                        with open(dir_path + key_name_y+\".pickle\", 'wb') as handle:\n",
    "                            pickle.dump(y[key_name_y], handle, protocol=pickle.HIGHEST_PROTOCOL)\n",
    "                        y = {}\n",
    "                        print(\"y_success.\")\n",
    "                    key_name_X = \"X_\"\n",
    "                    key_name_y = \"y_\"\n",
    "    b = time.time()\n",
    "    print(\"Make_Dataset is Done.\")\n",
    "    #print(\"time: \", b-a)\n",
    "\n",
    "def make_cryptocurrency_dataset(coin_name_list, start_date, end_date, time_unit, window_size, gap, margin_rate, data_files_dir):\n",
    "    y_trv = []\n",
    "    y_btv = []\n",
    "    num_coins = len(coin_name_list)\n",
    "    #raw_data = get_all_raw_data_from_aws(coin_name_list, start_date, end_date)\n",
    "    raw_data = get_all_raw_data(coin_name_list, start_date, end_date, data_files_dir)    \n",
    "    num_sticks = len(raw_data['BTC'])\n",
    "    \n",
    "    if time_unit % 10 != 0 or num_sticks < (time_unit / 10) * window_size + gap:\n",
    "        return None, None\n",
    "    \n",
    "    num = int(num_sticks - ((time_unit / 10) * window_size + gap) + 1)\n",
    "        \n",
    "    X = []\n",
    "    y = []\n",
    "    # (윈도우 개수, 코인 개수, 윈도우 사이즈, 3)\n",
    "    for idx in range(num):\n",
    "        X.append([])\n",
    "        y.append([])\n",
    "        idx_coin = 0\n",
    "        for coin_name in coin_name_list:\n",
    "            X[idx].append([])\n",
    "            \n",
    "            last_idx_in_window = int(idx + time_unit / 10 * window_size - 1)\n",
    "            close_price_in_last_idx_in_window = float(raw_data[coin_name][last_idx_in_window][3])\n",
    "            \n",
    "            for idx_in_window in range(window_size):\n",
    "                X[idx][idx_coin].append([])\n",
    "                idx_stick = int(idx + time_unit / 10 * (idx_in_window + 1) - 1)\n",
    "                X[idx][idx_coin][idx_in_window].append(float(raw_data[coin_name][idx_stick][3]) / close_price_in_last_idx_in_window)\n",
    "                X[idx][idx_coin][idx_in_window].append(float(raw_data[coin_name][idx_stick][4]) / close_price_in_last_idx_in_window)\n",
    "                X[idx][idx_coin][idx_in_window].append(float(raw_data[coin_name][idx_stick][5]) / close_price_in_last_idx_in_window)\n",
    "                X[idx][idx_coin][idx_in_window].append(float(raw_data[coin_name][idx_stick][6]))\n",
    "                \n",
    "            target_idx_for_window = int(idx + time_unit / 10 * window_size - 1 + gap)\n",
    "            target_price = float(raw_data[coin_name][target_idx_for_window][3])\n",
    "            \n",
    "            target = 0\n",
    "            if target_price >= close_price_in_last_idx_in_window * (1.0 + float(margin_rate) / 100.0):\n",
    "                target = 1\n",
    "            y[idx].append(target)\n",
    "            \n",
    "            idx_coin += 1\n",
    "           \n",
    "    X = np.asarray(X)\n",
    "    y = np.asarray(y)\n",
    "    \n",
    "    return X, y\n",
    "    \n",
    "def make_cryptocurrency_dataset_X(coin_name_list, start_date, end_date, time_unit, window_size, gap, margin_rate, data_files_dir):\n",
    "    num_coins = len(coin_name_list)\n",
    "    #raw_data = get_all_raw_data_from_aws(coin_name_list, start_date, end_date)\n",
    "    raw_data = get_all_raw_data(coin_name_list, start_date, end_date, data_files_dir)    \n",
    "    num_sticks = len(raw_data['BTC'])\n",
    "    \n",
    "    if time_unit % 10 != 0 or num_sticks < (time_unit / 10) * window_size + gap:\n",
    "        return None, None\n",
    "    \n",
    "    num = int(num_sticks - ((time_unit / 10) * window_size + gap) + 1)\n",
    "        \n",
    "    X = []\n",
    "    # (윈도우 개수, 코인 개수, 윈도우 사이즈, 3)\n",
    "    for idx in range(num):\n",
    "        X.append([])\n",
    "        idx_coin = 0\n",
    "        for coin_name in coin_name_list:\n",
    "            X[idx].append([])\n",
    "            \n",
    "            last_idx_in_window = int(idx + time_unit / 10 * window_size - 1)\n",
    "            close_price_in_last_idx_in_window = float(raw_data[coin_name][last_idx_in_window][3])\n",
    "            \n",
    "            for idx_in_window in range(window_size):\n",
    "                X[idx][idx_coin].append([])\n",
    "                idx_stick = int(idx + time_unit / 10 * (idx_in_window + 1) - 1)\n",
    "                X[idx][idx_coin][idx_in_window].append(float(raw_data[coin_name][idx_stick][3]) / close_price_in_last_idx_in_window)\n",
    "                X[idx][idx_coin][idx_in_window].append(float(raw_data[coin_name][idx_stick][4]) / close_price_in_last_idx_in_window)\n",
    "                X[idx][idx_coin][idx_in_window].append(float(raw_data[coin_name][idx_stick][5]) / close_price_in_last_idx_in_window)\n",
    "                X[idx][idx_coin][idx_in_window].append(float(raw_data[coin_name][idx_stick][6]))\n",
    "                \n",
    "    X = np.array(X)\n",
    "    \n",
    "    return X\n",
    "\n",
    "def make_cryptocurrency_dataset_y(coin_name_list, start_date, end_date, time_unit, window_size, gap, margin_rate, data_files_dir):\n",
    "    y_trv = []\n",
    "    y_btv = []\n",
    "    num_coins = len(coin_name_list)\n",
    "    #raw_data = get_all_raw_data_from_aws(coin_name_list, start_date, end_date)\n",
    "    raw_data = get_all_raw_data(coin_name_list, start_date, end_date, data_files_dir)    \n",
    "    num_sticks = len(raw_data['BTC'])\n",
    "    \n",
    "    if time_unit % 10 != 0 or num_sticks < (time_unit / 10) * window_size + gap:\n",
    "        return None, None\n",
    "    \n",
    "    num = int(num_sticks - ((time_unit / 10) * window_size + gap) + 1)\n",
    "        \n",
    "    \n",
    "    y = []\n",
    "    # (윈도우 개수, 코인 개수, 윈도우 사이즈, 3)\n",
    "    for idx in range(num):\n",
    "        \n",
    "        y.append([])\n",
    "        idx_coin = 0\n",
    "        for coin_name in coin_name_list:            \n",
    "            last_idx_in_window = int(idx + time_unit / 10 * window_size - 1)\n",
    "            close_price_in_last_idx_in_window = float(raw_data[coin_name][last_idx_in_window][3])\n",
    "            target_idx_for_window = int(idx + time_unit / 10 * window_size - 1 + gap)\n",
    "            target_price = float(raw_data[coin_name][target_idx_for_window][3])\n",
    "            target = 0\n",
    "            \n",
    "            if target_price >= close_price_in_last_idx_in_window * (1.0 + float(margin_rate) / 100.0):\n",
    "                target = 1\n",
    "            y[idx].append(target)\n",
    "            \n",
    "            idx_coin += 1\n",
    "           \n",
    "    y = np.asarray(y)\n",
    "    \n",
    "    return y\n",
    "\n",
    "#\n",
    "\n",
    "def Load_Dataset_X(dir_path, time_unit, window_size, gap, margin_rate):\n",
    "    key_name_X = \"X_\" + \\\n",
    "                    str(time_unit) + \"_\" + \\\n",
    "                    str(window_size) + \"_\" + \\\n",
    "                    str(gap) + \"_\" + \\\n",
    "                    str(margin_rate)\n",
    "                    \n",
    "\n",
    "    with open(dir_path + key_name_X + \".pickle\", 'rb') as handle:\n",
    "        b_x = pickle.load(handle)\n",
    "    return b_x\n",
    "    \n",
    "def Load_Dataset_y(dir_path, time_unit, window_size, gap, margin_rate):\n",
    "    key_name_y = \"y_\" + \\\n",
    "                    str(time_unit) + \"_\" + \\\n",
    "                    str(window_size) + \"_\" + \\\n",
    "                    str(gap) + \"_\" + \\\n",
    "                    str(margin_rate)\n",
    "                    \n",
    "\n",
    "    with open(dir_path + key_name_y + \".pickle\", 'rb') as handle:\n",
    "        b_y = pickle.load(handle)\n",
    "    return b_y\n",
    "    \n",
    "def Make_Dataset_numpy(dir_path, coin_list, start_date, end_date, time_unit, window_size, gap, margin_rate, data_files_dir):\n",
    "    print(\"Make_Dataset_numpy is Started.\")\n",
    "    a = time.time()\n",
    "\n",
    "    X = {}\n",
    "    y = {}\n",
    "    idx = []\n",
    "\n",
    "    key_name_X = \"X_\"\n",
    "    key_name_y = \"y_\"\n",
    "    for idx_time_unit in time_unit:\n",
    "        for idx_window_size in window_size:\n",
    "            for idx_gap in gap:\n",
    "                for idx_margix_rate in margin_rate:\n",
    "                    key_name_X += str(idx_time_unit) + \"_\" + str(idx_window_size) + \"_\" + str(idx_gap) + \"_\" + str(idx_margix_rate)\n",
    "                    key_name_y += str(idx_time_unit) + \"_\" + str(idx_window_size) + \"_\" + str(idx_gap) + \"_\" + str(idx_margix_rate)\n",
    "                    if (os.path.isfile(dir_path + key_name_X+\".pickle\")) is not True:\n",
    "                        print(key_name_X)\n",
    "                        X[key_name_X] = \\\n",
    "                        make_cryptocurrency_dataset_X(\n",
    "                                                    coin_list, \n",
    "                                                    start_date, \n",
    "                                                    end_date,\n",
    "                                                    idx_time_unit,\n",
    "                                                    idx_window_size,\n",
    "                                                    idx_gap,\n",
    "                                                    idx_margix_rate,\n",
    "                                                    data_files_dir)\n",
    "                        with open(dir_path + key_name_X+\".pickle\", 'wb') as handle:\n",
    "                            pickle.dump(X[key_name_X], handle, protocol=pickle.HIGHEST_PROTOCOL)\n",
    "                        X = {}\n",
    "                        print(\"X_success.\")\n",
    "                    if (os.path.isfile(dir_path+key_name_y+\".pickle\")) is not True:\n",
    "                        print(key_name_y)\n",
    "                        y[key_name_y] = \\\n",
    "                        make_cryptocurrency_dataset_y(\n",
    "                                                    coin_list, \n",
    "                                                    start_date, \n",
    "                                                    end_date,\n",
    "                                                    idx_time_unit,\n",
    "                                                    idx_window_size,\n",
    "                                                    idx_gap,\n",
    "                                                    idx_margix_rate,\n",
    "                                                    data_files_dir)\n",
    "                        with open(dir_path + key_name_y+\".pickle\", 'wb') as handle:\n",
    "                            pickle.dump(y[key_name_y], handle, protocol=pickle.HIGHEST_PROTOCOL)\n",
    "                        y = {}\n",
    "                        print(\"y_success.\")\n",
    "                    key_name_X = \"X_\"\n",
    "                    key_name_y = \"y_\"\n",
    "    b = time.time()\n",
    "    print(\"Make_Dataset_numpy is Done.\")\n",
    "    print()\n",
    "    #print(\"time: \", b-a)\n",
    "    \n",
    "    \n",
    "def Make_Dataset_tuple(dir_path, coin_list, start_date, end_date, time_unit, window_size, gap, margin_rate, data_files_dir):\n",
    "    print(\"Make_Dataset_tuple is Started.\")\n",
    "    a = time.time()\n",
    "\n",
    "    X = {}\n",
    "    y = {}\n",
    "    idx = []\n",
    "\n",
    "    key_name_X = \"X_\"\n",
    "    key_name_y = \"y_\"\n",
    "    for idx_time_unit in time_unit:\n",
    "        for idx_window_size in window_size:\n",
    "            for idx_gap in gap:\n",
    "                for idx_margix_rate in margin_rate:\n",
    "                    key_name_X += str(idx_time_unit) + \"_\" + str(idx_window_size) + \"_\" + str(idx_gap) + \"_\" + str(idx_margix_rate)\n",
    "                    key_name_y += str(idx_time_unit) + \"_\" + str(idx_window_size) + \"_\" + str(idx_gap) + \"_\" + str(idx_margix_rate)\n",
    "                    if (os.path.isfile(dir_path + key_name_X+\".pickle\")) is not True:\n",
    "                        print(key_name_X)\n",
    "                        X[key_name_X], _ = \\\n",
    "                        make_cryptocurrency_dataset(\n",
    "                                                    coin_list, \n",
    "                                                    start_date, \n",
    "                                                    end_date,\n",
    "                                                    idx_time_unit,\n",
    "                                                    idx_window_size,\n",
    "                                                    idx_gap,\n",
    "                                                    idx_margix_rate,\n",
    "                                                    data_files_dir)\n",
    "                        with open(dir_path + key_name_X+\".pickle\", 'wb') as handle:\n",
    "                            pickle.dump(X[key_name_X], handle, protocol=pickle.HIGHEST_PROTOCOL)\n",
    "                        X = {}\n",
    "                        print(\"X_success.\")\n",
    "                    if (os.path.isfile(dir_path+key_name_y+\".pickle\")) is not True:\n",
    "                        print(key_name_y)\n",
    "                        _, y[key_name_y] = \\\n",
    "                        make_cryptocurrency_dataset(\n",
    "                                                    coin_list, \n",
    "                                                    start_date, \n",
    "                                                    end_date,\n",
    "                                                    idx_time_unit,\n",
    "                                                    idx_window_size,\n",
    "                                                    idx_gap,\n",
    "                                                    idx_margix_rate,\n",
    "                                                    data_files_dir)\n",
    "                        with open(dir_path + key_name_y+\".pickle\", 'wb') as handle:\n",
    "                            pickle.dump(y[key_name_y], handle, protocol=pickle.HIGHEST_PROTOCOL)\n",
    "                        y = {}\n",
    "                        print(\"y_success.\")\n",
    "                    key_name_X = \"X_\"\n",
    "                    key_name_y = \"y_\"\n",
    "    b = time.time()\n",
    "    print(\"Make_Dataset_tuple is Done.\")\n",
    "    print()\n",
    "    #print(\"time: \", b-a)\n",
    "\n",
    "def Check_Dataset(dir_path, coin_list, start_date, end_date, time_unit, window_size, gap, margin_rate, data_files_dir):\n",
    "    print(\"Check_Dataset is Started.\")\n",
    "    a = time.time()\n",
    "\n",
    "    X = {}\n",
    "    y = {}\n",
    "    idx = []\n",
    "\n",
    "    key_name_X = \"X_\"\n",
    "    key_name_y = \"y_\"\n",
    "    for idx_time_unit in time_unit:\n",
    "        for idx_window_size in window_size:\n",
    "            for idx_gap in gap:\n",
    "                for idx_margix_rate in margin_rate:\n",
    "                    key_name_X += str(idx_time_unit) + \"_\" + str(idx_window_size) + \"_\" + str(idx_gap) + \"_\" + str(idx_margix_rate)\n",
    "                    key_name_y += str(idx_time_unit) + \"_\" + str(idx_window_size) + \"_\" + str(idx_gap) + \"_\" + str(idx_margix_rate)\n",
    "                    \n",
    "                    with open(dir_path + key_name_X+\".pickle\", 'rb') as handle:\n",
    "                        data = pickle.load(handle)\n",
    "    \n",
    "                    if type(data) == tuple:\n",
    "#                         print()\n",
    "                        os.system('rm '+dir_path + key_name_X+\".pickle\")\n",
    "                        print(key_name_X,\".pickle is removed.\")\n",
    "                        os.system('rm '+dir_path + key_name_y+\".pickle\")\n",
    "                        print(key_name_y,\".pickle is removed.\")\n",
    "        \n",
    "                    if (os.path.isfile(dir_path+key_name_X+\".pickle\")) is not True:\n",
    "                        print(key_name_X)\n",
    "                        X[key_name_X] = \\\n",
    "                        make_cryptocurrency_dataset_X(\n",
    "                                                    coin_list, \n",
    "                                                    start_date, \n",
    "                                                    end_date,\n",
    "                                                    idx_time_unit,\n",
    "                                                    idx_window_size,\n",
    "                                                    idx_gap,\n",
    "                                                    idx_margix_rate,\n",
    "                                                    data_files_dir)\n",
    "                        with open(dir_path + key_name_X+\".pickle\", 'wb') as handle:\n",
    "                            pickle.dump(X[key_name_X], handle, protocol=pickle.HIGHEST_PROTOCOL)\n",
    "                        X = {}\n",
    "                        print(\"[SUCCESS] \",key_name_X,\".pickle is created.\")\n",
    "                    if (os.path.isfile(dir_path+key_name_y+\".pickle\")) is not True:\n",
    "                        print(key_name_y)\n",
    "                        y[key_name_y] = \\\n",
    "                        make_cryptocurrency_dataset_y(\n",
    "                                                    coin_list, \n",
    "                                                    start_date, \n",
    "                                                    end_date,\n",
    "                                                    idx_time_unit,\n",
    "                                                    idx_window_size,\n",
    "                                                    idx_gap,\n",
    "                                                    idx_margix_rate,\n",
    "                                                    data_files_dir)\n",
    "                        with open(dir_path + key_name_y+\".pickle\", 'wb') as handle:\n",
    "                            pickle.dump(y[key_name_y], handle, protocol=pickle.HIGHEST_PROTOCOL)\n",
    "                        y = {}\n",
    "                        print(\"[SUCCESS] \",key_name_y,\".pickle is created.\")\n",
    "                    key_name_X = \"X_\"\n",
    "                    key_name_y = \"y_\"\n",
    "    b = time.time()\n",
    "    print(\"Check_Dataset is Done.\")\n",
    "    #print(\"time: \", b-a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create Dataset\n",
    "data_files_dir = \"/Users/dohyung/OneDrive/2018-RNN/RNN_python/AWS_dataset/\"\n",
    "dataset_dir_path_tuple_type = \"./dataset_pickle_tuple_type/\"\n",
    "dataset_dir_path_numpy_type = \"./dataset_pickle_numpy.ndarray_type)/\"\n",
    "coin_list = [\"KRW\", \"BTC\", \"ETH\", \"XRP\", \"BCH\", \"LTC\", \"DASH\", \"ETC\"]\n",
    "start_date = \"2017-08-04 21:40:00\"\n",
    "end_date = \"2018-08-20 23:50:00\"\n",
    "# time_unit = [10,30,60]     # candle stick minutes\n",
    "# window_size = [10,25,50,75,100]  # Unit: num. of candle sticks\n",
    "# gap = [1,2,3]            # Unit: num. of candle sticks\n",
    "# margin_rate = [0.1,0.25,0.5]  # Unit: percent\n",
    "\n",
    "# for slave04\n",
    "time_unit = [10,30,60]     # candle stick minutes\n",
    "window_size = [10,25,50]  # Unit: num. of candle sticks\n",
    "gap = [1,2,3]            # Unit: num. of candle sticks\n",
    "margin_rate = [0.1,0.25,0.5]  # Unit: percent\n",
    "\n",
    "# for slave05\n",
    "# time_unit = [10,30,60]     # candle stick minutes\n",
    "# window_size = [75,100]  # Unit: num. of candle sticks\n",
    "# gap = [1,2,3]            # Unit: num. of candle sticks\n",
    "# margin_rate = [0.1,0.25,0.5]  # Unit: percent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#Make_Dataset_tuple(dataset_dir_path_tuple_type, coin_list, start_date, end_date, time_unit, window_size, gap, margin_rate)\n",
    "#Make_Dataset_numpy(dataset_dir_path_numpy_type, coin_list, start_date, end_date, time_unit, window_size, gap, margin_rate)\n",
    "#Check_Dataset(dataset_dir_path, coin_list, start_date, end_date, time_unit, window_size, gap, margin_rate)\n",
    "# Tuple 형태의 데이터셋이 나오지 않도록."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_history(history):\n",
    "    loss_list = [s for s in history.history.keys() if 'loss' in s and 'val' not in s]\n",
    "    val_loss_list = [s for s in history.history.keys() if 'loss' in s and 'val' in s]\n",
    "    acc_list = [s for s in history.history.keys() if 'acc' in s and 'val' not in s]\n",
    "    val_acc_list = [s for s in history.history.keys() if 'acc' in s and 'val' in s]\n",
    "    \n",
    "    if len(loss_list) == 0:\n",
    "        print('Loss is missing in history')\n",
    "        return \n",
    "    \n",
    "    ## As loss always exists\n",
    "    epochs = range(1,len(history.history[loss_list[0]]) + 1)\n",
    "    \n",
    "    ## Loss\n",
    "    plt.figure(1)\n",
    "    for l in loss_list:\n",
    "        plt.plot(epochs, history.history[l], 'b', label='Training loss (' + str(str(format(history.history[l][-1],'.5f'))+')'))\n",
    "    for l in val_loss_list:\n",
    "        plt.plot(epochs, history.history[l], 'g', label='Validation loss (' + str(str(format(history.history[l][-1],'.5f'))+')'))\n",
    "    \n",
    "    plt.title('Loss')\n",
    "    plt.xlabel('Epochs')\n",
    "    plt.ylabel('Loss')\n",
    "    plt.legend()\n",
    "    \n",
    "    ## Accuracy\n",
    "    plt.figure(2)\n",
    "    for l in acc_list:\n",
    "        plt.plot(epochs, history.history[l], 'b', label='Training accuracy (' + str(format(history.history[l][-1],'.5f'))+')')\n",
    "    for l in val_acc_list:    \n",
    "        plt.plot(epochs, history.history[l], 'g', label='Validation accuracy (' + str(format(history.history[l][-1],'.5f'))+')')\n",
    "\n",
    "    plt.title('Accuracy')\n",
    "    plt.xlabel('Epochs')\n",
    "    plt.ylabel('Accuracy')\n",
    "    plt.legend()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def categorical_accuracy(y_true, y_pred):\n",
    "    return math_ops.cast(\n",
    "      math_ops.equal(\n",
    "          math_ops.argmax(y_true, axis=-1), math_ops.argmax(y_pred, axis=-1)),\n",
    "      K.floatx())\n",
    "\n",
    "def f1_score(y_true, y_pred):\n",
    "    def recall(y_true, y_pred):\n",
    "        \"\"\"Recall metric.\n",
    "\n",
    "        Only computes a batch-wise average of recall.\n",
    "\n",
    "        Computes the recall, a metric for multi-label classification of\n",
    "        how many relevant items are selected.\n",
    "        \"\"\"\n",
    "        true_positives = K.sum(K.round(K.clip(y_true * y_pred, 0, 1)))\n",
    "        possible_positives = K.sum(K.round(K.clip(y_true, 0, 1)))\n",
    "        recall = true_positives / (possible_positives + K.epsilon())\n",
    "        return recall\n",
    "\n",
    "    def precision(y_true, y_pred):\n",
    "        \"\"\"Precision metric.\n",
    "\n",
    "        Only computes a batch-wise average of precision.\n",
    "\n",
    "        Computes the precision, a metric for multi-label classification of\n",
    "        how many selected items are relevant.\n",
    "        \"\"\"\n",
    "        true_positives = K.sum(K.round(K.clip(y_true * y_pred, 0, 1)))\n",
    "        predicted_positives = K.sum(K.round(K.clip(y_pred, 0, 1)))\n",
    "        precision = true_positives / (predicted_positives + K.epsilon())\n",
    "        return precision\n",
    "    precision = precision(y_true, y_pred)\n",
    "    recall = recall(y_true, y_pred)\n",
    "    return 2*((precision*recall)/(precision+recall+K.epsilon()))\n",
    "\n",
    "def f1_score_(y_true, y_pred):\n",
    "\n",
    "    # Count positive samples.\n",
    "    c1 = K.sum(K.round(K.clip(y_true * y_pred, 0, 1)))\n",
    "    c2 = K.sum(K.round(K.clip(y_pred, 0, 1)))\n",
    "    c3 = K.sum(K.round(K.clip(y_true, 0, 1)))\n",
    "\n",
    "    # If there are no true samples, fix the F1 score at 0.\n",
    "    if c3 == 0:\n",
    "        return 0\n",
    "\n",
    "    # How many selected items are relevant?\n",
    "    precision = c1 / (c2 + 1e-7)\n",
    "    \n",
    "    # How many relevant items are selected?\n",
    "    recall = c1 / (c3 +  + 1e-7)\n",
    "\n",
    "    # Calculate f1_score\n",
    "    f1_score = 2 * (precision * recall) / ((precision + recall) + 1e-7)\n",
    "    return f1_score \n",
    "\n",
    "\n",
    "def create_model_RNN(window_size, n_state_units=32, activation='softmax', optimizer='adam', init='glorot_uniform', dropout_rate=0.0, neurons=2):\n",
    "    model = Sequential()\n",
    "    model.add(# if문을 통해 여러 RNN모델 쓸 수 있도록 하기, SimpleRNN외에 다른 RNN모델 찾아보기\n",
    "        RNN(n_state_units, \n",
    "                  input_shape=(window_size, 32),\n",
    "                  use_bias=True, \n",
    "                  activation='tanh',\n",
    "                  kernel_initializer='glorot_uniform', \n",
    "                  recurrent_initializer='orthogonal', \n",
    "                  bias_initializer='zeros', \n",
    "                  dropout=0.0,\n",
    "                  recurrent_dropout=0.0))\n",
    "    \n",
    "    model.add(Dense(units=neurons))\n",
    "    model.add(Dropout(dropout_rate))\n",
    "#     model.add(Dense(units=2))\n",
    "#     model = multi_gpu_model(model, gpus=2)\n",
    "\n",
    "    model.compile(loss=keras.losses.categorical_crossentropy, \n",
    "                  optimizer=optimizer, \n",
    "                  metrics=[\"accuracy\", f1_score])\n",
    "\n",
    "    return model\n",
    "\n",
    "\n",
    "def create_model_SimpleRNN(window_size, n_state_units=32, activation='softmax', optimizer='adam', init='glorot_uniform', dropout_rate=0.0, neurons=2):\n",
    "    model = Sequential()\n",
    "    model.add(# if문을 통해 여러 RNN모델 쓸 수 있도록 하기, SimpleRNN외에 다른 RNN모델 찾아보기\n",
    "        SimpleRNN(n_state_units, \n",
    "                  input_shape=(window_size, 32),\n",
    "                  use_bias=True, \n",
    "                  activation='tanh',\n",
    "                  kernel_initializer='glorot_uniform', \n",
    "                  recurrent_initializer='orthogonal', \n",
    "                  bias_initializer='zeros', \n",
    "                  dropout=0.0,\n",
    "                  recurrent_dropout=0.0))\n",
    "    \n",
    "    model.add(Dense(units=neurons))\n",
    "    model.add(Dropout(dropout_rate))\n",
    "#     model.add(Dense(units=2))\n",
    "#     model = multi_gpu_model(model, gpus=2)\n",
    "\n",
    "    model.compile(loss=keras.losses.categorical_crossentropy, \n",
    "                  optimizer=optimizer, \n",
    "                  metrics=[\"accuracy\", f1_score])\n",
    "\n",
    "    return model\n",
    "\n",
    "def create_model_LSTM(window_size, n_state_units=32, activation='softmax', optimizer='adam', init='glorot_uniform', dropout_rate=0.0, neurons=2):\n",
    "    model = Sequential()\n",
    "    model.add(# if문을 통해 여러 RNN모델 쓸 수 있도록 하기, SimpleRNN외에 다른 RNN모델 찾아보기\n",
    "        LSTM(n_state_units, \n",
    "                  input_shape=(window_size, 32),\n",
    "                  use_bias=True, \n",
    "                  activation='tanh',\n",
    "                  kernel_initializer='glorot_uniform', \n",
    "                  recurrent_initializer='orthogonal', \n",
    "                  bias_initializer='zeros', \n",
    "                  dropout=0.0,\n",
    "                  recurrent_dropout=0.0))\n",
    "    \n",
    "    model.add(Dense(units=neurons))\n",
    "    model.add(Dropout(dropout_rate))\n",
    "#     model.add(Dense(units=2))\n",
    "#     model = multi_gpu_model(model, gpus=2)\n",
    "\n",
    "    model.compile(loss=keras.losses.categorical_crossentropy, \n",
    "                  optimizer=optimizer, \n",
    "                  metrics=[\"accuracy\", f1_score])\n",
    "\n",
    "    return model\n",
    "\n",
    "\n",
    "def create_model_GRU(window_size, n_state_units=32, activation='softmax', optimizer='adam', init='glorot_uniform', dropout_rate=0.0, neurons=2):\n",
    "    model = Sequential()\n",
    "    model.add(# if문을 통해 여러 RNN모델 쓸 수 있도록 하기, SimpleRNN외에 다른 RNN모델 찾아보기\n",
    "        GRU(n_state_units, \n",
    "                  input_shape=(window_size, 32),\n",
    "                  use_bias=True, \n",
    "                  activation='tanh',\n",
    "                  kernel_initializer='glorot_uniform', \n",
    "                  recurrent_initializer='orthogonal', \n",
    "                  bias_initializer='zeros', \n",
    "                  dropout=0.0,\n",
    "                  recurrent_dropout=0.0))\n",
    "    \n",
    "    model.add(Dense(units=neurons))\n",
    "    model.add(Dropout(dropout_rate))\n",
    "#     model.add(Dense(units=2))\n",
    "#     model = multi_gpu_model(model, gpus=2)\n",
    "\n",
    "    model.compile(loss=keras.losses.categorical_crossentropy, \n",
    "                  optimizer=optimizer, \n",
    "                  metrics=[\"accuracy\", f1_score])\n",
    "\n",
    "    return model\n",
    "\n",
    "\n",
    "def input_reshape(X_train_data, X_test_data, n_steps, n_coins, n_price):\n",
    "    X_train_reshape = X_train_data.reshape(\n",
    "        -1, \n",
    "        n_steps, \n",
    "        n_coins * n_price\n",
    "    )\n",
    "    X_test_reshape = X_test_data.reshape(\n",
    "        -1, \n",
    "        n_steps, \n",
    "        n_coins * n_price\n",
    "    )\n",
    "    return X_train_reshape, X_test_reshape\n",
    "\n",
    "def onehottify(x, n=None, dtype=np.int):\n",
    "    \"\"\"1-hot encode x with the max value n (computed from data if n is None).\"\"\"\n",
    "    x = np.asarray(x)\n",
    "    n = np.max(x) + 1 if n is None else n\n",
    "    return np.eye(n, dtype=dtype)[x]\n",
    "\n",
    "\n",
    "\n",
    "def Evaluate(pickle_load_dir_path, data_files_dir, epochs, pickle_result_dir_path, time_unit, window_size, gap, margin_rate, _TEST, _ENHANCE):\n",
    "    X = {}\n",
    "    y = {}\n",
    "    MODEL_list = [\"SimpleRNN\", \"RNN\", \"LSTM\", \"GRU\"]\n",
    "#     MODEL_list = [\"SimpleRNN\"]\n",
    "    idx = []\n",
    "\n",
    "    key_name_X = \"X_\"\n",
    "    key_name_y = \"y_\"\n",
    "    \n",
    "    for MODEL_idx in MODEL_list:\n",
    "        MODEL = MODEL_idx\n",
    "        for idx_time_unit in time_unit:\n",
    "                for idx_window_size in window_size:\n",
    "                    for idx_gap in gap:\n",
    "                        for idx_margix_rate in margin_rate:\n",
    "                            key_name_X += str(idx_time_unit) + \"_\" + str(idx_window_size) + \"_\" + str(idx_gap) + \"_\" + str(idx_margix_rate)\n",
    "                            key_name_y += str(idx_time_unit) + \"_\" + str(idx_window_size) + \"_\" + str(idx_gap) + \"_\" + str(idx_margix_rate)\n",
    "\n",
    "\n",
    "                            X = Load_Dataset_X(pickle_load_dir_path, idx_time_unit, idx_window_size, idx_gap, idx_margix_rate)\n",
    "                            y = Load_Dataset_y(pickle_load_dir_path, idx_time_unit, idx_window_size, idx_gap, idx_margix_rate)\n",
    "\n",
    "                            y_single = {}\n",
    "                            y_single['BTC'] = y[:, 1]\n",
    "                            y_single['ETH'] = y[:, 2]\n",
    "                            y_single['XRP'] = y[:, 3]\n",
    "                            y_single['BCH'] = y[:, 4]\n",
    "                            y_single['LTC'] = y[:, 5]\n",
    "                            y_single['DASH'] = y[:, 6]\n",
    "                            y_single['ETC'] = y[:, 7]\n",
    "\n",
    "                            coin_list2 = [\"BTC\", \"ETH\", \"XRP\", \"BCH\", \"LTC\", \"DASH\", \"ETC\"]\n",
    "\n",
    "    #                         for coin in coin_list2:\n",
    "    #                             print(\"y_single[\"+coin+\"]\"+\".shape\")\n",
    "    #                             print(y_single[coin].shape)\n",
    "    #                             print()\n",
    "                            \n",
    "                            \n",
    "\n",
    "                            for coin in coin_list2:\n",
    "                                y2 = onehottify(y_single[coin], n=2)\n",
    "#                             for coin in range(1):\n",
    "#                                 y2 = onehottify(y_single['BTC'], n=2)\n",
    "                                \n",
    "\n",
    "\n",
    "                                X_train, X_test, y_train, y_test = train_test_split(X, y2, test_size=0.1, random_state=42)\n",
    "    #                             print(\"X_train.shape\")\n",
    "    #                             print(X_train.shape)\n",
    "    #                             print(\"y_train.shape\")\n",
    "    #                             print(y_train.shape)\n",
    "    #                             print()\n",
    "    #                             print(\"X_test.shape\")\n",
    "    #                             print(X_test.shape)\n",
    "    #                             print(\"y_test.shape\")\n",
    "    #                             print(y_test.shape)\n",
    "    #                             print()\n",
    "\n",
    "                                n_coins = 8\n",
    "                                n_price = 4\n",
    "                                n_steps = idx_window_size # 원래 100이었음. reshape 문제 때문에 수정함\n",
    "\n",
    "                                X_train_2 = X_train.transpose([0, 2, 1, 3])\n",
    "                                X_test_2 = X_test.transpose([0, 2, 1, 3])\n",
    "    #                             print(\"X_train_2.shape\")\n",
    "    #                             print(X_train_2.shape)\n",
    "    #                             print(\"X_test_2.shape\")\n",
    "    #                             print(X_test_2.shape)\n",
    "    #                             print()\n",
    "\n",
    "                                X_train_3 = X_train_2.reshape([X_train.shape[0], n_steps, n_coins * n_price])\n",
    "                                X_test_3 = X_test_2.reshape([X_test.shape[0], n_steps, n_coins * n_price])\n",
    "    #                             print(\"X_train_3.shape\")\n",
    "    #                             print(X_train_3.shape)\n",
    "    #                             print(\"X_test_3.shape\")\n",
    "    #                             print(X_test_3.shape)\n",
    "    #                             print()\n",
    "\n",
    "                                if (_TEST==True and _ENHANCE==False):\n",
    "                                    param_grid = {'window_size' : [n_steps], \n",
    "                                                  'n_state_units': [100],\n",
    "                                                  'activation': ['relu'], \n",
    "                                                  'optimizer': ['rmsprop'], #sgd 추가\n",
    "                                                  'init': ['glorot_uniform'], #he 추가\n",
    "                                                  'batch_size': [10]}\n",
    "\n",
    "                                elif (_TEST==False and _ENHANCE==False):\n",
    "                                    param_grid = {'window_size' : [n_steps], \n",
    "                                                  'n_state_units': [40, 80, 160],\n",
    "                                                  'activation': ['relu', 'softmax'], \n",
    "                                                  'optimizer': ['rmsprop', 'adam'], #sgd 추가\n",
    "                                                  'init': ['glorot_uniform', 'uniform', 'normal', 'he_uniform'], #he 추가\n",
    "                                                  'batch_size': [10, 50]}\n",
    "\n",
    "                                elif (_TEST==False and _ENHANCE==True):\n",
    "                                    param_grid = {'window_size' : [n_steps], \n",
    "                                                  'n_state_units': [40, 80, 160],\n",
    "                                                  'activation': ['relu', 'softmax'], \n",
    "                                                  'optimizer': ['rmsprop', 'adam'], #sgd 추가\n",
    "                                                  'init': ['glorot_uniform', 'uniform', 'normal', 'he_uniform'], #he 추가\n",
    "                                                  'batch_size': [10, 50],\n",
    "                                                  'dropout_rate':[0.0, 0.1, 0.2, 0.3, 0.4, 0.5], # after paramter select. when epochs raise..\n",
    "                                                  'neurons':[2,10,20,50]}\n",
    "\n",
    "\n",
    "\n",
    "                                X_train_reshape = X_train_2.reshape([X_train.shape[0], n_steps*n_coins * n_price])\n",
    "                                X_test_reshape = X_test_2.reshape([X_test.shape[0], n_steps*n_coins * n_price])\n",
    "    #                             print(\"X_train_reshape.shape\")\n",
    "    #                             print(X_train_reshape.shape)\n",
    "    #                             print(\"X_test_reshape.shape\")\n",
    "    #                             print(X_test_reshape.shape)\n",
    "    #                             print()\n",
    "\n",
    "                                scaler = MinMaxScaler()\n",
    "                                scaler.fit(X_train_reshape)\n",
    "                                X_train_scaled = scaler.transform(X_train_reshape)\n",
    "                                X_test_scaled = scaler.transform(X_test_reshape)\n",
    "\n",
    "                                X_train_scaled = X_train_scaled.reshape(-1, n_steps, n_coins * n_price)\n",
    "\n",
    "                                X_test_scaled = X_test_scaled.reshape(-1, n_steps, n_coins * n_price)\n",
    "\n",
    "                                                               \n",
    "                                if MODEL == \"SimpleRNN\":\n",
    "                                    model = KerasClassifier(build_fn=create_model_SimpleRNN, \n",
    "                                                            epochs=epochs, # epochs는 실험을 최종적으로 수행하고자 할 때 높일 것(100~150정도)\n",
    "                                                            batch_size=10, \n",
    "                                                            verbose=True)\n",
    "                                    \n",
    "                                elif MODEL == \"LSTM\":\n",
    "                                    model = KerasClassifier(build_fn=create_model_LSTM, \n",
    "                                                            epochs=epochs, # epochs는 실험을 최종적으로 수행하고자 할 때 높일 것(100~150정도)\n",
    "                                                            batch_size=10, \n",
    "                                                            verbose=True)\n",
    "                                    \n",
    "                                elif MODEL == \"RNN\":\n",
    "                                    model = KerasClassifier(build_fn=create_model_RNN, \n",
    "                                                            epochs=epochs, # epochs는 실험을 최종적으로 수행하고자 할 때 높일 것(100~150정도)\n",
    "                                                            batch_size=10, \n",
    "                                                            verbose=True)\n",
    "                                    \n",
    "                                elif MODEL == \"GRU\":\n",
    "                                    model = KerasClassifier(build_fn=create_model_GRU, \n",
    "                                                            epochs=epochs, # epochs는 실험을 최종적으로 수행하고자 할 때 높일 것(100~150정도)\n",
    "                                                            batch_size=10, \n",
    "                                                            verbose=True)\n",
    "\n",
    "                                grid = GridSearchCV(\n",
    "                                    estimator=model, \n",
    "                                    cv=5, \n",
    "                                    param_grid=param_grid,\n",
    "                                    verbose=1)\n",
    "\n",
    "                                X_train_scaled, X_test_scaled = input_reshape(X_train_scaled, X_test_scaled, n_steps, n_coins, n_price)\n",
    "                                print()\n",
    "                                print()\n",
    "                                print(\"----------------------\")\n",
    "                                print(\"<\"+MODEL+\">\")\n",
    "                                print(\"----------------------\")\n",
    "                                print(\"__\"+coin+\"__\" + \\\n",
    "#                                 print(\"__\"+\"BTC\"+\"__\" + \\\n",
    "                                        \"time unit: \"+str(idx_time_unit) + \"  |  \" + \\\n",
    "                                        \"window_size :\"+str(idx_window_size) + \"  |  \" + \\\n",
    "                                        \"gap :\"+str(idx_gap) + \"  |  \" + \\\n",
    "                                        \"margin_rate :\"+str(idx_margix_rate) + \\\n",
    "                                        \"  started.\")\n",
    "                                grid_result = grid.fit(X_train_scaled, \n",
    "                                                       y_train, \n",
    "                                                       validation_data=(X_test_scaled,y_test))\n",
    "                                print(\"----------------------\")\n",
    "\n",
    "                                # \n",
    "                                means = grid_result.cv_results_['mean_test_score']\n",
    "                                stds = grid_result.cv_results_['std_test_score']\n",
    "                                params = grid_result.cv_results_['params']\n",
    "\n",
    "                                evaluate_result = {}\n",
    "                                evaluate_result[MODEL + \"_\" + \\\n",
    "#                                               \"BTC\" + \"_\" + \\\n",
    "                                              coin + \"_\" + \\\n",
    "                                              str(idx_time_unit) + \"_\" + \\\n",
    "                                              str(idx_window_size) + \"_\" + \\\n",
    "                                              str(idx_gap) + \"_\" + \\\n",
    "                                              str(idx_margix_rate)] = {\"MODEL: \":MODEL,\\\n",
    "#                                                                \"Cryptocurrency: \":\"BTC\", \\\n",
    "                                                                \"Cryptocurrency: \":coin, \\\n",
    "                                                                \"grid_result.best_score_\":means, \\\n",
    "                                                                \"grid_result.best_params_\":params}\n",
    "                                print()\n",
    "                                print(\"evaluate result dict: \", evaluate_result)\n",
    "                                print()\n",
    "\n",
    "                                # summarize results\n",
    "                                print(\"Best: %f using %s\" % (grid_result.best_score_, grid_result.best_params_))\n",
    "                                print()\n",
    "                                print(\"---pickle saving..\")\n",
    "                                with open(pickle_result_dir_path + \\\n",
    "                                          MODEL + \"_\" + \\\n",
    "#                                           \"BTC\" + \"_\" + \\\n",
    "                                          coin + \"_\" + \\\n",
    "                                          str(idx_time_unit) + \"_\" + \\\n",
    "                                          str(idx_window_size) + \"_\" + \\\n",
    "                                          str(idx_gap) + \"_\" + \\\n",
    "                                          str(idx_margix_rate) + \\\n",
    "                                          \"_result.pickle\", 'wb') as handle:\n",
    "                                    pickle.dump(evaluate_result, handle, protocol=pickle.HIGHEST_PROTOCOL)\n",
    "                                print()\n",
    "\n",
    "\n",
    "    #                             for mean, stdev, param in zip(means, stds, params):\n",
    "    #                                 print(\"%f (%f) with: %r\" % (mean, stdev, param))\n",
    "    #                             print()\n",
    "\n",
    "\n",
    "                            key_name_X = \"X_\"\n",
    "                            key_name_y = \"y_\"\n",
    "\n",
    "                        \n",
    "# 저장된 pickle 파일의 데이터 구조\n",
    "# tmp = {}\n",
    "# tmp[\"10_1_1_0.1\"] = {\"grid_result.best_score_\":{}}, {\"grid_result.best_params_\":{}}\n",
    "# type(tmp[\"10_1_1_0.1\"][0])\n",
    "# print(tmp[\"10_1_1_0.1\"])\n",
    "# print(tmp[\"10_1_1_0.1\"])\n",
    "# print(tmp[\"10_1_1_0.1\"][0])\n",
    "# print(tmp[\"10_1_1_0.1\"][0]['grid_result.best_score_'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create Dataset\n",
    "\n",
    "# time_unit = [10,30,60]     # candle stick minutes\n",
    "# window_size = [10,25,50,75,100]  # Unit: num. of candle sticks\n",
    "# gap = [1,2,3]            # Unit: num. of candle sticks\n",
    "# margin_rate = [0.1,0.25,0.5]  # Unit: percent\n",
    "\n",
    "# for link01\n",
    "time_unit = [10,30,60]     # candle stick minutes\n",
    "window_size = [10,25,50]  # Unit: num. of candle sticks\n",
    "gap = [1,2,3]            # Unit: num. of candle sticks\n",
    "margin_rate = [0.1,0.25,0.5]  # Unit: percent\n",
    "\n",
    "# for slave04\n",
    "# time_unit = [10,30,60]     # candle stick minutes\n",
    "# window_size = [75]  # Unit: num. of candle sticks\n",
    "# gap = [1,2,3]            # Unit: num. of candle sticks\n",
    "# margin_rate = [0.1,0.25,0.5]  # Unit: percent\n",
    "\n",
    "# for slave05\n",
    "# time_unit = [10,30,60]     # candle stick minutes\n",
    "# window_size = [100]  # Unit: num. of candle sticks\n",
    "# gap = [1,2,3]            # Unit: num. of candle sticks\n",
    "# margin_rate = [0.1,0.25,0.5]  # Unit: percent\n",
    "\n",
    "# for test\n",
    "# time_unit = [10]     # candle stick minutes\n",
    "# window_size = [10]  # Unit: num. of candle sticks\n",
    "# gap = [1]            # Unit: num. of candle sticks\n",
    "# margin_rate = [0.1]  # Unit: percent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "----------------------\n",
      "<SimpleRNN>\n",
      "----------------------\n",
      "__BTC__time unit: 10  |  window_size :10  |  gap :1  |  margin_rate :0.1  started.\n",
      "Fitting 5 folds for each of 96 candidates, totalling 480 fits\n",
      "Train on 37535 samples, validate on 5214 samples\n",
      "Epoch 1/1\n",
      "37535/37535 [==============================] - 18s 470us/step - loss: 5.3073 - acc: 0.3293 - f1_score: 0.3265 - val_loss: 5.4654 - val_acc: 0.3393 - val_f1_score: 0.3393\n",
      "9384/9384 [==============================] - 2s 210us/step\n",
      "37535/37535 [==============================] - 8s 206us/step\n",
      "Train on 37535 samples, validate on 5214 samples\n",
      "Epoch 1/1\n",
      " 1900/37535 [>.............................] - ETA: 18s - loss: 5.2257 - acc: 0.6774 - f1_score: 0.6767"
     ]
    }
   ],
   "source": [
    "# Find Best Parameter\n",
    "Evaluate(pickle_load_dir_path = \"./dataset_pickle_tuple_type/\",  \n",
    "         data_files_dir = dataset_dir_path_tuple_type, \n",
    "         epochs=1, \n",
    "         pickle_result_dir_path = \"./evaluate_result/\", \n",
    "         time_unit = time_unit, \n",
    "         window_size = window_size, \n",
    "         gap = gap, \n",
    "         margin_rate = margin_rate, \n",
    "         _TEST=False, \n",
    "         _ENHANCE=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load parameter result\n",
    "# BTC_coin_X_10_10_1_0.1_result.pickle\n",
    "\n",
    "b_x = pickle.load(open(\"./evaluate_result/SimpleRNN_BTC_10_10_1_0.1__result.pickle\", \"rb\"))\n",
    "b_x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Boost-up Acc, F1\n",
    "evaluate_result_dir_path = \"./evaluate_result/acc_f1/\"\n",
    "dataset_dir_path = dataset_dir_path_tuple_type \n",
    "# dataset_dir_path = dataset_dir_path_numpy_type\n",
    "epochs = 100\n",
    "Evaluate(dataset_dir_path, data_files_dir, epochs, evaluate_result_dir_path, time_unit, window_size, gap, margin_rate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tmp=True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tmp==True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
