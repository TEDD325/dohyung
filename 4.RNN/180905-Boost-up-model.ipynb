{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/anaconda3/lib/python3.6/importlib/_bootstrap.py:219: RuntimeWarning: compiletime version 3.5 of module 'tensorflow.python.framework.fast_tensor_util' does not match runtime version 3.6\n",
      "  return f(*args, **kwds)\n",
      "/anaconda3/lib/python3.6/site-packages/h5py/__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n",
      "Using TensorFlow backend.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using Keras version: 2.2.2 backend: tensorflow\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/anaconda3/lib/python3.6/site-packages/sklearn/ensemble/weight_boosting.py:29: DeprecationWarning: numpy.core.umath_tests is an internal NumPy module and should not be imported. It will be removed in a future NumPy release.\n",
      "  from numpy.core.umath_tests import inner1d\n"
     ]
    }
   ],
   "source": [
    "# import library\n",
    "import tensorflow as tf\n",
    "\n",
    "import keras\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Dropout, Flatten, Activation\n",
    "from keras.layers import SimpleRNN, LSTM, GRU\n",
    "from keras import layers, models\n",
    "from keras.wrappers.scikit_learn import KerasClassifier\n",
    "from keras import backend as K\n",
    "from keras.utils import multi_gpu_model\n",
    "from keras import metrics\n",
    "from keras import losses\n",
    "from keras import __version__\n",
    "print('Using Keras version:', __version__, 'backend:', K.backend())\n",
    "# assert(LV(__version__) >= LV(\"2.0.0\"))\n",
    "from keras.backend.tensorflow_backend import set_session\n",
    "# config = tf.ConfigProto()\n",
    "# config.gpu_options.allow_growth = True\n",
    "# sess = tf.Session(config=config)\n",
    "# set_session(sess)\n",
    "# cfg = K.tf.ConfigProto()\n",
    "# cfg.gpu_options.allow_growth = True\n",
    "# K.set_session(K.tf.Session(config=cfg))\n",
    "\n",
    "# import os\n",
    "# os.environ[\"CUDA_DEVICE_ORDER\"]=\"PCI_BUS_ID\"\n",
    "# # The GPU id to use, usually either \"0\" or \"1\"\n",
    "# os.environ[\"CUDA_VISIBLE_DEVICES\"]=\"0\" \n",
    "\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "# from sklearn.pipeline import Pipeline\n",
    "from sklearn.preprocessing import MinMaxScaler, StandardScaler, PolynomialFeatures\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "from sklearn.metrics import precision_recall_fscore_support\n",
    "from sklearn.metrics import confusion_matrix, f1_score, precision_score, recall_score\n",
    "from sklearn.metrics import classification_report # https://stackoverflow.com/questions/50065484/getting-precision-recall-and-f1-score-per-class-in-keras\n",
    "\n",
    "from IPython.display import Javascript\n",
    "import numpy as np\n",
    "from distutils.version import LooseVersion as LV\n",
    "from datetime import datetime\n",
    "import csv\n",
    "import sys\n",
    "from os import listdir\n",
    "from os.path import isfile, join\n",
    "import pprint\n",
    "#import boto3\n",
    "import pickle\n",
    "import time\n",
    "import os.path\n",
    "import pickle\n",
    "sys.path.append(os.getcwd())\n",
    "from matplotlib import pyplot\n",
    "\n",
    "import smtplib\n",
    "from email.mime.text import MIMEText # simple MSG\n",
    "from email.mime.multipart import MIMEMultipart # complex MSG\n",
    "        \n",
    "from link_aws_key import *\n",
    "pp = pprint.PrettyPrinter(indent=4)\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "coins = {\n",
    "    0: 'KRW',\n",
    "    1: 'BTC',\n",
    "    2: 'ETH',\n",
    "    3: 'XRP',\n",
    "    4: 'BCH',\n",
    "    5: 'LTC',\n",
    "    6: 'DASH',\n",
    "    7: 'ETC'\n",
    "}\n",
    "\n",
    "# aws_client = boto3.client(\n",
    "#     's3',\n",
    "#     aws_access_key_id=LINK_AWSAccessKeyId,\n",
    "#     aws_secret_access_key=LINK_AWSSecretKey\n",
    "# )\n",
    "\n",
    "bucket = \"bithumb10\"\n",
    "cleanup_file_name = \"coin_{0}_{1}_cleanup.csv\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "def get_all_raw_data_from_aws(coin_name_list, start_date, end_date):\n",
    "    start_ms_time = datetime.strptime(start_date + \" +0900\", \"%Y-%m-%d %H:%M:%S %z\").timestamp() * 1000\n",
    "    end_ms_time = datetime.strptime(end_date + \" +0900\", \"%Y-%m-%d %H:%M:%S %z\").timestamp() * 1000\n",
    "    \n",
    "    year_temp = start_date[:4]\n",
    "    years = [year_temp]\n",
    "    while year_temp < end_date[:4]:\n",
    "        year_temp = str(int(start_date[:4]) + 1)\n",
    "        years.append(year_temp)\n",
    "    raw_data = {}  # 전체 CSV Raw 데이터\n",
    "    for coin_name in coin_name_list:\n",
    "        raw_data[coin_name] = []\n",
    "\n",
    "    # KRW 제외한 나머지 CSV Raw 데이터 수집\n",
    "    for coin_name in coin_name_list:\n",
    "        if coin_name == 'KRW':\n",
    "            continue\n",
    "        lines = []\n",
    "        for year in years:\n",
    "            obj = aws_client.get_object(\n",
    "                Bucket=bucket,\n",
    "                Key='cleanup/' + year + '/' + cleanup_file_name.format(coin_name, year)\n",
    "            )\n",
    "            if lines != []:\n",
    "                lines += obj.get('Body')._raw_stream.readlines()\n",
    "            else:\n",
    "                lines = obj.get('Body')._raw_stream.readlines()\n",
    "\n",
    "        for line in lines:\n",
    "            line = str(line.strip())[2:-1]\n",
    "            line = line.split(',')\n",
    "            if start_ms_time <= int(line[0]) and int(line[0]) <= end_ms_time:\n",
    "                raw_data[coin_name].append(line)\n",
    "\n",
    "    raw_data['KRW'] = list()\n",
    "    for line in raw_data['BTC']:\n",
    "        raw_data['KRW'].append([line[0], line[1], 1, 1, 1, 1, 1.0, 'normal'])\n",
    "\n",
    "    return raw_data\n",
    "\n",
    "def get_all_raw_data(coin_name_list, start_date, end_date, data_files_dir):\n",
    "    trading_files = []\n",
    "    for coin_name in coin_name_list:\n",
    "        for data_file_name in [f for f in listdir(data_files_dir) if isfile(join(data_files_dir, f))]:\n",
    "            if coin_name in data_file_name:\n",
    "                trading_files.append(data_file_name)\n",
    "\n",
    "    start_ms_time = datetime.strptime(start_date, \"%Y-%m-%d %H:%M:%S\").timestamp() * 1000\n",
    "    end_ms_time = datetime.strptime(end_date, \"%Y-%m-%d %H:%M:%S\").timestamp() * 1000\n",
    "    \n",
    "    raw_data = {} #전체 CSV Raw 데이터\n",
    "    for coin_name in coin_name_list:\n",
    "        raw_data[coin_name] = []\n",
    "    \n",
    "    #KRW 제외한 나머지 CSV Raw 데이터 수집\n",
    "    for coin_name in coin_name_list:\n",
    "        for data_file_name in trading_files:\n",
    "            if coin_name in data_file_name:\n",
    "                file = open(data_files_dir + data_file_name, 'r', encoding='utf-8')\n",
    "                rdr = csv.reader(file)\n",
    "                for line in rdr:\n",
    "                    if start_ms_time <= int(line[0]) and int(line[0]) <= end_ms_time:\n",
    "                        raw_data[coin_name].append(line)\n",
    "                file.close()\n",
    "    \n",
    "    for line in raw_data['BTC']:\n",
    "        raw_data['KRW'].append([line[0], line[1], 1, 1, 1, 1, 1.0, 'normal'])\n",
    "#     print(\"test\")\n",
    "    return raw_data\n",
    "\n",
    "def Make_Dataset(dir_path, coin_list, start_date, end_date, time_unit, window_size, gap, margin_rate):\n",
    "    print(\"Make_Dataset is Started.\")\n",
    "    a = time.time()\n",
    "\n",
    "    X = {}\n",
    "    y = {}\n",
    "    idx = []\n",
    "\n",
    "    key_name_X = \"X_\"\n",
    "    key_name_y = \"y_\"\n",
    "    for idx_time_unit in time_unit:\n",
    "        for idx_window_size in window_size:\n",
    "            for idx_gap in gap:\n",
    "                for idx_margin_rate in margin_rate:\n",
    "                    key_name_X += str(idx_time_unit) + \"_\" + str(idx_window_size) + \"_\" + str(idx_gap) + \"_\" + str(idx_margin_rate)\n",
    "                    key_name_y += str(idx_time_unit) + \"_\" + str(idx_window_size) + \"_\" + str(idx_gap) + \"_\" + str(idx_margin_rate)\n",
    "                    if (os.path.isfile(dir_path+key_name_X+\".pickle\")) is not True:\n",
    "                        print(key_name_X)\n",
    "                        X[key_name_X] = \\\n",
    "                        make_cryptocurrency_dataset(\n",
    "                                                    coin_list, \n",
    "                                                    start_date, \n",
    "                                                    end_date,\n",
    "                                                    idx_time_unit,\n",
    "                                                    idx_window_size,\n",
    "                                                    idx_gap,\n",
    "                                                    idx_margin_rate,\n",
    "                                                    data_files_dir)\n",
    "                        with open(dir_path + key_name_X+\".pickle\", 'wb') as handle:\n",
    "                            pickle.dump(X[key_name_X], handle, protocol=pickle.HIGHEST_PROTOCOL)\n",
    "                        X = {}\n",
    "                        print(\"X_success.\")\n",
    "                    if (os.path.isfile(dir_path + key_name_y+\".pickle\")) is not True:\n",
    "                        print(key_name_y)\n",
    "                        y[key_name_y] = \\\n",
    "                        make_cryptocurrency_dataset(\n",
    "                                                    coin_list, \n",
    "                                                    start_date, \n",
    "                                                    end_date,\n",
    "                                                    idx_time_unit,\n",
    "                                                    idx_window_size,\n",
    "                                                    idx_gap,\n",
    "                                                    idx_margin_rate,\n",
    "                                                    data_files_dir)\n",
    "                        with open(dir_path + key_name_y+\".pickle\", 'wb') as handle:\n",
    "                            pickle.dump(y[key_name_y], handle, protocol=pickle.HIGHEST_PROTOCOL)\n",
    "                        y = {}\n",
    "                        print(\"y_success.\")\n",
    "                    key_name_X = \"X_\"\n",
    "                    key_name_y = \"y_\"\n",
    "    b = time.time()\n",
    "    print(\"Make_Dataset is Done.\")\n",
    "    #print(\"time: \", b-a)\n",
    "\n",
    "def make_cryptocurrency_dataset(coin_name_list, start_date, end_date, time_unit, window_size, gap, margin_rate, data_files_dir):\n",
    "    y_trv = []\n",
    "    y_btv = []\n",
    "    num_coins = len(coin_name_list)\n",
    "    #raw_data = get_all_raw_data_from_aws(coin_name_list, start_date, end_date)\n",
    "    raw_data = get_all_raw_data(coin_name_list, start_date, end_date, data_files_dir)    \n",
    "    num_sticks = len(raw_data['BTC'])\n",
    "    \n",
    "    if time_unit % 10 != 0 or num_sticks < (time_unit / 10) * window_size + gap:\n",
    "        return None, None\n",
    "    \n",
    "    num = int(num_sticks - ((time_unit / 10) * window_size + gap) + 1)\n",
    "        \n",
    "    X = []\n",
    "    y = []\n",
    "    # (윈도우 개수, 코인 개수, 윈도우 사이즈, 3)\n",
    "    for idx in range(num):\n",
    "        X.append([])\n",
    "        y.append([])\n",
    "        idx_coin = 0\n",
    "        for coin_name in coin_name_list:\n",
    "            X[idx].append([])\n",
    "            \n",
    "            last_idx_in_window = int(idx + time_unit / 10 * window_size - 1)\n",
    "            close_price_in_last_idx_in_window = float(raw_data[coin_name][last_idx_in_window][3])\n",
    "            \n",
    "            for idx_in_window in range(window_size):\n",
    "                X[idx][idx_coin].append([])\n",
    "                idx_stick = int(idx + time_unit / 10 * (idx_in_window + 1) - 1)\n",
    "                X[idx][idx_coin][idx_in_window].append(float(raw_data[coin_name][idx_stick][3]) / close_price_in_last_idx_in_window)\n",
    "                X[idx][idx_coin][idx_in_window].append(float(raw_data[coin_name][idx_stick][4]) / close_price_in_last_idx_in_window)\n",
    "                X[idx][idx_coin][idx_in_window].append(float(raw_data[coin_name][idx_stick][5]) / close_price_in_last_idx_in_window)\n",
    "                X[idx][idx_coin][idx_in_window].append(float(raw_data[coin_name][idx_stick][6]))\n",
    "                \n",
    "            target_idx_for_window = int(idx + time_unit / 10 * window_size - 1 + gap)\n",
    "            target_price = float(raw_data[coin_name][target_idx_for_window][3])\n",
    "            \n",
    "            target = 0\n",
    "            if target_price >= close_price_in_last_idx_in_window * (1.0 + float(margin_rate) / 100.0):\n",
    "                target = 1\n",
    "            y[idx].append(target)\n",
    "            \n",
    "            idx_coin += 1\n",
    "           \n",
    "    X = np.asarray(X)\n",
    "    y = np.asarray(y)\n",
    "    \n",
    "    return X, y\n",
    "    \n",
    "def make_cryptocurrency_dataset_X(coin_name_list, start_date, end_date, time_unit, window_size, gap, margin_rate, data_files_dir):\n",
    "    num_coins = len(coin_name_list)\n",
    "    #raw_data = get_all_raw_data_from_aws(coin_name_list, start_date, end_date)\n",
    "    raw_data = get_all_raw_data(coin_name_list, start_date, end_date, data_files_dir)    \n",
    "    num_sticks = len(raw_data['BTC'])\n",
    "    \n",
    "    if time_unit % 10 != 0 or num_sticks < (time_unit / 10) * window_size + gap:\n",
    "        return None, None\n",
    "    \n",
    "    num = int(num_sticks - ((time_unit / 10) * window_size + gap) + 1)\n",
    "        \n",
    "    X = []\n",
    "    # (윈도우 개수, 코인 개수, 윈도우 사이즈, 3)\n",
    "    for idx in range(num):\n",
    "        X.append([])\n",
    "        idx_coin = 0\n",
    "        for coin_name in coin_name_list:\n",
    "            X[idx].append([])\n",
    "            \n",
    "            last_idx_in_window = int(idx + time_unit / 10 * window_size - 1)\n",
    "            close_price_in_last_idx_in_window = float(raw_data[coin_name][last_idx_in_window][3])\n",
    "            \n",
    "            for idx_in_window in range(window_size):\n",
    "                X[idx][idx_coin].append([])\n",
    "                idx_stick = int(idx + time_unit / 10 * (idx_in_window + 1) - 1)\n",
    "                X[idx][idx_coin][idx_in_window].append(float(raw_data[coin_name][idx_stick][3]) / close_price_in_last_idx_in_window)\n",
    "                X[idx][idx_coin][idx_in_window].append(float(raw_data[coin_name][idx_stick][4]) / close_price_in_last_idx_in_window)\n",
    "                X[idx][idx_coin][idx_in_window].append(float(raw_data[coin_name][idx_stick][5]) / close_price_in_last_idx_in_window)\n",
    "                X[idx][idx_coin][idx_in_window].append(float(raw_data[coin_name][idx_stick][6]))\n",
    "                \n",
    "    X = np.array(X)\n",
    "    \n",
    "    return X\n",
    "\n",
    "def make_cryptocurrency_dataset_y(coin_name_list, start_date, end_date, time_unit, window_size, gap, margin_rate, data_files_dir):\n",
    "    y_trv = []\n",
    "    y_btv = []\n",
    "    num_coins = len(coin_name_list)\n",
    "    #raw_data = get_all_raw_data_from_aws(coin_name_list, start_date, end_date)\n",
    "    raw_data = get_all_raw_data(coin_name_list, start_date, end_date, data_files_dir)    \n",
    "    num_sticks = len(raw_data['BTC'])\n",
    "    \n",
    "    if time_unit % 10 != 0 or num_sticks < (time_unit / 10) * window_size + gap:\n",
    "        return None, None\n",
    "    \n",
    "    num = int(num_sticks - ((time_unit / 10) * window_size + gap) + 1)\n",
    "        \n",
    "    \n",
    "    y = []\n",
    "    # (윈도우 개수, 코인 개수, 윈도우 사이즈, 3)\n",
    "    for idx in range(num):\n",
    "        \n",
    "        y.append([])\n",
    "        idx_coin = 0\n",
    "        for coin_name in coin_name_list:            \n",
    "            last_idx_in_window = int(idx + time_unit / 10 * window_size - 1)\n",
    "            close_price_in_last_idx_in_window = float(raw_data[coin_name][last_idx_in_window][3])\n",
    "            target_idx_for_window = int(idx + time_unit / 10 * window_size - 1 + gap)\n",
    "            target_price = float(raw_data[coin_name][target_idx_for_window][3])\n",
    "            target = 0\n",
    "            \n",
    "            if target_price >= close_price_in_last_idx_in_window * (1.0 + float(margin_rate) / 100.0):\n",
    "                target = 1\n",
    "            y[idx].append(target)\n",
    "            \n",
    "            idx_coin += 1\n",
    "           \n",
    "    y = np.asarray(y)\n",
    "    \n",
    "    return y\n",
    "\n",
    "#\n",
    "\n",
    "def Load_Dataset_X(dir_path, time_unit, window_size, gap, margin_rate):\n",
    "    key_name_X = \"X_\" + \\\n",
    "                    str(time_unit) + \"_\" + \\\n",
    "                    str(window_size) + \"_\" + \\\n",
    "                    str(gap) + \"_\" + \\\n",
    "                    str(margin_rate)\n",
    "                    \n",
    "\n",
    "    with open(dir_path + key_name_X + \".pickle\", 'rb') as handle:\n",
    "        b_x = pickle.load(handle)\n",
    "    return b_x\n",
    "    \n",
    "def Load_Dataset_y(dir_path, time_unit, window_size, gap, margin_rate):\n",
    "    key_name_y = \"y_\" + \\\n",
    "                    str(time_unit) + \"_\" + \\\n",
    "                    str(window_size) + \"_\" + \\\n",
    "                    str(gap) + \"_\" + \\\n",
    "                    str(margin_rate)\n",
    "                    \n",
    "\n",
    "    with open(dir_path + key_name_y + \".pickle\", 'rb') as handle:\n",
    "        b_y = pickle.load(handle)\n",
    "    return b_y\n",
    "    \n",
    "def Make_Dataset_numpy(dir_path, coin_list, start_date, end_date, time_unit, window_size, gap, margin_rate, data_files_dir):\n",
    "    print(\"Make_Dataset_numpy is Started.\")\n",
    "    a = time.time()\n",
    "\n",
    "    X = {}\n",
    "    y = {}\n",
    "    idx = []\n",
    "\n",
    "    key_name_X = \"X_\"\n",
    "    key_name_y = \"y_\"\n",
    "    for idx_time_unit in time_unit:\n",
    "        for idx_window_size in window_size:\n",
    "            for idx_gap in gap:\n",
    "                for idx_margin_rate in margin_rate:\n",
    "                    key_name_X += str(idx_time_unit) + \"_\" + str(idx_window_size) + \"_\" + str(idx_gap) + \"_\" + str(idx_margin_rate)\n",
    "                    key_name_y += str(idx_time_unit) + \"_\" + str(idx_window_size) + \"_\" + str(idx_gap) + \"_\" + str(idx_margin_rate)\n",
    "                    if (os.path.isfile(dir_path + key_name_X+\".pickle\")) is not True:\n",
    "                        print(key_name_X)\n",
    "                        X[key_name_X] = \\\n",
    "                        make_cryptocurrency_dataset_X(\n",
    "                                                    coin_list, \n",
    "                                                    start_date, \n",
    "                                                    end_date,\n",
    "                                                    idx_time_unit,\n",
    "                                                    idx_window_size,\n",
    "                                                    idx_gap,\n",
    "                                                    idx_margin_rate,\n",
    "                                                    data_files_dir)\n",
    "                        with open(dir_path + key_name_X+\".pickle\", 'wb') as handle:\n",
    "                            pickle.dump(X[key_name_X], handle, protocol=pickle.HIGHEST_PROTOCOL)\n",
    "                        X = {}\n",
    "                        print(\"X_success.\")\n",
    "                    if (os.path.isfile(dir_path+key_name_y+\".pickle\")) is not True:\n",
    "                        print(key_name_y)\n",
    "                        y[key_name_y] = \\\n",
    "                        make_cryptocurrency_dataset_y(\n",
    "                                                    coin_list, \n",
    "                                                    start_date, \n",
    "                                                    end_date,\n",
    "                                                    idx_time_unit,\n",
    "                                                    idx_window_size,\n",
    "                                                    idx_gap,\n",
    "                                                    idx_margin_rate,\n",
    "                                                    data_files_dir)\n",
    "                        with open(dir_path + key_name_y+\".pickle\", 'wb') as handle:\n",
    "                            pickle.dump(y[key_name_y], handle, protocol=pickle.HIGHEST_PROTOCOL)\n",
    "                        y = {}\n",
    "                        print(\"y_success.\")\n",
    "                    key_name_X = \"X_\"\n",
    "                    key_name_y = \"y_\"\n",
    "    b = time.time()\n",
    "    print(\"Make_Dataset_numpy is Done.\")\n",
    "    print()\n",
    "    #print(\"time: \", b-a)\n",
    "    \n",
    "    \n",
    "def Make_Dataset_tuple(dir_path, coin_list, start_date, end_date, time_unit, window_size, gap, margin_rate, data_files_dir):\n",
    "    print(\"Make_Dataset_tuple is Started.\")\n",
    "    a = time.time()\n",
    "\n",
    "    X = {}\n",
    "    y = {}\n",
    "    idx = []\n",
    "\n",
    "    key_name_X = \"X_\"\n",
    "    key_name_y = \"y_\"\n",
    "    for idx_time_unit in time_unit:\n",
    "        for idx_window_size in window_size:\n",
    "            for idx_gap in gap:\n",
    "                for idx_margin_rate in margin_rate:\n",
    "                    key_name_X += str(idx_time_unit) + \"_\" + str(idx_window_size) + \"_\" + str(idx_gap) + \"_\" + str(idx_margin_rate)\n",
    "                    key_name_y += str(idx_time_unit) + \"_\" + str(idx_window_size) + \"_\" + str(idx_gap) + \"_\" + str(idx_margin_rate)\n",
    "                    if (os.path.isfile(dir_path + key_name_X+\".pickle\")) is not True:\n",
    "                        print(key_name_X)\n",
    "                        X[key_name_X], _ = \\\n",
    "                        make_cryptocurrency_dataset(\n",
    "                                                    coin_list, \n",
    "                                                    start_date, \n",
    "                                                    end_date,\n",
    "                                                    idx_time_unit,\n",
    "                                                    idx_window_size,\n",
    "                                                    idx_gap,\n",
    "                                                    idx_margix_rate,\n",
    "                                                    data_files_dir)\n",
    "                        with open(dir_path + key_name_X+\".pickle\", 'wb') as handle:\n",
    "                            pickle.dump(X[key_name_X], handle, protocol=pickle.HIGHEST_PROTOCOL)\n",
    "                        X = {}\n",
    "                        print(\"X_success.\")\n",
    "                    if (os.path.isfile(dir_path+key_name_y+\".pickle\")) is not True:\n",
    "                        print(key_name_y)\n",
    "                        _, y[key_name_y] = \\\n",
    "                        make_cryptocurrency_dataset(\n",
    "                                                    coin_list, \n",
    "                                                    start_date, \n",
    "                                                    end_date,\n",
    "                                                    idx_time_unit,\n",
    "                                                    idx_window_size,\n",
    "                                                    idx_gap,\n",
    "                                                    idx_margix_rate,\n",
    "                                                    data_files_dir)\n",
    "                        with open(dir_path + key_name_y+\".pickle\", 'wb') as handle:\n",
    "                            pickle.dump(y[key_name_y], handle, protocol=pickle.HIGHEST_PROTOCOL)\n",
    "                        y = {}\n",
    "                        print(\"y_success.\")\n",
    "                    key_name_X = \"X_\"\n",
    "                    key_name_y = \"y_\"\n",
    "    b = time.time()\n",
    "    print(\"Make_Dataset_tuple is Done.\")\n",
    "    print()\n",
    "    #print(\"time: \", b-a)\n",
    "\n",
    "def Check_Dataset(dir_path, coin_list, start_date, end_date, time_unit, window_size, gap, margin_rate, data_files_dir):\n",
    "    print(\"Check_Dataset is Started.\")\n",
    "    a = time.time()\n",
    "\n",
    "    X = {}\n",
    "    y = {}\n",
    "    idx = []\n",
    "\n",
    "    key_name_X = \"X_\"\n",
    "    key_name_y = \"y_\"\n",
    "    for idx_time_unit in time_unit:\n",
    "        for idx_window_size in window_size:\n",
    "            for idx_gap in gap:\n",
    "                for idx_margix_rate in margin_rate:\n",
    "                    key_name_X += str(idx_time_unit) + \"_\" + str(idx_window_size) + \"_\" + str(idx_gap) + \"_\" + str(idx_margix_rate)\n",
    "                    key_name_y += str(idx_time_unit) + \"_\" + str(idx_window_size) + \"_\" + str(idx_gap) + \"_\" + str(idx_margix_rate)\n",
    "                    \n",
    "                    with open(dir_path + key_name_X+\".pickle\", 'rb') as handle:\n",
    "                        data = pickle.load(handle)\n",
    "    \n",
    "                    if type(data) == tuple:\n",
    "#                         print()\n",
    "                        os.system('rm '+dir_path + key_name_X+\".pickle\")\n",
    "                        print(key_name_X,\".pickle is removed.\")\n",
    "                        os.system('rm '+dir_path + key_name_y+\".pickle\")\n",
    "                        print(key_name_y,\".pickle is removed.\")\n",
    "        \n",
    "                    if (os.path.isfile(dir_path+key_name_X+\".pickle\")) is not True:\n",
    "                        print(key_name_X)\n",
    "                        X[key_name_X] = \\\n",
    "                        make_cryptocurrency_dataset_X(\n",
    "                                                    coin_list, \n",
    "                                                    start_date, \n",
    "                                                    end_date,\n",
    "                                                    idx_time_unit,\n",
    "                                                    idx_window_size,\n",
    "                                                    idx_gap,\n",
    "                                                    idx_margix_rate,\n",
    "                                                    data_files_dir)\n",
    "                        with open(dir_path + key_name_X+\".pickle\", 'wb') as handle:\n",
    "                            pickle.dump(X[key_name_X], handle, protocol=pickle.HIGHEST_PROTOCOL)\n",
    "                        X = {}\n",
    "                        print(\"[SUCCESS] \",key_name_X,\".pickle is created.\")\n",
    "                    if (os.path.isfile(dir_path+key_name_y+\".pickle\")) is not True:\n",
    "                        print(key_name_y)\n",
    "                        y[key_name_y] = \\\n",
    "                        make_cryptocurrency_dataset_y(\n",
    "                                                    coin_list, \n",
    "                                                    start_date, \n",
    "                                                    end_date,\n",
    "                                                    idx_time_unit,\n",
    "                                                    idx_window_size,\n",
    "                                                    idx_gap,\n",
    "                                                    idx_margix_rate,\n",
    "                                                    data_files_dir)\n",
    "                        with open(dir_path + key_name_y+\".pickle\", 'wb') as handle:\n",
    "                            pickle.dump(y[key_name_y], handle, protocol=pickle.HIGHEST_PROTOCOL)\n",
    "                        y = {}\n",
    "                        print(\"[SUCCESS] \",key_name_y,\".pickle is created.\")\n",
    "                    key_name_X = \"X_\"\n",
    "                    key_name_y = \"y_\"\n",
    "    b = time.time()\n",
    "    print(\"Check_Dataset is Done.\")\n",
    "    #print(\"time: \", b-a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create Dataset\n",
    "data_files_dir = \"/Users/dohyung/OneDrive/2018-RNN/RNN_python/AWS_dataset/\"\n",
    "dataset_dir_path_tuple_type = \"./dataset_pickle_tuple_type/\"\n",
    "dataset_dir_path_numpy_type = \"./dataset_pickle_numpy.ndarray_type)/\"\n",
    "coin_list = [\"KRW\", \"BTC\", \"ETH\", \"XRP\", \"BCH\", \"LTC\", \"DASH\", \"ETC\"]\n",
    "start_date = \"2017-08-04 21:40:00\"\n",
    "end_date = \"2018-08-20 23:50:00\"\n",
    "# time_unit = [10,30,60]     # candle stick minutes\n",
    "# window_size = [10,25,50,75,100]  # Unit: num. of candle sticks\n",
    "# gap = [1,2,3]            # Unit: num. of candle sticks\n",
    "# margin_rate = [0.1,0.25,0.5]  # Unit: percent\n",
    "\n",
    "# for slave04\n",
    "#time_unit = [10,30,60]     # candle stick minutes\n",
    "#window_size = [10,25,50]  # Unit: num. of candle sticks\n",
    "#gap = [1,2,3]            # Unit: num. of candle sticks\n",
    "#margin_rate = [0.1,0.25,0.5]  # Unit: percent\n",
    "\n",
    "# for slave05\n",
    "# time_unit = [10,30,60]     # candle stick minutes\n",
    "# window_size = [75,100]  # Unit: num. of candle sticks\n",
    "# gap = [1,2,3]            # Unit: num. of candle sticks\n",
    "# margin_rate = [0.1,0.25,0.5]  # Unit: percent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#Make_Dataset_tuple(dataset_dir_path_tuple_type, coin_list, start_date, end_date, time_unit, window_size, gap, margin_rate)\n",
    "#Make_Dataset_numpy(dataset_dir_path_numpy_type, coin_list, start_date, end_date, time_unit, window_size, gap, margin_rate)\n",
    "#Check_Dataset(dataset_dir_path, coin_list, start_date, end_date, time_unit, window_size, gap, margin_rate)\n",
    "# Tuple 형태의 데이터셋이 나오지 않도록."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_history(history):\n",
    "    loss_list = [s for s in history.history.keys() if 'loss' in s and 'val' not in s]\n",
    "    val_loss_list = [s for s in history.history.keys() if 'loss' in s and 'val' in s]\n",
    "    acc_list = [s for s in history.history.keys() if 'acc' in s and 'val' not in s]\n",
    "    val_acc_list = [s for s in history.history.keys() if 'acc' in s and 'val' in s]\n",
    "    \n",
    "    if len(loss_list) == 0:\n",
    "        print('Loss is missing in history')\n",
    "        return \n",
    "    \n",
    "    ## As loss always exists\n",
    "    epochs = range(1,len(history.history[loss_list[0]]) + 1)\n",
    "    \n",
    "    ## Loss\n",
    "    plt.figure(1)\n",
    "    for l in loss_list:\n",
    "        plt.plot(epochs, history.history[l], 'b', label='Training loss (' + str(str(format(history.history[l][-1],'.5f'))+')'))\n",
    "    for l in val_loss_list:\n",
    "        plt.plot(epochs, history.history[l], 'g', label='Validation loss (' + str(str(format(history.history[l][-1],'.5f'))+')'))\n",
    "    \n",
    "    plt.title('Loss')\n",
    "    plt.xlabel('Epochs')\n",
    "    plt.ylabel('Loss')\n",
    "    plt.legend()\n",
    "    \n",
    "    ## Accuracy\n",
    "    plt.figure(2)\n",
    "    for l in acc_list:\n",
    "        plt.plot(epochs, history.history[l], 'b', label='Training accuracy (' + str(format(history.history[l][-1],'.5f'))+')')\n",
    "    for l in val_acc_list:    \n",
    "        plt.plot(epochs, history.history[l], 'g', label='Validation accuracy (' + str(format(history.history[l][-1],'.5f'))+')')\n",
    "\n",
    "    plt.title('Accuracy')\n",
    "    plt.xlabel('Epochs')\n",
    "    plt.ylabel('Accuracy')\n",
    "    plt.legend()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def f1_score(y_true, y_pred):\n",
    "    def recall(y_true, y_pred):\n",
    "        \"\"\"Recall metric.\n",
    "\n",
    "        Only computes a batch-wise average of recall.\n",
    "\n",
    "        Computes the recall, a metric for multi-label classification of\n",
    "        how many relevant items are selected.\n",
    "        \"\"\"\n",
    "        true_positives = K.sum(K.round(K.clip(y_true * y_pred, 0, 1)))\n",
    "        possible_positives = K.sum(K.round(K.clip(y_true, 0, 1)))\n",
    "        recall = true_positives / (possible_positives + K.epsilon())\n",
    "        return recall\n",
    "\n",
    "    def precision(y_true, y_pred):\n",
    "        \"\"\"Precision metric.\n",
    "\n",
    "        Only computes a batch-wise average of precision.\n",
    "\n",
    "        Computes the precision, a metric for multi-label classification of\n",
    "        how many selected items are relevant.\n",
    "        \"\"\"\n",
    "        true_positives = K.sum(K.round(K.clip(y_true * y_pred, 0, 1)))\n",
    "        predicted_positives = K.sum(K.round(K.clip(y_pred, 0, 1)))\n",
    "        precision = true_positives / (predicted_positives + K.epsilon())\n",
    "        return precision\n",
    "    precision = precision(y_true, y_pred)\n",
    "    recall = recall(y_true, y_pred)\n",
    "    return 2*((precision*recall)/(precision+recall+K.epsilon()))\n",
    "\n",
    "def f1_score_(y_true, y_pred):\n",
    "\n",
    "    # Count positive samples.\n",
    "    c1 = K.sum(K.round(K.clip(y_true * y_pred, 0, 1)))\n",
    "    c2 = K.sum(K.round(K.clip(y_pred, 0, 1)))\n",
    "    c3 = K.sum(K.round(K.clip(y_true, 0, 1)))\n",
    "\n",
    "    # If there are no true samples, fix the F1 score at 0.\n",
    "    if c3 == 0:\n",
    "        return 0\n",
    "\n",
    "    # How many selected items are relevant?\n",
    "    precision = c1 / (c2 + 1e-7)\n",
    "    \n",
    "    # How many relevant items are selected?\n",
    "    recall = c1 / (c3 +  + 1e-7)\n",
    "\n",
    "    # Calculate f1_score\n",
    "    f1_score = 2 * (precision * recall) / ((precision + recall) + 1e-7)\n",
    "    return f1_score \n",
    "\n",
    "\n",
    "def create_model_RNN(window_size, n_state_units=32, activation='softmax', optimizer='adam', init='glorot_uniform', dropout_rate=0.0, neurons=2):\n",
    "    model = Sequential()\n",
    "    model.add(# if문을 통해 여러 RNN모델 쓸 수 있도록 하기, SimpleRNN외에 다른 RNN모델 찾아보기\n",
    "        RNN(  n_state_units, \n",
    "              input_shape=(window_size, 32),\n",
    "              use_bias=True, \n",
    "              activation='tanh',\n",
    "              kernel_initializer='glorot_uniform', \n",
    "              recurrent_initializer='orthogonal', \n",
    "              bias_initializer='zeros', \n",
    "              dropout=0.0,\n",
    "              recurrent_dropout=0.0))\n",
    "    \n",
    "    model.add(Dense(units=neurons))\n",
    "    model.add(Dropout(dropout_rate))\n",
    "#     model.add(Dense(units=2))\n",
    "    model = multi_gpu_model(model, gpus=2)\n",
    "\n",
    "    model.compile(loss=keras.losses.categorical_crossentropy, \n",
    "                  optimizer=optimizer, \n",
    "                  metrics=[\"accuracy\", f1_score])\n",
    "\n",
    "    return model\n",
    "\n",
    "\n",
    "def create_model_SimpleRNN(window_size, n_state_units=32, activation='softmax', optimizer='adam', init='glorot_uniform', dropout_rate=0.0, neurons=2):\n",
    "    model = Sequential()\n",
    "    model.add(# if문을 통해 여러 RNN모델 쓸 수 있도록 하기, SimpleRNN외에 다른 RNN모델 찾아보기\n",
    "        SimpleRNN(n_state_units, \n",
    "                  input_shape=(window_size, 32),\n",
    "                  use_bias=True, \n",
    "                  activation='tanh',\n",
    "                  kernel_initializer='glorot_uniform', \n",
    "                  recurrent_initializer='orthogonal', \n",
    "                  bias_initializer='zeros', \n",
    "                  dropout=0.0,\n",
    "                  recurrent_dropout=0.0))\n",
    "    \n",
    "    model.add(Dense(units=neurons))\n",
    "    model.add(Dropout(dropout_rate))\n",
    "#     model.add(Dense(units=2))\n",
    "    model = multi_gpu_model(model, gpus=2)\n",
    "\n",
    "    model.compile(loss=keras.losses.categorical_crossentropy, \n",
    "                  optimizer=optimizer, \n",
    "                  metrics=[\"accuracy\", f1_score])\n",
    "        \n",
    "    return model\n",
    "\n",
    "def create_model_LSTM(window_size, n_state_units=32, activation='softmax', optimizer='adam', init='glorot_uniform', dropout_rate=0.0, neurons=2):\n",
    "    model = Sequential()\n",
    "    model.add(# if문을 통해 여러 RNN모델 쓸 수 있도록 하기, SimpleRNN외에 다른 RNN모델 찾아보기\n",
    "        LSTM( n_state_units, \n",
    "              input_shape=(window_size, 32),\n",
    "              use_bias=True, \n",
    "              activation='tanh',\n",
    "              kernel_initializer='glorot_uniform', \n",
    "              recurrent_initializer='orthogonal', \n",
    "              bias_initializer='zeros', \n",
    "              dropout=0.0,\n",
    "              recurrent_dropout=0.0))\n",
    "    \n",
    "    model.add(Dense(units=neurons))\n",
    "    model.add(Dropout(dropout_rate))\n",
    "#     model.add(Dense(units=2))\n",
    "    model = multi_gpu_model(model, gpus=2)\n",
    "\n",
    "    model.compile(loss=keras.losses.categorical_crossentropy, \n",
    "                  optimizer=optimizer, \n",
    "                  metrics=[\"accuracy\", f1_score])\n",
    "    \n",
    "    return model\n",
    "\n",
    "\n",
    "def create_model_GRU(window_size, n_state_units=32, activation='softmax', optimizer='adam', init='glorot_uniform', dropout_rate=0.0, neurons=2):\n",
    "    model = Sequential()\n",
    "    model.add(# if문을 통해 여러 RNN모델 쓸 수 있도록 하기, SimpleRNN외에 다른 RNN모델 찾아보기\n",
    "        GRU(  n_state_units, \n",
    "              input_shape=(window_size, 32),\n",
    "              use_bias=True, \n",
    "              activation='tanh',\n",
    "              kernel_initializer='glorot_uniform', \n",
    "              recurrent_initializer='orthogonal', \n",
    "              bias_initializer='zeros', \n",
    "              dropout=0.0,\n",
    "              recurrent_dropout=0.0))\n",
    "    \n",
    "    model.add(Dense(units=neurons))\n",
    "    model.add(Dropout(dropout_rate))\n",
    "#     model.add(Dense(units=2))\n",
    "    model = multi_gpu_model(model, gpus=2)\n",
    "\n",
    "    model.compile(loss=keras.losses.categorical_crossentropy, \n",
    "                  optimizer=optimizer, \n",
    "                  metrics=[\"accuracy\", f1_score])\n",
    "\n",
    "    return model\n",
    "\n",
    "\n",
    "def create_model_RNN_non_GPU(window_size, n_state_units=32, activation='softmax', optimizer='adam', init='glorot_uniform', dropout_rate=0.0, neurons=2):\n",
    "    model = Sequential()\n",
    "    model.add(# if문을 통해 여러 RNN모델 쓸 수 있도록 하기, SimpleRNN외에 다른 RNN모델 찾아보기\n",
    "        RNN(  n_state_units, \n",
    "              input_shape=(window_size, 32),\n",
    "              use_bias=True, \n",
    "              activation='tanh',\n",
    "              kernel_initializer='glorot_uniform', \n",
    "              recurrent_initializer='orthogonal', \n",
    "              bias_initializer='zeros', \n",
    "              dropout=0.0,\n",
    "              recurrent_dropout=0.0))\n",
    "    \n",
    "    model.add(Dense(units=neurons))\n",
    "    model.add(Dropout(dropout_rate))\n",
    "#     model.add(Dense(units=2))\n",
    "#     model = multi_gpu_model(model, gpus=2)\n",
    "\n",
    "    model.compile(loss=keras.losses.categorical_crossentropy, \n",
    "                  optimizer=optimizer, \n",
    "                  metrics=[\"accuracy\", f1_score])\n",
    "\n",
    "    return model\n",
    "\n",
    "\n",
    "def create_model_SimpleRNN_non_GPU(window_size, n_state_units=32, activation='softmax', optimizer='adam', init='glorot_uniform', dropout_rate=0.0, neurons=2):\n",
    "    model = Sequential()\n",
    "    model.add(# if문을 통해 여러 RNN모델 쓸 수 있도록 하기, SimpleRNN외에 다른 RNN모델 찾아보기\n",
    "        SimpleRNN(n_state_units, \n",
    "                  input_shape=(window_size, 32),\n",
    "                  use_bias=True, \n",
    "                  activation='tanh',\n",
    "                  kernel_initializer='glorot_uniform', \n",
    "                  recurrent_initializer='orthogonal', \n",
    "                  bias_initializer='zeros', \n",
    "                  dropout=0.0,\n",
    "                  recurrent_dropout=0.0))\n",
    "    \n",
    "    model.add(Dense(units=neurons))\n",
    "    model.add(Dropout(dropout_rate))\n",
    "#     model.add(Dense(units=2))\n",
    "#     model = multi_gpu_model(model, gpus=2)\n",
    "\n",
    "    model.compile(loss=keras.losses.categorical_crossentropy, \n",
    "                  optimizer=optimizer, \n",
    "                  metrics=[\"accuracy\", f1_score])\n",
    "\n",
    "    return model\n",
    "\n",
    "def create_model_LSTM_non_GPU(window_size, n_state_units=32, activation='softmax', optimizer='adam', init='glorot_uniform', dropout_rate=0.0, neurons=2):\n",
    "    model = Sequential()\n",
    "    model.add(# if문을 통해 여러 RNN모델 쓸 수 있도록 하기, SimpleRNN외에 다른 RNN모델 찾아보기\n",
    "        LSTM( n_state_units, \n",
    "              input_shape=(window_size, 32),\n",
    "              use_bias=True, \n",
    "              activation='tanh',\n",
    "              kernel_initializer='glorot_uniform', \n",
    "              recurrent_initializer='orthogonal', \n",
    "              bias_initializer='zeros', \n",
    "              dropout=0.0,\n",
    "              recurrent_dropout=0.0))\n",
    "    \n",
    "    model.add(Dense(units=neurons))\n",
    "    model.add(Dropout(dropout_rate))\n",
    "#     model.add(Dense(units=2))\n",
    "#     model = multi_gpu_model(model, gpus=2)\n",
    "\n",
    "    model.compile(loss=keras.losses.categorical_crossentropy, \n",
    "                  optimizer=optimizer, \n",
    "                  metrics=[\"accuracy\", f1_score])\n",
    "\n",
    "    return model\n",
    "\n",
    "\n",
    "def create_model_GRU_non_GPU(window_size, n_state_units=32, activation='softmax', optimizer='adam', init='glorot_uniform', dropout_rate=0.0, neurons=2):\n",
    "    model = Sequential()\n",
    "    model.add(# if문을 통해 여러 RNN모델 쓸 수 있도록 하기, SimpleRNN외에 다른 RNN모델 찾아보기\n",
    "        GRU(  n_state_units, \n",
    "              input_shape=(window_size, 32),\n",
    "              use_bias=True, \n",
    "              activation='tanh',\n",
    "              kernel_initializer='glorot_uniform', \n",
    "              recurrent_initializer='orthogonal', \n",
    "              bias_initializer='zeros', \n",
    "              dropout=0.0,\n",
    "              recurrent_dropout=0.0))\n",
    "    \n",
    "    model.add(Dense(units=neurons))\n",
    "    model.add(Dropout(dropout_rate))\n",
    "#     model.add(Dense(units=2))\n",
    "#     model = multi_gpu_model(model, gpus=2)\n",
    "\n",
    "    model.compile(loss=keras.losses.categorical_crossentropy, \n",
    "                  optimizer=optimizer, \n",
    "                  metrics=[\"accuracy\", f1_score])\n",
    "\n",
    "    return model\n",
    "\n",
    "\n",
    "\n",
    "def input_reshape(X_train_data, X_test_data, n_steps, n_coins, n_price):\n",
    "    X_train_reshape = X_train_data.reshape(\n",
    "        -1, \n",
    "        n_steps, \n",
    "        n_coins * n_price\n",
    "    )\n",
    "    X_test_reshape = X_test_data.reshape(\n",
    "        -1, \n",
    "        n_steps, \n",
    "        n_coins * n_price\n",
    "    )\n",
    "    return X_train_reshape, X_test_reshape\n",
    "\n",
    "def onehottify(x, n=None, dtype=np.int):\n",
    "    \"\"\"1-hot encode x with the max value n (computed from data if n is None).\"\"\"\n",
    "    x = np.asarray(x)\n",
    "    n = np.max(x) + 1 if n is None else n\n",
    "    return np.eye(n, dtype=dtype)[x]\n",
    "\n",
    "def Start_Model(pickle_load_dir_path, data_files_dir, epochs, pickle_result_dir_path, boost_up_result_dir_path, MODEL, idx_time_unit, idx_window_size, idx_gap, idx_margin_rate, _TEST, _ENHANCE, _GPU, n_jobs, machine, Internet_connection, params):\n",
    "    X = {}\n",
    "    y = {}\n",
    "    #if (_TEST == True): \n",
    "        #MODEL_list = [\"SimpleRNN\", \"LSTM\", \"GRU\"]\n",
    "    #    time_unit = [10]     # candle stick minutes\n",
    "    #    window_size = [10]  # Unit: num. of candle sticks\n",
    "    #    gap = [1]            # Unit: num. of candle sticks\n",
    "    #    margin_rate = [0.1]  # Unit: percent\n",
    "    #elif (_TEST == False):\n",
    "        #MODEL_list = [\"SimpleRNN\", \"LSTM\", \"GRU\"]\n",
    "\n",
    "    key_name_X = \"X_\"\n",
    "    key_name_y = \"y_\"\n",
    "\n",
    "    #for MODEL_idx in MODEL_list:\n",
    "        #MODEL = MODEL_idx\n",
    "        #for idx_time_unit in time_unit:\n",
    "                #for idx_window_size in window_size:\n",
    "                    #for idx_gap in gap:\n",
    "                        #for idx_margin_rate in margin_rate:\n",
    "    key_name_X += str(idx_time_unit) + \"_\" + str(idx_window_size) + \"_\" + str(idx_gap) + \"_\" + str(idx_margin_rate)\n",
    "    key_name_y += str(idx_time_unit) + \"_\" + str(idx_window_size) + \"_\" + str(idx_gap) + \"_\" + str(idx_margin_rate)\n",
    "\n",
    "    X = Load_Dataset_X(pickle_load_dir_path, idx_time_unit, idx_window_size, idx_gap, idx_margin_rate)\n",
    "    y = Load_Dataset_y(pickle_load_dir_path, idx_time_unit, idx_window_size, idx_gap, idx_margin_rate)\n",
    "\n",
    "    y_single = {}\n",
    "    y_single['BTC'] = y[:, 1]\n",
    "    y_single['ETH'] = y[:, 2]\n",
    "    y_single['XRP'] = y[:, 3]\n",
    "    y_single['BCH'] = y[:, 4]\n",
    "    y_single['LTC'] = y[:, 5]\n",
    "    y_single['DASH'] = y[:, 6]\n",
    "    y_single['ETC'] = y[:, 7]\n",
    "\n",
    "    coin_list2 = [\"BTC\", \"ETH\", \"XRP\", \"BCH\", \"LTC\", \"DASH\", \"ETC\"]\n",
    "\n",
    "#                         for coin in coin_list2:\n",
    "#                             print(\"y_single[\"+coin+\"]\"+\".shape\")\n",
    "#                             print(y_single[coin].shape)\n",
    "#                             print()\n",
    "\n",
    "\n",
    "    if (_TEST == False and _ENHANCE == False):\n",
    "        for coin in coin_list2:\n",
    "            if (os.path.isfile(pickle_result_dir_path + \\\n",
    "                              MODEL + \"_\" + \\\n",
    "                              coin + \"_\" + \\\n",
    "                              str(idx_time_unit) + \"_\" + \\\n",
    "                              str(idx_window_size) + \"_\" + \\\n",
    "                              str(idx_gap) + \"_\" + \\\n",
    "                              str(idx_margin_rate) + \\\n",
    "                              \"_result.pickle\")) is True:\n",
    "                print(MODEL + \"_\" + \\\n",
    "                      coin + \"_\" + \\\n",
    "                      str(idx_time_unit) + \"_\" + \\\n",
    "                      str(idx_window_size) + \"_\" + \\\n",
    "                      str(idx_gap) + \"_\" + \\\n",
    "                      str(idx_margin_rate) + \\\n",
    "                      \"_result.pickle FILE ALREADY EXIST.\")\n",
    "                continue\n",
    "            else:\n",
    "                y2 = onehottify(y_single[coin], n=2)\n",
    "\n",
    "                Evaluate(pickle_load_dir_path, \n",
    "                         data_files_dir, \n",
    "                         epochs, \n",
    "                         pickle_result_dir_path,\n",
    "                         _TEST, \n",
    "                         _ENHANCE,\n",
    "                         coin,\n",
    "                         X, y2,\n",
    "                         key_name_X,\n",
    "                         key_name_y,\n",
    "                         idx_time_unit,\n",
    "                         idx_window_size,\n",
    "                         idx_gap,\n",
    "                         idx_margin_rate, \n",
    "                         MODEL,\n",
    "                         _GPU,\n",
    "                         n_jobs,\n",
    "                         machine,\n",
    "                         Internet_connection,\n",
    "                         params)\n",
    "            #Javascript('IPython.notebook.kernel.restart()')\n",
    "            #time.sleep(1)\n",
    "            #Javascript('IPython.notebook.execute_all_cells()')\n",
    "\n",
    "\n",
    "    if (_TEST == True and _ENHANCE == False):\n",
    "        # for test                                \n",
    "        for coin in range(1):\n",
    "            if (os.path.isfile(pickle_result_dir_path + \\\n",
    "                              \"_test_\" + \\\n",
    "                              MODEL + \"_\" + \\\n",
    "                              \"BTC\" + \"_\" + \\\n",
    "                              str(idx_time_unit) + \"_\" + \\\n",
    "                              str(idx_window_size) + \"_\" + \\\n",
    "                              str(idx_gap) + \"_\" + \\\n",
    "                              str(idx_margin_rate) + \\\n",
    "                              \"_result.pickle\")) is True:\n",
    "                print(\"_test_\" + \\\n",
    "                      MODEL + \"_\" + \\\n",
    "                      \"BTC\" + \"_\" + \\\n",
    "                      str(idx_time_unit) + \"_\" + \\\n",
    "                      str(idx_window_size) + \"_\" + \\\n",
    "                      str(idx_gap) + \"_\" + \\\n",
    "                      str(idx_margin_rate) + \\\n",
    "                      \"_result.pickle FILE ALREADY EXIST.\")\n",
    "                #Javascript('IPython.notebook.kernel.restart()')\n",
    "                #Javascript('IPython.notebook.execute_all_cells()')\n",
    "                continue\n",
    "            else:\n",
    "                y2 = onehottify(y_single['BTC'], n=2)                          \n",
    "\n",
    "                Evaluate(pickle_load_dir_path, \n",
    "                         data_files_dir, \n",
    "                         epochs, \n",
    "                         pickle_result_dir_path, \n",
    "                         _TEST, \n",
    "                         _ENHANCE,\n",
    "                         coin,\n",
    "                         X, y2,\n",
    "                         key_name_X,\n",
    "                         key_name_y,\n",
    "                         idx_time_unit,\n",
    "                         idx_window_size,\n",
    "                         idx_gap,\n",
    "                         idx_margin_rate, \n",
    "                         MODEL,\n",
    "                         _GPU,\n",
    "                         n_jobs, \n",
    "                         machine,\n",
    "                         Internet_connection,\n",
    "                         params)\n",
    "    #Javascript('IPython.notebook.kernel.restart()')\n",
    "    #time.sleep(1)\n",
    "    #Javascript('IPython.notebook.execute_all_cells()')\n",
    "                #Javascript('IPython.notebook.kernel.restart()')\n",
    "                #Javascript('IPython.notebook.execute_all_cells()')\n",
    "    if (_TEST == False and _ENHANCE == True):\n",
    "        for coin in coin_list2:\n",
    "            if (os.path.isfile(boost_up_result_dir_path + \\\n",
    "                              MODEL + \"_\" + \\\n",
    "                              coin + \"_\" + \\\n",
    "                              str(idx_time_unit) + \"_\" + \\\n",
    "                              str(idx_window_size) + \"_\" + \\\n",
    "                              str(idx_gap) + \"_\" + \\\n",
    "                              str(idx_margin_rate) + \\\n",
    "                              \"_boost_up_result.pickle\")) is True:\n",
    "                print(MODEL + \"_\" + \\\n",
    "                      coin + \"_\" + \\\n",
    "                      str(idx_time_unit) + \"_\" + \\\n",
    "                      str(idx_window_size) + \"_\" + \\\n",
    "                      str(idx_gap) + \"_\" + \\\n",
    "                      str(idx_margin_rate) + \\\n",
    "                      \"_boost_up_result.pickle FILE ALREADY EXIST.\")\n",
    "                continue\n",
    "            else:\n",
    "                print(\"[BOOST UP THE \"+MODEL + \"_\" + \\\n",
    "                                        coin + \"_\" + \\\n",
    "                                        str(idx_time_unit) + \"_\" + \\\n",
    "                                        str(idx_window_size) + \"_\" + \\\n",
    "                                        str(idx_gap) + \"_\" + \\\n",
    "                                        str(idx_margin_rate) + \" STARTED.]\")\n",
    "                y2 = onehottify(y_single[coin], n=2)\n",
    "\n",
    "                Evaluate(pickle_load_dir_path, \n",
    "                         data_files_dir, \n",
    "                         epochs, \n",
    "                         boost_up_result_dir_path,\n",
    "                         _TEST, \n",
    "                         _ENHANCE,\n",
    "                         coin,\n",
    "                         X, y2,\n",
    "                         key_name_X,\n",
    "                         key_name_y,\n",
    "                         idx_time_unit,\n",
    "                         idx_window_size,\n",
    "                         idx_gap,\n",
    "                         idx_margin_rate, \n",
    "                         MODEL,\n",
    "                         _GPU,\n",
    "                         n_jobs,\n",
    "                         machine,\n",
    "                         Internet_connection,\n",
    "                         params)\n",
    "            #Javascript('IPython.notebook.kernel.restart()')\n",
    "            #time.sleep(1)\n",
    "            #Javascript('IPython.notebook.execute_all_cells()')\n",
    "\n",
    "# 저장된 pickle 파일의 데이터 구조\n",
    "# tmp = {}\n",
    "# tmp[\"10_1_1_0.1\"] = {\"grid_result.best_score_\":{}}, {\"grid_result.best_params_\":{}}\n",
    "# type(tmp[\"10_1_1_0.1\"][0])\n",
    "# print(tmp[\"10_1_1_0.1\"])\n",
    "# print(tmp[\"10_1_1_0.1\"])\n",
    "# print(tmp[\"10_1_1_0.1\"][0])\n",
    "# print(tmp[\"10_1_1_0.1\"][0]['grid_result.best_score_'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Evaluate(pickle_load_dir_path, \n",
    "             data_files_dir, \n",
    "             epochs, \n",
    "             pickle_result_dir_path,\n",
    "             _TEST, \n",
    "             _ENHANCE, \n",
    "             coin, \n",
    "             X, y2, \n",
    "             key_name_X,\n",
    "             key_name_y,\n",
    "             idx_time_unit,\n",
    "             idx_window_size,\n",
    "             idx_gap,\n",
    "             idx_margin_rate, \n",
    "             MODEL,\n",
    "             _GPU,\n",
    "             n_jobs, \n",
    "             machine,\n",
    "             Internet_connection,\n",
    "             parameter):\n",
    "    \n",
    "\n",
    "    \n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y2, test_size=0.1, random_state=42)\n",
    "#     print(\"X_train.shape\")\n",
    "#     print(X_train.shape)\n",
    "#     print(\"y_train.shape\")\n",
    "#     print(y_train.shape)\n",
    "#     print()\n",
    "#     print(\"X_test.shape\")\n",
    "#     print(X_test.shape)\n",
    "#     print(\"y_test.shape\")\n",
    "#     print(y_test.shape)\n",
    "#     print()\n",
    "\n",
    "    n_coins = 8\n",
    "    n_price = 4\n",
    "    n_steps = idx_window_size # 원래 100이었음. reshape 문제 때문에 수정함\n",
    "\n",
    "    X_train_2 = X_train.transpose([0, 2, 1, 3])\n",
    "    X_test_2 = X_test.transpose([0, 2, 1, 3])\n",
    "#     print(\"X_train_2.shape\")\n",
    "#     print(X_train_2.shape)\n",
    "#     print(\"X_test_2.shape\")\n",
    "#     print(X_test_2.shape)\n",
    "#     print()\n",
    "\n",
    "    X_train_3 = X_train_2.reshape([X_train.shape[0], n_steps, n_coins * n_price])\n",
    "    X_test_3 = X_test_2.reshape([X_test.shape[0], n_steps, n_coins * n_price])\n",
    "#     print(\"X_train_3.shape\")\n",
    "#     print(X_train_3.shape)\n",
    "#     print(\"X_test_3.shape\")\n",
    "#     print(X_test_3.shape)\n",
    "#     print()\n",
    "\n",
    "    if (_TEST==True and _ENHANCE==False):\n",
    "        param_grid = {'window_size' : [n_steps], \n",
    "                      'n_state_units': [100],\n",
    "                      'activation': ['relu'], \n",
    "                      'optimizer': ['rmsprop'], #sgd 추가\n",
    "                      'init': ['glorot_uniform'], #he 추가\n",
    "                      'batch_size': [2048]}\n",
    "\n",
    "    elif (_TEST==False and _ENHANCE==False):\n",
    "        param_grid = {'window_size' : [n_steps], \n",
    "                      'n_state_units': [40, 80, 160],\n",
    "                      'activation': ['relu', 'softmax'], \n",
    "                      'optimizer': ['rmsprop', 'adam'], #sgd 추가\n",
    "                      'init': ['glorot_uniform', 'uniform', 'he_uniform'], #he 추가\n",
    "                      'batch_size': [64,128,256]}\n",
    "        \n",
    "    elif (_TEST==False and _ENHANCE==True):\n",
    "        param_grid = parameter\n",
    "        \n",
    "        \n",
    "\n",
    "\n",
    "\n",
    "    X_train_reshape = X_train_2.reshape([X_train.shape[0], n_steps*n_coins * n_price])\n",
    "    X_test_reshape = X_test_2.reshape([X_test.shape[0], n_steps*n_coins * n_price])\n",
    "#     print(\"X_train_reshape.shape\")\n",
    "#     print(X_train_reshape.shape)\n",
    "#     print(\"X_test_reshape.shape\")\n",
    "#     print(X_test_reshape.shape)\n",
    "#     print()\n",
    "\n",
    "    scaler = MinMaxScaler()\n",
    "    scaler.fit(X_train_reshape)\n",
    "    X_train_scaled = scaler.transform(X_train_reshape)\n",
    "    X_test_scaled = scaler.transform(X_test_reshape)\n",
    "\n",
    "    X_train_scaled = X_train_scaled.reshape(-1, n_steps, n_coins * n_price)\n",
    "\n",
    "    X_test_scaled = X_test_scaled.reshape(-1, n_steps, n_coins * n_price)\n",
    "\n",
    "    if _GPU == True:\n",
    "        if MODEL == \"SimpleRNN\" :\n",
    "            model = KerasClassifier(build_fn=create_model_SimpleRNN, \n",
    "                                    epochs=epochs, # epochs는 실험을 최종적으로 수행하고자 할 때 높일 것(100~150정도)\n",
    "#                                     batch_size=100, \n",
    "                                    verbose=True)\n",
    "\n",
    "        elif MODEL == \"LSTM\":\n",
    "            model = KerasClassifier(build_fn=create_model_LSTM, \n",
    "                                    epochs=epochs, # epochs는 실험을 최종적으로 수행하고자 할 때 높일 것(100~150정도)\n",
    "#                                     batch_size=100, \n",
    "                                    verbose=True)\n",
    "\n",
    "        elif MODEL == \"RNN\":\n",
    "            model = KerasClassifier(build_fn=create_model_RNN, \n",
    "                                    epochs=epochs, # epochs는 실험을 최종적으로 수행하고자 할 때 높일 것(100~150정도)\n",
    "#                                     batch_size=100, \n",
    "                                    verbose=True)\n",
    "\n",
    "        elif MODEL == \"GRU\":\n",
    "            model = KerasClassifier(build_fn=create_model_GRU, \n",
    "                                    epochs=epochs, # epochs는 실험을 최종적으로 수행하고자 할 때 높일 것(100~150정도)\n",
    "#                                     batch_size=100, \n",
    "                                    verbose=True)\n",
    "    elif _GPU == False:\n",
    "        if MODEL == \"SimpleRNN\" :\n",
    "            model = KerasClassifier(build_fn=create_model_SimpleRNN_non_GPU, \n",
    "                                    epochs=epochs, # epochs는 실험을 최종적으로 수행하고자 할 때 높일 것(100~150정도)\n",
    "                                    batch_size=10, \n",
    "                                    verbose=True)\n",
    "\n",
    "        elif MODEL == \"LSTM\":\n",
    "            model = KerasClassifier(build_fn=create_model_LSTM_non_GPU, \n",
    "                                    epochs=epochs, # epochs는 실험을 최종적으로 수행하고자 할 때 높일 것(100~150정도)\n",
    "                                    batch_size=10, \n",
    "                                    verbose=True)\n",
    "\n",
    "        elif MODEL == \"RNN\":\n",
    "            model = KerasClassifier(build_fn=create_model_RNN_non_GPU, \n",
    "                                    epochs=epochs, # epochs는 실험을 최종적으로 수행하고자 할 때 높일 것(100~150정도)\n",
    "                                    batch_size=10, \n",
    "                                    verbose=True)\n",
    "\n",
    "        elif MODEL == \"GRU\":\n",
    "            model = KerasClassifier(build_fn=create_model_GRU_non_GPU, \n",
    "                                    epochs=epochs, # epochs는 실험을 최종적으로 수행하고자 할 때 높일 것(100~150정도)\n",
    "                                    batch_size=10, \n",
    "                                    verbose=True)\n",
    "    if (_ENHANCE == False):\n",
    "        grid = GridSearchCV(\n",
    "            estimator=model, \n",
    "            cv=5, \n",
    "            n_jobs=n_jobs, # test\n",
    "            param_grid=param_grid,\n",
    "            verbose=1)\n",
    "      \n",
    "        X_train_scaled, X_test_scaled = input_reshape(X_train_scaled, X_test_scaled, n_steps, n_coins, n_price)\n",
    "\n",
    "        if (_TEST == True): \n",
    "            print()\n",
    "            print()\n",
    "            print(\"TEST!\")\n",
    "            print()\n",
    "            print(\"----------------------\")\n",
    "            print(\"<\"+MODEL+\">\")\n",
    "            print(\"----------------------\")\n",
    "            print(\"__\"+\"BTC\"+\"__\" + \\\n",
    "                    \"time unit: \"+str(idx_time_unit) + \"  |  \" + \\\n",
    "                    \"window_size :\"+str(idx_window_size) + \"  |  \" + \\\n",
    "                    \"gap :\"+str(idx_gap) + \"  |  \" + \\\n",
    "                    \"margin_rate :\"+str(idx_margin_rate) + \\\n",
    "                    \"  started.\")\n",
    "        elif (_TEST == False):\n",
    "            print()\n",
    "            print()\n",
    "            print(\"----------------------\")\n",
    "            print(\"<\"+MODEL+\">\")\n",
    "            print(\"----------------------\")\n",
    "            print(\"__\"+coin+\"__\" + \\\n",
    "                    \"time unit: \"+str(idx_time_unit) + \"  |  \" + \\\n",
    "                    \"window_size :\"+str(idx_window_size) + \"  |  \" + \\\n",
    "                    \"gap :\"+str(idx_gap) + \"  |  \" + \\\n",
    "                    \"margin_rate :\"+str(idx_margin_rate) + \\\n",
    "                    \"  started.\")\n",
    "\n",
    "        grid_result = grid.fit(X_train_scaled, \n",
    "                               y_train, \n",
    "                               validation_data=(X_test_scaled,y_test))\n",
    "\n",
    "\n",
    "\n",
    "        print(\"----------------------\")\n",
    "\n",
    "        # \n",
    "    #     means = grid_result.cv_results_['mean_test_score']\n",
    "    #     stds = grid_result.cv_results_['std_test_score']\n",
    "    #     params = grid_result.cv_results_['params']\n",
    "    #     print(\"grid_result.cv_results_\",grid_result.cv_results_)\n",
    "    #     print(\"grid_result.best_estimator_\",grid_result.best_estimator_)\n",
    "        print(\"grid_result.score(X_test_scaled, y_test): \",grid_result.score(X_test_scaled, y_test))\n",
    "\n",
    "\n",
    "\n",
    "        evaluate_result = {}\n",
    "\n",
    "        if (_TEST == True): \n",
    "            test_score = grid_result.score(X_test_scaled, y_test)\n",
    "            evaluate_result[MODEL + \"_\" + \\\n",
    "                          \"BTC\" + \"_\" + \\\n",
    "                          str(idx_time_unit) + \"_\" + \\\n",
    "                          str(idx_window_size) + \"_\" + \\\n",
    "                          str(idx_gap) + \"_\" + \\\n",
    "                          str(idx_margin_rate)] = {\"MODEL\":MODEL,\\\n",
    "                                            \"Cryptocurrency\":\"BTC\",\\\n",
    "    #                                         \"grid_result.cv_results_\":grid_result.cv_results_, \\\n",
    "    #                                         \"grid_result.best_estimator_\":grid_result.best_estimator_, \\\n",
    "                                            \"Score\":grid_result.cv_results_['mean_test_score'], \\\n",
    "                                            \"Params\":grid_result.cv_results_['params'],\\\n",
    "                                            \"test_score\":test_score}  \n",
    "\n",
    "\n",
    "        elif (_TEST == False): \n",
    "            test_score = grid_result.score(X_test_scaled, y_test)\n",
    "            evaluate_result[MODEL + \"_\" + \\\n",
    "                          coin + \"_\" + \\\n",
    "                          str(idx_time_unit) + \"_\" + \\\n",
    "                          str(idx_window_size) + \"_\" + \\\n",
    "                          str(idx_gap) + \"_\" + \\\n",
    "                          str(idx_margin_rate)] = {\"MODEL\":MODEL,\\\n",
    "                                            \"Cryptocurrency\":coin, \\\n",
    "                                            \"Score\":grid_result.cv_results_['mean_test_score'], \\\n",
    "                                            \"Params\":grid_result.cv_results_['params'],\\\n",
    "                                            \"test_score\":test_score}\n",
    "    #     print()\n",
    "    #     print(\"evaluate result dict: \", evaluate_result)\n",
    "    #     print()\n",
    "\n",
    "        # summarize results\n",
    "        print()\n",
    "        print(\"Best: %f using %s\" % (grid_result.best_score_, grid_result.best_params_))\n",
    "        print()\n",
    "        \n",
    "    elif (_ENHANCE == True):\n",
    "        \n",
    "        \n",
    "    # for checking pickle file exist\n",
    "    print(\"---pickle saving..\")\n",
    "    if (_TEST == True):\n",
    "        X = {}\n",
    "        y = {}\n",
    "        key_name_X = \"X_\"\n",
    "        key_name_y = \"y_\"\n",
    "        for idx_time_unit in time_unit:\n",
    "            for idx_window_size in window_size:\n",
    "                for idx_gap in gap:\n",
    "                    for idx_margin_rate in margin_rate:\n",
    "                        key_name_X += str(idx_time_unit) + \"_\" + str(idx_window_size) + \"_\" + str(idx_gap) + \"_\" + str(idx_margin_rate)\n",
    "                        key_name_y += str(idx_time_unit) + \"_\" + str(idx_window_size) + \"_\" + str(idx_gap) + \"_\" + str(idx_margin_rate)\n",
    "                        if (os.path.isfile(pickle_result_dir_path + \\\n",
    "                                          \"_test_\" + \\\n",
    "                                          MODEL + \"_\" + \\\n",
    "                                          \"BTC\" + \"_\" + \\\n",
    "                                          str(idx_time_unit) + \"_\" + \\\n",
    "                                          str(idx_window_size) + \"_\" + \\\n",
    "                                          str(idx_gap) + \"_\" + \\\n",
    "                                          str(idx_margin_rate) + \\\n",
    "                                          \"_result.pickle\")) is not True:\n",
    "                            with open(pickle_result_dir_path + \\\n",
    "                                      \"_test_\" + \\\n",
    "                                      MODEL + \"_\" + \\\n",
    "                                      \"BTC\" + \"_\" + \\\n",
    "                                      str(idx_time_unit) + \"_\" + \\\n",
    "                                      str(idx_window_size) + \"_\" + \\\n",
    "                                      str(idx_gap) + \"_\" + \\\n",
    "                                      str(idx_margin_rate) + \\\n",
    "                                      \"_result.pickle\", 'wb') as handle:\n",
    "                                pickle.dump(evaluate_result, handle, protocol=pickle.HIGHEST_PROTOCOL)\n",
    "                                \n",
    "                                # Sending Email\n",
    "                                if Internet_connection == True:\n",
    "                                    smtp = smtplib.SMTP('smtp.naver.com', 587)\n",
    "                                    smtp.ehlo()      # say Hello\n",
    "                                    smtp.starttls()  # TLS 사용시 필요\n",
    "                                    smtp.login('dhgdohk@naver.com', '30892793@dohk')\n",
    "\n",
    "                                    msg = MIMEText(pickle_result_dir_path + \\\n",
    "                                                   \"_test_\" + \\\n",
    "                                                   MODEL + \"_\" + \\\n",
    "                                                   \"BTC\" + \"_\" + \\\n",
    "                                                   str(idx_time_unit) + \"_\" + \\\n",
    "                                                   str(idx_window_size) + \"_\" + \\\n",
    "                                                   str(idx_gap) + \"_\" + \\\n",
    "                                                   str(idx_margin_rate) + \\\n",
    "                                                   \"_result.pickle\")\n",
    "                                    msg['Subject'] =   pickle_result_dir_path + \\\n",
    "                                                       MODEL + \"_\" + \\\n",
    "                                                       \"BTC\" + \"_\" + \\\n",
    "                                                       str(idx_time_unit) + \"_\" + \\\n",
    "                                                       str(idx_window_size) + \"_\" + \\\n",
    "                                                       str(idx_gap) + \"_\" + \\\n",
    "                                                       str(idx_margin_rate) + \\\n",
    "                                                       \"_result.pickle\"\n",
    "                                    msg['To'] = 'dhgdohk@naver.com'\n",
    "                                    smtp.sendmail('dhgdohk@naver.com', 'dhgdohk@naver.com', msg.as_string())\n",
    "\n",
    "                                    smtp.quit()\n",
    "                        else:\n",
    "                            print(\"Already exist the file: \", pickle_result_dir_path + \\\n",
    "                                                              \"_test_\" + \\\n",
    "                                                              MODEL + \"_\" + \\\n",
    "                                                              \"BTC\" + \"_\" + \\\n",
    "                                                              str(idx_time_unit) + \"_\" + \\\n",
    "                                                              str(idx_window_size) + \"_\" + \\\n",
    "                                                              str(idx_gap) + \"_\" + \\\n",
    "                                                              str(idx_margin_rate) + \\\n",
    "                                                              \"_result.pickle\")\n",
    "\n",
    "    elif (_TEST == False): \n",
    "        X = {}\n",
    "        y = {}\n",
    "        key_name_X = \"X_\"\n",
    "        key_name_y = \"y_\"\n",
    "        \n",
    "        key_name_X += str(idx_time_unit) + \"_\" + str(idx_window_size) + \"_\" + str(idx_gap) + \"_\" + str(idx_margin_rate)\n",
    "        key_name_y += str(idx_time_unit) + \"_\" + str(idx_window_size) + \"_\" + str(idx_gap) + \"_\" + str(idx_margin_rate)\n",
    "        if (os.path.isfile(pickle_result_dir_path + \\\n",
    "                          MODEL + \"_\" + \\\n",
    "                          coin + \"_\" + \\\n",
    "                          str(idx_time_unit) + \"_\" + \\\n",
    "                          str(idx_window_size) + \"_\" + \\\n",
    "                          str(idx_gap) + \"_\" + \\\n",
    "                          str(idx_margin_rate) + \\\n",
    "                          \"_result.pickle\")) is not True:\n",
    "            with open(pickle_result_dir_path + \\\n",
    "                      MODEL + \"_\" + \\\n",
    "                      coin + \"_\" + \\\n",
    "                      str(idx_time_unit) + \"_\" + \\\n",
    "                      str(idx_window_size) + \"_\" + \\\n",
    "                      str(idx_gap) + \"_\" + \\\n",
    "                      str(idx_margin_rate) + \\\n",
    "                      \"_result.pickle\", 'wb') as handle:\n",
    "                pickle.dump(evaluate_result, handle, protocol=pickle.HIGHEST_PROTOCOL)\n",
    "\n",
    "                # Sending Email\n",
    "                if Internet_connection == True:\n",
    "                    smtp = smtplib.SMTP('smtp.naver.com', 587)\n",
    "                    smtp.ehlo()      # say Hello\n",
    "                    smtp.starttls()  # TLS 사용시 필요\n",
    "                    smtp.login('dhgdohk@naver.com', '30892793@dohk')\n",
    "\n",
    "                    msg = MIMEText(machine + \\\n",
    "                                   pickle_result_dir_path + \\\n",
    "                                   MODEL + \"_\" + \\\n",
    "                                   coin + \"_\" + \\\n",
    "                                   str(idx_time_unit) + \"_\" + \\\n",
    "                                   str(idx_window_size) + \"_\" + \\\n",
    "                                   str(idx_gap) + \"_\" + \\\n",
    "                                   str(idx_margin_rate) + \\\n",
    "                                   \"_result.pickle\")\n",
    "                    msg['Subject'] =   machine + \\\n",
    "                                       pickle_result_dir_path + \\\n",
    "                                       MODEL + \"_\" + \\\n",
    "                                       coin + \"_\" + \\\n",
    "                                       str(idx_time_unit) + \"_\" + \\\n",
    "                                       str(idx_window_size) + \"_\" + \\\n",
    "                                       str(idx_gap) + \"_\" + \\\n",
    "                                       str(idx_margin_rate) + \\\n",
    "                                       \"_result.pickle\"\n",
    "                    msg['To'] = 'dhgdohk@naver.com'\n",
    "                    smtp.sendmail('dhgdohk@naver.com', 'dhgdohk@naver.com', msg.as_string())\n",
    "\n",
    "                    smtp.quit()\n",
    "        else:\n",
    "            print(\"Already exist the file: \", pickle_result_dir_path + \\\n",
    "                                              \"_test_\" + \\\n",
    "                                              MODEL + \"_\" + \\\n",
    "                                              \"BTC\" + \"_\" + \\\n",
    "                                              str(idx_time_unit) + \"_\" + \\\n",
    "                                              str(idx_window_size) + \"_\" + \\\n",
    "                                              str(idx_gap) + \"_\" + \\\n",
    "                                              str(idx_margin_rate) + \\\n",
    "                                              \"_result.pickle\")\n",
    "    \n",
    "    \n",
    "    #print(\"!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!\")\n",
    "    #Javascript('Jupyter.notebook.session.delete()')\n",
    "    #print(\"Jupyter notebook kernel restart\")\n",
    "    #time.sleep(1)\n",
    "    #Javascript('Jupyter.notebook.kernel.restart()')\n",
    "    #time.sleep(1)\n",
    "    #print(\"Done\")\n",
    "    #Javascript('Jupyter.notebook.execute_all_cells()')\n",
    "    print()\n",
    "\n",
    "\n",
    "#     for mean, stdev, param in zip(means, stds, params):\n",
    "#         print(\"%f (%f) with: %r\" % (mean, stdev, param))\n",
    "#     print()\n",
    "\n",
    "\n",
    "    key_name_X = \"X_\"\n",
    "    key_name_y = \"y_\"\n",
    "    \n",
    "    \n",
    "#     return eval_result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def start(machine, Internet_connection, pickle_result_dir_path, boost_up_result_dir_path, _TEST, _GPU, _ENHANCED, n_jobs, MODEL, idx_time_unit, idx_window_size, idx_gap, idx_margin_rate, epochs, best_params):\n",
    "    '''\n",
    "        [ATTENTION] In create_model METHOD part, need to set appropriate about GPU\n",
    "        \n",
    "        LINK01 -> GPU OFF\n",
    "        MSI -> GPU OFF\n",
    "        SLAVE04 -> GPU ON\n",
    "        SLAVE05 -> GPU ON\n",
    "    ''' \n",
    "    # \n",
    "    \n",
    "    \n",
    "    if (_ENHANCED == False and machine==\"slave05\"):\n",
    "        #time_unit = [10,30,60]     # candle stick minutes\n",
    "        #window_size = [25]  # Unit: num. of candle sticks\n",
    "        #gap = [1]            # Unit: num. of candle sticks\n",
    "        #margin_rate = [0.1]  # Unit: percent\n",
    "\n",
    "        start_time = time.time()\n",
    "        Start_Model( pickle_load_dir_path = \"./dataset_pickle_tuple_type/\",  \n",
    "                     data_files_dir = dataset_dir_path_tuple_type, \n",
    "                     epochs=epochs, \n",
    "                     pickle_result_dir_path = \"./evaluate_result/\", \n",
    "                     boost_up_result_dir_path = \"./boost_up_result/\",\n",
    "                     MODEL=MODEL, \n",
    "                    idx_time_unit=idx_time_unit,\n",
    "                    idx_window_size=idx_window_size, \n",
    "                    idx_gap=idx_gap, \n",
    "                    idx_margin_rate=idx_margin_rate, \n",
    "                     _TEST=False, \n",
    "                     _ENHANCE=_ENHANCED,\n",
    "                     _GPU=True,\n",
    "                     n_jobs=2,\n",
    "                     machine=machine, \n",
    "                     Internet_connection=Internet_connection, \n",
    "                     params=best_params)\n",
    "        \n",
    "        end_time = time.time()\n",
    "        print()\n",
    "        print(\"TIME: \", end_time-start_time)\n",
    "        Javascript('IPython.notebook.kernel.restart()')\n",
    "        time.sleep(1)\n",
    "        Javascript('IPython.notebook.execute_all_cells()')\n",
    "\n",
    "    elif (_ENHANCED == False and machine==\"link-koreatech\"):\n",
    "        #time_unit = [10]     # candle stick minutes\n",
    "        #window_size = [10,25,50]  # Unit: num. of candle sticks\n",
    "        #gap = [1]            # Unit: num. of candle sticks\n",
    "        #margin_rate = [0.1]  # Unit: percent\n",
    "\n",
    "        Start_Model( pickle_load_dir_path = \"./dataset_pickle_tuple_type/\",  \n",
    "                     data_files_dir = dataset_dir_path_tuple_type, \n",
    "                     epochs=epochs, \n",
    "                     pickle_result_dir_path = \"./evaluate_result/\", \n",
    "                    boost_up_result_dir_path = \"./boost_up_result/\",\n",
    "                     MODEL=MODEL, \n",
    "                    idx_time_unit=idx_time_unit,\n",
    "                    idx_window_size=idx_window_size, \n",
    "                    idx_gap=idx_gap, \n",
    "                    idx_margin_rate=idx_margin_rate,\n",
    "                     _TEST=False, \n",
    "                     _ENHANCE=_ENHANCED,\n",
    "                     _GPU=False,\n",
    "                     n_jobs=1,\n",
    "                     machine=machine, \n",
    "                     Internet_connection=Internet_connection, \n",
    "                     params=best_params)\n",
    "        end_time = time.time()\n",
    "        print()\n",
    "        print(\"TIME: \", end_time-start_time)\n",
    "        Javascript('IPython.notebook.kernel.restart()')\n",
    "        time.sleep(1)\n",
    "        Javascript('IPython.notebook.execute_all_cells()')\n",
    "\n",
    "    elif (_ENHANCED == False and machine==\"slave04\"):\n",
    "        #time_unit = [10]     # candle stick minutes\n",
    "        #window_size = [75]  # Unit: num. of candle sticks\n",
    "        #gap = [1]            # Unit: num. of candle sticks\n",
    "        #margin_rate = [0.1]  # Unit: percent\n",
    "\n",
    "        start_time = time.time()\n",
    "\n",
    "        Start_Model(pickle_load_dir_path = \"./dataset_pickle_tuple_type/\",  \n",
    "                 data_files_dir = dataset_dir_path_tuple_type, \n",
    "                 epochs=epochs, \n",
    "                 pickle_result_dir_path = \"./evaluate_result/\", \n",
    "                    boost_up_result_dir_path = \"./boost_up_result/\",\n",
    "                 MODEL=MODEL, \n",
    "                idx_time_unit=idx_time_unit,\n",
    "                idx_window_size=idx_window_size, \n",
    "                idx_gap=idx_gap, \n",
    "                idx_margin_rate=idx_margin_rate,\n",
    "                 _TEST=False, \n",
    "                 _ENHANCE=_ENHANCED,\n",
    "                 _GPU=True,\n",
    "                 n_jobs=2,\n",
    "                 machine=machine,\n",
    "                 Internet_connection=Internet_connection, \n",
    "                 params=best_params)\n",
    "\n",
    "        end_time = time.time()\n",
    "        print()\n",
    "        print(\"TIME: \", end_time-start_time)\n",
    "        Javascript('IPython.notebook.kernel.restart()')\n",
    "        time.sleep(1)\n",
    "        Javascript('IPython.notebook.execute_all_cells()')\n",
    "\n",
    "    elif (_ENHANCED == False and machine==\"link01\"):\n",
    "        #time_unit = [10]     # candle stick minutes\n",
    "        #window_size = [25]  # Unit: num. of candle sticks\n",
    "        #gap = [1,2,3]            # Unit: num. of candle sticks\n",
    "        #margin_rate = [0.1]  # Unit: percent\n",
    "\n",
    "        start_time = time.time()\n",
    "\n",
    "        Start_Model(pickle_load_dir_path = \"./dataset_pickle_tuple_type/\",  \n",
    "                 data_files_dir = dataset_dir_path_tuple_type, \n",
    "                 epochs=epochs, \n",
    "                 pickle_result_dir_path = \"./evaluate_result/\", \n",
    "                    boost_up_result_dir_path = \"./boost_up_result/\",\n",
    "                 MODEL=MODEL, \n",
    "                idx_time_unit=idx_time_unit,\n",
    "                idx_window_size=idx_window_size, \n",
    "                idx_gap=idx_gap, \n",
    "                idx_margin_rate=idx_margin_rate,\n",
    "                 _TEST=False,\n",
    "                 _ENHANCE=_ENHANCED,\n",
    "                 _GPU=False,\n",
    "                 n_jobs=1,\n",
    "                 machine=machine, \n",
    "                 Internet_connection=Internet_connection,\n",
    "                 params=best_params)\n",
    "\n",
    "        end_time = time.time()\n",
    "        print()\n",
    "        print(\"TIME: \", end_time-start_time)\n",
    "        Javascript('IPython.notebook.kernel.restart()')\n",
    "        time.sleep(1)\n",
    "        Javascript('IPython.notebook.execute_all_cells()')\n",
    "\n",
    "    elif (_ENHANCED == False and machine==\"MSI\"):\n",
    "        #time_unit = [10]     # candle stick minutes\n",
    "        #window_size = [25]  # Unit: num. of candle sticks\n",
    "        #gap = [1]            # Unit: num. of candle sticks\n",
    "        #margin_rate = [0.1,0.25,0.5]  # Unit: percent\n",
    "\n",
    "        start_time = time.time()\n",
    "\n",
    "        Start_Model(pickle_load_dir_path = \"./dataset_pickle_tuple_type/\",  \n",
    "                 data_files_dir = dataset_dir_path_tuple_type, \n",
    "                 epochs=epochs, \n",
    "                 pickle_result_dir_path = \"./evaluate_result/\", \n",
    "                    boost_up_result_dir_path = \"./boost_up_result/\",\n",
    "                 MODEL=MODEL, \n",
    "                idx_time_unit=idx_time_unit,\n",
    "                idx_window_size=idx_window_size, \n",
    "                idx_gap=idx_gap, \n",
    "                idx_margin_rate=idx_margin_rate,\n",
    "                 _TEST=False, \n",
    "                 _ENHANCE=_ENHANCED,\n",
    "                 _GPU=False,\n",
    "                 n_jobs=1,\n",
    "                 machine=machine, \n",
    "                 Internet_connection=Internet_connection,\n",
    "                 params=best_params)\n",
    "\n",
    "        end_time = time.time()\n",
    "        print()\n",
    "        print(\"TIME: \", end_time-start_time)\n",
    "        Javascript('IPython.notebook.kernel.restart()')\n",
    "        time.sleep(1)\n",
    "        Javascript('IPython.notebook.execute_all_cells()')\n",
    "        \n",
    "    elif (_ENHANCED == False and machine==\"test\"):\n",
    "        \n",
    "        #time_unit = [10]     # candle stick minutes\n",
    "        #window_size = [10]  # Unit: num. of candle sticks\n",
    "        #gap = [1]            # Unit: num. of candle sticks\n",
    "        #margin_rate = [0.1]  # Unit: percent\n",
    "\n",
    "        start_time = time.time()\n",
    "\n",
    "        Start_Model(pickle_load_dir_path = \"./dataset_pickle_tuple_type/\",  \n",
    "                 data_files_dir = dataset_dir_path_tuple_type, \n",
    "                 epochs=epochs, \n",
    "                 pickle_result_dir_path = \"./evaluate_result/\", \n",
    "                    boost_up_result_dir_path = \"./boost_up_result/\",\n",
    "                 MODEL=MODEL, \n",
    "                idx_time_unit=idx_time_unit,\n",
    "                idx_window_size=idx_window_size, \n",
    "                idx_gap=idx_gap, \n",
    "                idx_margin_rate=idx_margin_rate,\n",
    "                 _TEST=_TEST, \n",
    "                 _ENHANCE=_ENHANCED,\n",
    "                 _GPU=_GPU,\n",
    "                 n_jobs=1,\n",
    "                 machine=\"test\", \n",
    "                 Internet_connection=Internet_connection,\n",
    "                 params=best_params)\n",
    "\n",
    "        end_time = time.time()\n",
    "        print()\n",
    "        print(\"TIME: \", end_time-start_time)\n",
    "        Javascript('IPython.notebook.kernel.restart()')\n",
    "        time.sleep(1)\n",
    "        Javascript('IPython.notebook.execute_all_cells()')\n",
    "    \n",
    "    elif (_ENHANCED == True):\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # evaluate model using test data\n",
    "# score = "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # load pickle file \n",
    "# import pickle\n",
    "# b_x = pickle.load(open(\"./evaluate_result/_test_SimpleRNN_BTC_10_10_1_0.1_result.pickle\", \"rb\"))\n",
    "# b_x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Boost-up Acc, F1\n",
    "# evaluate_result_dir_path = \"./evaluate_result/acc_f1/\"\n",
    "# dataset_dir_path = dataset_dir_path_tuple_type \n",
    "# # dataset_dir_path = dataset_dir_path_numpy_type\n",
    "# epochs = 100\n",
    "# Evaluate(dataset_dir_path, data_files_dir, epochs, evaluate_result_dir_path, time_unit, window_size, gap, margin_rate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_info = {}\n",
    "model_info[\"test\"] = {#\"MODEL_list\":[\"SimpleRNN\", \"LSTM\", \"GRU\"],\n",
    "                      \"MODEL_list\":[\"SimpleRNN\"],\n",
    "                      \"time_unit\":[10], \n",
    "                      \"window_size\":[10],\n",
    "                      \"gap\":[1], \n",
    "                      \"margin_rate\":[0.1]}\n",
    "\n",
    "model_info[\"slave05\"] = {\"MODEL_list\":[\"SimpleRNN\", \"LSTM\", \"GRU\"],\n",
    "                      \"time_unit\":[10,30,60], \n",
    "                      \"window_size\":[25],\n",
    "                      \"gap\":[1], \n",
    "                      \"margin_rate\":[0.1]}\n",
    "model_info[\"slave04\"] = {\"MODEL_list\":[\"SimpleRNN\", \"LSTM\", \"GRU\"],\n",
    "                      \"time_unit\":[10], \n",
    "                      \"window_size\":[10,50,75],\n",
    "                      \"gap\":[1], \n",
    "                      \"margin_rate\":[0.1]}\n",
    "model_info[\"link01\"] = {\"MODEL_list\":[\"SimpleRNN\", \"LSTM\", \"GRU\"],\n",
    "                      \"time_unit\":[10], \n",
    "                      \"window_size\":[25],\n",
    "                      \"gap\":[2,3], \n",
    "                      \"margin_rate\":[0.1]}\n",
    "model_info[\"link-koreatech\"] = {\"MODEL_list\":[\"SimpleRNN\", \"LSTM\", \"GRU\"],\n",
    "                      \"time_unit\":[10], \n",
    "                      \"window_size\":[25],\n",
    "                      \"gap\":[1], \n",
    "                      \"margin_rate\":[0.25,0.5]}\n",
    "\n",
    "model_info[\"MSI\"] = {\"MODEL_list\":[\"SimpleRNN\", \"LSTM\", \"GRU\"],\n",
    "                      \"time_unit\":[10], \n",
    "                      \"window_size\":[25],\n",
    "                      \"gap\":[1], \n",
    "                      \"margin_rate\":[0.1,0.25,0.5]}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Hyperio test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from __future__ import print_function\n",
    "# import keras\n",
    "# from keras.datasets import mnist\n",
    "# from keras.models import Sequential\n",
    "# from keras.layers import Dense, Dropout, Flatten\n",
    "# from keras.layers import Conv2D, MaxPooling2D\n",
    "# from keras import backend as K\n",
    "\n",
    "# def create_model_SimpleRNN_non_GPU_test(x_train, y_train, x_val, y_val, params):\n",
    "#     model = Sequential()\n",
    "#     model.add(# if문을 통해 여러 RNN모델 쓸 수 있도록 하기, SimpleRNN외에 다른 RNN모델 찾아보기\n",
    "#         SimpleRNN(\n",
    "# #             n_state_units, \n",
    "#                   params['first_neuron'],\n",
    "#                   input_dim=x_train.shape[2], # [dataset 크기, 윈도우 사이즈, 32(코인개수*OLHC)]\n",
    "# #                   input_shape=(window_size, 32),\n",
    "# #                   use_bias=True, \n",
    "#                   #activation='relu'\n",
    "#         ))\n",
    "# #                   kernel_initializer='glorot_uniform', \n",
    "# #                   recurrent_initializer='orthogonal', \n",
    "# #                   bias_initializer='zeros', \n",
    "# #                   dropout=0.0,\n",
    "# #                   recurrent_dropout=0.0))\n",
    "    \n",
    "# #     model.add(Dense(units=neurons))\n",
    "# #     model.add(Dropout(dropout_rate))\n",
    "        \n",
    "#     model.add(Dropout(params['dropout']))\n",
    "#     model.add(Dense(y_train.shape[1],\n",
    "#                     activation=params['last_activation']))\n",
    "        \n",
    "# #     model.add(Dense(units=2))\n",
    "# #     model = multi_gpu_model(model, gpus=2)\n",
    "# #     model.compile(optimizer=params['optimizer'](lr=lr_normalizer(params['lr'], params['optimizer'])),\n",
    "# #                   loss=params['loss'],\n",
    "# #                   metrics=['acc'])\n",
    "#     model = multi_gpu_model(model, gpus=2)\n",
    "#     model.compile(optimizer=params['optimizer'](),\n",
    "#                   loss=params['loss'],\n",
    "#                   metrics=['acc', f1_score])\n",
    "    \n",
    "#     out = model.fit(x_train, y_train,\n",
    "#                     batch_size=params['batch_size'],\n",
    "#                     epochs=params['epochs'],\n",
    "#                     verbose=1,\n",
    "#                     validation_data=[x_val, y_val])\n",
    "# #                     callbacks=early_stopper(params['epochs'], mode='strict'))\n",
    "    \n",
    "#     return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# from keras.optimizers import Adam, Nadam\n",
    "# from keras.activations import softmax\n",
    "# from keras.losses import categorical_crossentropy, logcosh\n",
    "\n",
    "# pickle_load_dir_path = \"./dataset_pickle_tuple_type/\"\n",
    "# X = {}\n",
    "# y = {}\n",
    "\n",
    "\n",
    "# MODEL = [\"SimpleRNN\"]\n",
    "# idx_time_unit = 10     # candle stick minutes\n",
    "# idx_window_size = 25  # Unit: num. of candle sticks\n",
    "# idx_gap = 1            # Unit: num. of candle sticks\n",
    "# idx_margix_rate = 0.1  # Unit: percent\n",
    "\n",
    "# key_name_X = \"X_\"\n",
    "# key_name_y = \"y_\"\n",
    "\n",
    "\n",
    "# key_name_X += str(idx_time_unit) + \"_\" + str(idx_window_size) + \"_\" + str(idx_gap) + \"_\" + str(idx_margix_rate)\n",
    "# key_name_y += str(idx_time_unit) + \"_\" + str(idx_window_size) + \"_\" + str(idx_gap) + \"_\" + str(idx_margix_rate)\n",
    "\n",
    "# X = Load_Dataset_X(pickle_load_dir_path, idx_time_unit, idx_window_size, idx_gap, idx_margix_rate)\n",
    "# y = Load_Dataset_y(pickle_load_dir_path, idx_time_unit, idx_window_size, idx_gap, idx_margix_rate)\n",
    "\n",
    "# y_single = {}\n",
    "# y_single['BTC'] = y[:, 1]\n",
    "# y_single['ETH'] = y[:, 2]\n",
    "# y_single['XRP'] = y[:, 3]\n",
    "# y_single['BCH'] = y[:, 4]\n",
    "# y_single['LTC'] = y[:, 5]\n",
    "# y_single['DASH'] = y[:, 6]\n",
    "# y_single['ETC'] = y[:, 7]\n",
    "\n",
    "# coin_list2 = [\"BTC\", \"ETH\", \"XRP\", \"BCH\", \"LTC\", \"DASH\", \"ETC\"]\n",
    "\n",
    "# y2 = onehottify(y_single['BTC'], n=2) \n",
    "# #                         for coin in coin_list2:\n",
    "# #                             print(\"y_single[\"+coin+\"]\"+\".shape\")\n",
    "# #                             print(y_single[coin].shape)\n",
    "# #                             print()\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# X_train, X_test, y_train, y_test = train_test_split(X, y2, test_size=0.1, random_state=42)\n",
    "# #     print(\"X_train.shape\")\n",
    "# #     print(X_train.shape)\n",
    "# #     print(\"y_train.shape\")\n",
    "# #     print(y_train.shape)\n",
    "# #     print()\n",
    "# #     print(\"X_test.shape\")\n",
    "# #     print(X_test.shape)\n",
    "# #     print(\"y_test.shape\")\n",
    "# #     print(y_test.shape)\n",
    "# #     print()\n",
    "\n",
    "# n_coins = 8\n",
    "# n_price = 4\n",
    "# n_steps = idx_window_size # 원래 100이었음. reshape 문제 때문에 수정함\n",
    "\n",
    "# X_train_2 = X_train.transpose([0, 2, 1, 3])\n",
    "# X_test_2 = X_test.transpose([0, 2, 1, 3])\n",
    "# #     print(\"X_train_2.shape\")\n",
    "# #     print(X_train_2.shape)\n",
    "# #     print(\"X_test_2.shape\")\n",
    "# #     print(X_test_2.shape)\n",
    "# #     print()\n",
    "\n",
    "# X_train_3 = X_train_2.reshape([X_train.shape[0], n_steps, n_coins * n_price])\n",
    "# X_test_3 = X_test_2.reshape([X_test.shape[0], n_steps, n_coins * n_price])\n",
    "# #     print(\"X_train_3.shape\")\n",
    "# #     print(X_train_3.shape)\n",
    "# #     print(\"X_test_3.shape\")\n",
    "# #     print(X_test_3.shape)\n",
    "# #     print()\n",
    "\n",
    "# X_train_reshape = X_train_2.reshape([X_train.shape[0], n_steps*n_coins * n_price])\n",
    "# X_test_reshape = X_test_2.reshape([X_test.shape[0], n_steps*n_coins * n_price])\n",
    "# #     print(\"X_train_reshape.shape\")\n",
    "# #     print(X_train_reshape.shape)\n",
    "# #     print(\"X_test_reshape.shape\")\n",
    "# #     print(X_test_reshape.shape)\n",
    "# #     print()\n",
    "\n",
    "# scaler = MinMaxScaler()\n",
    "# scaler.fit(X_train_reshape)\n",
    "# X_train_scaled = scaler.transform(X_train_reshape)\n",
    "# X_test_scaled = scaler.transform(X_test_reshape)\n",
    "\n",
    "# X_train_scaled = X_train_scaled.reshape(-1, n_steps, n_coins * n_price)\n",
    "\n",
    "# X_test_scaled = X_test_scaled.reshape(-1, n_steps, n_coins * n_price)\n",
    "\n",
    "# X_train_scaled, X_test_scaled = input_reshape(X_train_scaled, X_test_scaled, n_steps, n_coins, n_price)\n",
    "\n",
    "                        \n",
    "# p = {'lr': (0.1, 0.01, 0.001),\n",
    "#      'first_neuron':[4, 8, 16, 32, 64, 128],\n",
    "#      'batch_size': [64,128,256],\n",
    "#      'epochs': [100],\n",
    "#      'activation':['relu', 'softmax'],\n",
    "#      'dropout': (0, 0.40, 10),\n",
    "#      'optimizer': [Adam, Nadam],\n",
    "#      'kernel_initializer':['glorot_uniform', 'uniform', 'he_uniform'],\n",
    "#      'recurrent_initializer':['orthogonal'], \n",
    "#      'bias_initializer':['zeros'],\n",
    "#      'loss': ['categorical_crossentropy', 'logcosh'],\n",
    "#      'last_activation': ['softmax'],\n",
    "#      'weight_regulizer':[None],\n",
    "#      'emb_output_dims': [None]}\n",
    "\n",
    "# import hyperio as hy\n",
    "# h = hy.Hyperio(X_train_scaled, y_train, \n",
    "#                params=p, \n",
    "#                dataset_name='coin', \n",
    "#                experiment_no='1', \n",
    "#                model=create_model_SimpleRNN_non_GPU_test,\n",
    "#                # create_model_SimpleRNN_non_GPU_test(x_train, y_train, x_val, y_val, params)\n",
    "#                grid_downsample=.01)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# h"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# h.result"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Boost-Up"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'activation': ['relu'], 'batch_size': [64], 'init': ['he_uniform'], 'n_state_units': [160], 'optimizer': ['adam'], 'window_size': [10]}\n"
     ]
    }
   ],
   "source": [
    "evaluate_result = pickle.load(open(\"./evaluate_result/SimpleRNN_BTC_10_10_1_0.1_result.pickle\", \"rb\"))\n",
    "key = str(evaluate_result.keys())[12:-3]\n",
    "print(evaluate_result[key]['Best_Params'])\n",
    "best_params = evaluate_result[key]['Best_Params']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[BOOST UP THE SimpleRNN_BTC_10_10_1_0.1 STARTED.]\n",
      "\n",
      "\n",
      "----------------------\n",
      "<SimpleRNN>\n",
      "----------------------\n",
      "__BTC__time unit: 10  |  window_size :10  |  gap :1  |  margin_rate :0.1  started.\n",
      "Fitting 5 folds for each of 1 candidates, totalling 5 fits\n",
      "Train on 37535 samples, validate on 5214 samples\n",
      "Epoch 1/1\n",
      "37535/37535 [==============================] - 6s 149us/step - loss: 5.3102 - acc: 0.5800 - f1_score: 0.5767 - val_loss: 5.4685 - val_acc: 0.3393 - val_f1_score: 0.3393\n",
      "9384/9384 [==============================] - 1s 58us/step\n",
      "37535/37535 [==============================] - 2s 56us/step\n",
      "Train on 37535 samples, validate on 5214 samples\n",
      "Epoch 1/1\n",
      "37535/37535 [==============================] - 6s 160us/step - loss: 3.2014 - acc: 0.3300 - f1_score: 2.6232e-04 - val_loss: 0.6410 - val_acc: 0.3393 - val_f1_score: 0.0000e+00\n",
      "9384/9384 [==============================] - 1s 69us/step\n",
      "37535/37535 [==============================] - 3s 67us/step\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m--------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0mTraceback (most recent call last)",
      "\u001b[0;32m<ipython-input-18-37f14c2ba837>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     31\u001b[0m                          \u001b[0midx_margin_rate\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0midx_margin_rate\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     32\u001b[0m                          \u001b[0mepochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mepochs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 33\u001b[0;31m                          best_params=best_params)\n\u001b[0m\u001b[1;32m     34\u001b[0m                     \u001b[0mJavascript\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'IPython.notebook.kernel.restart()'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     35\u001b[0m                     \u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msleep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-8-5987ea6b6f4b>\u001b[0m in \u001b[0;36mstart\u001b[0;34m(machine, Internet_connection, pickle_result_dir_path, boost_up_result_dir_path, _TEST, _GPU, _ENHANCED, n_jobs, MODEL, idx_time_unit, idx_window_size, idx_gap, idx_margin_rate, epochs, best_params)\u001b[0m\n\u001b[1;32m    197\u001b[0m                  \u001b[0mmachine\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"test\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    198\u001b[0m                  \u001b[0mInternet_connection\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mInternet_connection\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 199\u001b[0;31m                  params=best_params)\n\u001b[0m\u001b[1;32m    200\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    201\u001b[0m         \u001b[0mend_time\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtime\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-6-effb8693ba60>\u001b[0m in \u001b[0;36mStart_Model\u001b[0;34m(pickle_load_dir_path, data_files_dir, epochs, pickle_result_dir_path, boost_up_result_dir_path, MODEL, idx_time_unit, idx_window_size, idx_gap, idx_margin_rate, _TEST, _ENHANCE, _GPU, n_jobs, machine, Internet_connection, params)\u001b[0m\n\u001b[1;32m    410\u001b[0m                          \u001b[0mmachine\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    411\u001b[0m                          \u001b[0mInternet_connection\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 412\u001b[0;31m                          params)\n\u001b[0m\u001b[1;32m    413\u001b[0m             \u001b[0;31m#Javascript('IPython.notebook.kernel.restart()')\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    414\u001b[0m             \u001b[0;31m#time.sleep(1)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-7-8515c654c847>\u001b[0m in \u001b[0;36mEvaluate\u001b[0;34m(pickle_load_dir_path, data_files_dir, epochs, pickle_result_dir_path, _TEST, _ENHANCE, coin, X, y2, key_name_X, key_name_y, idx_time_unit, idx_window_size, idx_gap, idx_margin_rate, MODEL, _GPU, n_jobs, machine, Internet_connection, parameter)\u001b[0m\n\u001b[1;32m    171\u001b[0m     grid_result = grid.fit(X_train_scaled, \n\u001b[1;32m    172\u001b[0m                            \u001b[0my_train\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 173\u001b[0;31m                            validation_data=(X_test_scaled,y_test))\n\u001b[0m\u001b[1;32m    174\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    175\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/anaconda3/lib/python3.6/site-packages/sklearn/model_selection/_search.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, X, y, groups, **fit_params)\u001b[0m\n\u001b[1;32m    637\u001b[0m                                   error_score=self.error_score)\n\u001b[1;32m    638\u001b[0m           for parameters, (train, test) in product(candidate_params,\n\u001b[0;32m--> 639\u001b[0;31m                                                    cv.split(X, y, groups)))\n\u001b[0m\u001b[1;32m    640\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    641\u001b[0m         \u001b[0;31m# if one choose to see train score, \"out\" will contain train score info\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/anaconda3/lib/python3.6/site-packages/sklearn/externals/joblib/parallel.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, iterable)\u001b[0m\n\u001b[1;32m    777\u001b[0m             \u001b[0;31m# was dispatched. In particular this covers the edge\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    778\u001b[0m             \u001b[0;31m# case of Parallel used with an exhausted iterator.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 779\u001b[0;31m             \u001b[0;32mwhile\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdispatch_one_batch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0miterator\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    780\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_iterating\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    781\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/anaconda3/lib/python3.6/site-packages/sklearn/externals/joblib/parallel.py\u001b[0m in \u001b[0;36mdispatch_one_batch\u001b[0;34m(self, iterator)\u001b[0m\n\u001b[1;32m    623\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    624\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 625\u001b[0;31m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_dispatch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtasks\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    626\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    627\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/anaconda3/lib/python3.6/site-packages/sklearn/externals/joblib/parallel.py\u001b[0m in \u001b[0;36m_dispatch\u001b[0;34m(self, batch)\u001b[0m\n\u001b[1;32m    586\u001b[0m         \u001b[0mdispatch_timestamp\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtime\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    587\u001b[0m         \u001b[0mcb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mBatchCompletionCallBack\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdispatch_timestamp\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 588\u001b[0;31m         \u001b[0mjob\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_backend\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mapply_async\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcallback\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcb\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    589\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jobs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mjob\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    590\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/anaconda3/lib/python3.6/site-packages/sklearn/externals/joblib/_parallel_backends.py\u001b[0m in \u001b[0;36mapply_async\u001b[0;34m(self, func, callback)\u001b[0m\n\u001b[1;32m    109\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mapply_async\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcallback\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    110\u001b[0m         \u001b[0;34m\"\"\"Schedule a func to be run\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 111\u001b[0;31m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mImmediateResult\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfunc\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    112\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mcallback\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    113\u001b[0m             \u001b[0mcallback\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mresult\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/anaconda3/lib/python3.6/site-packages/sklearn/externals/joblib/_parallel_backends.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, batch)\u001b[0m\n\u001b[1;32m    330\u001b[0m         \u001b[0;31m# Don't delay the application, to avoid keeping the input\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    331\u001b[0m         \u001b[0;31m# arguments in memory\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 332\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mresults\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbatch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    333\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    334\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/anaconda3/lib/python3.6/site-packages/sklearn/externals/joblib/parallel.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    129\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    130\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__call__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 131\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitems\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    132\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    133\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__len__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/anaconda3/lib/python3.6/site-packages/sklearn/externals/joblib/parallel.py\u001b[0m in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m    129\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    130\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__call__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 131\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitems\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    132\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    133\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__len__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/anaconda3/lib/python3.6/site-packages/sklearn/model_selection/_validation.py\u001b[0m in \u001b[0;36m_fit_and_score\u001b[0;34m(estimator, X, y, scorer, train, test, verbose, parameters, fit_params, return_train_score, return_parameters, return_n_test_samples, return_times, error_score)\u001b[0m\n\u001b[1;32m    456\u001b[0m             \u001b[0mestimator\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mfit_params\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    457\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 458\u001b[0;31m             \u001b[0mestimator\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mfit_params\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    459\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    460\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/anaconda3/lib/python3.6/site-packages/keras/wrappers/scikit_learn.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, x, y, sample_weight, **kwargs)\u001b[0m\n\u001b[1;32m    208\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0msample_weight\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    209\u001b[0m             \u001b[0mkwargs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'sample_weight'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msample_weight\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 210\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0msuper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mKerasClassifier\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    211\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    212\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/anaconda3/lib/python3.6/site-packages/keras/wrappers/scikit_learn.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, x, y, **kwargs)\u001b[0m\n\u001b[1;32m    150\u001b[0m         \u001b[0mfit_args\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    151\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 152\u001b[0;31m         \u001b[0mhistory\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mfit_args\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    153\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    154\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mhistory\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/anaconda3/lib/python3.6/site-packages/keras/engine/training.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, **kwargs)\u001b[0m\n\u001b[1;32m   1006\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1007\u001b[0m             \u001b[0mins\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mx\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0my\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0msample_weights\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1008\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_make_train_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1009\u001b[0m         \u001b[0mf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain_function\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1010\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/anaconda3/lib/python3.6/site-packages/keras/engine/training.py\u001b[0m in \u001b[0;36m_make_train_function\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    496\u001b[0m                     training_updates = self.optimizer.get_updates(\n\u001b[1;32m    497\u001b[0m                         \u001b[0mparams\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_collected_trainable_weights\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 498\u001b[0;31m                         loss=self.total_loss)\n\u001b[0m\u001b[1;32m    499\u001b[0m                 updates = (self.updates +\n\u001b[1;32m    500\u001b[0m                            \u001b[0mtraining_updates\u001b[0m \u001b[0;34m+\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/anaconda3/lib/python3.6/site-packages/keras/legacy/interfaces.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     89\u001b[0m                 warnings.warn('Update your `' + object_name +\n\u001b[1;32m     90\u001b[0m                               '` call to the Keras 2 API: ' + signature, stacklevel=2)\n\u001b[0;32m---> 91\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     92\u001b[0m         \u001b[0mwrapper\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_original_function\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     93\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mwrapper\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/anaconda3/lib/python3.6/site-packages/keras/optimizers.py\u001b[0m in \u001b[0;36mget_updates\u001b[0;34m(self, loss, params)\u001b[0m\n\u001b[1;32m    468\u001b[0m     \u001b[0;34m@\u001b[0m\u001b[0minterfaces\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlegacy_get_updates_support\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    469\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mget_updates\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mloss\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mparams\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 470\u001b[0;31m         \u001b[0mgrads\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_gradients\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mloss\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mparams\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    471\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupdates\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mK\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupdate_add\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0miterations\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    472\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/anaconda3/lib/python3.6/site-packages/keras/optimizers.py\u001b[0m in \u001b[0;36mget_gradients\u001b[0;34m(self, loss, params)\u001b[0m\n\u001b[1;32m     87\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     88\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mget_gradients\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mloss\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mparams\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 89\u001b[0;31m         \u001b[0mgrads\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mK\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgradients\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mloss\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mparams\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     90\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mgrads\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     91\u001b[0m             raise ValueError('An operation has `None` for gradient. '\n",
      "\u001b[0;32m/anaconda3/lib/python3.6/site-packages/keras/backend/tensorflow_backend.py\u001b[0m in \u001b[0;36mgradients\u001b[0;34m(loss, variables)\u001b[0m\n\u001b[1;32m   2706\u001b[0m         \u001b[0mA\u001b[0m \u001b[0mgradients\u001b[0m \u001b[0mtensor\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2707\u001b[0m     \"\"\"\n\u001b[0;32m-> 2708\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgradients\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mloss\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvariables\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcolocate_gradients_with_ops\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2709\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2710\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/anaconda3/lib/python3.6/site-packages/tensorflow/python/ops/gradients_impl.py\u001b[0m in \u001b[0;36mgradients\u001b[0;34m(ys, xs, grad_ys, name, colocate_gradients_with_ops, gate_gradients, aggregation_method, stop_gradients)\u001b[0m\n\u001b[1;32m    594\u001b[0m   \u001b[0;32mwith\u001b[0m \u001b[0mops\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_default_graph\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_mutation_lock\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m  \u001b[0;31m# pylint: disable=protected-access\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    595\u001b[0m     return _GradientsHelper(ys, xs, grad_ys, name, colocate_gradients_with_ops,\n\u001b[0;32m--> 596\u001b[0;31m                             gate_gradients, aggregation_method, stop_gradients)\n\u001b[0m\u001b[1;32m    597\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    598\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/anaconda3/lib/python3.6/site-packages/tensorflow/python/ops/gradients_impl.py\u001b[0m in \u001b[0;36m_GradientsHelper\u001b[0;34m(ys, xs, grad_ys, name, colocate_gradients_with_ops, gate_gradients, aggregation_method, stop_gradients, src_graph)\u001b[0m\n\u001b[1;32m    661\u001b[0m     \u001b[0mstop_gradient_ops\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mop\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mt\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mstop_gradients\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    662\u001b[0m     reachable_to_ops, pending_count, loop_state = _PendingCount(\n\u001b[0;32m--> 663\u001b[0;31m         to_ops, from_ops, colocate_gradients_with_ops, func_graphs, xs)\n\u001b[0m\u001b[1;32m    664\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    665\u001b[0m     \u001b[0;31m# Iterate over the collected ops.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/anaconda3/lib/python3.6/site-packages/tensorflow/python/ops/gradients_impl.py\u001b[0m in \u001b[0;36m_PendingCount\u001b[0;34m(to_ops, from_ops, colocate_gradients_with_ops, func_graphs, xs)\u001b[0m\n\u001b[1;32m    188\u001b[0m   \u001b[0;31m# 'loop_state' is None if there are no while loops.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    189\u001b[0m   loop_state = control_flow_ops.MaybeCreateControlFlowState(\n\u001b[0;32m--> 190\u001b[0;31m       between_op_list, between_ops, colocate_gradients_with_ops)\n\u001b[0m\u001b[1;32m    191\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    192\u001b[0m   \u001b[0;31m# Initialize pending count for between ops.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/anaconda3/lib/python3.6/site-packages/tensorflow/python/ops/control_flow_ops.py\u001b[0m in \u001b[0;36mMaybeCreateControlFlowState\u001b[0;34m(between_op_list, between_ops, colocate_gradients_with_ops)\u001b[0m\n\u001b[1;32m   1428\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0mcolocate_gradients_with_ops\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1429\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0mops\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcolocate_with\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mop\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1430\u001b[0;31m           \u001b[0mloop_state\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mAddWhileContext\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mop\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbetween_op_list\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbetween_ops\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1431\u001b[0m       \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1432\u001b[0m         \u001b[0mloop_state\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mAddWhileContext\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mop\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbetween_op_list\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbetween_ops\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/anaconda3/lib/python3.6/site-packages/tensorflow/python/ops/control_flow_ops.py\u001b[0m in \u001b[0;36mAddWhileContext\u001b[0;34m(self, op, between_op_list, between_ops)\u001b[0m\n\u001b[1;32m   1242\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0mouter_forward_ctxt\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1243\u001b[0m         \u001b[0mouter_grad_state\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_map\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mouter_forward_ctxt\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1244\u001b[0;31m       \u001b[0mgrad_state\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mGradLoopState\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mforward_ctxt\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mouter_grad_state\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1245\u001b[0m       \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_map\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mforward_ctxt\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgrad_state\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1246\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/anaconda3/lib/python3.6/site-packages/tensorflow/python/ops/control_flow_ops.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, forward_ctxt, outer_grad_state)\u001b[0m\n\u001b[1;32m    820\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mouter_forward_ctxt\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    821\u001b[0m       \u001b[0mouter_forward_ctxt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mEnter\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 822\u001b[0;31m     \u001b[0mcnt\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mforward_index\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mforward_ctxt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mAddForwardLoopCounter\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mouter_grad_state\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    823\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mouter_forward_ctxt\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    824\u001b[0m       \u001b[0mouter_forward_ctxt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mExit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/anaconda3/lib/python3.6/site-packages/tensorflow/python/ops/control_flow_ops.py\u001b[0m in \u001b[0;36mAddForwardLoopCounter\u001b[0;34m(self, outer_grad_state)\u001b[0m\n\u001b[1;32m   2542\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2543\u001b[0m     \u001b[0mmerge_n\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmerge\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0menter_n\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0menter_n\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2544\u001b[0;31m     \u001b[0mswitch_n\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mswitch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmerge_n\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_pivot\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2545\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2546\u001b[0m     \u001b[0mindex\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmath_ops\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0madd\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mswitch_n\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/anaconda3/lib/python3.6/site-packages/tensorflow/python/ops/control_flow_ops.py\u001b[0m in \u001b[0;36mswitch\u001b[0;34m(data, pred, dtype, name)\u001b[0m\n\u001b[1;32m    349\u001b[0m     \u001b[0mpred\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mops\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconvert_to_tensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpred\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"pred\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    350\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mops\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTensor\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 351\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mgen_control_flow_ops\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mswitch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpred\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    352\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    353\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mops\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mIndexedSlices\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msparse_tensor\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mSparseTensor\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/anaconda3/lib/python3.6/site-packages/tensorflow/python/ops/gen_control_flow_ops.py\u001b[0m in \u001b[0;36mswitch\u001b[0;34m(data, pred, name)\u001b[0m\n\u001b[1;32m    798\u001b[0m   \u001b[0;32mif\u001b[0m \u001b[0m_ctx\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0m_ctx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_eager_context\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mis_eager\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    799\u001b[0m     _, _, _op = _op_def_lib._apply_op_helper(\n\u001b[0;32m--> 800\u001b[0;31m         \"Switch\", data=data, pred=pred, name=name)\n\u001b[0m\u001b[1;32m    801\u001b[0m     \u001b[0m_result\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_op\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moutputs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    802\u001b[0m     \u001b[0m_inputs_flat\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_op\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/anaconda3/lib/python3.6/site-packages/tensorflow/python/framework/op_def_library.py\u001b[0m in \u001b[0;36m_apply_op_helper\u001b[0;34m(self, op_type_name, name, **keywords)\u001b[0m\n\u001b[1;32m    785\u001b[0m         op = g.create_op(op_type_name, inputs, output_types, name=scope,\n\u001b[1;32m    786\u001b[0m                          \u001b[0minput_types\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minput_types\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mattrs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mattr_protos\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 787\u001b[0;31m                          op_def=op_def)\n\u001b[0m\u001b[1;32m    788\u001b[0m       \u001b[0;32mreturn\u001b[0m \u001b[0moutput_structure\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mop_def\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mis_stateful\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mop\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    789\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/anaconda3/lib/python3.6/site-packages/tensorflow/python/util/deprecation.py\u001b[0m in \u001b[0;36mnew_func\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    425\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0mis_in_graph_mode\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mIS_IN_GRAPH_MODE\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0m_PRINT_DEPRECATION_WARNINGS\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    426\u001b[0m         \u001b[0minvalid_args\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 427\u001b[0;31m         \u001b[0mnamed_args\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf_inspect\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgetcallargs\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfunc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    428\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0marg_name\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mspec\u001b[0m \u001b[0;32min\u001b[0m \u001b[0miter\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdeprecated_positions\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitems\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    429\u001b[0m           if (spec.position < len(args) and\n",
      "\u001b[0;32m/anaconda3/lib/python3.6/site-packages/tensorflow/python/util/tf_inspect.py\u001b[0m in \u001b[0;36mgetcallargs\u001b[0;34m(func, *positional, **named)\u001b[0m\n\u001b[1;32m    233\u001b[0m   \u001b[0margspec\u001b[0m \u001b[0mwill\u001b[0m \u001b[0mbe\u001b[0m \u001b[0mused\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    234\u001b[0m   \"\"\"\n\u001b[0;32m--> 235\u001b[0;31m   \u001b[0margspec\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgetfullargspec\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfunc\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    236\u001b[0m   \u001b[0mcall_args\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnamed\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcopy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    237\u001b[0m   \u001b[0mthis\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgetattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfunc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'im_self'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mgetattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfunc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'__self__'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/anaconda3/lib/python3.6/site-packages/tensorflow/python/util/tf_inspect.py\u001b[0m in \u001b[0;36mgetfullargspec\u001b[0;34m(obj)\u001b[0m\n\u001b[1;32m    211\u001b[0m     \u001b[0mdirectly\u001b[0m \u001b[0mon\u001b[0m \u001b[0mthe\u001b[0m \u001b[0mcallable\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    212\u001b[0m   \"\"\"\n\u001b[0;32m--> 213\u001b[0;31m   \u001b[0mdecorators\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf_decorator\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0munwrap\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mobj\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    214\u001b[0m   return next((d.decorator_argspec\n\u001b[1;32m    215\u001b[0m                \u001b[0;32mfor\u001b[0m \u001b[0md\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mdecorators\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/anaconda3/lib/python3.6/site-packages/tensorflow/python/util/tf_decorator.py\u001b[0m in \u001b[0;36munwrap\u001b[0;34m(maybe_tf_decorator)\u001b[0m\n\u001b[1;32m    121\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcur\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mTFDecorator\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    122\u001b[0m       \u001b[0mdecorators\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcur\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 123\u001b[0;31m     \u001b[0;32melif\u001b[0m \u001b[0mhasattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcur\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'_tf_decorator'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    124\u001b[0m       \u001b[0mdecorators\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgetattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcur\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'_tf_decorator'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    125\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "Machine = \"test\"\n",
    "# If Machine is \"test\" then, _TEST argument must be True\n",
    "test = False\n",
    "_GPU= False\n",
    "_ENHANCED = True\n",
    "\n",
    "Internet_connection=True\n",
    "n_jobs=-1\n",
    "epochs=1\n",
    "\n",
    "pickle_result_dir_path = \"./evaluate_result/\"\n",
    "boost_up_result_dir_path = \"./boost_up_result/\"\n",
    "\n",
    "for model in model_info[Machine][\"MODEL_list\"]:\n",
    "    for idx_time_unit in model_info[Machine][\"time_unit\"]:\n",
    "        for idx_window_size in model_info[Machine][\"window_size\"]:\n",
    "            for idx_gap in model_info[Machine][\"gap\"]:\n",
    "                for idx_margin_rate in model_info[Machine][\"margin_rate\"]:\n",
    "                    start(machine=Machine, \n",
    "                         Internet_connection=Internet_connection, \n",
    "                         pickle_result_dir_path=pickle_result_dir_path, \n",
    "                         boost_up_result_dir_path=boost_up_result_dir_path,\n",
    "                         _TEST=test, \n",
    "                         _GPU=_GPU, \n",
    "                         _ENHANCED=_ENHANCED,\n",
    "                         n_jobs=n_jobs,\n",
    "                         MODEL = model, \n",
    "                         idx_time_unit=idx_time_unit, \n",
    "                         idx_window_size=idx_window_size, \n",
    "                         idx_gap=idx_gap, \n",
    "                         idx_margin_rate=idx_margin_rate,\n",
    "                         epochs=epochs,\n",
    "                         best_params=best_params)\n",
    "                    Javascript('IPython.notebook.kernel.restart()')\n",
    "                    time.sleep(1)\n",
    "                    Javascript('IPython.notebook.execute_all_cells()')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
