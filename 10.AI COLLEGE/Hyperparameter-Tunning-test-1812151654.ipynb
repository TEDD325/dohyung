{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/anaconda3/lib/python3.6/importlib/_bootstrap.py:219: RuntimeWarning: compiletime version 3.5 of module 'tensorflow.python.framework.fast_tensor_util' does not match runtime version 3.6\n",
      "  return f(*args, **kwds)\n",
      "/anaconda3/lib/python3.6/site-packages/h5py/__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n",
      "Using TensorFlow backend.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using Keras version: 2.2.2 backend: tensorflow\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/anaconda3/lib/python3.6/site-packages/sklearn/ensemble/weight_boosting.py:29: DeprecationWarning: numpy.core.umath_tests is an internal NumPy module and should not be imported. It will be removed in a future NumPy release.\n",
      "  from numpy.core.umath_tests import inner1d\n"
     ]
    }
   ],
   "source": [
    "# import library\n",
    "import tensorflow as tf\n",
    "\n",
    "import keras\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Dropout, Flatten, Activation\n",
    "from keras.layers import SimpleRNN, LSTM, GRU\n",
    "from keras.wrappers.scikit_learn import KerasClassifier\n",
    "from keras import backend as K\n",
    "from keras.utils import multi_gpu_model\n",
    "from keras import metrics\n",
    "from keras import losses\n",
    "from keras import __version__\n",
    "print('Using Keras version:', __version__, 'backend:', K.backend())\n",
    "# assert(LV(__version__) >= LV(\"2.0.0\"))\n",
    "from keras.backend.tensorflow_backend import set_session\n",
    "# config = tf.ConfigProto()\n",
    "# config.gpu_options.allow_growth = True\n",
    "# sess = tf.Session(config=config)\n",
    "# set_session(sess)\n",
    "# cfg = K.tf.ConfigProto()\n",
    "# cfg.gpu_options.allow_growth = True\n",
    "# K.set_session(K.tf.Session(config=cfg))\n",
    "\n",
    "# import os\n",
    "# os.environ[\"CUDA_DEVICE_ORDER\"]=\"PCI_BUS_ID\"\n",
    "# # The GPU id to use, usually either \"0\" or \"1\"\n",
    "# os.environ[\"CUDA_VISIBLE_DEVICES\"]=\"0\" \n",
    "\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "# from sklearn.pipeline import Pipeline\n",
    "from sklearn.preprocessing import MinMaxScaler, StandardScaler, PolynomialFeatures\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "from sklearn.metrics import precision_recall_fscore_support\n",
    "from sklearn.metrics import confusion_matrix, f1_score, precision_score, recall_score\n",
    "from sklearn.metrics import classification_report # https://stackoverflow.com/questions/50065484/getting-precision-recall-and-f1-score-per-class-in-keras\n",
    "\n",
    "from IPython.display import Javascript\n",
    "import numpy as np\n",
    "from distutils.version import LooseVersion as LV\n",
    "from datetime import datetime\n",
    "import csv\n",
    "import sys\n",
    "from os import listdir\n",
    "from os.path import isfile, join\n",
    "import pprint\n",
    "#import boto3\n",
    "import pickle\n",
    "import time\n",
    "import os.path\n",
    "import pickle\n",
    "sys.path.append(os.getcwd())\n",
    "from matplotlib import pyplot\n",
    "\n",
    "import smtplib\n",
    "from email.mime.text import MIMEText # simple MSG\n",
    "from email.mime.multipart import MIMEMultipart # complex MSG\n",
    "        \n",
    "pp = pprint.PrettyPrinter(indent=4)\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "def get_all_raw_data(coin_name_list, start_date, end_date, data_files_dir):\n",
    "    trading_files = []\n",
    "    for coin_name in coin_name_list:\n",
    "        for data_file_name in [f for f in listdir(data_files_dir) if isfile(join(data_files_dir, f))]:\n",
    "            if coin_name in data_file_name:\n",
    "                trading_files.append(data_file_name)\n",
    "\n",
    "    start_ms_time = datetime.strptime(start_date, \"%Y-%m-%d %H:%M:%S\").timestamp() * 1000\n",
    "    end_ms_time = datetime.strptime(end_date, \"%Y-%m-%d %H:%M:%S\").timestamp() * 1000\n",
    "    \n",
    "    raw_data = {} #전체 CSV Raw 데이터\n",
    "    for coin_name in coin_name_list:\n",
    "        raw_data[coin_name] = []\n",
    "    \n",
    "    #KRW 제외한 나머지 CSV Raw 데이터 수집\n",
    "    for coin_name in coin_name_list:\n",
    "        for data_file_name in trading_files:\n",
    "            if coin_name in data_file_name:\n",
    "                file = open(data_files_dir + data_file_name, 'r', encoding='utf-8')\n",
    "                rdr = csv.reader(file)\n",
    "                for line in rdr:\n",
    "                    if start_ms_time <= int(line[0]) and int(line[0]) <= end_ms_time:\n",
    "                        raw_data[coin_name].append(line)\n",
    "                file.close()\n",
    "    \n",
    "    for line in raw_data['BTC']:\n",
    "        raw_data['KRW'].append([line[0], line[1], 1, 1, 1, 1, 1.0, 'normal'])\n",
    "#     print(\"test\")\n",
    "    return raw_data\n",
    "\n",
    "def Make_Dataset(dir_path, coin_list, start_date, end_date, time_unit, window_size, gap, margin_rate):\n",
    "    print(\"Make_Dataset is Started.\")\n",
    "    a = time.time()\n",
    "\n",
    "    X = {}\n",
    "    y = {}\n",
    "    idx = []\n",
    "\n",
    "    key_name_X = \"X_\"\n",
    "    key_name_y = \"y_\"\n",
    "    for idx_time_unit in time_unit:\n",
    "        for idx_window_size in window_size:\n",
    "            for idx_gap in gap:\n",
    "                for idx_margix_rate in margin_rate:\n",
    "                    key_name_X += str(idx_time_unit) + \"_\" + str(idx_window_size) + \"_\" + str(idx_gap) + \"_\" + str(idx_margix_rate)\n",
    "                    key_name_y += str(idx_time_unit) + \"_\" + str(idx_window_size) + \"_\" + str(idx_gap) + \"_\" + str(idx_margix_rate)\n",
    "                    if (os.path.isfile(dir_path+key_name_X+\".pickle\")) is not True:\n",
    "                        print(key_name_X)\n",
    "                        X[key_name_X] = \\\n",
    "                        make_cryptocurrency_dataset(\n",
    "                                                    coin_list, \n",
    "                                                    start_date, \n",
    "                                                    end_date,\n",
    "                                                    idx_time_unit,\n",
    "                                                    idx_window_size,\n",
    "                                                    idx_gap,\n",
    "                                                    idx_margix_rate,\n",
    "                                                    data_files_dir)\n",
    "                        with open(dir_path + key_name_X+\".pickle\", 'wb') as handle:\n",
    "                            pickle.dump(X[key_name_X], handle, protocol=pickle.HIGHEST_PROTOCOL)\n",
    "                        X = {}\n",
    "                        print(\"X_success.\")\n",
    "                    if (os.path.isfile(dir_path + key_name_y+\".pickle\")) is not True:\n",
    "                        print(key_name_y)\n",
    "                        y[key_name_y] = \\\n",
    "                        make_cryptocurrency_dataset(\n",
    "                                                    coin_list, \n",
    "                                                    start_date, \n",
    "                                                    end_date,\n",
    "                                                    idx_time_unit,\n",
    "                                                    idx_window_size,\n",
    "                                                    idx_gap,\n",
    "                                                    idx_margix_rate,\n",
    "                                                    data_files_dir)\n",
    "                        with open(dir_path + key_name_y+\".pickle\", 'wb') as handle:\n",
    "                            pickle.dump(y[key_name_y], handle, protocol=pickle.HIGHEST_PROTOCOL)\n",
    "                        y = {}\n",
    "                        print(\"y_success.\")\n",
    "                    key_name_X = \"X_\"\n",
    "                    key_name_y = \"y_\"\n",
    "    b = time.time()\n",
    "    print(\"Make_Dataset is Done.\")\n",
    "    #print(\"time: \", b-a)\n",
    "\n",
    "def make_cryptocurrency_dataset(coin_name_list, start_date, end_date, time_unit, window_size, gap, margin_rate, data_files_dir):\n",
    "    y_trv = []\n",
    "    y_btv = []\n",
    "    num_coins = len(coin_name_list)\n",
    "    #raw_data = get_all_raw_data_from_aws(coin_name_list, start_date, end_date)\n",
    "    raw_data = get_all_raw_data(coin_name_list, start_date, end_date, data_files_dir)    \n",
    "    num_sticks = len(raw_data['BTC'])\n",
    "    \n",
    "    if time_unit % 10 != 0 or num_sticks < (time_unit / 10) * window_size + gap:\n",
    "        return None, None\n",
    "    \n",
    "    num = int(num_sticks - ((time_unit / 10) * window_size + gap) + 1)\n",
    "        \n",
    "    X = []\n",
    "    y = []\n",
    "    # (윈도우 개수, 코인 개수, 윈도우 사이즈, 3)\n",
    "    for idx in range(num):\n",
    "        X.append([])\n",
    "        y.append([])\n",
    "        idx_coin = 0\n",
    "        for coin_name in coin_name_list:\n",
    "            X[idx].append([])\n",
    "            \n",
    "            last_idx_in_window = int(idx + time_unit / 10 * window_size - 1)\n",
    "            close_price_in_last_idx_in_window = float(raw_data[coin_name][last_idx_in_window][3])\n",
    "            \n",
    "            for idx_in_window in range(window_size):\n",
    "                X[idx][idx_coin].append([])\n",
    "                idx_stick = int(idx + time_unit / 10 * (idx_in_window + 1) - 1)\n",
    "                X[idx][idx_coin][idx_in_window].append(float(raw_data[coin_name][idx_stick][3]) / close_price_in_last_idx_in_window)\n",
    "                X[idx][idx_coin][idx_in_window].append(float(raw_data[coin_name][idx_stick][4]) / close_price_in_last_idx_in_window)\n",
    "                X[idx][idx_coin][idx_in_window].append(float(raw_data[coin_name][idx_stick][5]) / close_price_in_last_idx_in_window)\n",
    "                X[idx][idx_coin][idx_in_window].append(float(raw_data[coin_name][idx_stick][6]))\n",
    "                \n",
    "            target_idx_for_window = int(idx + time_unit / 10 * window_size - 1 + gap)\n",
    "            target_price = float(raw_data[coin_name][target_idx_for_window][3])\n",
    "            \n",
    "            target = 0\n",
    "            if target_price >= close_price_in_last_idx_in_window * (1.0 + float(margin_rate) / 100.0):\n",
    "                target = 1\n",
    "            y[idx].append(target)\n",
    "            \n",
    "            idx_coin += 1\n",
    "           \n",
    "    X = np.asarray(X)\n",
    "    y = np.asarray(y)\n",
    "    \n",
    "    return X, y\n",
    "    \n",
    "def make_cryptocurrency_dataset_X(coin_name_list, start_date, end_date, time_unit, window_size, gap, margin_rate, data_files_dir):\n",
    "    num_coins = len(coin_name_list)\n",
    "    #raw_data = get_all_raw_data_from_aws(coin_name_list, start_date, end_date)\n",
    "    raw_data = get_all_raw_data(coin_name_list, start_date, end_date, data_files_dir)    \n",
    "    num_sticks = len(raw_data['BTC'])\n",
    "    \n",
    "    if time_unit % 10 != 0 or num_sticks < (time_unit / 10) * window_size + gap:\n",
    "        return None, None\n",
    "    \n",
    "    num = int(num_sticks - ((time_unit / 10) * window_size + gap) + 1)\n",
    "        \n",
    "    X = []\n",
    "    # (윈도우 개수, 코인 개수, 윈도우 사이즈, 3)\n",
    "    for idx in range(num):\n",
    "        X.append([])\n",
    "        idx_coin = 0\n",
    "        for coin_name in coin_name_list:\n",
    "            X[idx].append([])\n",
    "            \n",
    "            last_idx_in_window = int(idx + time_unit / 10 * window_size - 1)\n",
    "            close_price_in_last_idx_in_window = float(raw_data[coin_name][last_idx_in_window][3])\n",
    "            \n",
    "            for idx_in_window in range(window_size):\n",
    "                X[idx][idx_coin].append([])\n",
    "                idx_stick = int(idx + time_unit / 10 * (idx_in_window + 1) - 1)\n",
    "                X[idx][idx_coin][idx_in_window].append(float(raw_data[coin_name][idx_stick][3]) / close_price_in_last_idx_in_window)\n",
    "                X[idx][idx_coin][idx_in_window].append(float(raw_data[coin_name][idx_stick][4]) / close_price_in_last_idx_in_window)\n",
    "                X[idx][idx_coin][idx_in_window].append(float(raw_data[coin_name][idx_stick][5]) / close_price_in_last_idx_in_window)\n",
    "                X[idx][idx_coin][idx_in_window].append(float(raw_data[coin_name][idx_stick][6]))\n",
    "                \n",
    "    X = np.array(X)\n",
    "    \n",
    "    return X\n",
    "\n",
    "def make_cryptocurrency_dataset_y(coin_name_list, start_date, end_date, time_unit, window_size, gap, margin_rate, data_files_dir):\n",
    "    y_trv = []\n",
    "    y_btv = []\n",
    "    num_coins = len(coin_name_list)\n",
    "    #raw_data = get_all_raw_data_from_aws(coin_name_list, start_date, end_date)\n",
    "    raw_data = get_all_raw_data(coin_name_list, start_date, end_date, data_files_dir)    \n",
    "    num_sticks = len(raw_data['BTC'])\n",
    "    \n",
    "    if time_unit % 10 != 0 or num_sticks < (time_unit / 10) * window_size + gap:\n",
    "        return None, None\n",
    "    \n",
    "    num = int(num_sticks - ((time_unit / 10) * window_size + gap) + 1)\n",
    "        \n",
    "    \n",
    "    y = []\n",
    "    # (윈도우 개수, 코인 개수, 윈도우 사이즈, 3)\n",
    "    for idx in range(num):\n",
    "        \n",
    "        y.append([])\n",
    "        idx_coin = 0\n",
    "        for coin_name in coin_name_list:            \n",
    "            last_idx_in_window = int(idx + time_unit / 10 * window_size - 1)\n",
    "            close_price_in_last_idx_in_window = float(raw_data[coin_name][last_idx_in_window][3])\n",
    "            target_idx_for_window = int(idx + time_unit / 10 * window_size - 1 + gap)\n",
    "            target_price = float(raw_data[coin_name][target_idx_for_window][3])\n",
    "            target = 0\n",
    "            \n",
    "            if target_price >= close_price_in_last_idx_in_window * (1.0 + float(margin_rate) / 100.0):\n",
    "                target = 1\n",
    "            y[idx].append(target)\n",
    "            \n",
    "            idx_coin += 1\n",
    "           \n",
    "    y = np.asarray(y)\n",
    "    \n",
    "    return y\n",
    "\n",
    "#\n",
    "\n",
    "def Load_Dataset_X(dir_path, time_unit, window_size, gap, margin_rate):\n",
    "    key_name_X = \"X_\" + \\\n",
    "                    str(time_unit) + \"_\" + \\\n",
    "                    str(window_size) + \"_\" + \\\n",
    "                    str(gap) + \"_\" + \\\n",
    "                    str(margin_rate)\n",
    "                    \n",
    "\n",
    "    with open(dir_path + key_name_X + \".pickle\", 'rb') as handle:\n",
    "        b_x = pickle.load(handle)\n",
    "    return b_x\n",
    "    \n",
    "def Load_Dataset_y(dir_path, time_unit, window_size, gap, margin_rate):\n",
    "    key_name_y = \"y_\" + \\\n",
    "                    str(time_unit) + \"_\" + \\\n",
    "                    str(window_size) + \"_\" + \\\n",
    "                    str(gap) + \"_\" + \\\n",
    "                    str(margin_rate)\n",
    "                    \n",
    "\n",
    "    with open(dir_path + key_name_y + \".pickle\", 'rb') as handle:\n",
    "        b_y = pickle.load(handle)\n",
    "    return b_y\n",
    "    \n",
    "def Make_Dataset_numpy(dir_path, coin_list, start_date, end_date, time_unit, window_size, gap, margin_rate, data_files_dir):\n",
    "    print(\"Make_Dataset_numpy is Started.\")\n",
    "    a = time.time()\n",
    "\n",
    "    X = {}\n",
    "    y = {}\n",
    "    idx = []\n",
    "\n",
    "    key_name_X = \"X_\"\n",
    "    key_name_y = \"y_\"\n",
    "    for idx_time_unit in time_unit:\n",
    "        for idx_window_size in window_size:\n",
    "            for idx_gap in gap:\n",
    "                for idx_margix_rate in margin_rate:\n",
    "                    key_name_X += str(idx_time_unit) + \"_\" + str(idx_window_size) + \"_\" + str(idx_gap) + \"_\" + str(idx_margix_rate)\n",
    "                    key_name_y += str(idx_time_unit) + \"_\" + str(idx_window_size) + \"_\" + str(idx_gap) + \"_\" + str(idx_margix_rate)\n",
    "                    if (os.path.isfile(dir_path + key_name_X+\".pickle\")) is not True:\n",
    "                        print(key_name_X)\n",
    "                        X[key_name_X] = \\\n",
    "                        make_cryptocurrency_dataset_X(\n",
    "                                                    coin_list, \n",
    "                                                    start_date, \n",
    "                                                    end_date,\n",
    "                                                    idx_time_unit,\n",
    "                                                    idx_window_size,\n",
    "                                                    idx_gap,\n",
    "                                                    idx_margix_rate,\n",
    "                                                    data_files_dir)\n",
    "                        with open(dir_path + key_name_X+\".pickle\", 'wb') as handle:\n",
    "                            pickle.dump(X[key_name_X], handle, protocol=pickle.HIGHEST_PROTOCOL)\n",
    "                        X = {}\n",
    "                        print(\"X_success.\")\n",
    "                    if (os.path.isfile(dir_path+key_name_y+\".pickle\")) is not True:\n",
    "                        print(key_name_y)\n",
    "                        y[key_name_y] = \\\n",
    "                        make_cryptocurrency_dataset_y(\n",
    "                                                    coin_list, \n",
    "                                                    start_date, \n",
    "                                                    end_date,\n",
    "                                                    idx_time_unit,\n",
    "                                                    idx_window_size,\n",
    "                                                    idx_gap,\n",
    "                                                    idx_margix_rate,\n",
    "                                                    data_files_dir)\n",
    "                        with open(dir_path + key_name_y+\".pickle\", 'wb') as handle:\n",
    "                            pickle.dump(y[key_name_y], handle, protocol=pickle.HIGHEST_PROTOCOL)\n",
    "                        y = {}\n",
    "                        print(\"y_success.\")\n",
    "                    key_name_X = \"X_\"\n",
    "                    key_name_y = \"y_\"\n",
    "    b = time.time()\n",
    "    print(\"Make_Dataset_numpy is Done.\")\n",
    "    print()\n",
    "    #print(\"time: \", b-a)\n",
    "    \n",
    "    \n",
    "def Make_Dataset_tuple(dir_path, coin_list, start_date, end_date, time_unit, window_size, gap, margin_rate, data_files_dir):\n",
    "    print(\"Make_Dataset_tuple is Started.\")\n",
    "    a = time.time()\n",
    "\n",
    "    X = {}\n",
    "    y = {}\n",
    "    idx = []\n",
    "\n",
    "    key_name_X = \"X_\"\n",
    "    key_name_y = \"y_\"\n",
    "    for idx_time_unit in time_unit:\n",
    "        for idx_window_size in window_size:\n",
    "            for idx_gap in gap:\n",
    "                for idx_margix_rate in margin_rate:\n",
    "                    key_name_X += str(idx_time_unit) + \"_\" + str(idx_window_size) + \"_\" + str(idx_gap) + \"_\" + str(idx_margix_rate)\n",
    "                    key_name_y += str(idx_time_unit) + \"_\" + str(idx_window_size) + \"_\" + str(idx_gap) + \"_\" + str(idx_margix_rate)\n",
    "                    if (os.path.isfile(dir_path + key_name_X+\".pickle\")) is not True:\n",
    "                        print(key_name_X)\n",
    "                        X[key_name_X], _ = \\\n",
    "                        make_cryptocurrency_dataset(\n",
    "                                                    coin_list, \n",
    "                                                    start_date, \n",
    "                                                    end_date,\n",
    "                                                    idx_time_unit,\n",
    "                                                    idx_window_size,\n",
    "                                                    idx_gap,\n",
    "                                                    idx_margix_rate,\n",
    "                                                    data_files_dir)\n",
    "                        with open(dir_path + key_name_X+\".pickle\", 'wb') as handle:\n",
    "                            pickle.dump(X[key_name_X], handle, protocol=pickle.HIGHEST_PROTOCOL)\n",
    "                        X = {}\n",
    "                        print(\"X_success.\")\n",
    "                    if (os.path.isfile(dir_path+key_name_y+\".pickle\")) is not True:\n",
    "                        print(key_name_y)\n",
    "                        _, y[key_name_y] = \\\n",
    "                        make_cryptocurrency_dataset(\n",
    "                                                    coin_list, \n",
    "                                                    start_date, \n",
    "                                                    end_date,\n",
    "                                                    idx_time_unit,\n",
    "                                                    idx_window_size,\n",
    "                                                    idx_gap,\n",
    "                                                    idx_margix_rate,\n",
    "                                                    data_files_dir)\n",
    "                        with open(dir_path + key_name_y+\".pickle\", 'wb') as handle:\n",
    "                            pickle.dump(y[key_name_y], handle, protocol=pickle.HIGHEST_PROTOCOL)\n",
    "                        y = {}\n",
    "                        print(\"y_success.\")\n",
    "                    key_name_X = \"X_\"\n",
    "                    key_name_y = \"y_\"\n",
    "    b = time.time()\n",
    "    print(\"Make_Dataset_tuple is Done.\")\n",
    "    print()\n",
    "    #print(\"time: \", b-a)\n",
    "\n",
    "def Check_Dataset(dir_path, coin_list, start_date, end_date, time_unit, window_size, gap, margin_rate, data_files_dir):\n",
    "    print(\"Check_Dataset is Started.\")\n",
    "    a = time.time()\n",
    "\n",
    "    X = {}\n",
    "    y = {}\n",
    "    idx = []\n",
    "\n",
    "    key_name_X = \"X_\"\n",
    "    key_name_y = \"y_\"\n",
    "    for idx_time_unit in time_unit:\n",
    "        for idx_window_size in window_size:\n",
    "            for idx_gap in gap:\n",
    "                for idx_margix_rate in margin_rate:\n",
    "                    key_name_X += str(idx_time_unit) + \"_\" + str(idx_window_size) + \"_\" + str(idx_gap) + \"_\" + str(idx_margix_rate)\n",
    "                    key_name_y += str(idx_time_unit) + \"_\" + str(idx_window_size) + \"_\" + str(idx_gap) + \"_\" + str(idx_margix_rate)\n",
    "                    \n",
    "                    with open(dir_path + key_name_X+\".pickle\", 'rb') as handle:\n",
    "                        data = pickle.load(handle)\n",
    "    \n",
    "                    if type(data) == tuple:\n",
    "#                         print()\n",
    "                        os.system('rm '+dir_path + key_name_X+\".pickle\")\n",
    "                        print(key_name_X,\".pickle is removed.\")\n",
    "                        os.system('rm '+dir_path + key_name_y+\".pickle\")\n",
    "                        print(key_name_y,\".pickle is removed.\")\n",
    "        \n",
    "                    if (os.path.isfile(dir_path+key_name_X+\".pickle\")) is not True:\n",
    "                        print(key_name_X)\n",
    "                        X[key_name_X] = \\\n",
    "                        make_cryptocurrency_dataset_X(\n",
    "                                                    coin_list, \n",
    "                                                    start_date, \n",
    "                                                    end_date,\n",
    "                                                    idx_time_unit,\n",
    "                                                    idx_window_size,\n",
    "                                                    idx_gap,\n",
    "                                                    idx_margix_rate,\n",
    "                                                    data_files_dir)\n",
    "                        with open(dir_path + key_name_X+\".pickle\", 'wb') as handle:\n",
    "                            pickle.dump(X[key_name_X], handle, protocol=pickle.HIGHEST_PROTOCOL)\n",
    "                        X = {}\n",
    "                        print(\"[SUCCESS] \",key_name_X,\".pickle is created.\")\n",
    "                    if (os.path.isfile(dir_path+key_name_y+\".pickle\")) is not True:\n",
    "                        print(key_name_y)\n",
    "                        y[key_name_y] = \\\n",
    "                        make_cryptocurrency_dataset_y(\n",
    "                                                    coin_list, \n",
    "                                                    start_date, \n",
    "                                                    end_date,\n",
    "                                                    idx_time_unit,\n",
    "                                                    idx_window_size,\n",
    "                                                    idx_gap,\n",
    "                                                    idx_margix_rate,\n",
    "                                                    data_files_dir)\n",
    "                        with open(dir_path + key_name_y+\".pickle\", 'wb') as handle:\n",
    "                            pickle.dump(y[key_name_y], handle, protocol=pickle.HIGHEST_PROTOCOL)\n",
    "                        y = {}\n",
    "                        print(\"[SUCCESS] \",key_name_y,\".pickle is created.\")\n",
    "                    key_name_X = \"X_\"\n",
    "                    key_name_y = \"y_\"\n",
    "    b = time.time()\n",
    "    print(\"Check_Dataset is Done.\")\n",
    "    #print(\"time: \", b-a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create Dataset\n",
    "data_files_dir = \"/Users/dohyung/OneDrive/2018-RNN/RNN_python/AWS_dataset/\"\n",
    "dataset_dir_path_tuple_type = \"./dataset_pickle_tuple_type/\"\n",
    "dataset_dir_path_numpy_type = \"./dataset_pickle_numpy.ndarray_type)/\"\n",
    "coin_list = [\"KRW\", \"BTC\", \"ETH\", \"XRP\", \"BCH\", \"LTC\", \"DASH\", \"ETC\"]\n",
    "start_date = \"2017-08-04 21:40:00\"\n",
    "end_date = \"2018-08-20 23:50:00\"\n",
    "# time_unit = [10,30,60]     # candle stick minutes\n",
    "# window_size = [10,25,50,75,100]  # Unit: num. of candle sticks\n",
    "# gap = [1,2,3]            # Unit: num. of candle sticks\n",
    "# margin_rate = [0.1,0.25,0.5]  # Unit: percent\n",
    "\n",
    "# for slave04\n",
    "time_unit = [10,30,60]     # candle stick minutes\n",
    "window_size = [10,25,50]  # Unit: num. of candle sticks\n",
    "gap = [1,2,3]            # Unit: num. of candle sticks\n",
    "margin_rate = [0.1,0.25,0.5]  # Unit: percent\n",
    "\n",
    "# for slave05\n",
    "# time_unit = [10,30,60]     # candle stick minutes\n",
    "# window_size = [75,100]  # Unit: num. of candle sticks\n",
    "# gap = [1,2,3]            # Unit: num. of candle sticks\n",
    "# margin_rate = [0.1,0.25,0.5]  # Unit: percent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#Make_Dataset_tuple(dataset_dir_path_tuple_type, coin_list, start_date, end_date, time_unit, window_size, gap, margin_rate)\n",
    "#Make_Dataset_numpy(dataset_dir_path_numpy_type, coin_list, start_date, end_date, time_unit, window_size, gap, margin_rate)\n",
    "#Check_Dataset(dataset_dir_path, coin_list, start_date, end_date, time_unit, window_size, gap, margin_rate)\n",
    "# Tuple 형태의 데이터셋이 나오지 않도록."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_history(history):\n",
    "    loss_list = [s for s in history.history.keys() if 'loss' in s and 'val' not in s]\n",
    "    val_loss_list = [s for s in history.history.keys() if 'loss' in s and 'val' in s]\n",
    "    acc_list = [s for s in history.history.keys() if 'acc' in s and 'val' not in s]\n",
    "    val_acc_list = [s for s in history.history.keys() if 'acc' in s and 'val' in s]\n",
    "    \n",
    "    if len(loss_list) == 0:\n",
    "        print('Loss is missing in history')\n",
    "        return \n",
    "    \n",
    "    ## As loss always exists\n",
    "    epochs = range(1,len(history.history[loss_list[0]]) + 1)\n",
    "    \n",
    "    ## Loss\n",
    "    plt.figure(1)\n",
    "    for l in loss_list:\n",
    "        plt.plot(epochs, history.history[l], 'b', label='Training loss (' + str(str(format(history.history[l][-1],'.5f'))+')'))\n",
    "    for l in val_loss_list:\n",
    "        plt.plot(epochs, history.history[l], 'g', label='Validation loss (' + str(str(format(history.history[l][-1],'.5f'))+')'))\n",
    "    \n",
    "    plt.title('Loss')\n",
    "    plt.xlabel('Epochs')\n",
    "    plt.ylabel('Loss')\n",
    "    plt.legend()\n",
    "    \n",
    "    ## Accuracy\n",
    "    plt.figure(2)\n",
    "    for l in acc_list:\n",
    "        plt.plot(epochs, history.history[l], 'b', label='Training accuracy (' + str(format(history.history[l][-1],'.5f'))+')')\n",
    "    for l in val_acc_list:    \n",
    "        plt.plot(epochs, history.history[l], 'g', label='Validation accuracy (' + str(format(history.history[l][-1],'.5f'))+')')\n",
    "\n",
    "    plt.title('Accuracy')\n",
    "    plt.xlabel('Epochs')\n",
    "    plt.ylabel('Accuracy')\n",
    "    plt.legend()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def f1_score(y_true, y_pred):\n",
    "    def recall(y_true, y_pred):\n",
    "        \"\"\"Recall metric.\n",
    "\n",
    "        Only computes a batch-wise average of recall.\n",
    "\n",
    "        Computes the recall, a metric for multi-label classification of\n",
    "        how many relevant items are selected.\n",
    "        \"\"\"\n",
    "        true_positives = K.sum(K.round(K.clip(y_true * y_pred, 0, 1)))\n",
    "        possible_positives = K.sum(K.round(K.clip(y_true, 0, 1)))\n",
    "        recall = true_positives / (possible_positives + K.epsilon())\n",
    "        return recall\n",
    "\n",
    "    def precision(y_true, y_pred):\n",
    "        \"\"\"Precision metric.\n",
    "\n",
    "        Only computes a batch-wise average of precision.\n",
    "\n",
    "        Computes the precision, a metric for multi-label classification of\n",
    "        how many selected items are relevant.\n",
    "        \"\"\"\n",
    "        true_positives = K.sum(K.round(K.clip(y_true * y_pred, 0, 1)))\n",
    "        predicted_positives = K.sum(K.round(K.clip(y_pred, 0, 1)))\n",
    "        precision = true_positives / (predicted_positives + K.epsilon())\n",
    "        return precision\n",
    "    precision = precision(y_true, y_pred)\n",
    "    recall = recall(y_true, y_pred)\n",
    "    return 2*((precision*recall)/(precision+recall+K.epsilon()))\n",
    "\n",
    "def f1_score_(y_true, y_pred):\n",
    "\n",
    "    # Count positive samples.\n",
    "    c1 = K.sum(K.round(K.clip(y_true * y_pred, 0, 1)))\n",
    "    c2 = K.sum(K.round(K.clip(y_pred, 0, 1)))\n",
    "    c3 = K.sum(K.round(K.clip(y_true, 0, 1)))\n",
    "\n",
    "    # If there are no true samples, fix the F1 score at 0.\n",
    "    if c3 == 0:\n",
    "        return 0\n",
    "\n",
    "    # How many selected items are relevant?\n",
    "    precision = c1 / (c2 + 1e-7)\n",
    "    \n",
    "    # How many relevant items are selected?\n",
    "    recall = c1 / (c3 +  + 1e-7)\n",
    "\n",
    "    # Calculate f1_score\n",
    "    f1_score = 2 * (precision * recall) / ((precision + recall) + 1e-7)\n",
    "    return f1_score \n",
    "\n",
    "\n",
    "def create_model_RNN(window_size, n_state_units=32, activation='softmax', optimizer='adam', init='glorot_uniform', dropout_rate=0.0, neurons=2):\n",
    "    model = Sequential()\n",
    "    model.add(# if문을 통해 여러 RNN모델 쓸 수 있도록 하기, SimpleRNN외에 다른 RNN모델 찾아보기\n",
    "        RNN(  n_state_units, \n",
    "              input_shape=(window_size, 32),\n",
    "              use_bias=True, \n",
    "              activation='tanh',\n",
    "              kernel_initializer='glorot_uniform', \n",
    "              recurrent_initializer='orthogonal', \n",
    "              bias_initializer='zeros', \n",
    "              dropout=0.0,\n",
    "              recurrent_dropout=0.0))\n",
    "    \n",
    "    model.add(Dense(units=neurons))\n",
    "    model.add(Dropout(dropout_rate))\n",
    "#     model.add(Dense(units=2))\n",
    "    model = multi_gpu_model(model, gpus=2)\n",
    "\n",
    "    model.compile(loss=keras.losses.categorical_crossentropy, \n",
    "                  optimizer=optimizer, \n",
    "                  metrics=[\"accuracy\", f1_score])\n",
    "\n",
    "    return model\n",
    "\n",
    "\n",
    "def create_model_SimpleRNN(window_size, n_state_units=32, activation='softmax', optimizer='adam', init='glorot_uniform', dropout_rate=0.0, neurons=2):\n",
    "    model = Sequential()\n",
    "    model.add(# if문을 통해 여러 RNN모델 쓸 수 있도록 하기, SimpleRNN외에 다른 RNN모델 찾아보기\n",
    "        SimpleRNN(n_state_units, \n",
    "                  input_shape=(window_size, 32),\n",
    "                  use_bias=True, \n",
    "                  activation='tanh',\n",
    "                  kernel_initializer='glorot_uniform', \n",
    "                  recurrent_initializer='orthogonal', \n",
    "                  bias_initializer='zeros', \n",
    "                  dropout=0.0,\n",
    "                  recurrent_dropout=0.0))\n",
    "    \n",
    "    model.add(Dense(units=neurons))\n",
    "    model.add(Dropout(dropout_rate))\n",
    "#     model.add(Dense(units=2))\n",
    "    model = multi_gpu_model(model, gpus=2)\n",
    "\n",
    "    model.compile(loss=keras.losses.categorical_crossentropy, \n",
    "                  optimizer=optimizer, \n",
    "                  metrics=[\"accuracy\", f1_score])\n",
    "\n",
    "    return model\n",
    "\n",
    "def create_model_LSTM(window_size, n_state_units=32, activation='softmax', optimizer='adam', init='glorot_uniform', dropout_rate=0.0, neurons=2):\n",
    "    model = Sequential()\n",
    "    model.add(# if문을 통해 여러 RNN모델 쓸 수 있도록 하기, SimpleRNN외에 다른 RNN모델 찾아보기\n",
    "        LSTM( n_state_units, \n",
    "              input_shape=(window_size, 32),\n",
    "              use_bias=True, \n",
    "              activation='tanh',\n",
    "              kernel_initializer='glorot_uniform', \n",
    "              recurrent_initializer='orthogonal', \n",
    "              bias_initializer='zeros', \n",
    "              dropout=0.0,\n",
    "              recurrent_dropout=0.0))\n",
    "    \n",
    "    model.add(Dense(units=neurons))\n",
    "    model.add(Dropout(dropout_rate))\n",
    "#     model.add(Dense(units=2))\n",
    "    model = multi_gpu_model(model, gpus=2)\n",
    "\n",
    "    model.compile(loss=keras.losses.categorical_crossentropy, \n",
    "                  optimizer=optimizer, \n",
    "                  metrics=[\"accuracy\", f1_score])\n",
    "\n",
    "    return model\n",
    "\n",
    "\n",
    "def create_model_GRU(window_size, n_state_units=32, activation='softmax', optimizer='adam', init='glorot_uniform', dropout_rate=0.0, neurons=2):\n",
    "    model = Sequential()\n",
    "    model.add(# if문을 통해 여러 RNN모델 쓸 수 있도록 하기, SimpleRNN외에 다른 RNN모델 찾아보기\n",
    "        GRU(  n_state_units, \n",
    "              input_shape=(window_size, 32),\n",
    "              use_bias=True, \n",
    "              activation='tanh',\n",
    "              kernel_initializer='glorot_uniform', \n",
    "              recurrent_initializer='orthogonal', \n",
    "              bias_initializer='zeros', \n",
    "              dropout=0.0,\n",
    "              recurrent_dropout=0.0))\n",
    "    \n",
    "    model.add(Dense(units=neurons))\n",
    "    model.add(Dropout(dropout_rate))\n",
    "#     model.add(Dense(units=2))\n",
    "    model = multi_gpu_model(model, gpus=2)\n",
    "\n",
    "    model.compile(loss=keras.losses.categorical_crossentropy, \n",
    "                  optimizer=optimizer, \n",
    "                  metrics=[\"accuracy\", f1_score])\n",
    "\n",
    "    return model\n",
    "\n",
    "\n",
    "def create_model_RNN_non_GPU(window_size, n_state_units=32, activation='softmax', optimizer='adam', init='glorot_uniform', dropout_rate=0.0, neurons=2):\n",
    "    model = Sequential()\n",
    "    model.add(# if문을 통해 여러 RNN모델 쓸 수 있도록 하기, SimpleRNN외에 다른 RNN모델 찾아보기\n",
    "        RNN(  n_state_units, \n",
    "              input_shape=(window_size, 32),\n",
    "              use_bias=True, \n",
    "              activation='tanh',\n",
    "              kernel_initializer='glorot_uniform', \n",
    "              recurrent_initializer='orthogonal', \n",
    "              bias_initializer='zeros', \n",
    "              dropout=0.0,\n",
    "              recurrent_dropout=0.0))\n",
    "    \n",
    "    model.add(Dense(units=neurons))\n",
    "    model.add(Dropout(dropout_rate))\n",
    "#     model.add(Dense(units=2))\n",
    "#     model = multi_gpu_model(model, gpus=2)\n",
    "\n",
    "    model.compile(loss=keras.losses.categorical_crossentropy, \n",
    "                  optimizer=optimizer, \n",
    "                  metrics=[\"accuracy\", f1_score])\n",
    "\n",
    "    return model\n",
    "\n",
    "\n",
    "def create_model_SimpleRNN_non_GPU_test(x_train, y_train, x_val, y_val, params):\n",
    "    model = Sequential()\n",
    "    model.add(# if문을 통해 여러 RNN모델 쓸 수 있도록 하기, SimpleRNN외에 다른 RNN모델 찾아보기\n",
    "        SimpleRNN(n_state_units, \n",
    "                  params['first_neuron'],\n",
    "                  input_dim=x_train.shape[2], # [dataset 크기, 윈도우 사이즈, 32(코인개수*OLHC)]\n",
    "#                   input_shape=(window_size, 32),\n",
    "#                   use_bias=True, \n",
    "                  activation='relu'))\n",
    "#                   kernel_initializer='glorot_uniform', \n",
    "#                   recurrent_initializer='orthogonal', \n",
    "#                   bias_initializer='zeros', \n",
    "#                   dropout=0.0,\n",
    "#                   recurrent_dropout=0.0))\n",
    "    \n",
    "#     model.add(Dense(units=neurons))\n",
    "#     model.add(Dropout(dropout_rate))\n",
    "        \n",
    "    model.add(Dropout(params['dropout']))\n",
    "    model.add(Dense(y_train.shape[1],\n",
    "                    activation=params['last_activation']))\n",
    "        \n",
    "#     model.add(Dense(units=2))\n",
    "#     model = multi_gpu_model(model, gpus=2)\n",
    "    model.compile(optimizer=params['optimizer'](lr=lr_normalizer(params['lr'], params['optimizer'])),\n",
    "                  loss=params['loss'],\n",
    "                  metrics=['acc'])\n",
    "        \n",
    "    out = model.fit(x_train, y_train,\n",
    "                    batch_size=params['batch_size'],\n",
    "                    epochs=params['epochs'],\n",
    "                    verbose=1,\n",
    "                    validation_data=[x_val, y_val],\n",
    "                    callbacks=early_stopper(params['epochs'], mode='strict'))\n",
    "    \n",
    "    return out\n",
    "        \n",
    "def create_model_SimpleRNN_non_GPU(window_size, n_state_units=32, activation='softmax', optimizer='adam', init='glorot_uniform', dropout_rate=0.0, neurons=2):\n",
    "    model = Sequential()\n",
    "    model.add(# if문을 통해 여러 RNN모델 쓸 수 있도록 하기, SimpleRNN외에 다른 RNN모델 찾아보기\n",
    "        SimpleRNN(n_state_units, \n",
    "                  input_shape=(window_size, 32),\n",
    "                  use_bias=True, \n",
    "                  activation='tanh',\n",
    "                  kernel_initializer='glorot_uniform', \n",
    "                  recurrent_initializer='orthogonal', \n",
    "                  bias_initializer='zeros', \n",
    "                  dropout=0.0,\n",
    "                  recurrent_dropout=0.0))\n",
    "    \n",
    "    model.add(Dense(units=neurons))\n",
    "    model.add(Dropout(dropout_rate))\n",
    "#     model.add(Dense(units=2))\n",
    "#     model = multi_gpu_model(model, gpus=2)\n",
    "\n",
    "    model.compile(loss=keras.losses.categorical_crossentropy, \n",
    "                  optimizer=optimizer, \n",
    "                  metrics=[\"accuracy\", f1_score])\n",
    "\n",
    "    model.fit(X_train_scaled, y_train, \n",
    "              validation_data=(X_test_scaled,y_test))\n",
    "    return model\n",
    "\n",
    "def create_model_LSTM_non_GPU(window_size, n_state_units=32, activation='softmax', optimizer='adam', init='glorot_uniform', dropout_rate=0.0, neurons=2):\n",
    "    model = Sequential()\n",
    "    model.add(# if문을 통해 여러 RNN모델 쓸 수 있도록 하기, SimpleRNN외에 다른 RNN모델 찾아보기\n",
    "        LSTM( n_state_units, \n",
    "              input_shape=(window_size, 32),\n",
    "              use_bias=True, \n",
    "              activation='tanh',\n",
    "              kernel_initializer='glorot_uniform', \n",
    "              recurrent_initializer='orthogonal', \n",
    "              bias_initializer='zeros', \n",
    "              dropout=0.0,\n",
    "              recurrent_dropout=0.0))\n",
    "    \n",
    "    model.add(Dense(units=neurons))\n",
    "    model.add(Dropout(dropout_rate))\n",
    "#     model.add(Dense(units=2))\n",
    "#     model = multi_gpu_model(model, gpus=2)\n",
    "\n",
    "    model.compile(loss=keras.losses.categorical_crossentropy, \n",
    "                  optimizer=optimizer, \n",
    "                  metrics=[\"accuracy\", f1_score])\n",
    "\n",
    "    return model\n",
    "\n",
    "\n",
    "def create_model_GRU_non_GPU(window_size, n_state_units=32, activation='softmax', optimizer='adam', init='glorot_uniform', dropout_rate=0.0, neurons=2):\n",
    "    model = Sequential()\n",
    "    model.add(# if문을 통해 여러 RNN모델 쓸 수 있도록 하기, SimpleRNN외에 다른 RNN모델 찾아보기\n",
    "        GRU(  n_state_units, \n",
    "              input_shape=(window_size, 32),\n",
    "              use_bias=True, \n",
    "              activation='tanh',\n",
    "              kernel_initializer='glorot_uniform', \n",
    "              recurrent_initializer='orthogonal', \n",
    "              bias_initializer='zeros', \n",
    "              dropout=0.0,\n",
    "              recurrent_dropout=0.0))\n",
    "    \n",
    "    model.add(Dense(units=neurons))\n",
    "    model.add(Dropout(dropout_rate))\n",
    "#     model.add(Dense(units=2))\n",
    "#     model = multi_gpu_model(model, gpus=2)\n",
    "\n",
    "    model.compile(loss=keras.losses.categorical_crossentropy, \n",
    "                  optimizer=optimizer, \n",
    "                  metrics=[\"accuracy\", f1_score])\n",
    "\n",
    "    return model\n",
    "\n",
    "\n",
    "def input_reshape(X_train_data, X_test_data, n_steps, n_coins, n_price):\n",
    "    X_train_reshape = X_train_data.reshape(\n",
    "        -1, \n",
    "        n_steps, \n",
    "        n_coins * n_price\n",
    "    )\n",
    "    X_test_reshape = X_test_data.reshape(\n",
    "        -1, \n",
    "        n_steps, \n",
    "        n_coins * n_price\n",
    "    )\n",
    "    return X_train_reshape, X_test_reshape\n",
    "\n",
    "def onehottify(x, n=None, dtype=np.int):\n",
    "    \"\"\"1-hot encode x with the max value n (computed from data if n is None).\"\"\"\n",
    "    x = np.asarray(x)\n",
    "    n = np.max(x) + 1 if n is None else n\n",
    "    return np.eye(n, dtype=dtype)[x]\n",
    "\n",
    "def Start_Model(pickle_load_dir_path, data_files_dir, epochs, pickle_result_dir_path, time_unit, window_size, gap, margin_rate, _TEST, _ENHANCE, _GPU, n_jobs, machine, Internet_connection):\n",
    "    X = {}\n",
    "    y = {}\n",
    "    if (_TEST == True): \n",
    "        MODEL_list = [\"SimpleRNN\"]\n",
    "        time_unit = [10]     # candle stick minutes\n",
    "        window_size = [10]  # Unit: num. of candle sticks\n",
    "        gap = [1]            # Unit: num. of candle sticks\n",
    "        margin_rate = [0.1]  # Unit: percent\n",
    "    elif (_TEST == False):\n",
    "        MODEL_list = [\"SimpleRNN\", \"RNN\", \"LSTM\", \"GRU\"]\n",
    "\n",
    "    key_name_X = \"X_\"\n",
    "    key_name_y = \"y_\"\n",
    "    \n",
    "    for MODEL_idx in MODEL_list:\n",
    "        MODEL = MODEL_idx\n",
    "        for idx_time_unit in time_unit:\n",
    "                for idx_window_size in window_size:\n",
    "                    for idx_gap in gap:\n",
    "                        for idx_margix_rate in margin_rate:\n",
    "                            key_name_X += str(idx_time_unit) + \"_\" + str(idx_window_size) + \"_\" + str(idx_gap) + \"_\" + str(idx_margix_rate)\n",
    "                            key_name_y += str(idx_time_unit) + \"_\" + str(idx_window_size) + \"_\" + str(idx_gap) + \"_\" + str(idx_margix_rate)\n",
    "                            \n",
    "                            X = Load_Dataset_X(pickle_load_dir_path, idx_time_unit, idx_window_size, idx_gap, idx_margix_rate)\n",
    "                            y = Load_Dataset_y(pickle_load_dir_path, idx_time_unit, idx_window_size, idx_gap, idx_margix_rate)\n",
    "\n",
    "                            y_single = {}\n",
    "                            y_single['BTC'] = y[:, 1]\n",
    "                            y_single['ETH'] = y[:, 2]\n",
    "                            y_single['XRP'] = y[:, 3]\n",
    "                            y_single['BCH'] = y[:, 4]\n",
    "                            y_single['LTC'] = y[:, 5]\n",
    "                            y_single['DASH'] = y[:, 6]\n",
    "                            y_single['ETC'] = y[:, 7]\n",
    "\n",
    "                            coin_list2 = [\"BTC\", \"ETH\", \"XRP\", \"BCH\", \"LTC\", \"DASH\", \"ETC\"]\n",
    "\n",
    "    #                         for coin in coin_list2:\n",
    "    #                             print(\"y_single[\"+coin+\"]\"+\".shape\")\n",
    "    #                             print(y_single[coin].shape)\n",
    "    #                             print()\n",
    "                            \n",
    "                            \n",
    "                            if (_TEST == False):\n",
    "                                for coin in coin_list2:\n",
    "                                    if (os.path.isfile(pickle_result_dir_path + \\\n",
    "                                                      MODEL + \"_\" + \\\n",
    "                                                      coin + \"_\" + \\\n",
    "                                                      str(idx_time_unit) + \"_\" + \\\n",
    "                                                      str(idx_window_size) + \"_\" + \\\n",
    "                                                      str(idx_gap) + \"_\" + \\\n",
    "                                                      str(idx_margix_rate) + \\\n",
    "                                                      \"_result.pickle\")) is True:\n",
    "                                        print(MODEL + \"_\" + \\\n",
    "                                              coin + \"_\" + \\\n",
    "                                              str(idx_time_unit) + \"_\" + \\\n",
    "                                              str(idx_window_size) + \"_\" + \\\n",
    "                                              str(idx_gap) + \"_\" + \\\n",
    "                                              str(idx_margix_rate) + \\\n",
    "                                              \"_result.pickle FILE ALREADY EXIST.\")\n",
    "                                        continue\n",
    "                                    else:\n",
    "                                        y2 = onehottify(y_single[coin], n=2)\n",
    "\n",
    "                                        Evaluate(pickle_load_dir_path, \n",
    "                                                 data_files_dir, \n",
    "                                                 epochs, \n",
    "                                                 pickle_result_dir_path, \n",
    "                                                 time_unit, \n",
    "                                                 window_size, \n",
    "                                                 gap, \n",
    "                                                 margin_rate, \n",
    "                                                 _TEST, \n",
    "                                                 _ENHANCE,\n",
    "                                                 coin,\n",
    "                                                 X, y2,\n",
    "                                                 key_name_X,\n",
    "                                                 key_name_y,\n",
    "                                                 idx_time_unit,\n",
    "                                                 idx_window_size,\n",
    "                                                 idx_gap,\n",
    "                                                 idx_margix_rate, \n",
    "                                                 MODEL,\n",
    "                                                 _GPU,\n",
    "                                                 n_jobs,\n",
    "                                                 machine,\n",
    "                                                 Internet_connection)\n",
    "\n",
    "                                    \n",
    "                            if (_TEST == True):\n",
    "                                # for test                                \n",
    "                                for coin in range(1):\n",
    "                                    if (os.path.isfile(pickle_result_dir_path + \\\n",
    "                                                      \"_test_\" + \\\n",
    "                                                      MODEL + \"_\" + \\\n",
    "                                                      \"BTC\" + \"_\" + \\\n",
    "                                                      str(idx_time_unit) + \"_\" + \\\n",
    "                                                      str(idx_window_size) + \"_\" + \\\n",
    "                                                      str(idx_gap) + \"_\" + \\\n",
    "                                                      str(idx_margix_rate) + \\\n",
    "                                                      \"_result.pickle\")) is True:\n",
    "                                        print(\"_test_\" + \\\n",
    "                                              MODEL + \"_\" + \\\n",
    "                                              \"BTC\" + \"_\" + \\\n",
    "                                              str(idx_time_unit) + \"_\" + \\\n",
    "                                              str(idx_window_size) + \"_\" + \\\n",
    "                                              str(idx_gap) + \"_\" + \\\n",
    "                                              str(idx_margix_rate) + \\\n",
    "                                              \"_result.pickle FILE ALREADY EXIST.\")\n",
    "                                        Javascript('IPython.notebook.kernel.restart()')\n",
    "                                        Javascript('IPython.notebook.execute_all_cells()')\n",
    "                                        continue\n",
    "                                    else:\n",
    "                                        y2 = onehottify(y_single['BTC'], n=2)                          \n",
    "\n",
    "                                        Evaluate(pickle_load_dir_path, \n",
    "                                                 data_files_dir, \n",
    "                                                 epochs, \n",
    "                                                 pickle_result_dir_path, \n",
    "                                                 time_unit, \n",
    "                                                 window_size, \n",
    "                                                 gap, \n",
    "                                                 margin_rate, \n",
    "                                                 _TEST, \n",
    "                                                 _ENHANCE,\n",
    "                                                 coin,\n",
    "                                                 X, y2,\n",
    "                                                 key_name_X,\n",
    "                                                 key_name_y,\n",
    "                                                 idx_time_unit,\n",
    "                                                 idx_window_size,\n",
    "                                                 idx_gap,\n",
    "                                                 idx_margix_rate, \n",
    "                                                 MODEL,\n",
    "                                                 _GPU,\n",
    "                                                 n_jobs, \n",
    "                                                 machine,\n",
    "                                                 Internet_connection)\n",
    "                                        Javascript('IPython.notebook.kernel.restart()')\n",
    "                                        Javascript('IPython.notebook.execute_all_cells()')\n",
    "\n",
    "# 저장된 pickle 파일의 데이터 구조\n",
    "# tmp = {}\n",
    "# tmp[\"10_1_1_0.1\"] = {\"grid_result.best_score_\":{}}, {\"grid_result.best_params_\":{}}\n",
    "# type(tmp[\"10_1_1_0.1\"][0])\n",
    "# print(tmp[\"10_1_1_0.1\"])\n",
    "# print(tmp[\"10_1_1_0.1\"])\n",
    "# print(tmp[\"10_1_1_0.1\"][0])\n",
    "# print(tmp[\"10_1_1_0.1\"][0]['grid_result.best_score_'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Evaluate(pickle_load_dir_path, \n",
    "             data_files_dir, \n",
    "             epochs, \n",
    "             pickle_result_dir_path, \n",
    "             time_unit, \n",
    "             window_size, \n",
    "             gap, \n",
    "             margin_rate, \n",
    "             _TEST, \n",
    "             _ENHANCE, \n",
    "             coin, \n",
    "             X, y2, \n",
    "             key_name_X,\n",
    "             key_name_y,\n",
    "             idx_time_unit,\n",
    "             idx_window_size,\n",
    "             idx_gap,\n",
    "             idx_margix_rate, \n",
    "             MODEL,\n",
    "             _GPU,\n",
    "             n_jobs, \n",
    "             machine,\n",
    "             Internet_connection):\n",
    "    \n",
    "    \n",
    "                        \n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y2, test_size=0.1, random_state=42)\n",
    "#     print(\"X_train.shape\")\n",
    "#     print(X_train.shape)\n",
    "#     print(\"y_train.shape\")\n",
    "#     print(y_train.shape)\n",
    "#     print()\n",
    "#     print(\"X_test.shape\")\n",
    "#     print(X_test.shape)\n",
    "#     print(\"y_test.shape\")\n",
    "#     print(y_test.shape)\n",
    "#     print()\n",
    "\n",
    "    n_coins = 8\n",
    "    n_price = 4\n",
    "    n_steps = idx_window_size # 원래 100이었음. reshape 문제 때문에 수정함\n",
    "\n",
    "    X_train_2 = X_train.transpose([0, 2, 1, 3])\n",
    "    X_test_2 = X_test.transpose([0, 2, 1, 3])\n",
    "#     print(\"X_train_2.shape\")\n",
    "#     print(X_train_2.shape)\n",
    "#     print(\"X_test_2.shape\")\n",
    "#     print(X_test_2.shape)\n",
    "#     print()\n",
    "\n",
    "    X_train_3 = X_train_2.reshape([X_train.shape[0], n_steps, n_coins * n_price])\n",
    "    X_test_3 = X_test_2.reshape([X_test.shape[0], n_steps, n_coins * n_price])\n",
    "#     print(\"X_train_3.shape\")\n",
    "#     print(X_train_3.shape)\n",
    "#     print(\"X_test_3.shape\")\n",
    "#     print(X_test_3.shape)\n",
    "#     print()\n",
    "\n",
    "    if (_TEST==True and _ENHANCE==False):\n",
    "        param_grid = {'window_size' : [n_steps], \n",
    "                      'n_state_units': [100],\n",
    "                      'activation': ['relu'], \n",
    "                      'optimizer': ['rmsprop'], #sgd 추가\n",
    "                      'init': ['glorot_uniform'], #he 추가\n",
    "                      'batch_size': [2048]}\n",
    "\n",
    "    elif (_TEST==False and _ENHANCE==False):\n",
    "        param_grid = {'window_size' : [n_steps], \n",
    "                      'n_state_units': [40, 80, 160],\n",
    "                      'activation': ['relu', 'softmax'], \n",
    "                      'optimizer': ['rmsprop', 'adam'], #sgd 추가\n",
    "                      'init': ['glorot_uniform', 'uniform', 'he_uniform'], #he 추가\n",
    "                      'batch_size': [64,128,256]}\n",
    "        \n",
    "    elif (_TEST==False and _ENHANCE==True):\n",
    "        param_grid = {'window_size' : [], \n",
    "                      'n_state_units': [],\n",
    "                      'activation': [], \n",
    "                      'optimizer': [], #sgd 추가\n",
    "                      'init': [], #he 추가\n",
    "                      'batch_size': [10, 50],\n",
    "                      'dropout_rate':[0.0, 0.1, 0.2, 0.3, 0.4, 0.5], # after paramter select. when epochs raise..\n",
    "                      'neurons':[2,10,100]}\n",
    "\n",
    "\n",
    "\n",
    "    X_train_reshape = X_train_2.reshape([X_train.shape[0], n_steps*n_coins * n_price])\n",
    "    X_test_reshape = X_test_2.reshape([X_test.shape[0], n_steps*n_coins * n_price])\n",
    "#     print(\"X_train_reshape.shape\")\n",
    "#     print(X_train_reshape.shape)\n",
    "#     print(\"X_test_reshape.shape\")\n",
    "#     print(X_test_reshape.shape)\n",
    "#     print()\n",
    "\n",
    "    scaler = MinMaxScaler()\n",
    "    scaler.fit(X_train_reshape)\n",
    "    X_train_scaled = scaler.transform(X_train_reshape)\n",
    "    X_test_scaled = scaler.transform(X_test_reshape)\n",
    "\n",
    "    X_train_scaled = X_train_scaled.reshape(-1, n_steps, n_coins * n_price)\n",
    "\n",
    "    X_test_scaled = X_test_scaled.reshape(-1, n_steps, n_coins * n_price)\n",
    "\n",
    "    \n",
    "    if _GPU == True:\n",
    "        if MODEL == \"SimpleRNN\" :\n",
    "            model = KerasClassifier(build_fn=create_model_SimpleRNN, \n",
    "                                    epochs=epochs, # epochs는 실험을 최종적으로 수행하고자 할 때 높일 것(100~150정도)\n",
    "#                                     batch_size=100, \n",
    "                                    verbose=True)\n",
    "\n",
    "        elif MODEL == \"LSTM\":\n",
    "            model = KerasClassifier(build_fn=create_model_LSTM, \n",
    "                                    epochs=epochs, # epochs는 실험을 최종적으로 수행하고자 할 때 높일 것(100~150정도)\n",
    "#                                     batch_size=100, \n",
    "                                    verbose=True)\n",
    "\n",
    "        elif MODEL == \"RNN\":\n",
    "            model = KerasClassifier(build_fn=create_model_RNN, \n",
    "                                    epochs=epochs, # epochs는 실험을 최종적으로 수행하고자 할 때 높일 것(100~150정도)\n",
    "#                                     batch_size=100, \n",
    "                                    verbose=True)\n",
    "\n",
    "        elif MODEL == \"GRU\":\n",
    "            model = KerasClassifier(build_fn=create_model_GRU, \n",
    "                                    epochs=epochs, # epochs는 실험을 최종적으로 수행하고자 할 때 높일 것(100~150정도)\n",
    "#                                     batch_size=100, \n",
    "                                    verbose=True)\n",
    "    elif _GPU == False:\n",
    "        if MODEL == \"SimpleRNN\" :\n",
    "            model = KerasClassifier(build_fn=create_model_SimpleRNN_non_GPU, \n",
    "                                    epochs=epochs, # epochs는 실험을 최종적으로 수행하고자 할 때 높일 것(100~150정도)\n",
    "                                    batch_size=10, \n",
    "                                    verbose=True)\n",
    "\n",
    "        elif MODEL == \"LSTM\":\n",
    "            model = KerasClassifier(build_fn=create_model_LSTM_non_GPU, \n",
    "                                    epochs=epochs, # epochs는 실험을 최종적으로 수행하고자 할 때 높일 것(100~150정도)\n",
    "                                    batch_size=10, \n",
    "                                    verbose=True)\n",
    "\n",
    "        elif MODEL == \"RNN\":\n",
    "            model = KerasClassifier(build_fn=create_model_RNN_non_GPU, \n",
    "                                    epochs=epochs, # epochs는 실험을 최종적으로 수행하고자 할 때 높일 것(100~150정도)\n",
    "                                    batch_size=10, \n",
    "                                    verbose=True)\n",
    "\n",
    "        elif MODEL == \"GRU\":\n",
    "            model = KerasClassifier(build_fn=create_model_GRU_non_GPU, \n",
    "                                    epochs=epochs, # epochs는 실험을 최종적으로 수행하고자 할 때 높일 것(100~150정도)\n",
    "                                    batch_size=10, \n",
    "                                    verbose=True)\n",
    "\n",
    "    grid = GridSearchCV(\n",
    "        estimator=model, \n",
    "        cv=5, \n",
    "        n_jobs=n_jobs, # test\n",
    "        param_grid=param_grid,\n",
    "        verbose=1)\n",
    "\n",
    "    X_train_scaled, X_test_scaled = input_reshape(X_train_scaled, X_test_scaled, n_steps, n_coins, n_price)\n",
    "    \n",
    "    if (_TEST == True): \n",
    "        print()\n",
    "        print()\n",
    "        print(\"TEST!\")\n",
    "        print()\n",
    "        print(\"----------------------\")\n",
    "        print(\"<\"+MODEL+\">\")\n",
    "        print(\"----------------------\")\n",
    "        print(\"__\"+\"BTC\"+\"__\" + \\\n",
    "                \"time unit: \"+str(idx_time_unit) + \"  |  \" + \\\n",
    "                \"window_size :\"+str(idx_window_size) + \"  |  \" + \\\n",
    "                \"gap :\"+str(idx_gap) + \"  |  \" + \\\n",
    "                \"margin_rate :\"+str(idx_margix_rate) + \\\n",
    "                \"  started.\")\n",
    "    elif (_TEST == False):\n",
    "        print()\n",
    "        print()\n",
    "        print(\"----------------------\")\n",
    "        print(\"<\"+MODEL+\">\")\n",
    "        print(\"----------------------\")\n",
    "        print(\"__\"+coin+\"__\" + \\\n",
    "                \"time unit: \"+str(idx_time_unit) + \"  |  \" + \\\n",
    "                \"window_size :\"+str(idx_window_size) + \"  |  \" + \\\n",
    "                \"gap :\"+str(idx_gap) + \"  |  \" + \\\n",
    "                \"margin_rate :\"+str(idx_margix_rate) + \\\n",
    "                \"  started.\")\n",
    "    \n",
    "#     from keras.optimizers import Adam, Nadam\n",
    "#     from keras.activations import softmax\n",
    "#     from keras.losses import categorical_crossentropy, logcosh\n",
    "\n",
    "#     p = {'lr': (2, 10, 30),\n",
    "#          'first_neuron':[4, 8, 16, 32, 64, 128],\n",
    "#          'batch_size': [2, 3, 4],\n",
    "#          'epochs': [500],\n",
    "#          'dropout': (0, 0.40, 10),\n",
    "#          'optimizer': [Adam, Nadam],\n",
    "#          'loss': [categorical_crossentropy, logcosh],\n",
    "#          'last_activation': [softmax],\n",
    "#          'weight_regulizer':[None],\n",
    "#          'emb_output_dims': [None]}\n",
    "\n",
    "#     import hyperio as hy\n",
    "#     h = hy.Hyperio(X_train_scaled, y_train, \n",
    "#                    params=p, \n",
    "#     #                dataset_name='iris', \n",
    "#                    experiment_no='1', \n",
    "#                    model=create_model_SimpleRNN_non_GPU_test,\n",
    "#                    # create_model_SimpleRNN_non_GPU_test(x_train, y_train, x_val, y_val, params)\n",
    "#                    grid_downsample=.01)\n",
    "\n",
    "###########################################################\n",
    "#     grid_result = grid.fit(X_train_scaled, \n",
    "#                            y_train, \n",
    "#                            validation_data=(X_test_scaled,y_test))\n",
    "    \n",
    "    \n",
    "    \n",
    "    print(\"----------------------\")\n",
    "\n",
    "    # \n",
    "#     means = grid_result.cv_results_['mean_test_score']\n",
    "#     stds = grid_result.cv_results_['std_test_score']\n",
    "#     params = grid_result.cv_results_['params']\n",
    "#     print(\"grid_result.cv_results_\",grid_result.cv_results_)\n",
    "#     print(\"grid_result.best_estimator_\",grid_result.best_estimator_)\n",
    "    print(\"grid_result.score(X_test_scaled, y_test): \",grid_result.score(X_test_scaled, y_test))\n",
    "    \n",
    "    \n",
    "            \n",
    "    evaluate_result = {}\n",
    "    \n",
    "    if (_TEST == True): \n",
    "        test_score = grid_result.score(X_test_scaled, y_test)\n",
    "        evaluate_result[MODEL + \"_\" + \\\n",
    "                      \"BTC\" + \"_\" + \\\n",
    "                      str(idx_time_unit) + \"_\" + \\\n",
    "                      str(idx_window_size) + \"_\" + \\\n",
    "                      str(idx_gap) + \"_\" + \\\n",
    "                      str(idx_margix_rate)] = {\"MODEL: \":MODEL,\\\n",
    "                                        \"Cryptocurrency: \":\"BTC\",\\\n",
    "#                                         \"grid_result.cv_results_\":grid_result.cv_results_, \\\n",
    "#                                         \"grid_result.best_estimator_\":grid_result.best_estimator_, \\\n",
    "                                        \"grid_result.cv_results_['mean_test_score']\":grid_result.cv_results_['mean_test_score'], \\\n",
    "                                        \"grid_result.cv_results_['params']\":grid_result.cv_results_['params'],\\\n",
    "                                        \"grid_result.test_score\":test_score}     \n",
    "                                        \n",
    "\n",
    "    elif (_TEST == False): \n",
    "        test_score = grid_result.score(X_test_scaled, y_test)\n",
    "        evaluate_result[MODEL + \"_\" + \\\n",
    "                      coin + \"_\" + \\\n",
    "                      str(idx_time_unit) + \"_\" + \\\n",
    "                      str(idx_window_size) + \"_\" + \\\n",
    "                      str(idx_gap) + \"_\" + \\\n",
    "                      str(idx_margix_rate)] = {\"MODEL: \":MODEL,\\\n",
    "                                        \"Cryptocurrency: \":coin, \\\n",
    "                                        \"grid_result.cv_results_['mean_test_score']\":grid_result.cv_results_['mean_test_score'], \\\n",
    "                                        \"grid_result.cv_results_['params']\":grid_result.cv_results_['params'],\\\n",
    "                                        \"grid_result.test_score\":test_score}\n",
    "#     print()\n",
    "#     print(\"evaluate result dict: \", evaluate_result)\n",
    "#     print()\n",
    "\n",
    "    # summarize results\n",
    "    print()\n",
    "    print(\"Best: %f using %s\" % (grid_result.best_score_, grid_result.best_params_))\n",
    "    print()\n",
    "    # for checking pickle file exist\n",
    "    print(\"---pickle saving..\")\n",
    "    if (_TEST == True):\n",
    "        X = {}\n",
    "        y = {}\n",
    "        key_name_X = \"X_\"\n",
    "        key_name_y = \"y_\"\n",
    "        for idx_time_unit in time_unit:\n",
    "            for idx_window_size in window_size:\n",
    "                for idx_gap in gap:\n",
    "                    for idx_margix_rate in margin_rate:\n",
    "                        key_name_X += str(idx_time_unit) + \"_\" + str(idx_window_size) + \"_\" + str(idx_gap) + \"_\" + str(idx_margix_rate)\n",
    "                        key_name_y += str(idx_time_unit) + \"_\" + str(idx_window_size) + \"_\" + str(idx_gap) + \"_\" + str(idx_margix_rate)\n",
    "                        if (os.path.isfile(pickle_result_dir_path + \\\n",
    "                                          \"_test_\" + \\\n",
    "                                          MODEL + \"_\" + \\\n",
    "                                          \"BTC\" + \"_\" + \\\n",
    "                                          str(idx_time_unit) + \"_\" + \\\n",
    "                                          str(idx_window_size) + \"_\" + \\\n",
    "                                          str(idx_gap) + \"_\" + \\\n",
    "                                          str(idx_margix_rate) + \\\n",
    "                                          \"_result.pickle\")) is not True:\n",
    "                            with open(pickle_result_dir_path + \\\n",
    "                                      \"_test_\" + \\\n",
    "                                      MODEL + \"_\" + \\\n",
    "                                      \"BTC\" + \"_\" + \\\n",
    "                                      str(idx_time_unit) + \"_\" + \\\n",
    "                                      str(idx_window_size) + \"_\" + \\\n",
    "                                      str(idx_gap) + \"_\" + \\\n",
    "                                      str(idx_margix_rate) + \\\n",
    "                                      \"_result.pickle\", 'wb') as handle:\n",
    "                                pickle.dump(evaluate_result, handle, protocol=pickle.HIGHEST_PROTOCOL)\n",
    "                                \n",
    "                                # Sending Email\n",
    "                                if Internet_connection == True:\n",
    "                                    smtp = smtplib.SMTP('smtp.naver.com', 587)\n",
    "                                    smtp.ehlo()      # say Hello\n",
    "                                    smtp.starttls()  # TLS 사용시 필요\n",
    "                                    smtp.login('dhgdohk@naver.com', '30892793@dohk')\n",
    "\n",
    "                                    msg = MIMEText(pickle_result_dir_path + \\\n",
    "                                                   \"_test_\" + \\\n",
    "                                                   MODEL + \"_\" + \\\n",
    "                                                   \"BTC\" + \"_\" + \\\n",
    "                                                   str(idx_time_unit) + \"_\" + \\\n",
    "                                                   str(idx_window_size) + \"_\" + \\\n",
    "                                                   str(idx_gap) + \"_\" + \\\n",
    "                                                   str(idx_margix_rate) + \\\n",
    "                                                   \"_result.pickle\")\n",
    "                                    msg['Subject'] =   pickle_result_dir_path + \\\n",
    "                                                       MODEL + \"_\" + \\\n",
    "                                                       \"BTC\" + \"_\" + \\\n",
    "                                                       str(idx_time_unit) + \"_\" + \\\n",
    "                                                       str(idx_window_size) + \"_\" + \\\n",
    "                                                       str(idx_gap) + \"_\" + \\\n",
    "                                                       str(idx_margix_rate) + \\\n",
    "                                                       \"_result.pickle\"\n",
    "                                    msg['To'] = 'dhgdohk@naver.com'\n",
    "                                    smtp.sendmail('dhgdohk@naver.com', 'dhgdohk@naver.com', msg.as_string())\n",
    "\n",
    "                                    smtp.quit()\n",
    "                        else:\n",
    "                            print(\"Already exist the file: \", pickle_result_dir_path + \\\n",
    "                                                              \"_test_\" + \\\n",
    "                                                              MODEL + \"_\" + \\\n",
    "                                                              \"BTC\" + \"_\" + \\\n",
    "                                                              str(idx_time_unit) + \"_\" + \\\n",
    "                                                              str(idx_window_size) + \"_\" + \\\n",
    "                                                              str(idx_gap) + \"_\" + \\\n",
    "                                                              str(idx_margix_rate) + \\\n",
    "                                                              \"_result.pickle\")\n",
    "\n",
    "    elif (_TEST == False): \n",
    "        X = {}\n",
    "        y = {}\n",
    "        key_name_X = \"X_\"\n",
    "        key_name_y = \"y_\"\n",
    "        for idx_time_unit in time_unit:\n",
    "            for idx_window_size in window_size:\n",
    "                for idx_gap in gap:\n",
    "                    for idx_margix_rate in margin_rate:\n",
    "                        key_name_X += str(idx_time_unit) + \"_\" + str(idx_window_size) + \"_\" + str(idx_gap) + \"_\" + str(idx_margix_rate)\n",
    "                        key_name_y += str(idx_time_unit) + \"_\" + str(idx_window_size) + \"_\" + str(idx_gap) + \"_\" + str(idx_margix_rate)\n",
    "                        if (os.path.isfile(pickle_result_dir_path + \\\n",
    "                                          MODEL + \"_\" + \\\n",
    "                                          coin + \"_\" + \\\n",
    "                                          str(idx_time_unit) + \"_\" + \\\n",
    "                                          str(idx_window_size) + \"_\" + \\\n",
    "                                          str(idx_gap) + \"_\" + \\\n",
    "                                          str(idx_margix_rate) + \\\n",
    "                                          \"_result.pickle\")) is not True:\n",
    "                            with open(pickle_result_dir_path + \\\n",
    "                                      MODEL + \"_\" + \\\n",
    "                                      coin + \"_\" + \\\n",
    "                                      str(idx_time_unit) + \"_\" + \\\n",
    "                                      str(idx_window_size) + \"_\" + \\\n",
    "                                      str(idx_gap) + \"_\" + \\\n",
    "                                      str(idx_margix_rate) + \\\n",
    "                                      \"_result.pickle\", 'wb') as handle:\n",
    "                                pickle.dump(evaluate_result, handle, protocol=pickle.HIGHEST_PROTOCOL)\n",
    "                                \n",
    "                                # Sending Email\n",
    "                                if Internet_connection == True:\n",
    "                                    smtp = smtplib.SMTP('smtp.naver.com', 587)\n",
    "                                    smtp.ehlo()      # say Hello\n",
    "                                    smtp.starttls()  # TLS 사용시 필요\n",
    "                                    smtp.login('dhgdohk@naver.com', '30892793@dohk')\n",
    "\n",
    "                                    msg = MIMEText(machine + \\\n",
    "                                                   pickle_result_dir_path + \\\n",
    "                                                   MODEL + \"_\" + \\\n",
    "                                                   coin + \"_\" + \\\n",
    "                                                   str(idx_time_unit) + \"_\" + \\\n",
    "                                                   str(idx_window_size) + \"_\" + \\\n",
    "                                                   str(idx_gap) + \"_\" + \\\n",
    "                                                   str(idx_margix_rate) + \\\n",
    "                                                   \"_result.pickle\")\n",
    "                                    msg['Subject'] =   machine + \\\n",
    "                                                       pickle_result_dir_path + \\\n",
    "                                                       MODEL + \"_\" + \\\n",
    "                                                       coin + \"_\" + \\\n",
    "                                                       str(idx_time_unit) + \"_\" + \\\n",
    "                                                       str(idx_window_size) + \"_\" + \\\n",
    "                                                       str(idx_gap) + \"_\" + \\\n",
    "                                                       str(idx_margix_rate) + \\\n",
    "                                                       \"_result.pickle\"\n",
    "                                    msg['To'] = 'dhgdohk@naver.com'\n",
    "                                    smtp.sendmail('dhgdohk@naver.com', 'dhgdohk@naver.com', msg.as_string())\n",
    "\n",
    "                                    smtp.quit()\n",
    "                        else:\n",
    "                            print(\"Already exist the file: \", pickle_result_dir_path + \\\n",
    "                                                              \"_test_\" + \\\n",
    "                                                              MODEL + \"_\" + \\\n",
    "                                                              \"BTC\" + \"_\" + \\\n",
    "                                                              str(idx_time_unit) + \"_\" + \\\n",
    "                                                              str(idx_window_size) + \"_\" + \\\n",
    "                                                              str(idx_gap) + \"_\" + \\\n",
    "                                                              str(idx_margix_rate) + \\\n",
    "                                                              \"_result.pickle\")\n",
    "                        Javascript('IPython.notebook.kernel.restart()')\n",
    "                        Javascript('IPython.notebook.execute_all_cells()')\n",
    "    print()\n",
    "\n",
    "\n",
    "#     for mean, stdev, param in zip(means, stds, params):\n",
    "#         print(\"%f (%f) with: %r\" % (mean, stdev, param))\n",
    "#     print()\n",
    "\n",
    "\n",
    "    key_name_X = \"X_\"\n",
    "    key_name_y = \"y_\"\n",
    "    \n",
    "    \n",
    "#     return eval_result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def start(machine, Internet_connection):\n",
    "    '''\n",
    "        [ATTENTION] In create_model METHOD part, need to set appropriate about GPU\n",
    "        \n",
    "        LINK01 -> GPU OFF\n",
    "        MSI -> GPU OFF\n",
    "        SLAVE04 -> GPU ON\n",
    "        SLAVE05 -> GPU ON\n",
    "    ''' \n",
    "    # \n",
    "    if machine==\"slave05-1\":\n",
    "        time_unit = [10,30,60]     # candle stick minutes\n",
    "        window_size = [25]  # Unit: num. of candle sticks\n",
    "        gap = [1]            # Unit: num. of candle sticks\n",
    "        margin_rate = [0.1]  # Unit: percent\n",
    "\n",
    "        start_time = time.time()\n",
    "        Start_Model( pickle_load_dir_path = \"./dataset_pickle_tuple_type/\",  \n",
    "                     data_files_dir = dataset_dir_path_tuple_type, \n",
    "                     epochs=1, \n",
    "                     pickle_result_dir_path = \"./evaluate_result/\", \n",
    "                     time_unit = time_unit, \n",
    "                     window_size = window_size, \n",
    "                     gap = gap, \n",
    "                     margin_rate = margin_rate, \n",
    "                     _TEST=False, \n",
    "                     _ENHANCE=False,\n",
    "                     _GPU=True,\n",
    "                     n_jobs=2,\n",
    "                     machine=\"slave05\", \n",
    "                     Internet_connection=Internet_connection)\n",
    "        end_time = time.time()\n",
    "        print()\n",
    "        print(\"TIME: \", end_time-start_time)\n",
    "        Javascript('IPython.notebook.kernel.restart()')\n",
    "        time.sleep(1)\n",
    "        Javascript('IPython.notebook.execute_all_cells()')\n",
    "\n",
    "    elif machine==\"slave05-2\":\n",
    "        time_unit = [10]     # candle stick minutes\n",
    "        window_size = [10,25,50]  # Unit: num. of candle sticks\n",
    "        gap = [1]            # Unit: num. of candle sticks\n",
    "        margin_rate = [0.1]  # Unit: percent\n",
    "\n",
    "        Start_Model( pickle_load_dir_path = \"./dataset_pickle_tuple_type/\",  \n",
    "                     data_files_dir = dataset_dir_path_tuple_type, \n",
    "                     epochs=1, \n",
    "                     pickle_result_dir_path = \"./evaluate_result/\", \n",
    "                     time_unit = time_unit, \n",
    "                     window_size = window_size, \n",
    "                     gap = gap, \n",
    "                     margin_rate = margin_rate, \n",
    "                     _TEST=False, \n",
    "                     _ENHANCE=False,\n",
    "                     _GPU=False,\n",
    "                     n_jobs=1,\n",
    "                     machine=\"slave05\", \n",
    "                     Internet_connection=Internet_connection)\n",
    "        end_time = time.time()\n",
    "        print()\n",
    "        print(\"TIME: \", end_time-start_time)\n",
    "        Javascript('IPython.notebook.kernel.restart()')\n",
    "        time.sleep(1)\n",
    "        Javascript('IPython.notebook.execute_all_cells()')\n",
    "\n",
    "    elif machine==\"slave04\":\n",
    "        time_unit = [10]     # candle stick minutes\n",
    "        window_size = [75]  # Unit: num. of candle sticks\n",
    "        gap = [1]            # Unit: num. of candle sticks\n",
    "        margin_rate = [0.1]  # Unit: percent\n",
    "\n",
    "        start_time = time.time()\n",
    "\n",
    "        Start_Model(pickle_load_dir_path = \"./dataset_pickle_tuple_type/\",  \n",
    "                 data_files_dir = dataset_dir_path_tuple_type, \n",
    "                 epochs=1, \n",
    "                 pickle_result_dir_path = \"./evaluate_result/\", \n",
    "                 time_unit = time_unit, \n",
    "                 window_size = window_size, \n",
    "                 gap = gap, \n",
    "                 margin_rate = margin_rate, \n",
    "                 _TEST=False, \n",
    "                 _ENHANCE=False,\n",
    "                 _GPU=True,\n",
    "                 n_jobs=2,\n",
    "                 machine=\"slave04\",\n",
    "                 Internet_connection=Internet_connection)\n",
    "\n",
    "        end_time = time.time()\n",
    "        print()\n",
    "        print(\"TIME: \", end_time-start_time)\n",
    "        Javascript('IPython.notebook.kernel.restart()')\n",
    "        time.sleep(1)\n",
    "        Javascript('IPython.notebook.execute_all_cells()')\n",
    "\n",
    "    elif machine==\"link01\":\n",
    "        time_unit = [10]     # candle stick minutes\n",
    "        window_size = [10]  # Unit: num. of candle sticks\n",
    "        gap = [1,2,3]            # Unit: num. of candle sticks\n",
    "        margin_rate = [0.1]  # Unit: percent\n",
    "\n",
    "        start_time = time.time()\n",
    "\n",
    "        Start_Model(pickle_load_dir_path = \"./dataset_pickle_tuple_type/\",  \n",
    "                 data_files_dir = dataset_dir_path_tuple_type, \n",
    "                 epochs=1, \n",
    "                 pickle_result_dir_path = \"./evaluate_result/\", \n",
    "                 time_unit = time_unit, \n",
    "                 window_size = window_size, \n",
    "                 gap = gap, \n",
    "                 margin_rate = margin_rate, \n",
    "                 _TEST=False,\n",
    "                 _ENHANCE=False,\n",
    "                 _GPU=False,\n",
    "                 n_jobs=1,\n",
    "                 machine=\"link01\", \n",
    "                 Internet_connection=Internet_connection)\n",
    "\n",
    "        end_time = time.time()\n",
    "        print()\n",
    "        print(\"TIME: \", end_time-start_time)\n",
    "        Javascript('IPython.notebook.kernel.restart()')\n",
    "        time.sleep(1)\n",
    "        Javascript('IPython.notebook.execute_all_cells()')\n",
    "\n",
    "    elif machine==\"MSI\":\n",
    "        time_unit = [10]     # candle stick minutes\n",
    "        window_size = [10]  # Unit: num. of candle sticks\n",
    "        gap = [1]            # Unit: num. of candle sticks\n",
    "        margin_rate = [0.1,0.25,0.5]  # Unit: percent\n",
    "\n",
    "        start_time = time.time()\n",
    "\n",
    "        Start_Model(pickle_load_dir_path = \"./dataset_pickle_tuple_type/\",  \n",
    "                 data_files_dir = dataset_dir_path_tuple_type, \n",
    "                 epochs=1, \n",
    "                 pickle_result_dir_path = \"./evaluate_result/\", \n",
    "                 time_unit = time_unit, \n",
    "                 window_size = window_size, \n",
    "                 gap = gap, \n",
    "                 margin_rate = margin_rate, \n",
    "                 _TEST=False, \n",
    "                 _ENHANCE=False,\n",
    "                 _GPU=False,\n",
    "                 n_jobs=1,\n",
    "                 machine=\"MSI\", \n",
    "                 Internet_connection=Internet_connection)\n",
    "\n",
    "        end_time = time.time()\n",
    "        print()\n",
    "        print(\"TIME: \", end_time-start_time)\n",
    "        Javascript('IPython.notebook.kernel.restart()')\n",
    "        time.sleep(1)\n",
    "        Javascript('IPython.notebook.execute_all_cells()')\n",
    "    elif machine==\"MacBook\":\n",
    "        time_unit = [10]     # candle stick minutes\n",
    "        window_size = [10]  # Unit: num. of candle sticks\n",
    "        gap = [1]            # Unit: num. of candle sticks\n",
    "        margin_rate = [0.1,0.25,0.5]  # Unit: percent\n",
    "\n",
    "        start_time = time.time()\n",
    "\n",
    "        Start_Model(pickle_load_dir_path = \"./dataset_pickle_tuple_type/\",  \n",
    "                 data_files_dir = dataset_dir_path_tuple_type, \n",
    "                 epochs=1, \n",
    "                 pickle_result_dir_path = \"./evaluate_result/\", \n",
    "                 time_unit = time_unit, \n",
    "                 window_size = window_size, \n",
    "                 gap = gap, \n",
    "                 margin_rate = margin_rate, \n",
    "                 _TEST=False, \n",
    "                 _ENHANCE=False,\n",
    "                 _GPU=False,\n",
    "                 n_jobs=1,\n",
    "                 machine=\"MSI\", \n",
    "                 Internet_connection=Internet_connection)\n",
    "\n",
    "        end_time = time.time()\n",
    "        print()\n",
    "        print(\"TIME: \", end_time-start_time)\n",
    "        Javascript('IPython.notebook.kernel.restart()')\n",
    "        time.sleep(1)\n",
    "        Javascript('IPython.notebook.execute_all_cells()')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# start(machine=\"MacBook\", Internet_connection=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # evaluate model using test data\n",
    "# score = "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # load pickle file \n",
    "# import pickle\n",
    "# b_x = pickle.load(open(\"./evaluate_result/_test_SimpleRNN_BTC_10_10_1_0.1_result.pickle\", \"rb\"))\n",
    "# b_x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Boost-up Acc, F1\n",
    "# evaluate_result_dir_path = \"./evaluate_result/acc_f1/\"\n",
    "# dataset_dir_path = dataset_dir_path_tuple_type \n",
    "# # dataset_dir_path = dataset_dir_path_numpy_type\n",
    "# epochs = 100\n",
    "# Evaluate(dataset_dir_path, data_files_dir, epochs, evaluate_result_dir_path, time_unit, window_size, gap, margin_rate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'x' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-16-9cacd3857fcc>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mhyperio\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mhy\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m h = hy.Hyperio(x, y, \n\u001b[0m\u001b[1;32m      3\u001b[0m                \u001b[0mparams\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mp\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;31m#                dataset_name='iris',\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m                \u001b[0mexperiment_no\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'1'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'x' is not defined"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import print_function\n",
    "import keras\n",
    "from keras.datasets import mnist\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Dropout, Flatten\n",
    "from keras.layers import Conv2D, MaxPooling2D\n",
    "from keras import backend as K\n",
    "\n",
    "def create_model_SimpleRNN_non_GPU_test(x_train, y_train, x_val, y_val, params):\n",
    "    model = Sequential()\n",
    "    model.add(# if문을 통해 여러 RNN모델 쓸 수 있도록 하기, SimpleRNN외에 다른 RNN모델 찾아보기\n",
    "        SimpleRNN(\n",
    "#             n_state_units, \n",
    "                  params['first_neuron'],\n",
    "                  input_dim=x_train.shape[2], # [dataset 크기, 윈도우 사이즈, 32(코인개수*OLHC)]\n",
    "#                   input_shape=(window_size, 32),\n",
    "#                   use_bias=True, \n",
    "                  activation='relu'))\n",
    "#                   kernel_initializer='glorot_uniform', \n",
    "#                   recurrent_initializer='orthogonal', \n",
    "#                   bias_initializer='zeros', \n",
    "#                   dropout=0.0,\n",
    "#                   recurrent_dropout=0.0))\n",
    "    \n",
    "#     model.add(Dense(units=neurons))\n",
    "#     model.add(Dropout(dropout_rate))\n",
    "        \n",
    "    model.add(Dropout(params['dropout']))\n",
    "    model.add(Dense(y_train.shape[1],\n",
    "                    activation=params['last_activation']))\n",
    "        \n",
    "#     model.add(Dense(units=2))\n",
    "#     model = multi_gpu_model(model, gpus=2)\n",
    "#     model.compile(optimizer=params['optimizer'](lr=lr_normalizer(params['lr'], params['optimizer'])),\n",
    "#                   loss=params['loss'],\n",
    "#                   metrics=['acc'])\n",
    "    model.compile(optimizer=params['optimizer'](),\n",
    "                  loss=params['loss'],\n",
    "                  metrics=['acc'])\n",
    "        \n",
    "    out = model.fit(x_train, y_train,\n",
    "                    batch_size=params['batch_size'],\n",
    "                    epochs=params['epochs'],\n",
    "                    verbose=1,\n",
    "                    validation_data=[x_val, y_val])\n",
    "#                     callbacks=early_stopper(params['epochs'], mode='strict'))\n",
    "    \n",
    "    return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/anaconda3/lib/python3.6/site-packages/ipykernel_launcher.py:18: UserWarning: The `input_dim` and `input_length` arguments in recurrent layers are deprecated. Use `input_shape` instead.\n",
      "/anaconda3/lib/python3.6/site-packages/ipykernel_launcher.py:18: UserWarning: Update your `SimpleRNN` call to the Keras 2 API: `SimpleRNN(64, activation=\"relu\", input_shape=(None, 32))`\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 32843 samples, validate on 14076 samples\n",
      "Epoch 1/10\n",
      "32843/32843 [==============================] - 61s 2ms/step - loss: 0.6315 - acc: 0.6699 - val_loss: 0.6348 - val_acc: 0.6694\n",
      "Epoch 2/10\n",
      "32843/32843 [==============================] - 60s 2ms/step - loss: 0.6271 - acc: 0.6705 - val_loss: 0.6268 - val_acc: 0.6697\n",
      "Epoch 3/10\n",
      "32843/32843 [==============================] - 61s 2ms/step - loss: 0.6255 - acc: 0.6702 - val_loss: 0.6317 - val_acc: 0.6697\n",
      "Epoch 4/10\n",
      "32843/32843 [==============================] - 61s 2ms/step - loss: 0.6254 - acc: 0.6704 - val_loss: 0.6256 - val_acc: 0.6695\n",
      "Epoch 5/10\n",
      "32843/32843 [==============================] - 61s 2ms/step - loss: 0.6243 - acc: 0.6707 - val_loss: 0.6259 - val_acc: 0.6697\n",
      "Epoch 6/10\n",
      "32843/32843 [==============================] - 61s 2ms/step - loss: 0.6236 - acc: 0.6700 - val_loss: 0.6261 - val_acc: 0.6697\n",
      "Epoch 7/10\n",
      "32843/32843 [==============================] - 61s 2ms/step - loss: 0.6238 - acc: 0.6702 - val_loss: 0.6249 - val_acc: 0.6696\n",
      "Epoch 8/10\n",
      "32843/32843 [==============================] - 61s 2ms/step - loss: 0.6232 - acc: 0.6702 - val_loss: 0.6257 - val_acc: 0.6697\n",
      "Epoch 9/10\n",
      "32843/32843 [==============================] - 61s 2ms/step - loss: 0.6237 - acc: 0.6707 - val_loss: 0.6244 - val_acc: 0.6696\n",
      "Epoch 10/10\n",
      "32843/32843 [==============================] - 61s 2ms/step - loss: 0.6238 - acc: 0.6704 - val_loss: 0.6242 - val_acc: 0.6694\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/anaconda3/lib/python3.6/site-packages/ipykernel_launcher.py:18: UserWarning: The `input_dim` and `input_length` arguments in recurrent layers are deprecated. Use `input_shape` instead.\n",
      "/anaconda3/lib/python3.6/site-packages/ipykernel_launcher.py:18: UserWarning: Update your `SimpleRNN` call to the Keras 2 API: `SimpleRNN(64, activation=\"relu\", input_shape=(None, 32))`\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 32843 samples, validate on 14076 samples\n",
      "Epoch 1/10\n",
      "32843/32843 [==============================] - 59s 2ms/step - loss: 0.6313 - acc: 0.6690 - val_loss: 0.6278 - val_acc: 0.6696\n",
      "Epoch 2/10\n",
      "32843/32843 [==============================] - 58s 2ms/step - loss: 0.6267 - acc: 0.6702 - val_loss: 0.6265 - val_acc: 0.6697\n",
      "Epoch 3/10\n",
      "32843/32843 [==============================] - 58s 2ms/step - loss: 0.6257 - acc: 0.6700 - val_loss: 0.6256 - val_acc: 0.6697\n",
      "Epoch 4/10\n",
      "32843/32843 [==============================] - 62s 2ms/step - loss: 0.6252 - acc: 0.6699 - val_loss: 0.6267 - val_acc: 0.6699\n",
      "Epoch 5/10\n",
      "32843/32843 [==============================] - 56s 2ms/step - loss: 0.6246 - acc: 0.6702 - val_loss: 0.6271 - val_acc: 0.6700\n",
      "Epoch 6/10\n",
      "32843/32843 [==============================] - 55s 2ms/step - loss: 0.6246 - acc: 0.6700 - val_loss: 0.6261 - val_acc: 0.6700\n",
      "Epoch 7/10\n",
      "32843/32843 [==============================] - 55s 2ms/step - loss: 0.6232 - acc: 0.6702 - val_loss: 0.6250 - val_acc: 0.6697\n",
      "Epoch 8/10\n",
      "32843/32843 [==============================] - 55s 2ms/step - loss: 0.6235 - acc: 0.6706 - val_loss: 0.6241 - val_acc: 0.6699\n",
      "Epoch 9/10\n",
      "32843/32843 [==============================] - 55s 2ms/step - loss: 0.6233 - acc: 0.6698 - val_loss: 0.6252 - val_acc: 0.6699\n",
      "Epoch 10/10\n",
      "32843/32843 [==============================] - 55s 2ms/step - loss: 0.6234 - acc: 0.6696 - val_loss: 0.6239 - val_acc: 0.6698\n",
      "57 scans will take roughly 32376 seconds\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/anaconda3/lib/python3.6/site-packages/ipykernel_launcher.py:18: UserWarning: The `input_dim` and `input_length` arguments in recurrent layers are deprecated. Use `input_shape` instead.\n",
      "/anaconda3/lib/python3.6/site-packages/ipykernel_launcher.py:18: UserWarning: Update your `SimpleRNN` call to the Keras 2 API: `SimpleRNN(64, activation=\"relu\", input_shape=(None, 32))`\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 32843 samples, validate on 14076 samples\n",
      "Epoch 1/10\n",
      "32843/32843 [==============================] - 37s 1ms/step - loss: 0.1040 - acc: 0.6701 - val_loss: 0.1040 - val_acc: 0.6582\n",
      "Epoch 2/10\n",
      "32843/32843 [==============================] - 36s 1ms/step - loss: 0.1031 - acc: 0.6699 - val_loss: 0.1033 - val_acc: 0.6697\n",
      "Epoch 3/10\n",
      "32843/32843 [==============================] - 37s 1ms/step - loss: 0.1028 - acc: 0.6704 - val_loss: 0.1031 - val_acc: 0.6695\n",
      "Epoch 4/10\n",
      "32843/32843 [==============================] - 36s 1ms/step - loss: 0.1027 - acc: 0.6700 - val_loss: 0.1029 - val_acc: 0.6697\n",
      "Epoch 5/10\n",
      "32843/32843 [==============================] - 36s 1ms/step - loss: 0.1026 - acc: 0.6704 - val_loss: 0.1028 - val_acc: 0.6697\n",
      "Epoch 6/10\n",
      "32843/32843 [==============================] - 37s 1ms/step - loss: 0.1025 - acc: 0.6704 - val_loss: 0.1027 - val_acc: 0.6698\n",
      "Epoch 7/10\n",
      "32843/32843 [==============================] - 36s 1ms/step - loss: 0.1025 - acc: 0.6701 - val_loss: 0.1028 - val_acc: 0.6697\n",
      "Epoch 8/10\n",
      "32843/32843 [==============================] - 36s 1ms/step - loss: 0.1023 - acc: 0.6708 - val_loss: 0.1035 - val_acc: 0.6697\n",
      "Epoch 9/10\n",
      "32843/32843 [==============================] - 36s 1ms/step - loss: 0.1022 - acc: 0.6711 - val_loss: 0.1036 - val_acc: 0.6694\n",
      "Epoch 10/10\n",
      "32843/32843 [==============================] - 36s 1ms/step - loss: 0.1023 - acc: 0.6697 - val_loss: 0.1035 - val_acc: 0.6698\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/anaconda3/lib/python3.6/site-packages/ipykernel_launcher.py:18: UserWarning: The `input_dim` and `input_length` arguments in recurrent layers are deprecated. Use `input_shape` instead.\n",
      "/anaconda3/lib/python3.6/site-packages/ipykernel_launcher.py:18: UserWarning: Update your `SimpleRNN` call to the Keras 2 API: `SimpleRNN(16, activation=\"relu\", input_shape=(None, 32))`\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 32843 samples, validate on 14076 samples\n",
      "Epoch 1/10\n",
      "32843/32843 [==============================] - 28s 844us/step - loss: 0.1044 - acc: 0.6695 - val_loss: 0.1043 - val_acc: 0.6694\n",
      "Epoch 2/10\n",
      "32843/32843 [==============================] - 27s 833us/step - loss: 0.1034 - acc: 0.6705 - val_loss: 0.1038 - val_acc: 0.6696\n",
      "Epoch 3/10\n",
      "32843/32843 [==============================] - 27s 827us/step - loss: 0.1030 - acc: 0.6702 - val_loss: 0.1031 - val_acc: 0.6699\n",
      "Epoch 4/10\n",
      "32843/32843 [==============================] - 27s 832us/step - loss: 0.1030 - acc: 0.6702 - val_loss: 0.1034 - val_acc: 0.6697\n",
      "Epoch 5/10\n",
      "32843/32843 [==============================] - 27s 829us/step - loss: 0.1028 - acc: 0.6703 - val_loss: 0.1030 - val_acc: 0.6698\n",
      "Epoch 6/10\n",
      "32843/32843 [==============================] - 27s 830us/step - loss: 0.1119 - acc: 0.6694 - val_loss: 0.1029 - val_acc: 0.6701\n",
      "Epoch 7/10\n",
      "32843/32843 [==============================] - 27s 833us/step - loss: 0.1027 - acc: 0.6696 - val_loss: 0.1027 - val_acc: 0.6704\n",
      "Epoch 8/10\n",
      "32843/32843 [==============================] - 27s 829us/step - loss: 0.1028 - acc: 0.6697 - val_loss: 0.1028 - val_acc: 0.6701\n",
      "Epoch 9/10\n",
      "32843/32843 [==============================] - 27s 833us/step - loss: 0.1025 - acc: 0.6708 - val_loss: 0.1028 - val_acc: 0.6697\n",
      "Epoch 10/10\n",
      "32843/32843 [==============================] - 27s 830us/step - loss: 0.1027 - acc: 0.6703 - val_loss: 0.1029 - val_acc: 0.6695\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/anaconda3/lib/python3.6/site-packages/ipykernel_launcher.py:18: UserWarning: The `input_dim` and `input_length` arguments in recurrent layers are deprecated. Use `input_shape` instead.\n",
      "/anaconda3/lib/python3.6/site-packages/ipykernel_launcher.py:18: UserWarning: Update your `SimpleRNN` call to the Keras 2 API: `SimpleRNN(128, activation=\"relu\", input_shape=(None, 32))`\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 32843 samples, validate on 14076 samples\n",
      "Epoch 1/10\n",
      "32843/32843 [==============================] - 37s 1ms/step - loss: 0.6318 - acc: 0.6698 - val_loss: 0.6279 - val_acc: 0.6697\n",
      "Epoch 2/10\n",
      "32843/32843 [==============================] - 37s 1ms/step - loss: 0.6267 - acc: 0.6698 - val_loss: 0.6266 - val_acc: 0.6699\n",
      "Epoch 3/10\n",
      "32843/32843 [==============================] - 37s 1ms/step - loss: 0.6254 - acc: 0.6701 - val_loss: 0.6322 - val_acc: 0.6696\n",
      "Epoch 4/10\n",
      "32843/32843 [==============================] - 37s 1ms/step - loss: 0.6246 - acc: 0.6698 - val_loss: 0.6297 - val_acc: 0.6699\n",
      "Epoch 5/10\n",
      "32843/32843 [==============================] - 37s 1ms/step - loss: 0.6240 - acc: 0.6703 - val_loss: 0.6248 - val_acc: 0.6698\n",
      "Epoch 6/10\n",
      "32843/32843 [==============================] - 37s 1ms/step - loss: 0.6229 - acc: 0.6701 - val_loss: 0.6292 - val_acc: 0.6696\n",
      "Epoch 7/10\n",
      "32843/32843 [==============================] - 37s 1ms/step - loss: 0.6233 - acc: 0.6702 - val_loss: 0.6242 - val_acc: 0.6701\n",
      "Epoch 8/10\n",
      "32843/32843 [==============================] - 37s 1ms/step - loss: 0.6231 - acc: 0.6705 - val_loss: 0.6241 - val_acc: 0.6697\n",
      "Epoch 9/10\n",
      "32843/32843 [==============================] - 37s 1ms/step - loss: 0.6231 - acc: 0.6700 - val_loss: 0.6236 - val_acc: 0.6699\n",
      "Epoch 10/10\n",
      "32843/32843 [==============================] - 37s 1ms/step - loss: 0.6221 - acc: 0.6703 - val_loss: 0.6249 - val_acc: 0.6699\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/anaconda3/lib/python3.6/site-packages/ipykernel_launcher.py:18: UserWarning: The `input_dim` and `input_length` arguments in recurrent layers are deprecated. Use `input_shape` instead.\n",
      "/anaconda3/lib/python3.6/site-packages/ipykernel_launcher.py:18: UserWarning: Update your `SimpleRNN` call to the Keras 2 API: `SimpleRNN(128, activation=\"relu\", input_shape=(None, 32))`\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 32843 samples, validate on 14076 samples\n",
      "Epoch 1/10\n",
      "32843/32843 [==============================] - 37s 1ms/step - loss: 0.1043 - acc: 0.6696 - val_loss: 0.1041 - val_acc: 0.6699\n",
      "Epoch 2/10\n",
      "32843/32843 [==============================] - 37s 1ms/step - loss: 0.1033 - acc: 0.6699 - val_loss: 0.1035 - val_acc: 0.6697\n",
      "Epoch 3/10\n",
      "32843/32843 [==============================] - 37s 1ms/step - loss: 0.1030 - acc: 0.6700 - val_loss: 0.1047 - val_acc: 0.6697\n",
      "Epoch 4/10\n",
      "32843/32843 [==============================] - 37s 1ms/step - loss: 0.1028 - acc: 0.6702 - val_loss: 0.1030 - val_acc: 0.6699\n",
      "Epoch 5/10\n",
      "32843/32843 [==============================] - 37s 1ms/step - loss: 0.1027 - acc: 0.6699 - val_loss: 0.1029 - val_acc: 0.6696\n",
      "Epoch 6/10\n",
      "32843/32843 [==============================] - 37s 1ms/step - loss: 0.1027 - acc: 0.6704 - val_loss: 0.1027 - val_acc: 0.6696\n",
      "Epoch 7/10\n",
      "32843/32843 [==============================] - 37s 1ms/step - loss: 0.1026 - acc: 0.6701 - val_loss: 0.1027 - val_acc: 0.6697\n",
      "Epoch 8/10\n",
      "32843/32843 [==============================] - 37s 1ms/step - loss: 0.1025 - acc: 0.6710 - val_loss: 0.1029 - val_acc: 0.6696\n",
      "Epoch 9/10\n",
      "32843/32843 [==============================] - 37s 1ms/step - loss: 0.1025 - acc: 0.6699 - val_loss: 0.1029 - val_acc: 0.6696\n",
      "Epoch 10/10\n",
      "32843/32843 [==============================] - 37s 1ms/step - loss: 0.1024 - acc: 0.6701 - val_loss: 0.1026 - val_acc: 0.6701\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/anaconda3/lib/python3.6/site-packages/ipykernel_launcher.py:18: UserWarning: The `input_dim` and `input_length` arguments in recurrent layers are deprecated. Use `input_shape` instead.\n",
      "/anaconda3/lib/python3.6/site-packages/ipykernel_launcher.py:18: UserWarning: Update your `SimpleRNN` call to the Keras 2 API: `SimpleRNN(16, activation=\"relu\", input_shape=(None, 32))`\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 32843 samples, validate on 14076 samples\n",
      "Epoch 1/10\n",
      "32843/32843 [==============================] - 36s 1ms/step - loss: 0.1042 - acc: 0.6700 - val_loss: 0.1037 - val_acc: 0.6692\n",
      "Epoch 2/10\n",
      "32843/32843 [==============================] - 36s 1ms/step - loss: 0.1031 - acc: 0.6701 - val_loss: 0.1033 - val_acc: 0.6694\n",
      "Epoch 3/10\n",
      "32843/32843 [==============================] - 36s 1ms/step - loss: 0.1029 - acc: 0.6700 - val_loss: 0.1031 - val_acc: 0.6696\n",
      "Epoch 4/10\n",
      "32843/32843 [==============================] - 36s 1ms/step - loss: 0.1027 - acc: 0.6699 - val_loss: 0.1031 - val_acc: 0.6695\n",
      "Epoch 5/10\n",
      "32843/32843 [==============================] - 36s 1ms/step - loss: 0.1025 - acc: 0.6698 - val_loss: 0.1034 - val_acc: 0.6689\n",
      "Epoch 6/10\n",
      "32843/32843 [==============================] - 36s 1ms/step - loss: 0.1024 - acc: 0.6701 - val_loss: 0.1029 - val_acc: 0.6687\n",
      "Epoch 7/10\n",
      "32843/32843 [==============================] - 36s 1ms/step - loss: 0.1024 - acc: 0.6703 - val_loss: 0.1035 - val_acc: 0.6694\n",
      "Epoch 8/10\n",
      "32843/32843 [==============================] - 36s 1ms/step - loss: 0.1024 - acc: 0.6702 - val_loss: 0.1027 - val_acc: 0.6693\n",
      "Epoch 9/10\n",
      "32843/32843 [==============================] - 36s 1ms/step - loss: 0.1024 - acc: 0.6704 - val_loss: 0.1034 - val_acc: 0.6694\n",
      "Epoch 10/10\n",
      "32843/32843 [==============================] - 36s 1ms/step - loss: 0.1024 - acc: 0.6701 - val_loss: 0.1028 - val_acc: 0.6698\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/anaconda3/lib/python3.6/site-packages/ipykernel_launcher.py:18: UserWarning: The `input_dim` and `input_length` arguments in recurrent layers are deprecated. Use `input_shape` instead.\n",
      "/anaconda3/lib/python3.6/site-packages/ipykernel_launcher.py:18: UserWarning: Update your `SimpleRNN` call to the Keras 2 API: `SimpleRNN(64, activation=\"relu\", input_shape=(None, 32))`\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 32843 samples, validate on 14076 samples\n",
      "Epoch 1/10\n",
      "32843/32843 [==============================] - 36s 1ms/step - loss: 0.6330 - acc: 0.6694 - val_loss: 0.6386 - val_acc: 0.6697\n",
      "Epoch 2/10\n",
      "32843/32843 [==============================] - 36s 1ms/step - loss: 0.6270 - acc: 0.6699 - val_loss: 0.6283 - val_acc: 0.6697\n",
      "Epoch 3/10\n",
      "32843/32843 [==============================] - 36s 1ms/step - loss: 0.6253 - acc: 0.6703 - val_loss: 0.6258 - val_acc: 0.6694\n",
      "Epoch 4/10\n",
      "32843/32843 [==============================] - 36s 1ms/step - loss: 0.6249 - acc: 0.6702 - val_loss: 0.6259 - val_acc: 0.6692\n",
      "Epoch 5/10\n",
      "32843/32843 [==============================] - 36s 1ms/step - loss: 0.6251 - acc: 0.6700 - val_loss: 0.6282 - val_acc: 0.6695\n",
      "Epoch 6/10\n",
      "32843/32843 [==============================] - 36s 1ms/step - loss: 0.6243 - acc: 0.6707 - val_loss: 0.6250 - val_acc: 0.6692\n",
      "Epoch 7/10\n",
      "32843/32843 [==============================] - 36s 1ms/step - loss: 0.6240 - acc: 0.6705 - val_loss: 0.6252 - val_acc: 0.6692\n",
      "Epoch 8/10\n",
      "32843/32843 [==============================] - 36s 1ms/step - loss: 0.6240 - acc: 0.6708 - val_loss: 0.6262 - val_acc: 0.6699\n",
      "Epoch 9/10\n",
      "32843/32843 [==============================] - 36s 1ms/step - loss: 0.6235 - acc: 0.6703 - val_loss: 0.6246 - val_acc: 0.6694\n",
      "Epoch 10/10\n",
      "32843/32843 [==============================] - 36s 1ms/step - loss: 0.6237 - acc: 0.6699 - val_loss: 0.6246 - val_acc: 0.6699\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/anaconda3/lib/python3.6/site-packages/ipykernel_launcher.py:18: UserWarning: The `input_dim` and `input_length` arguments in recurrent layers are deprecated. Use `input_shape` instead.\n",
      "/anaconda3/lib/python3.6/site-packages/ipykernel_launcher.py:18: UserWarning: Update your `SimpleRNN` call to the Keras 2 API: `SimpleRNN(4, activation=\"relu\", input_shape=(None, 32))`\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 32843 samples, validate on 14076 samples\n",
      "Epoch 1/10\n",
      "32843/32843 [==============================] - 27s 827us/step - loss: 0.6460 - acc: 0.6684 - val_loss: 0.6300 - val_acc: 0.6697\n",
      "Epoch 2/10\n",
      "32843/32843 [==============================] - 27s 815us/step - loss: 0.6291 - acc: 0.6700 - val_loss: 0.6283 - val_acc: 0.6697\n",
      "Epoch 3/10\n",
      "32843/32843 [==============================] - 27s 809us/step - loss: 0.6285 - acc: 0.6700 - val_loss: 0.6285 - val_acc: 0.6697\n",
      "Epoch 4/10\n",
      "32843/32843 [==============================] - 27s 813us/step - loss: 0.6285 - acc: 0.6700 - val_loss: 0.6303 - val_acc: 0.6697\n",
      "Epoch 5/10\n",
      "32843/32843 [==============================] - 27s 812us/step - loss: 0.6273 - acc: 0.6700 - val_loss: 0.6279 - val_acc: 0.6697\n",
      "Epoch 6/10\n",
      "32843/32843 [==============================] - 27s 815us/step - loss: 0.6276 - acc: 0.6700 - val_loss: 0.6271 - val_acc: 0.6697\n",
      "Epoch 7/10\n",
      "32843/32843 [==============================] - 27s 812us/step - loss: 0.6266 - acc: 0.6700 - val_loss: 0.6278 - val_acc: 0.6697\n",
      "Epoch 8/10\n",
      "32843/32843 [==============================] - 27s 811us/step - loss: 0.6267 - acc: 0.6700 - val_loss: 0.6271 - val_acc: 0.6697\n",
      "Epoch 9/10\n",
      "32843/32843 [==============================] - 27s 813us/step - loss: 0.6266 - acc: 0.6700 - val_loss: 0.6292 - val_acc: 0.6697\n",
      "Epoch 10/10\n",
      "32843/32843 [==============================] - 27s 810us/step - loss: 0.6266 - acc: 0.6700 - val_loss: 0.6268 - val_acc: 0.6697\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/anaconda3/lib/python3.6/site-packages/ipykernel_launcher.py:18: UserWarning: The `input_dim` and `input_length` arguments in recurrent layers are deprecated. Use `input_shape` instead.\n",
      "/anaconda3/lib/python3.6/site-packages/ipykernel_launcher.py:18: UserWarning: Update your `SimpleRNN` call to the Keras 2 API: `SimpleRNN(32, activation=\"relu\", input_shape=(None, 32))`\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 32843 samples, validate on 14076 samples\n",
      "Epoch 1/10\n",
      "32843/32843 [==============================] - 28s 845us/step - loss: 0.6326 - acc: 0.6695 - val_loss: 0.6288 - val_acc: 0.6697\n",
      "Epoch 2/10\n",
      "32843/32843 [==============================] - 27s 830us/step - loss: 0.6283 - acc: 0.6702 - val_loss: 0.6268 - val_acc: 0.6697\n",
      "Epoch 3/10\n",
      "32843/32843 [==============================] - 28s 844us/step - loss: 0.6265 - acc: 0.6704 - val_loss: 0.6271 - val_acc: 0.6703\n",
      "Epoch 4/10\n",
      "32843/32843 [==============================] - 27s 835us/step - loss: 0.6264 - acc: 0.6701 - val_loss: 0.6265 - val_acc: 0.6696\n",
      "Epoch 5/10\n",
      "32843/32843 [==============================] - 27s 828us/step - loss: 0.6259 - acc: 0.6702 - val_loss: 0.6251 - val_acc: 0.6696\n",
      "Epoch 6/10\n",
      "32843/32843 [==============================] - 28s 841us/step - loss: 0.6252 - acc: 0.6697 - val_loss: 0.6269 - val_acc: 0.6698\n",
      "Epoch 7/10\n",
      "32843/32843 [==============================] - 27s 828us/step - loss: 0.6252 - acc: 0.6699 - val_loss: 0.6244 - val_acc: 0.6698\n",
      "Epoch 8/10\n",
      "32843/32843 [==============================] - 27s 825us/step - loss: 0.6250 - acc: 0.6693 - val_loss: 0.6259 - val_acc: 0.6697\n",
      "Epoch 9/10\n",
      "32843/32843 [==============================] - 27s 826us/step - loss: 0.6244 - acc: 0.6698 - val_loss: 0.6257 - val_acc: 0.6696\n",
      "Epoch 10/10\n",
      "32843/32843 [==============================] - 27s 828us/step - loss: 0.6244 - acc: 0.6705 - val_loss: 0.6255 - val_acc: 0.6699\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/anaconda3/lib/python3.6/site-packages/ipykernel_launcher.py:18: UserWarning: The `input_dim` and `input_length` arguments in recurrent layers are deprecated. Use `input_shape` instead.\n",
      "/anaconda3/lib/python3.6/site-packages/ipykernel_launcher.py:18: UserWarning: Update your `SimpleRNN` call to the Keras 2 API: `SimpleRNN(16, activation=\"relu\", input_shape=(None, 32))`\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 32843 samples, validate on 14076 samples\n",
      "Epoch 1/10\n",
      "32843/32843 [==============================] - 28s 850us/step - loss: 0.6344 - acc: 0.6685 - val_loss: 0.6299 - val_acc: 0.6697\n",
      "Epoch 2/10\n",
      "32843/32843 [==============================] - 27s 835us/step - loss: 0.6285 - acc: 0.6702 - val_loss: 0.6276 - val_acc: 0.6697\n",
      "Epoch 3/10\n",
      "32843/32843 [==============================] - 27s 835us/step - loss: 0.6268 - acc: 0.6704 - val_loss: 0.6257 - val_acc: 0.6697\n",
      "Epoch 4/10\n",
      "32843/32843 [==============================] - 27s 832us/step - loss: 0.6254 - acc: 0.6695 - val_loss: 0.6257 - val_acc: 0.6697\n",
      "Epoch 5/10\n",
      "32843/32843 [==============================] - 27s 835us/step - loss: 0.6255 - acc: 0.6702 - val_loss: 0.6250 - val_acc: 0.6701\n",
      "Epoch 6/10\n",
      "32843/32843 [==============================] - 27s 836us/step - loss: 0.6251 - acc: 0.6700 - val_loss: 0.6246 - val_acc: 0.6699\n",
      "Epoch 7/10\n",
      "32843/32843 [==============================] - 27s 835us/step - loss: 0.6252 - acc: 0.6699 - val_loss: 0.6258 - val_acc: 0.6697\n",
      "Epoch 8/10\n",
      "32843/32843 [==============================] - 27s 836us/step - loss: 0.6247 - acc: 0.6699 - val_loss: 0.6250 - val_acc: 0.6699\n",
      "Epoch 9/10\n",
      "32843/32843 [==============================] - 27s 830us/step - loss: 0.6245 - acc: 0.6706 - val_loss: 0.6246 - val_acc: 0.6698\n",
      "Epoch 10/10\n",
      "32843/32843 [==============================] - 28s 841us/step - loss: 0.6250 - acc: 0.6702 - val_loss: 0.6246 - val_acc: 0.6698\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/anaconda3/lib/python3.6/site-packages/ipykernel_launcher.py:18: UserWarning: The `input_dim` and `input_length` arguments in recurrent layers are deprecated. Use `input_shape` instead.\n",
      "/anaconda3/lib/python3.6/site-packages/ipykernel_launcher.py:18: UserWarning: Update your `SimpleRNN` call to the Keras 2 API: `SimpleRNN(4, activation=\"relu\", input_shape=(None, 32))`\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 32843 samples, validate on 14076 samples\n",
      "Epoch 1/10\n",
      "32843/32843 [==============================] - 35s 1ms/step - loss: 0.6320 - acc: 0.6685 - val_loss: 0.6287 - val_acc: 0.6697\n",
      "Epoch 2/10\n",
      "32843/32843 [==============================] - 35s 1ms/step - loss: 0.6267 - acc: 0.6700 - val_loss: 0.6298 - val_acc: 0.6697\n",
      "Epoch 3/10\n",
      "32843/32843 [==============================] - 35s 1ms/step - loss: 0.6252 - acc: 0.6700 - val_loss: 0.6261 - val_acc: 0.6697\n",
      "Epoch 4/10\n",
      "32843/32843 [==============================] - 35s 1ms/step - loss: 0.6245 - acc: 0.6700 - val_loss: 0.6260 - val_acc: 0.6697\n",
      "Epoch 5/10\n",
      "32843/32843 [==============================] - 35s 1ms/step - loss: 0.6237 - acc: 0.6700 - val_loss: 0.6253 - val_acc: 0.6697\n",
      "Epoch 6/10\n",
      "32843/32843 [==============================] - 35s 1ms/step - loss: 0.6237 - acc: 0.6700 - val_loss: 0.6249 - val_acc: 0.6697\n",
      "Epoch 7/10\n",
      "32843/32843 [==============================] - 35s 1ms/step - loss: 0.6233 - acc: 0.6700 - val_loss: 0.6244 - val_acc: 0.6697\n",
      "Epoch 8/10\n",
      "32843/32843 [==============================] - 35s 1ms/step - loss: 0.6233 - acc: 0.6700 - val_loss: 0.6247 - val_acc: 0.6697\n",
      "Epoch 9/10\n",
      "32843/32843 [==============================] - 35s 1ms/step - loss: 0.6228 - acc: 0.6700 - val_loss: 0.6252 - val_acc: 0.6697\n",
      "Epoch 10/10\n",
      "32843/32843 [==============================] - 35s 1ms/step - loss: 0.6228 - acc: 0.6700 - val_loss: 0.6234 - val_acc: 0.6697\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/anaconda3/lib/python3.6/site-packages/ipykernel_launcher.py:18: UserWarning: The `input_dim` and `input_length` arguments in recurrent layers are deprecated. Use `input_shape` instead.\n",
      "/anaconda3/lib/python3.6/site-packages/ipykernel_launcher.py:18: UserWarning: Update your `SimpleRNN` call to the Keras 2 API: `SimpleRNN(8, activation=\"relu\", input_shape=(None, 32))`\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 32843 samples, validate on 14076 samples\n",
      "Epoch 1/10\n",
      "32843/32843 [==============================] - 28s 840us/step - loss: 0.6340 - acc: 0.6683 - val_loss: 0.6299 - val_acc: 0.6692\n",
      "Epoch 2/10\n",
      "32843/32843 [==============================] - 27s 831us/step - loss: 0.6284 - acc: 0.6703 - val_loss: 0.6279 - val_acc: 0.6695\n",
      "Epoch 3/10\n",
      "32843/32843 [==============================] - 27s 834us/step - loss: 0.6267 - acc: 0.6700 - val_loss: 0.6286 - val_acc: 0.6697\n",
      "Epoch 4/10\n",
      "32843/32843 [==============================] - 27s 833us/step - loss: 0.6268 - acc: 0.6700 - val_loss: 0.6289 - val_acc: 0.6694\n",
      "Epoch 5/10\n",
      "32843/32843 [==============================] - 27s 836us/step - loss: 0.6264 - acc: 0.6695 - val_loss: 0.6277 - val_acc: 0.6697\n",
      "Epoch 6/10\n",
      "32843/32843 [==============================] - 28s 839us/step - loss: 0.6259 - acc: 0.6699 - val_loss: 0.6260 - val_acc: 0.6699\n",
      "Epoch 7/10\n",
      "32843/32843 [==============================] - 27s 834us/step - loss: 0.6260 - acc: 0.6700 - val_loss: 0.6271 - val_acc: 0.6694\n",
      "Epoch 8/10\n",
      "32843/32843 [==============================] - 27s 836us/step - loss: 0.6254 - acc: 0.6706 - val_loss: 0.6251 - val_acc: 0.6701\n",
      "Epoch 9/10\n",
      "32843/32843 [==============================] - 27s 836us/step - loss: 0.6244 - acc: 0.6708 - val_loss: 0.6248 - val_acc: 0.6697\n",
      "Epoch 10/10\n",
      "32843/32843 [==============================] - 27s 836us/step - loss: 0.6256 - acc: 0.6701 - val_loss: 0.6250 - val_acc: 0.6697\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/anaconda3/lib/python3.6/site-packages/ipykernel_launcher.py:18: UserWarning: The `input_dim` and `input_length` arguments in recurrent layers are deprecated. Use `input_shape` instead.\n",
      "/anaconda3/lib/python3.6/site-packages/ipykernel_launcher.py:18: UserWarning: Update your `SimpleRNN` call to the Keras 2 API: `SimpleRNN(16, activation=\"relu\", input_shape=(None, 32))`\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 32843 samples, validate on 14076 samples\n",
      "Epoch 1/10\n",
      "32843/32843 [==============================] - 27s 823us/step - loss: 0.1040 - acc: 0.6701 - val_loss: 0.1039 - val_acc: 0.6695\n",
      "Epoch 2/10\n",
      "32843/32843 [==============================] - 27s 814us/step - loss: 0.1032 - acc: 0.6697 - val_loss: 0.1034 - val_acc: 0.6697\n",
      "Epoch 3/10\n",
      "32843/32843 [==============================] - 27s 812us/step - loss: 0.1029 - acc: 0.6702 - val_loss: 0.1035 - val_acc: 0.6697\n",
      "Epoch 4/10\n",
      "32843/32843 [==============================] - 27s 814us/step - loss: 0.1026 - acc: 0.6701 - val_loss: 0.1047 - val_acc: 0.6698\n",
      "Epoch 5/10\n",
      "32843/32843 [==============================] - 27s 811us/step - loss: 0.1026 - acc: 0.6702 - val_loss: 0.1032 - val_acc: 0.6697\n",
      "Epoch 6/10\n",
      "32843/32843 [==============================] - 27s 813us/step - loss: 0.1025 - acc: 0.6706 - val_loss: 0.1028 - val_acc: 0.6694\n",
      "Epoch 7/10\n",
      "32843/32843 [==============================] - 27s 810us/step - loss: 0.1024 - acc: 0.6709 - val_loss: 0.1031 - val_acc: 0.6697\n",
      "Epoch 8/10\n",
      "32843/32843 [==============================] - 27s 813us/step - loss: 0.1025 - acc: 0.6706 - val_loss: 0.1027 - val_acc: 0.6698\n",
      "Epoch 9/10\n",
      "32843/32843 [==============================] - 27s 811us/step - loss: 0.1023 - acc: 0.6707 - val_loss: 0.1032 - val_acc: 0.6694\n",
      "Epoch 10/10\n",
      "32843/32843 [==============================] - 27s 814us/step - loss: 0.1023 - acc: 0.6705 - val_loss: 0.1025 - val_acc: 0.6697\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/anaconda3/lib/python3.6/site-packages/ipykernel_launcher.py:18: UserWarning: The `input_dim` and `input_length` arguments in recurrent layers are deprecated. Use `input_shape` instead.\n",
      "/anaconda3/lib/python3.6/site-packages/ipykernel_launcher.py:18: UserWarning: Update your `SimpleRNN` call to the Keras 2 API: `SimpleRNN(64, activation=\"relu\", input_shape=(None, 32))`\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 32843 samples, validate on 14076 samples\n",
      "Epoch 1/10\n",
      "32843/32843 [==============================] - 36s 1ms/step - loss: 0.6331 - acc: 0.6694 - val_loss: 0.6287 - val_acc: 0.6697\n",
      "Epoch 2/10\n",
      "32843/32843 [==============================] - 36s 1ms/step - loss: 0.6278 - acc: 0.6707 - val_loss: 0.6277 - val_acc: 0.6697\n",
      "Epoch 3/10\n",
      "32843/32843 [==============================] - 36s 1ms/step - loss: 0.6261 - acc: 0.6704 - val_loss: 0.6266 - val_acc: 0.6697\n",
      "Epoch 4/10\n",
      "32843/32843 [==============================] - 36s 1ms/step - loss: 0.6257 - acc: 0.6699 - val_loss: 0.6295 - val_acc: 0.6696\n",
      "Epoch 5/10\n",
      "32843/32843 [==============================] - 36s 1ms/step - loss: 0.6251 - acc: 0.6699 - val_loss: 0.6290 - val_acc: 0.6698\n",
      "Epoch 6/10\n",
      "32843/32843 [==============================] - 36s 1ms/step - loss: 0.6249 - acc: 0.6702 - val_loss: 0.6252 - val_acc: 0.6699\n",
      "Epoch 7/10\n",
      "32843/32843 [==============================] - 36s 1ms/step - loss: 0.6239 - acc: 0.6700 - val_loss: 0.6268 - val_acc: 0.6699\n",
      "Epoch 8/10\n",
      "32843/32843 [==============================] - 36s 1ms/step - loss: 0.6250 - acc: 0.6699 - val_loss: 0.6251 - val_acc: 0.6701\n",
      "Epoch 9/10\n",
      "32843/32843 [==============================] - 36s 1ms/step - loss: 0.6243 - acc: 0.6702 - val_loss: 0.6258 - val_acc: 0.6697\n",
      "Epoch 10/10\n",
      "32843/32843 [==============================] - 36s 1ms/step - loss: 0.6248 - acc: 0.6700 - val_loss: 0.6259 - val_acc: 0.6695\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/anaconda3/lib/python3.6/site-packages/ipykernel_launcher.py:18: UserWarning: The `input_dim` and `input_length` arguments in recurrent layers are deprecated. Use `input_shape` instead.\n",
      "/anaconda3/lib/python3.6/site-packages/ipykernel_launcher.py:18: UserWarning: Update your `SimpleRNN` call to the Keras 2 API: `SimpleRNN(32, activation=\"relu\", input_shape=(None, 32))`\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 32843 samples, validate on 14076 samples\n",
      "Epoch 1/10\n",
      "32843/32843 [==============================] - 28s 837us/step - loss: 0.1041 - acc: 0.6696 - val_loss: 0.1036 - val_acc: 0.6698\n",
      "Epoch 2/10\n",
      "32843/32843 [==============================] - 27s 829us/step - loss: 0.1031 - acc: 0.6706 - val_loss: 0.1032 - val_acc: 0.6695\n",
      "Epoch 3/10\n",
      "32843/32843 [==============================] - 27s 828us/step - loss: 0.1028 - acc: 0.6703 - val_loss: 0.1030 - val_acc: 0.6697\n",
      "Epoch 4/10\n",
      "32843/32843 [==============================] - 27s 824us/step - loss: 0.1027 - acc: 0.6705 - val_loss: 0.1030 - val_acc: 0.6699\n",
      "Epoch 5/10\n",
      "32843/32843 [==============================] - 27s 827us/step - loss: 0.1027 - acc: 0.6706 - val_loss: 0.1036 - val_acc: 0.6697\n",
      "Epoch 6/10\n",
      "32843/32843 [==============================] - 27s 826us/step - loss: 0.1026 - acc: 0.6705 - val_loss: 0.1036 - val_acc: 0.6678\n",
      "Epoch 7/10\n",
      "32843/32843 [==============================] - 27s 831us/step - loss: 0.1026 - acc: 0.6706 - val_loss: 0.1028 - val_acc: 0.6696\n",
      "Epoch 8/10\n",
      "32843/32843 [==============================] - 27s 830us/step - loss: 0.1024 - acc: 0.6707 - val_loss: 0.1038 - val_acc: 0.6700\n",
      "Epoch 9/10\n",
      "32843/32843 [==============================] - 28s 867us/step - loss: 0.1024 - acc: 0.6704 - val_loss: 0.1030 - val_acc: 0.6701\n",
      "Epoch 10/10\n",
      "32843/32843 [==============================] - 29s 884us/step - loss: 0.1024 - acc: 0.6698 - val_loss: 0.1030 - val_acc: 0.6694\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/anaconda3/lib/python3.6/site-packages/ipykernel_launcher.py:18: UserWarning: The `input_dim` and `input_length` arguments in recurrent layers are deprecated. Use `input_shape` instead.\n",
      "/anaconda3/lib/python3.6/site-packages/ipykernel_launcher.py:18: UserWarning: Update your `SimpleRNN` call to the Keras 2 API: `SimpleRNN(64, activation=\"relu\", input_shape=(None, 32))`\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 32843 samples, validate on 14076 samples\n",
      "Epoch 1/10\n",
      "32843/32843 [==============================] - 32s 969us/step - loss: 0.1039 - acc: 0.6695 - val_loss: 0.1034 - val_acc: 0.6697\n",
      "Epoch 2/10\n",
      "32843/32843 [==============================] - 28s 866us/step - loss: 0.1030 - acc: 0.6701 - val_loss: 0.1039 - val_acc: 0.6698\n",
      "Epoch 3/10\n",
      "32843/32843 [==============================] - 28s 840us/step - loss: 0.1028 - acc: 0.6704 - val_loss: 0.1034 - val_acc: 0.6698\n",
      "Epoch 4/10\n",
      "32843/32843 [==============================] - 28s 841us/step - loss: 0.1027 - acc: 0.6703 - val_loss: 0.1028 - val_acc: 0.6701\n",
      "Epoch 5/10\n",
      "32843/32843 [==============================] - 28s 842us/step - loss: 0.1025 - acc: 0.6703 - val_loss: 0.1028 - val_acc: 0.6694\n",
      "Epoch 6/10\n",
      "32843/32843 [==============================] - 28s 845us/step - loss: 0.1023 - acc: 0.6704 - val_loss: 0.1083 - val_acc: 0.6695\n",
      "Epoch 7/10\n",
      "32843/32843 [==============================] - 28s 843us/step - loss: 0.1023 - acc: 0.6703 - val_loss: 0.1027 - val_acc: 0.6697\n",
      "Epoch 8/10\n",
      "32843/32843 [==============================] - 28s 841us/step - loss: 0.1023 - acc: 0.6705 - val_loss: 0.1026 - val_acc: 0.6697\n",
      "Epoch 9/10\n",
      "32843/32843 [==============================] - 28s 846us/step - loss: 0.1023 - acc: 0.6705 - val_loss: 0.1026 - val_acc: 0.6696\n",
      "Epoch 10/10\n",
      "32843/32843 [==============================] - 28s 844us/step - loss: 0.1021 - acc: 0.6706 - val_loss: 0.1029 - val_acc: 0.6697\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/anaconda3/lib/python3.6/site-packages/ipykernel_launcher.py:18: UserWarning: The `input_dim` and `input_length` arguments in recurrent layers are deprecated. Use `input_shape` instead.\n",
      "/anaconda3/lib/python3.6/site-packages/ipykernel_launcher.py:18: UserWarning: Update your `SimpleRNN` call to the Keras 2 API: `SimpleRNN(128, activation=\"relu\", input_shape=(None, 32))`\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 32843 samples, validate on 14076 samples\n",
      "Epoch 1/10\n",
      "32843/32843 [==============================] - 57s 2ms/step - loss: 0.6306 - acc: 0.6698 - val_loss: 0.6314 - val_acc: 0.6697\n",
      "Epoch 2/10\n",
      "32843/32843 [==============================] - 57s 2ms/step - loss: 0.6265 - acc: 0.6697 - val_loss: 0.6451 - val_acc: 0.6697\n",
      "Epoch 3/10\n",
      "32843/32843 [==============================] - 57s 2ms/step - loss: 0.6248 - acc: 0.6702 - val_loss: 0.6256 - val_acc: 0.6697\n",
      "Epoch 4/10\n",
      "32843/32843 [==============================] - 61s 2ms/step - loss: 0.6239 - acc: 0.6699 - val_loss: 0.6250 - val_acc: 0.6697\n",
      "Epoch 5/10\n",
      "32843/32843 [==============================] - 65s 2ms/step - loss: 0.6238 - acc: 0.6700 - val_loss: 0.6263 - val_acc: 0.6700\n",
      "Epoch 6/10\n",
      "32843/32843 [==============================] - 67s 2ms/step - loss: 0.6236 - acc: 0.6701 - val_loss: 0.6267 - val_acc: 0.6698\n",
      "Epoch 7/10\n",
      "32843/32843 [==============================] - 68s 2ms/step - loss: 0.6232 - acc: 0.6706 - val_loss: 0.6245 - val_acc: 0.6703\n",
      "Epoch 8/10\n",
      "32843/32843 [==============================] - 68s 2ms/step - loss: 0.6227 - acc: 0.6705 - val_loss: 0.6252 - val_acc: 0.6695\n",
      "Epoch 9/10\n",
      "32843/32843 [==============================] - 68s 2ms/step - loss: 0.6231 - acc: 0.6709 - val_loss: 0.6269 - val_acc: 0.6697\n",
      "Epoch 10/10\n",
      "32843/32843 [==============================] - 68s 2ms/step - loss: 0.6224 - acc: 0.6703 - val_loss: 0.6264 - val_acc: 0.6697\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/anaconda3/lib/python3.6/site-packages/ipykernel_launcher.py:18: UserWarning: The `input_dim` and `input_length` arguments in recurrent layers are deprecated. Use `input_shape` instead.\n",
      "/anaconda3/lib/python3.6/site-packages/ipykernel_launcher.py:18: UserWarning: Update your `SimpleRNN` call to the Keras 2 API: `SimpleRNN(8, activation=\"relu\", input_shape=(None, 32))`\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 32843 samples, validate on 14076 samples\n",
      "Epoch 1/10\n",
      "32843/32843 [==============================] - 29s 873us/step - loss: 0.1045 - acc: 0.6698 - val_loss: 0.1037 - val_acc: 0.6693\n",
      "Epoch 2/10\n",
      "32843/32843 [==============================] - 28s 854us/step - loss: 0.1035 - acc: 0.6701 - val_loss: 0.1035 - val_acc: 0.6697\n",
      "Epoch 3/10\n",
      "32843/32843 [==============================] - 28s 846us/step - loss: 0.1031 - acc: 0.6703 - val_loss: 0.1044 - val_acc: 0.6693\n",
      "Epoch 4/10\n",
      "32843/32843 [==============================] - 28s 854us/step - loss: 0.1032 - acc: 0.6703 - val_loss: 0.1033 - val_acc: 0.6695\n",
      "Epoch 5/10\n",
      "32843/32843 [==============================] - 28s 851us/step - loss: 0.1032 - acc: 0.6699 - val_loss: 0.1032 - val_acc: 0.6692TA: 2s - - ETA: 1s - loss: 0.1032 - a - ETA: 0s - loss: 0.\n",
      "Epoch 6/10\n",
      "32843/32843 [==============================] - 28s 859us/step - loss: 0.1033 - acc: 0.6696 - val_loss: 0.1032 - val_acc: 0.6692\n",
      "Epoch 7/10\n",
      "32843/32843 [==============================] - 28s 853us/step - loss: 0.1031 - acc: 0.6701 - val_loss: 0.1031 - val_acc: 0.6695 0.670\n",
      "Epoch 8/10\n",
      "32843/32843 [==============================] - 28s 858us/step - loss: 0.1030 - acc: 0.6702 - val_loss: 0.1029 - val_acc: 0.6695\n",
      "Epoch 9/10\n",
      "32843/32843 [==============================] - 28s 861us/step - loss: 0.1030 - acc: 0.6699 - val_loss: 0.1029 - val_acc: 0.6697\n",
      "Epoch 10/10\n",
      "32843/32843 [==============================] - 28s 857us/step - loss: 0.1031 - acc: 0.6700 - val_loss: 0.1031 - val_acc: 0.6697\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/anaconda3/lib/python3.6/site-packages/ipykernel_launcher.py:18: UserWarning: The `input_dim` and `input_length` arguments in recurrent layers are deprecated. Use `input_shape` instead.\n",
      "/anaconda3/lib/python3.6/site-packages/ipykernel_launcher.py:18: UserWarning: Update your `SimpleRNN` call to the Keras 2 API: `SimpleRNN(16, activation=\"relu\", input_shape=(None, 32))`\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 32843 samples, validate on 14076 samples\n",
      "Epoch 1/10\n",
      "32843/32843 [==============================] - 37s 1ms/step - loss: 0.1044 - acc: 0.6705 - val_loss: 0.1050 - val_acc: 0.6697\n",
      "Epoch 2/10\n",
      "32843/32843 [==============================] - 37s 1ms/step - loss: 0.1032 - acc: 0.6704 - val_loss: 0.1041 - val_acc: 0.6696\n",
      "Epoch 3/10\n",
      "32843/32843 [==============================] - 37s 1ms/step - loss: 0.1028 - acc: 0.6702 - val_loss: 0.1030 - val_acc: 0.6697\n",
      "Epoch 4/10\n",
      "32843/32843 [==============================] - 37s 1ms/step - loss: 0.1027 - acc: 0.6698 - val_loss: 0.1030 - val_acc: 0.6694\n",
      "Epoch 5/10\n",
      "32843/32843 [==============================] - 37s 1ms/step - loss: 0.1026 - acc: 0.6704 - val_loss: 0.1034 - val_acc: 0.6697\n",
      "Epoch 6/10\n",
      "32843/32843 [==============================] - 37s 1ms/step - loss: 0.1025 - acc: 0.6707 - val_loss: 0.1029 - val_acc: 0.6702\n",
      "Epoch 7/10\n",
      "32843/32843 [==============================] - 37s 1ms/step - loss: 0.1023 - acc: 0.6698 - val_loss: 0.1025 - val_acc: 0.6700 ETA: 0s - loss: 0.1023 -\n",
      "Epoch 8/10\n",
      "32843/32843 [==============================] - 37s 1ms/step - loss: 0.1023 - acc: 0.6707 - val_loss: 0.1032 - val_acc: 0.6694\n",
      "Epoch 9/10\n",
      "32843/32843 [==============================] - 37s 1ms/step - loss: 0.1024 - acc: 0.6703 - val_loss: 0.1027 - val_acc: 0.6698\n",
      "Epoch 10/10\n",
      "32843/32843 [==============================] - 37s 1ms/step - loss: 0.1022 - acc: 0.6702 - val_loss: 0.1049 - val_acc: 0.6702\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/anaconda3/lib/python3.6/site-packages/ipykernel_launcher.py:18: UserWarning: The `input_dim` and `input_length` arguments in recurrent layers are deprecated. Use `input_shape` instead.\n",
      "/anaconda3/lib/python3.6/site-packages/ipykernel_launcher.py:18: UserWarning: Update your `SimpleRNN` call to the Keras 2 API: `SimpleRNN(16, activation=\"relu\", input_shape=(None, 32))`\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 32843 samples, validate on 14076 samples\n",
      "Epoch 1/10\n",
      "32843/32843 [==============================] - 38s 1ms/step - loss: 0.6315 - acc: 0.6697 - val_loss: 0.6307 - val_acc: 0.6697\n",
      "Epoch 2/10\n",
      "32843/32843 [==============================] - 37s 1ms/step - loss: 0.6269 - acc: 0.6700 - val_loss: 0.6265 - val_acc: 0.6697\n",
      "Epoch 3/10\n",
      "32843/32843 [==============================] - 38s 1ms/step - loss: 0.6256 - acc: 0.6700 - val_loss: 0.6289 - val_acc: 0.6697\n",
      "Epoch 4/10\n",
      "32843/32843 [==============================] - 37s 1ms/step - loss: 0.6252 - acc: 0.6700 - val_loss: 0.6251 - val_acc: 0.6697\n",
      "Epoch 5/10\n",
      "32843/32843 [==============================] - 37s 1ms/step - loss: 0.6252 - acc: 0.6700 - val_loss: 0.6263 - val_acc: 0.6697\n",
      "Epoch 6/10\n",
      "32843/32843 [==============================] - 37s 1ms/step - loss: 0.6244 - acc: 0.6700 - val_loss: 0.6243 - val_acc: 0.6697\n",
      "Epoch 7/10\n",
      "32843/32843 [==============================] - 37s 1ms/step - loss: 0.6243 - acc: 0.6700 - val_loss: 0.6249 - val_acc: 0.6697\n",
      "Epoch 8/10\n",
      "32843/32843 [==============================] - 37s 1ms/step - loss: 0.6243 - acc: 0.6700 - val_loss: 0.6246 - val_acc: 0.6697\n",
      "Epoch 9/10\n",
      "32843/32843 [==============================] - 38s 1ms/step - loss: 0.6234 - acc: 0.6700 - val_loss: 0.6252 - val_acc: 0.6697lo - ETA: 0s - loss: 0.6232 - acc:\n",
      "Epoch 10/10\n",
      "32843/32843 [==============================] - 37s 1ms/step - loss: 0.6230 - acc: 0.6700 - val_loss: 0.6273 - val_acc: 0.6697\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/anaconda3/lib/python3.6/site-packages/ipykernel_launcher.py:18: UserWarning: The `input_dim` and `input_length` arguments in recurrent layers are deprecated. Use `input_shape` instead.\n",
      "/anaconda3/lib/python3.6/site-packages/ipykernel_launcher.py:18: UserWarning: Update your `SimpleRNN` call to the Keras 2 API: `SimpleRNN(8, activation=\"relu\", input_shape=(None, 32))`\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 32843 samples, validate on 14076 samples\n",
      "Epoch 1/10\n",
      "32843/32843 [==============================] - 27s 834us/step - loss: 0.6327 - acc: 0.6697 - val_loss: 0.6323 - val_acc: 0.6699\n",
      "Epoch 2/10\n",
      "32843/32843 [==============================] - 27s 825us/step - loss: 0.6284 - acc: 0.6701 - val_loss: 0.6291 - val_acc: 0.6686\n",
      "Epoch 3/10\n",
      "32843/32843 [==============================] - 27s 825us/step - loss: 0.6262 - acc: 0.6698 - val_loss: 0.6271 - val_acc: 0.6703669 - ETA: 4s - los - ETA: 2s - loss: 0.6 \n",
      "Epoch 4/10\n",
      "32843/32843 [==============================] - 27s 826us/step - loss: 0.6247 - acc: 0.6701 - val_loss: 0.6278 - val_acc: 0.67060s - loss: 0.6241 - acc: 0.6 - ETA: 0s - loss: 0.6\n",
      "Epoch 5/10\n",
      "32843/32843 [==============================] - 27s 823us/step - loss: 0.6237 - acc: 0.6697 - val_loss: 0.6259 - val_acc: 0.6703\n",
      "Epoch 6/10\n",
      "32843/32843 [==============================] - 27s 827us/step - loss: 0.6233 - acc: 0.6699 - val_loss: 0.6297 - val_acc: 0.6703\n",
      "Epoch 7/10\n",
      "32843/32843 [==============================] - 27s 830us/step - loss: 0.6228 - acc: 0.6702 - val_loss: 0.6251 - val_acc: 0.6709\n",
      "Epoch 8/10\n",
      "32843/32843 [==============================] - 27s 826us/step - loss: 0.6226 - acc: 0.6702 - val_loss: 0.6245 - val_acc: 0.6701\n",
      "Epoch 9/10\n",
      "32843/32843 [==============================] - 27s 817us/step - loss: 0.6226 - acc: 0.6705 - val_loss: 0.6295 - val_acc: 0.6697\n",
      "Epoch 10/10\n",
      "32843/32843 [==============================] - 27s 831us/step - loss: 0.6221 - acc: 0.6703 - val_loss: 0.6276 - val_acc: 0.6692\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/anaconda3/lib/python3.6/site-packages/ipykernel_launcher.py:18: UserWarning: The `input_dim` and `input_length` arguments in recurrent layers are deprecated. Use `input_shape` instead.\n",
      "/anaconda3/lib/python3.6/site-packages/ipykernel_launcher.py:18: UserWarning: Update your `SimpleRNN` call to the Keras 2 API: `SimpleRNN(8, activation=\"relu\", input_shape=(None, 32))`\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 32843 samples, validate on 14076 samples\n",
      "Epoch 1/10\n",
      "32843/32843 [==============================] - 37s 1ms/step - loss: 0.1040 - acc: 0.6700 - val_loss: 0.1046 - val_acc: 0.6687\n",
      "Epoch 2/10\n",
      "32843/32843 [==============================] - 36s 1ms/step - loss: 0.1033 - acc: 0.6699 - val_loss: 0.1037 - val_acc: 0.6698 0s - loss: 0.10\n",
      "Epoch 3/10\n",
      "32843/32843 [==============================] - 37s 1ms/step - loss: 0.1029 - acc: 0.6703 - val_loss: 0.1037 - val_acc: 0.6673\n",
      "Epoch 4/10\n",
      "32843/32843 [==============================] - 37s 1ms/step - loss: 0.1026 - acc: 0.6705 - val_loss: 0.1034 - val_acc: 0.6697 loss: 0.1027 - acc: 0\n",
      "Epoch 5/10\n",
      "32843/32843 [==============================] - 37s 1ms/step - loss: 0.1025 - acc: 0.6705 - val_loss: 0.1035 - val_acc: 0.6704: 5s  - ETA\n",
      "Epoch 6/10\n",
      "32843/32843 [==============================] - 37s 1ms/step - loss: 0.1023 - acc: 0.6707 - val_loss: 0.1031 - val_acc: 0.6683\n",
      "Epoch 7/10\n",
      "32843/32843 [==============================] - 37s 1ms/step - loss: 0.1022 - acc: 0.6705 - val_loss: 0.1030 - val_acc: 0.6705\n",
      "Epoch 8/10\n",
      "32843/32843 [==============================] - 37s 1ms/step - loss: 0.1022 - acc: 0.6710 - val_loss: 0.1029 - val_acc: 0.6699\n",
      "Epoch 9/10\n",
      "32843/32843 [==============================] - 37s 1ms/step - loss: 0.1021 - acc: 0.6706 - val_loss: 0.1026 - val_acc: 0.6706A: 2s - loss: 0.1020 -  - ETA: 1s - ETA: 0s - loss: 0.1021 - acc: 0.670\n",
      "Epoch 10/10\n",
      "32843/32843 [==============================] - 37s 1ms/step - loss: 0.1020 - acc: 0.6709 - val_loss: 0.1027 - val_acc: 0.6698\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/anaconda3/lib/python3.6/site-packages/ipykernel_launcher.py:18: UserWarning: The `input_dim` and `input_length` arguments in recurrent layers are deprecated. Use `input_shape` instead.\n",
      "/anaconda3/lib/python3.6/site-packages/ipykernel_launcher.py:18: UserWarning: Update your `SimpleRNN` call to the Keras 2 API: `SimpleRNN(128, activation=\"relu\", input_shape=(None, 32))`\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 32843 samples, validate on 14076 samples\n",
      "Epoch 1/10\n",
      "32843/32843 [==============================] - 32s 979us/step - loss: 0.6313 - acc: 0.6704 - val_loss: 0.6310 - val_acc: 0.6694\n",
      "Epoch 2/10\n",
      "32843/32843 [==============================] - 32s 965us/step - loss: 0.6266 - acc: 0.6693 - val_loss: 0.6273 - val_acc: 0.6700\n",
      "Epoch 3/10\n",
      "32843/32843 [==============================] - 32s 962us/step - loss: 0.6250 - acc: 0.6703 - val_loss: 0.6340 - val_acc: 0.6695\n",
      "Epoch 4/10\n",
      "32843/32843 [==============================] - 32s 959us/step - loss: 0.6242 - acc: 0.6704 - val_loss: 0.6413 - val_acc: 0.6695\n",
      "Epoch 5/10\n",
      "32843/32843 [==============================] - 31s 959us/step - loss: 0.6239 - acc: 0.6702 - val_loss: 0.6251 - val_acc: 0.6696\n",
      "Epoch 6/10\n",
      "32843/32843 [==============================] - 31s 952us/step - loss: 0.6235 - acc: 0.6706 - val_loss: 0.6248 - val_acc: 0.6699\n",
      "Epoch 7/10\n",
      "32843/32843 [==============================] - 31s 957us/step - loss: 0.6232 - acc: 0.6704 - val_loss: 0.6241 - val_acc: 0.6697 - los - ETA: 0s - loss: 0.6233 - acc: 0.670\n",
      "Epoch 8/10\n",
      "32843/32843 [==============================] - 32s 966us/step - loss: 0.6230 - acc: 0.6704 - val_loss: 0.6263 - val_acc: 0.6697\n",
      "Epoch 9/10\n",
      "32843/32843 [==============================] - 31s 957us/step - loss: 0.6228 - acc: 0.6706 - val_loss: 0.6244 - val_acc: 0.6697\n",
      "Epoch 10/10\n",
      "32843/32843 [==============================] - 32s 968us/step - loss: 0.6227 - acc: 0.6706 - val_loss: 0.6279 - val_acc: 0.6695\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/anaconda3/lib/python3.6/site-packages/ipykernel_launcher.py:18: UserWarning: The `input_dim` and `input_length` arguments in recurrent layers are deprecated. Use `input_shape` instead.\n",
      "/anaconda3/lib/python3.6/site-packages/ipykernel_launcher.py:18: UserWarning: Update your `SimpleRNN` call to the Keras 2 API: `SimpleRNN(64, activation=\"relu\", input_shape=(None, 32))`\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 32843 samples, validate on 14076 samples\n",
      "Epoch 1/10\n",
      "32843/32843 [==============================] - 39s 1ms/step - loss: 0.6319 - acc: 0.6695 - val_loss: 0.6285 - val_acc: 0.6697\n",
      "Epoch 2/10\n",
      "32843/32843 [==============================] - 39s 1ms/step - loss: 0.6268 - acc: 0.6701 - val_loss: 0.6295 - val_acc: 0.6697\n",
      "Epoch 3/10\n",
      "32843/32843 [==============================] - 39s 1ms/step - loss: 0.6254 - acc: 0.6700 - val_loss: 0.6275 - val_acc: 0.6697\n",
      "Epoch 4/10\n",
      "32843/32843 [==============================] - 39s 1ms/step - loss: 0.6248 - acc: 0.6704 - val_loss: 0.6566 - val_acc: 0.6695\n",
      "Epoch 5/10\n",
      "32843/32843 [==============================] - 38s 1ms/step - loss: 0.6247 - acc: 0.6703 - val_loss: 0.6252 - val_acc: 0.6699\n",
      "Epoch 6/10\n",
      "32843/32843 [==============================] - 39s 1ms/step - loss: 0.6238 - acc: 0.6698 - val_loss: 0.6270 - val_acc: 0.6699\n",
      "Epoch 7/10\n",
      "32843/32843 [==============================] - 39s 1ms/step - loss: 0.6237 - acc: 0.6705 - val_loss: 0.6299 - val_acc: 0.6697\n",
      "Epoch 8/10\n",
      "32843/32843 [==============================] - 39s 1ms/step - loss: 0.6232 - acc: 0.6706 - val_loss: 0.6294 - val_acc: 0.6695s - loss: 0\n",
      "Epoch 9/10\n",
      "32843/32843 [==============================] - 39s 1ms/step - loss: 0.6230 - acc: 0.6699 - val_loss: 0.6239 - val_acc: 0.6702\n",
      "Epoch 10/10\n",
      "32843/32843 [==============================] - 39s 1ms/step - loss: 0.6226 - acc: 0.6699 - val_loss: 0.6266 - val_acc: 0.6694oss: \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/anaconda3/lib/python3.6/site-packages/ipykernel_launcher.py:18: UserWarning: The `input_dim` and `input_length` arguments in recurrent layers are deprecated. Use `input_shape` instead.\n",
      "/anaconda3/lib/python3.6/site-packages/ipykernel_launcher.py:18: UserWarning: Update your `SimpleRNN` call to the Keras 2 API: `SimpleRNN(128, activation=\"relu\", input_shape=(None, 32))`\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 32843 samples, validate on 14076 samples\n",
      "Epoch 1/10\n",
      "32843/32843 [==============================] - 63s 2ms/step - loss: 0.6321 - acc: 0.6698 - val_loss: 0.6319 - val_acc: 0.6697\n",
      "Epoch 2/10\n",
      "32843/32843 [==============================] - 59s 2ms/step - loss: 0.6291 - acc: 0.6701 - val_loss: 0.6272 - val_acc: 0.6697\n",
      "Epoch 3/10\n",
      "32843/32843 [==============================] - 61s 2ms/step - loss: 0.6276 - acc: 0.6708 - val_loss: 0.6343 - val_acc: 0.6695\n",
      "Epoch 4/10\n",
      "32843/32843 [==============================] - 60s 2ms/step - loss: 0.6258 - acc: 0.6702 - val_loss: 0.6280 - val_acc: 0.6694\n",
      "Epoch 5/10\n",
      "32843/32843 [==============================] - 60s 2ms/step - loss: 0.6257 - acc: 0.6698 - val_loss: 0.6262 - val_acc: 0.6697\n",
      "Epoch 6/10\n",
      "32843/32843 [==============================] - 62s 2ms/step - loss: 0.6254 - acc: 0.6705 - val_loss: 0.6248 - val_acc: 0.6696\n",
      "Epoch 7/10\n",
      "32843/32843 [==============================] - 60s 2ms/step - loss: 0.6249 - acc: 0.6700 - val_loss: 0.6253 - val_acc: 0.6700\n",
      "Epoch 8/10\n",
      "32843/32843 [==============================] - 60s 2ms/step - loss: 0.6249 - acc: 0.6698 - val_loss: 0.6377 - val_acc: 0.6696\n",
      "Epoch 9/10\n",
      "32843/32843 [==============================] - 61s 2ms/step - loss: 0.6249 - acc: 0.6704 - val_loss: 0.6262 - val_acc: 0.6697\n",
      "Epoch 10/10\n",
      "32843/32843 [==============================] - 60s 2ms/step - loss: 0.6243 - acc: 0.6705 - val_loss: 0.6264 - val_acc: 0.6698\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/anaconda3/lib/python3.6/site-packages/ipykernel_launcher.py:18: UserWarning: The `input_dim` and `input_length` arguments in recurrent layers are deprecated. Use `input_shape` instead.\n",
      "/anaconda3/lib/python3.6/site-packages/ipykernel_launcher.py:18: UserWarning: Update your `SimpleRNN` call to the Keras 2 API: `SimpleRNN(64, activation=\"relu\", input_shape=(None, 32))`\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 32843 samples, validate on 14076 samples\n",
      "Epoch 1/10\n",
      "32843/32843 [==============================] - 55s 2ms/step - loss: 0.1045 - acc: 0.6694 - val_loss: 0.1049 - val_acc: 0.6697\n",
      "Epoch 2/10\n",
      "32843/32843 [==============================] - 55s 2ms/step - loss: 0.1038 - acc: 0.6700 - val_loss: 0.1032 - val_acc: 0.6697\n",
      "Epoch 3/10\n",
      "32843/32843 [==============================] - 55s 2ms/step - loss: 0.1035 - acc: 0.6702 - val_loss: 0.1032 - val_acc: 0.6699\n",
      "Epoch 4/10\n",
      "32843/32843 [==============================] - 55s 2ms/step - loss: 0.1034 - acc: 0.6699 - val_loss: 0.1030 - val_acc: 0.6697\n",
      "Epoch 5/10\n",
      "32843/32843 [==============================] - 55s 2ms/step - loss: 0.1033 - acc: 0.6695 - val_loss: 0.1031 - val_acc: 0.6696\n",
      "Epoch 6/10\n",
      "32843/32843 [==============================] - 55s 2ms/step - loss: 0.1031 - acc: 0.6700 - val_loss: 0.1033 - val_acc: 0.6701\n",
      "Epoch 7/10\n",
      "32843/32843 [==============================] - 55s 2ms/step - loss: 0.1030 - acc: 0.6702 - val_loss: 0.1032 - val_acc: 0.6697\n",
      "Epoch 8/10\n",
      "32843/32843 [==============================] - 55s 2ms/step - loss: 0.1029 - acc: 0.6699 - val_loss: 0.1027 - val_acc: 0.6701\n",
      "Epoch 9/10\n",
      "32843/32843 [==============================] - 55s 2ms/step - loss: 0.1028 - acc: 0.6702 - val_loss: 0.1031 - val_acc: 0.6697\n",
      "Epoch 10/10\n",
      "32843/32843 [==============================] - 55s 2ms/step - loss: 0.1029 - acc: 0.6695 - val_loss: 0.1033 - val_acc: 0.6697\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/anaconda3/lib/python3.6/site-packages/ipykernel_launcher.py:18: UserWarning: The `input_dim` and `input_length` arguments in recurrent layers are deprecated. Use `input_shape` instead.\n",
      "/anaconda3/lib/python3.6/site-packages/ipykernel_launcher.py:18: UserWarning: Update your `SimpleRNN` call to the Keras 2 API: `SimpleRNN(32, activation=\"relu\", input_shape=(None, 32))`\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 32843 samples, validate on 14076 samples\n",
      "Epoch 1/10\n",
      "32843/32843 [==============================] - 36s 1ms/step - loss: 0.6333 - acc: 0.6697 - val_loss: 0.6308 - val_acc: 0.6697\n",
      "Epoch 2/10\n",
      "32843/32843 [==============================] - 36s 1ms/step - loss: 0.6291 - acc: 0.6700 - val_loss: 0.6284 - val_acc: 0.6699\n",
      "Epoch 3/10\n",
      "32843/32843 [==============================] - 36s 1ms/step - loss: 0.6267 - acc: 0.6706 - val_loss: 0.6284 - val_acc: 0.6697\n",
      "Epoch 4/10\n",
      "32843/32843 [==============================] - 36s 1ms/step - loss: 0.6261 - acc: 0.6704 - val_loss: 0.6285 - val_acc: 0.6697\n",
      "Epoch 5/10\n",
      "32843/32843 [==============================] - 36s 1ms/step - loss: 0.6264 - acc: 0.6703 - val_loss: 0.6300 - val_acc: 0.6697\n",
      "Epoch 6/10\n",
      "32843/32843 [==============================] - 36s 1ms/step - loss: 0.6261 - acc: 0.6695 - val_loss: 0.6254 - val_acc: 0.6697\n",
      "Epoch 7/10\n",
      "32843/32843 [==============================] - 38s 1ms/step - loss: 0.6255 - acc: 0.6702 - val_loss: 0.6253 - val_acc: 0.6697\n",
      "Epoch 8/10\n",
      "32843/32843 [==============================] - 36s 1ms/step - loss: 0.6255 - acc: 0.6702 - val_loss: 0.6258 - val_acc: 0.6697\n",
      "Epoch 9/10\n",
      "32843/32843 [==============================] - 36s 1ms/step - loss: 0.6251 - acc: 0.6701 - val_loss: 0.6255 - val_acc: 0.6697\n",
      "Epoch 10/10\n",
      "32843/32843 [==============================] - 36s 1ms/step - loss: 0.6246 - acc: 0.6702 - val_loss: 0.6249 - val_acc: 0.6698\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/anaconda3/lib/python3.6/site-packages/ipykernel_launcher.py:18: UserWarning: The `input_dim` and `input_length` arguments in recurrent layers are deprecated. Use `input_shape` instead.\n",
      "/anaconda3/lib/python3.6/site-packages/ipykernel_launcher.py:18: UserWarning: Update your `SimpleRNN` call to the Keras 2 API: `SimpleRNN(16, activation=\"relu\", input_shape=(None, 32))`\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 32843 samples, validate on 14076 samples\n",
      "Epoch 1/10\n",
      "32843/32843 [==============================] - 53s 2ms/step - loss: 0.6312 - acc: 0.6698 - val_loss: 0.6287 - val_acc: 0.6697\n",
      "Epoch 2/10\n",
      "32843/32843 [==============================] - 53s 2ms/step - loss: 0.6268 - acc: 0.6700 - val_loss: 0.6275 - val_acc: 0.6697\n",
      "Epoch 3/10\n",
      "32843/32843 [==============================] - 53s 2ms/step - loss: 0.6263 - acc: 0.6698 - val_loss: 0.6276 - val_acc: 0.6695\n",
      "Epoch 4/10\n",
      "32843/32843 [==============================] - 56s 2ms/step - loss: 0.6247 - acc: 0.6703 - val_loss: 0.6265 - val_acc: 0.6697\n",
      "Epoch 5/10\n",
      "32843/32843 [==============================] - 53s 2ms/step - loss: 0.6247 - acc: 0.6705 - val_loss: 0.6255 - val_acc: 0.6695\n",
      "Epoch 6/10\n",
      "32843/32843 [==============================] - 53s 2ms/step - loss: 0.6235 - acc: 0.6701 - val_loss: 0.6292 - val_acc: 0.6697\n",
      "Epoch 7/10\n",
      "32843/32843 [==============================] - 54s 2ms/step - loss: 0.6234 - acc: 0.6704 - val_loss: 0.6247 - val_acc: 0.6697\n",
      "Epoch 8/10\n",
      "32843/32843 [==============================] - 53s 2ms/step - loss: 0.6233 - acc: 0.6704 - val_loss: 0.6251 - val_acc: 0.6699\n",
      "Epoch 9/10\n",
      "32843/32843 [==============================] - 55s 2ms/step - loss: 0.6228 - acc: 0.6699 - val_loss: 0.6244 - val_acc: 0.6701\n",
      "Epoch 10/10\n",
      "32843/32843 [==============================] - 54s 2ms/step - loss: 0.6225 - acc: 0.6703 - val_loss: 0.6293 - val_acc: 0.6697\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/anaconda3/lib/python3.6/site-packages/ipykernel_launcher.py:18: UserWarning: The `input_dim` and `input_length` arguments in recurrent layers are deprecated. Use `input_shape` instead.\n",
      "/anaconda3/lib/python3.6/site-packages/ipykernel_launcher.py:18: UserWarning: Update your `SimpleRNN` call to the Keras 2 API: `SimpleRNN(16, activation=\"relu\", input_shape=(None, 32))`\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 32843 samples, validate on 14076 samples\n",
      "Epoch 1/10\n",
      "32843/32843 [==============================] - 31s 943us/step - loss: 0.6325 - acc: 0.6702 - val_loss: 0.6304 - val_acc: 0.6697\n",
      "Epoch 2/10\n",
      "32843/32843 [==============================] - 30s 905us/step - loss: 0.6283 - acc: 0.6699 - val_loss: 0.6274 - val_acc: 0.6697\n",
      "Epoch 3/10\n",
      "32843/32843 [==============================] - 30s 916us/step - loss: 0.6264 - acc: 0.6704 - val_loss: 0.6258 - val_acc: 0.6697\n",
      "Epoch 4/10\n",
      "32843/32843 [==============================] - 30s 921us/step - loss: 0.6263 - acc: 0.6702 - val_loss: 0.6280 - val_acc: 0.6697\n",
      "Epoch 5/10\n",
      "32843/32843 [==============================] - 30s 921us/step - loss: 0.6260 - acc: 0.6702 - val_loss: 0.6250 - val_acc: 0.6696\n",
      "Epoch 6/10\n",
      "32843/32843 [==============================] - 30s 925us/step - loss: 0.6257 - acc: 0.6699 - val_loss: 0.6254 - val_acc: 0.6697\n",
      "Epoch 7/10\n",
      "32843/32843 [==============================] - 31s 931us/step - loss: 0.6256 - acc: 0.6698 - val_loss: 0.6250 - val_acc: 0.6700\n",
      "Epoch 8/10\n",
      "32843/32843 [==============================] - 31s 938us/step - loss: 0.6252 - acc: 0.6706 - val_loss: 0.6258 - val_acc: 0.6697\n",
      "Epoch 9/10\n",
      "32843/32843 [==============================] - 30s 921us/step - loss: 0.6258 - acc: 0.6702 - val_loss: 0.6295 - val_acc: 0.6698\n",
      "Epoch 10/10\n",
      "32843/32843 [==============================] - 31s 935us/step - loss: 0.6254 - acc: 0.6698 - val_loss: 0.6258 - val_acc: 0.6699\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/anaconda3/lib/python3.6/site-packages/ipykernel_launcher.py:18: UserWarning: The `input_dim` and `input_length` arguments in recurrent layers are deprecated. Use `input_shape` instead.\n",
      "/anaconda3/lib/python3.6/site-packages/ipykernel_launcher.py:18: UserWarning: Update your `SimpleRNN` call to the Keras 2 API: `SimpleRNN(8, activation=\"relu\", input_shape=(None, 32))`\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 32843 samples, validate on 14076 samples\n",
      "Epoch 1/10\n",
      "32843/32843 [==============================] - 28s 866us/step - loss: 0.6335 - acc: 0.6667 - val_loss: 0.6318 - val_acc: 0.6698\n",
      "Epoch 2/10\n",
      "32843/32843 [==============================] - 28s 852us/step - loss: 0.6283 - acc: 0.6703 - val_loss: 0.6283 - val_acc: 0.6698\n",
      "Epoch 3/10\n",
      "32843/32843 [==============================] - 27s 837us/step - loss: 0.6270 - acc: 0.6702 - val_loss: 0.6274 - val_acc: 0.6697\n",
      "Epoch 4/10\n",
      "32843/32843 [==============================] - 29s 897us/step - loss: 0.6266 - acc: 0.6711 - val_loss: 0.6275 - val_acc: 0.6699 0s - loss: 0.6268 - ac\n",
      "Epoch 5/10\n",
      "32843/32843 [==============================] - 29s 897us/step - loss: 0.6257 - acc: 0.6699 - val_loss: 0.6272 - val_acc: 0.6702\n",
      "Epoch 6/10\n",
      "32843/32843 [==============================] - 30s 899us/step - loss: 0.6250 - acc: 0.6703 - val_loss: 0.6254 - val_acc: 0.6700\n",
      "Epoch 7/10\n",
      "32843/32843 [==============================] - 29s 885us/step - loss: 0.6244 - acc: 0.6701 - val_loss: 0.6285 - val_acc: 0.6698\n",
      "Epoch 8/10\n",
      "32843/32843 [==============================] - 29s 876us/step - loss: 0.6242 - acc: 0.6700 - val_loss: 0.6250 - val_acc: 0.6697\n",
      "Epoch 9/10\n",
      "32843/32843 [==============================] - 27s 826us/step - loss: 0.6238 - acc: 0.6713 - val_loss: 0.6269 - val_acc: 0.6689\n",
      "Epoch 10/10\n",
      "32843/32843 [==============================] - 27s 826us/step - loss: 0.6243 - acc: 0.6697 - val_loss: 0.6253 - val_acc: 0.6702\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/anaconda3/lib/python3.6/site-packages/ipykernel_launcher.py:18: UserWarning: The `input_dim` and `input_length` arguments in recurrent layers are deprecated. Use `input_shape` instead.\n",
      "/anaconda3/lib/python3.6/site-packages/ipykernel_launcher.py:18: UserWarning: Update your `SimpleRNN` call to the Keras 2 API: `SimpleRNN(4, activation=\"relu\", input_shape=(None, 32))`\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 32843 samples, validate on 14076 samples\n",
      "Epoch 1/10\n",
      "32843/32843 [==============================] - 36s 1ms/step - loss: 0.1040 - acc: 0.6700 - val_loss: 0.1036 - val_acc: 0.6697\n",
      "Epoch 2/10\n",
      "32843/32843 [==============================] - 36s 1ms/step - loss: 0.1031 - acc: 0.6700 - val_loss: 0.1038 - val_acc: 0.6697\n",
      "Epoch 3/10\n",
      "32843/32843 [==============================] - 36s 1ms/step - loss: 0.1027 - acc: 0.6700 - val_loss: 0.1032 - val_acc: 0.6697\n",
      "Epoch 4/10\n",
      "32843/32843 [==============================] - 36s 1ms/step - loss: 0.1026 - acc: 0.6700 - val_loss: 0.1029 - val_acc: 0.6697\n",
      "Epoch 5/10\n",
      "32843/32843 [==============================] - 36s 1ms/step - loss: 0.1024 - acc: 0.6699 - val_loss: 0.1027 - val_acc: 0.6697\n",
      "Epoch 6/10\n",
      "32843/32843 [==============================] - 36s 1ms/step - loss: 0.1023 - acc: 0.6697 - val_loss: 0.1029 - val_acc: 0.6697\n",
      "Epoch 7/10\n",
      "32843/32843 [==============================] - 36s 1ms/step - loss: 0.1023 - acc: 0.6697 - val_loss: 0.1027 - val_acc: 0.6697\n",
      "Epoch 8/10\n",
      "32843/32843 [==============================] - 36s 1ms/step - loss: 0.1022 - acc: 0.6702 - val_loss: 0.1028 - val_acc: 0.6697\n",
      "Epoch 9/10\n",
      "32843/32843 [==============================] - 36s 1ms/step - loss: 0.1022 - acc: 0.6697 - val_loss: 0.1028 - val_acc: 0.6697\n",
      "Epoch 10/10\n",
      "32843/32843 [==============================] - 36s 1ms/step - loss: 0.1021 - acc: 0.6699 - val_loss: 0.1031 - val_acc: 0.6697\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/anaconda3/lib/python3.6/site-packages/ipykernel_launcher.py:18: UserWarning: The `input_dim` and `input_length` arguments in recurrent layers are deprecated. Use `input_shape` instead.\n",
      "/anaconda3/lib/python3.6/site-packages/ipykernel_launcher.py:18: UserWarning: Update your `SimpleRNN` call to the Keras 2 API: `SimpleRNN(4, activation=\"relu\", input_shape=(None, 32))`\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 32843 samples, validate on 14076 samples\n",
      "Epoch 1/10\n",
      "32843/32843 [==============================] - 37s 1ms/step - loss: 0.1050 - acc: 0.6697 - val_loss: 0.1048 - val_acc: 0.6697\n",
      "Epoch 2/10\n",
      "32843/32843 [==============================] - 37s 1ms/step - loss: 0.1048 - acc: 0.6700 - val_loss: 0.1048 - val_acc: 0.6697\n",
      "Epoch 3/10\n",
      "32843/32843 [==============================] - 37s 1ms/step - loss: 0.1047 - acc: 0.6700 - val_loss: 0.1048 - val_acc: 0.6697\n",
      "Epoch 4/10\n",
      "32843/32843 [==============================] - 37s 1ms/step - loss: 0.1047 - acc: 0.6700 - val_loss: 0.1048 - val_acc: 0.6697\n",
      "Epoch 5/10\n",
      "32843/32843 [==============================] - 36s 1ms/step - loss: 0.1047 - acc: 0.6700 - val_loss: 0.1048 - val_acc: 0.6697\n",
      "Epoch 6/10\n",
      "32843/32843 [==============================] - 37s 1ms/step - loss: 0.1047 - acc: 0.6700 - val_loss: 0.1048 - val_acc: 0.6697\n",
      "Epoch 7/10\n",
      "32843/32843 [==============================] - 36s 1ms/step - loss: 0.1047 - acc: 0.6700 - val_loss: 0.1048 - val_acc: 0.6697\n",
      "Epoch 8/10\n",
      "32843/32843 [==============================] - 36s 1ms/step - loss: 0.1047 - acc: 0.6700 - val_loss: 0.1048 - val_acc: 0.6697\n",
      "Epoch 9/10\n",
      "32843/32843 [==============================] - 36s 1ms/step - loss: 0.1047 - acc: 0.6700 - val_loss: 0.1048 - val_acc: 0.6697\n",
      "Epoch 10/10\n",
      "32843/32843 [==============================] - 36s 1ms/step - loss: 0.1047 - acc: 0.6700 - val_loss: 0.1050 - val_acc: 0.6697\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/anaconda3/lib/python3.6/site-packages/ipykernel_launcher.py:18: UserWarning: The `input_dim` and `input_length` arguments in recurrent layers are deprecated. Use `input_shape` instead.\n",
      "/anaconda3/lib/python3.6/site-packages/ipykernel_launcher.py:18: UserWarning: Update your `SimpleRNN` call to the Keras 2 API: `SimpleRNN(8, activation=\"relu\", input_shape=(None, 32))`\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 32843 samples, validate on 14076 samples\n",
      "Epoch 1/10\n",
      "32843/32843 [==============================] - 28s 844us/step - loss: 0.6317 - acc: 0.6687 - val_loss: 0.6390 - val_acc: 0.6696\n",
      "Epoch 2/10\n",
      "32843/32843 [==============================] - 27s 834us/step - loss: 0.6261 - acc: 0.6701 - val_loss: 0.6293 - val_acc: 0.6697\n",
      "Epoch 3/10\n",
      "32843/32843 [==============================] - 27s 837us/step - loss: 0.6247 - acc: 0.6700 - val_loss: 0.6269 - val_acc: 0.6697\n",
      "Epoch 4/10\n",
      "32843/32843 [==============================] - 28s 838us/step - loss: 0.6239 - acc: 0.6700 - val_loss: 0.6250 - val_acc: 0.6697\n",
      "Epoch 5/10\n",
      "32843/32843 [==============================] - 27s 835us/step - loss: 0.6226 - acc: 0.6702 - val_loss: 0.6252 - val_acc: 0.6699\n",
      "Epoch 6/10\n",
      "32843/32843 [==============================] - 27s 832us/step - loss: 0.6225 - acc: 0.6702 - val_loss: 0.6283 - val_acc: 0.6696\n",
      "Epoch 7/10\n",
      "32843/32843 [==============================] - 28s 840us/step - loss: 0.6222 - acc: 0.6702 - val_loss: 0.6248 - val_acc: 0.6697\n",
      "Epoch 8/10\n",
      "32843/32843 [==============================] - 27s 830us/step - loss: 0.6219 - acc: 0.6704 - val_loss: 0.6240 - val_acc: 0.6696\n",
      "Epoch 9/10\n",
      "32843/32843 [==============================] - 27s 832us/step - loss: 0.6218 - acc: 0.6704 - val_loss: 0.6238 - val_acc: 0.6696\n",
      "Epoch 10/10\n",
      "32843/32843 [==============================] - 28s 839us/step - loss: 0.6216 - acc: 0.6703 - val_loss: 0.6235 - val_acc: 0.6694\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/anaconda3/lib/python3.6/site-packages/ipykernel_launcher.py:18: UserWarning: The `input_dim` and `input_length` arguments in recurrent layers are deprecated. Use `input_shape` instead.\n",
      "/anaconda3/lib/python3.6/site-packages/ipykernel_launcher.py:18: UserWarning: Update your `SimpleRNN` call to the Keras 2 API: `SimpleRNN(32, activation=\"relu\", input_shape=(None, 32))`\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 32843 samples, validate on 14076 samples\n",
      "Epoch 1/10\n",
      "32843/32843 [==============================] - 28s 844us/step - loss: 0.1046 - acc: 0.6699 - val_loss: 0.1035 - val_acc: 0.6697\n",
      "Epoch 2/10\n",
      "32843/32843 [==============================] - 28s 839us/step - loss: 0.1035 - acc: 0.6702 - val_loss: 0.1045 - val_acc: 0.6697\n",
      "Epoch 3/10\n",
      "32843/32843 [==============================] - 28s 842us/step - loss: 0.1032 - acc: 0.6707 - val_loss: 0.1036 - val_acc: 0.6697\n",
      "Epoch 4/10\n",
      "32843/32843 [==============================] - 27s 836us/step - loss: 0.1031 - acc: 0.6699 - val_loss: 0.1029 - val_acc: 0.6697\n",
      "Epoch 5/10\n",
      "32843/32843 [==============================] - 28s 843us/step - loss: 0.1031 - acc: 0.6698 - val_loss: 0.1029 - val_acc: 0.6697\n",
      "Epoch 6/10\n",
      "32843/32843 [==============================] - 28s 842us/step - loss: 0.1030 - acc: 0.6699 - val_loss: 0.1028 - val_acc: 0.6697\n",
      "Epoch 7/10\n",
      "32843/32843 [==============================] - 27s 836us/step - loss: 0.1028 - acc: 0.6698 - val_loss: 0.1028 - val_acc: 0.6697\n",
      "Epoch 8/10\n",
      "32843/32843 [==============================] - 28s 844us/step - loss: 0.1028 - acc: 0.6703 - val_loss: 0.1029 - val_acc: 0.6698\n",
      "Epoch 9/10\n",
      "32843/32843 [==============================] - 27s 837us/step - loss: 0.1027 - acc: 0.6700 - val_loss: 0.1030 - val_acc: 0.6697\n",
      "Epoch 10/10\n",
      "32843/32843 [==============================] - 28s 840us/step - loss: 0.1030 - acc: 0.6699 - val_loss: 0.1033 - val_acc: 0.6699\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/anaconda3/lib/python3.6/site-packages/ipykernel_launcher.py:18: UserWarning: The `input_dim` and `input_length` arguments in recurrent layers are deprecated. Use `input_shape` instead.\n",
      "/anaconda3/lib/python3.6/site-packages/ipykernel_launcher.py:18: UserWarning: Update your `SimpleRNN` call to the Keras 2 API: `SimpleRNN(128, activation=\"relu\", input_shape=(None, 32))`\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 32843 samples, validate on 14076 samples\n",
      "Epoch 1/10\n",
      "32843/32843 [==============================] - 37s 1ms/step - loss: 0.6332 - acc: 0.6691 - val_loss: 0.6342 - val_acc: 0.6700\n",
      "Epoch 2/10\n",
      "32843/32843 [==============================] - 37s 1ms/step - loss: 0.6279 - acc: 0.6701 - val_loss: 0.6270 - val_acc: 0.6695\n",
      "Epoch 3/10\n",
      "32843/32843 [==============================] - 37s 1ms/step - loss: 0.6260 - acc: 0.6700 - val_loss: 0.6261 - val_acc: 0.6697\n",
      "Epoch 4/10\n",
      "32843/32843 [==============================] - 37s 1ms/step - loss: 0.6256 - acc: 0.6704 - val_loss: 0.6268 - val_acc: 0.6697\n",
      "Epoch 5/10\n",
      "32843/32843 [==============================] - 37s 1ms/step - loss: 0.6259 - acc: 0.6706 - val_loss: 0.6262 - val_acc: 0.6696\n",
      "Epoch 6/10\n",
      "32843/32843 [==============================] - 37s 1ms/step - loss: 0.6251 - acc: 0.6699 - val_loss: 0.6267 - val_acc: 0.6699\n",
      "Epoch 7/10\n",
      "32843/32843 [==============================] - 37s 1ms/step - loss: 0.6247 - acc: 0.6698 - val_loss: 0.6285 - val_acc: 0.6696\n",
      "Epoch 8/10\n",
      "32843/32843 [==============================] - 37s 1ms/step - loss: 0.6253 - acc: 0.6699 - val_loss: 0.6260 - val_acc: 0.6699\n",
      "Epoch 9/10\n",
      "32843/32843 [==============================] - 37s 1ms/step - loss: 0.6247 - acc: 0.6704 - val_loss: 0.6265 - val_acc: 0.6697\n",
      "Epoch 10/10\n",
      "32843/32843 [==============================] - 37s 1ms/step - loss: 0.6263 - acc: 0.6700 - val_loss: 0.6274 - val_acc: 0.6696\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/anaconda3/lib/python3.6/site-packages/ipykernel_launcher.py:18: UserWarning: The `input_dim` and `input_length` arguments in recurrent layers are deprecated. Use `input_shape` instead.\n",
      "/anaconda3/lib/python3.6/site-packages/ipykernel_launcher.py:18: UserWarning: Update your `SimpleRNN` call to the Keras 2 API: `SimpleRNN(32, activation=\"relu\", input_shape=(None, 32))`\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 32843 samples, validate on 14076 samples\n",
      "Epoch 1/10\n",
      "32843/32843 [==============================] - 28s 853us/step - loss: 0.1041 - acc: 0.6697 - val_loss: 0.1035 - val_acc: 0.6698\n",
      "Epoch 2/10\n",
      "32843/32843 [==============================] - 27s 837us/step - loss: 0.1031 - acc: 0.6701 - val_loss: 0.1054 - val_acc: 0.6697\n",
      "Epoch 3/10\n",
      "32843/32843 [==============================] - 28s 842us/step - loss: 0.1029 - acc: 0.6702 - val_loss: 0.1032 - val_acc: 0.6694\n",
      "Epoch 4/10\n",
      "32843/32843 [==============================] - 28s 839us/step - loss: 0.1027 - acc: 0.6702 - val_loss: 0.1030 - val_acc: 0.6695\n",
      "Epoch 5/10\n",
      "32843/32843 [==============================] - 28s 842us/step - loss: 0.1026 - acc: 0.6702 - val_loss: 0.1028 - val_acc: 0.6697\n",
      "Epoch 6/10\n",
      "32843/32843 [==============================] - 28s 841us/step - loss: 0.1024 - acc: 0.6703 - val_loss: 0.1029 - val_acc: 0.6694\n",
      "Epoch 7/10\n",
      "32843/32843 [==============================] - 28s 839us/step - loss: 0.1024 - acc: 0.6706 - val_loss: 0.1028 - val_acc: 0.6696\n",
      "Epoch 8/10\n",
      "32843/32843 [==============================] - 28s 840us/step - loss: 0.1023 - acc: 0.6699 - val_loss: 0.1033 - val_acc: 0.6682\n",
      "Epoch 9/10\n",
      "32843/32843 [==============================] - 28s 839us/step - loss: 0.1023 - acc: 0.6707 - val_loss: 0.1031 - val_acc: 0.6697\n",
      "Epoch 10/10\n",
      "32843/32843 [==============================] - 28s 840us/step - loss: 0.1023 - acc: 0.6702 - val_loss: 0.1025 - val_acc: 0.6701\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/anaconda3/lib/python3.6/site-packages/ipykernel_launcher.py:18: UserWarning: The `input_dim` and `input_length` arguments in recurrent layers are deprecated. Use `input_shape` instead.\n",
      "/anaconda3/lib/python3.6/site-packages/ipykernel_launcher.py:18: UserWarning: Update your `SimpleRNN` call to the Keras 2 API: `SimpleRNN(4, activation=\"relu\", input_shape=(None, 32))`\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 32843 samples, validate on 14076 samples\n",
      "Epoch 1/10\n",
      "32843/32843 [==============================] - 53s 2ms/step - loss: 0.1042 - acc: 0.6700 - val_loss: 0.1037 - val_acc: 0.6697\n",
      "Epoch 2/10\n",
      "32843/32843 [==============================] - 53s 2ms/step - loss: 0.1031 - acc: 0.6700 - val_loss: 0.1037 - val_acc: 0.6697\n",
      "Epoch 3/10\n",
      "32843/32843 [==============================] - 53s 2ms/step - loss: 0.1027 - acc: 0.6700 - val_loss: 0.1033 - val_acc: 0.6697\n",
      "Epoch 4/10\n",
      "32843/32843 [==============================] - 53s 2ms/step - loss: 0.1025 - acc: 0.6700 - val_loss: 0.1029 - val_acc: 0.6697\n",
      "Epoch 5/10\n",
      "32843/32843 [==============================] - 53s 2ms/step - loss: 0.1024 - acc: 0.6700 - val_loss: 0.1028 - val_acc: 0.6697\n",
      "Epoch 6/10\n",
      "32843/32843 [==============================] - 53s 2ms/step - loss: 0.1023 - acc: 0.6700 - val_loss: 0.1040 - val_acc: 0.6697\n",
      "Epoch 7/10\n",
      "32843/32843 [==============================] - 53s 2ms/step - loss: 0.1022 - acc: 0.6700 - val_loss: 0.1029 - val_acc: 0.6697\n",
      "Epoch 8/10\n",
      "32843/32843 [==============================] - 53s 2ms/step - loss: 0.1022 - acc: 0.6700 - val_loss: 0.1028 - val_acc: 0.6697\n",
      "Epoch 9/10\n",
      "32843/32843 [==============================] - 53s 2ms/step - loss: 0.1021 - acc: 0.6703 - val_loss: 0.1027 - val_acc: 0.6697\n",
      "Epoch 10/10\n",
      "32843/32843 [==============================] - 53s 2ms/step - loss: 0.1021 - acc: 0.6700 - val_loss: 0.1030 - val_acc: 0.6697\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/anaconda3/lib/python3.6/site-packages/ipykernel_launcher.py:18: UserWarning: The `input_dim` and `input_length` arguments in recurrent layers are deprecated. Use `input_shape` instead.\n",
      "/anaconda3/lib/python3.6/site-packages/ipykernel_launcher.py:18: UserWarning: Update your `SimpleRNN` call to the Keras 2 API: `SimpleRNN(4, activation=\"relu\", input_shape=(None, 32))`\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 32843 samples, validate on 14076 samples\n",
      "Epoch 1/10\n",
      "32843/32843 [==============================] - 55s 2ms/step - loss: 0.6358 - acc: 0.6692 - val_loss: 0.6344 - val_acc: 0.6697\n",
      "Epoch 2/10\n",
      "32843/32843 [==============================] - 55s 2ms/step - loss: 0.6345 - acc: 0.6700 - val_loss: 0.6346 - val_acc: 0.6697\n",
      "Epoch 3/10\n",
      "32843/32843 [==============================] - 54s 2ms/step - loss: 0.6343 - acc: 0.6700 - val_loss: 0.6353 - val_acc: 0.6697\n",
      "Epoch 4/10\n",
      "32843/32843 [==============================] - 55s 2ms/step - loss: 0.6346 - acc: 0.6700 - val_loss: 0.6345 - val_acc: 0.6697oss: 0.6357 - acc:  - ETA: 21s - loss: 0.6359 - acc - ETA: 21s - loss: 0.6361 - - ETA: 20s - loss: 0.6359 - acc: 0.66 - ETA: 20s - loss: 0.6359 - acc: 0.66 - ETA: 20s - loss: 0.6360 - acc: 0. - ETA: 19s - loss: 0.6353 - acc: 0. - ETA: 19s - loss: 0.6352 - acc: 0. - - ETA: 18s - loss:  - ETA: 17s - loss: 0.6351 - acc: 0.66 - ETA: 17s - loss: 0.6351 - - ETA: 17s - loss: 0.6352 - acc: 0. - ETA: 17s - loss: 0.6351 - acc:  - ETA: 17s - loss: 0.6351 - - E - ETA: 16s - loss: 0.6349 - - ETA:  - ETA: 15s - loss: 0.6346 - acc - ETA: 14s -  - ETA: 13s - loss: 0.6345 - acc: 0.67 - ETA: 13s - loss: 0.6344 - acc - ETA: 12s - loss: 0.63 - ETA: 12s - loss: 0.6346 - acc: 0.67 - ETA: 12s - loss: 0.6346 - acc: 0.67 - ETA: 12s - loss: 0.6346 - acc: 0.67 - ETA: 12s - loss: 0.6345 - - ETA: 11s - loss: 0.6351 - acc: 0. - ETA: 10s - loss: 0.6351 - acc: 0. - ETA: 10s - loss: 0.6351 - acc: 0.66 - ETA: 10s - loss: 0.6352 - acc: 0.66 - ETA: 10s - loss: 0.6352 - acc:  - ETA: 10s - loss: 0.6351 - acc: 0. - E - ETA: 9s - loss: 0.6345 - acc: 0 - ETA: 8s - loss: 0.6345 - acc: 0.67 - ETA: 8s - loss: 0.6345 -  - ETA: 8s - loss: 0.6344 - acc:  - ETA: 7s - loss: 0.6344 - acc: 0.6 - ETA: 7s - loss: 0.6 - ETA: 6s - loss: 0.6348 - acc: 0. - ETA: 6s - loss: 0.6349 - acc: 0.669 - ETA: 6s - loss: 0.6348 - a - ETA: 6s - loss: 0.6347 - acc: 0.670 - ETA: 6s - loss: - ETA: 4s - loss: 0.6346 - acc: 0.670 - ETA: 4s - loss: 0.6346 - acc: 0.670 - ETA: 4s - loss: 0.6346 - acc - ETA: 4s - loss: 0.6 - ETA: 3s - loss: 0.6346 - a - ETA: 3s - loss: 0.6346 - acc: 0.670 - ETA: 3s - loss: 0 - ETA: 2s - loss: 0.6346 - ETA: 1s - loss: 0.6344 - acc: 0.670 - ETA: 1s - loss: 0.6345 - acc: 0.670 - ETA: 1s - loss: 0.6345 - acc: 0.670 - ETA: 1s - los\n",
      "Epoch 5/10\n",
      "32843/32843 [==============================] - 55s 2ms/step - loss: 0.6346 - acc: 0.6700 - val_loss: 0.6353 - val_acc: 0.6697- ETA: 44s - loss: 0. - ETA: 43s - loss: 0.6445 - acc: 0.65 - E - ETA: 39s  - ETA: 36s - loss: 0.6364 - acc: 0.66 - ETA: 36s - loss: 0.6364 - acc:  - ETA: 36s  - ETA: 35s - loss: 0.6363 - acc: 0. - ETA: 35s - loss: 0.6366 - acc: 0. - ETA: 35s - loss: 0.6366 - acc - ETA: 35s - loss - ETA: 34s - loss: 0.6364 - acc: 0.66 - ETA: 34s - loss: 0. - ETA: 33s - loss: 0.6367 - acc: 0. - ETA: 32s - loss: 0.6363 - acc - ETA: 32s - loss: 0.6366 - acc: 0. - ETA: 31s - loss: 0.6368 - acc: 0. - E - ETA: 29s - loss: 0.6356 - acc: 0.66 - ETA: 29s - loss: 0.6358 - acc: 0. - - ETA:  - ETA: 27s - loss: 0.6346 - acc: 0.67 - ETA: 26s - lo - ETA: 26s - loss: 0.6351 - acc: 0.66 - ETA: 26s - loss: 0.6350 - acc: 0. - ETA: 26s - loss: 0.6350 - acc: 0.66 - ETA: 26s - loss: 0.6351 - a - ETA: 25s - loss: 0.6355 - acc: 0. - E - ETA: 25s - loss: 0.6353 - acc: 0.66 - ETA: 24s - loss: 0.6353 - acc: 0.66 - ETA: 24s - loss: 0.6353 - acc:  - ETA: 24s - loss: 0.6355 - - ETA: 24s - loss: 0.63 - ETA: 21s - loss: 0.6357 - - ETA: 18s - loss: 0.6353 - acc - ETA: 18s - lo - ETA: 17s - loss: 0.6355 - ETA:  - ETA: 16 - ETA: 15s - loss: 0.6363 - acc: 0. - ETA: 15s - loss: 0.6362 - acc:  - ETA: 14s - loss: 0. - ETA: 13s - lo - ETA: 13s - loss: 0. - ETA: 12s - loss: 0.6359 - acc: 0. - ETA: 11s - loss: 0.6360 - acc: 0.66 - ETA: 11s - loss: 0.6360 - acc: 0. - ETA: 11s - - ETA: 9s - loss: 0.6359 - acc: 0.66 - ETA: 9s - loss: 0.63 - ETA: 8s - loss: 0.6362 - acc: 0.667 - ETA: 8s - loss: 0.6 - ETA: 7s - loss: 0.6357 -  - ETA: 7s - loss: 0.6356 - ac - ETA: 6s - loss: 0.6358 - acc: 0.668 - ETA: 6s - loss: 0.6358 - acc: - ETA: 6s - loss - ETA: 5s - loss: 0.6360 - acc: 0.66 - ETA: 5s - loss: 0.63 - ETA: 4s - loss: 0.6358 - acc: 0.668 - ETA: 4s - loss: 0.6358 - acc - ETA: 3s - loss: 0.6357 - acc: 0.668 - ETA: 3s - loss: 0.6357 - acc:  - ETA: 3s - loss: 0.6357 - acc: 0.668 - ETA: 3s - loss: 0.6356 - acc: 0.668 - ETA: 3s - loss: 0.6356  - ETA: 2s - loss: 0.6355  - ETA: 2s - loss: 0.6 - ETA: 1s - lo\n",
      "Epoch 6/10\n",
      "32843/32843 [==============================] - 55s 2ms/step - loss: 0.6345 - acc: 0.6700 - val_loss: 0.6353 - val_acc: 0.6697c: 0.65 - ETA: 48s - loss: 0.6475 - acc: 0. - ETA: 47s - loss: 0.6339 - ETA: 46s - loss: 0.63 - ETA: 46s - loss: 0.6402 - acc: 0. - ETA: 46s - loss: 0.6406 - acc - ETA: 45s - loss: 0.6389 - a - - ETA: 44s - loss: 0.6359 - acc: 0. - ETA: 44s - loss: 0. - ETA: 43s - loss: 0.6345 - acc - ETA: 43s  - ETA: 42s - loss: 0.6332 - acc: 0.67 - ETA: 42s - loss: 0.6333 - acc: 0.67 - ETA: 42s - loss: 0.6331 - a - ETA: 42s - loss:  - ETA: 41s - loss: 0.6310 - a - ETA: 41s - lo - ETA: 41s - loss: 0.6326 - ETA: 40s - loss: 0.6335 - acc: 0.67 - ETA: 40s - loss: 0.6335 - acc: 0. - ETA: 40s - loss: 0.6343 - a - ETA: 40s - loss: 0.6324 - acc:  - E - ETA: 38s - loss: 0.6301 - acc: 0.67 - ETA: 38s - loss: 0.6302 - acc - ETA: 37s - loss - ETA: 36s - lo - ETA: 34s - loss: 0.6307 - acc:  - ETA: 34s -  - ETA: 33s - loss: 0.6312 - acc - ETA - ETA: 32s - loss: 0.6305 - acc: 0.67 - ETA: 32s - loss - ETA: 32s - loss: 0.6305 - acc: 0.67 - ETA: 32s - loss: 0.63 - ETA: 30s - loss: 0.6305 - acc: 0.67 - ETA: 30s - loss: 0.6306 - acc: 0.67 - ETA: 30s - loss: 0.6304 - acc - ETA: 30s - loss: 0.6310 - acc - ETA: 30s - loss: 0.6317 - acc: 0.67 - ETA: 29s - loss - ETA: 29s - loss: 0.6322 - - ETA: 26s  - ETA: 26s - loss: 0.6337 - ETA: 23s - loss: 0.6341 - acc: 0.67 - ETA: 23s - loss: 0.6342 - ETA: 22s - loss: 0.6339 - acc: 0. - ETA: 22s - loss: 0.6340 - acc: 0.67 - ETA: 22s - loss: 0.6340 - acc: 0.67 - ETA: 22s - loss: 0.6337 - acc: 0.67 - ETA:  - ETA: 21s - lo - ETA: 20s - loss: 0.6342 - acc:  - ETA: 20s - loss: 0.6340 - acc: 0.67 - ETA: 20s - loss: 0.6339 - acc: 0.67 - ETA: 20s - loss: 0.6339 - acc: 0.67 - ETA: 20 - ETA: 19s - loss: 0.6337 - acc: 0.67 - ETA: 19s - loss: 0.6339 - a - ETA: 19s - loss: 0.6335 - - ETA: 18s - loss: 0.6334 - a - ETA: 17s - loss: 0.6332 - acc: 0.67 - ETA: 17s - loss: 0.6333 - acc: 0.67 - ETA: 17s - loss:  - E - ETA: 16s - loss - - ETA: 14s - loss:  - ETA: 13s - loss: 0.6335 - acc: 0.67 - ETA: 13s - loss: 0.6334 - acc - ETA: 13s -  - ETA: 13s - loss: 0.6334 - acc: 0. - ETA: 12s - loss:  - ETA: 12s - loss: 0.6332 - - ETA: 11s - loss: 0.6329 - ETA: 10s - loss: 0.6331 - acc: 0. - ETA: 10s - loss: 0. - ETA: 10s - loss: 0.6332 - acc:  - ETA: 9s - loss: 0.6334 - acc: 0.671 - ETA: 9s - loss: 0.6334 -  - ETA: 9s - loss: 0.6335 - acc: 0.671 - ETA: 9s - loss: 0.6336 - acc: 0. - ETA: 9s - loss: 0. - ETA: 8s - loss: 0.6334 - acc - ETA: 7s  - ETA: 6s - loss: 0.6337 - acc - ETA: 5s - loss: 0.6340 - - ETA: 5s - - ETA: 3s - loss: 0.6339 - acc: 0.67 - ETA: 3s - loss: 0.6337 - acc: 0.671 - ETA: 3s - loss: 0.6338 - acc:  - ETA: 3s - loss: 0.6337 - acc: 0.6 - ETA - ETA: 1s - loss: 0.6338 - acc: 0.6 - ETA: 1s - loss: 0.6340 - acc: 0.670 - ETA: 1s - loss: 0.6340 - acc: 0.67 - ETA: 1s - loss: 0.6340 - acc: 0.670 - ETA: 1s - loss: 0.6340 - acc: 0.670 - ETA: 1s - loss: 0.6341 - acc: 0.670 - ETA: 1s - loss: 0.6341 - acc: 0.67 - ETA: 0s - loss: 0.6343 - acc: 0.670 - ETA: 0s - loss: 0.6343 - acc: 0.6 - ETA: 0s - loss: 0.6344 - acc: - ETA: 0s - loss: 0.6344 - acc:\n",
      "Epoch 7/10\n",
      "32843/32843 [==============================] - 55s 2ms/step - loss: 0.6346 - acc: 0.6700 - val_loss: 0.6346 - val_acc: 0.6697 loss: 0.6379 - ETA: 44s - loss: 0. - ETA: 43s - loss: 0.6385 - acc: 0.66 - ETA: 43s - loss: 0.6392 - acc: 0. - E - ETA: 42s - loss: 0.6372 - acc: 0. - ETA: 42s - loss: 0. - ETA - ETA: 40s - loss: 0.6396 - acc: 0.66 - ETA: 40s - loss: 0.6395 - acc: 0.66 - ETA: 40s - loss: 0.6396 - - ETA: 40s - loss: 0.6384 - acc: 0. - ETA: 40s - loss: 0.6380 - ETA: 39s - loss: 0.6381 - a - ETA: 39s - loss: 0.6368 - - ETA: 39s - loss: 0. - ETA: 38s - loss: 0.6364 - acc: 0.66 - ETA: 38s - loss: 0.6360 - acc: 0.66 - ETA: 38s - loss:  - ETA: 38s - loss: 0.6349 - acc: 0. - ETA: 38s - loss: 0.6349 - a - ETA: 36s - loss: 0.6353 - acc: 0.66 - ETA: 36s - loss: 0.6353 - acc - ETA: 36s - loss: 0.6350 - acc: 0.66 - ETA: 36s - loss: 0.6348 - acc: 0.66 - - ETA: 35s - loss: 0.6346 - acc: 0. - ETA: 35s - loss: 0.6344 - a - ETA: 34s - loss: 0.6342 - acc:  - ETA: 34s - loss: 0.6344 - acc: 0.67 - ETA: 34s - loss: 0.6342 - acc: 0.67 - ETA - ETA: 33s - loss: 0.6339 - acc: 0.67 - ETA: 33s - loss:  - E - ETA: 32s - loss:  - ETA: 30s - loss: 0.6338 - acc: 0.67 - ETA: 30s - loss: 0.6338 - acc: 0.67 - ETA: 30s - loss: 0. - ETA: 30s - loss: 0.6342 - acc: 0.67 - ETA: 30s -  - ETA: 29s - loss: 0.6343 - acc: 0.67 - ETA: 29s - loss: 0.6342 - acc: 0. - ETA: 29s - loss:  - ETA: 28s - loss: 0.6347 - acc: 0. - ETA: 28s - loss: 0.6348 - acc: 0.66 - ETA: 28s - loss:  - ETA - ETA: 24s - loss: 0.6345 - acc: 0. - ETA: 24s - loss:  - ETA:  - ETA: 22s - loss: 0.6341 - acc: 0.67 - ETA: 22s - loss: 0.6340 - acc: 0. - ETA: 22s - loss: 0.63 - ETA: 21s - loss: 0. - ETA:  - ETA: 19 - ETA: 19s - loss: 0.6335 - acc: 0.67 - ETA: 19s - loss: 0.6337 - ETA: 18s - loss: 0.6342 - acc: 0. - ETA: 18s - loss:  - ETA: 18s - loss: 0.6343 - acc:  - ETA: 17s - loss: 0.6345 - - ETA: 16s - loss: 0.6346 - acc: 0.67 - ETA: 16s - loss: 0.6347 - acc: 0.66 - ETA: 16s - loss: 0.6347 - acc: 0. - ETA - ETA: 14s - loss: 0.6348 - acc: 0.66 - ETA: 14s - loss: 0.6348 - a - ETA: 12s - loss: 0. - ETA: 11s - loss: 0.6343 - acc: 0. - ETA: 11s -  - ETA: 10s - loss:  - ETA: 10s - loss: 0.6344 - acc: 0.67 - ETA: 10s - loss: 0.6345 - ETA: 9s  - ETA: 8s - loss: 0.6341 - acc: 0.670 - ETA: 8s - loss: 0.6 - ETA: 7s - loss: 0.6346 - ac - ETA: 7s - los - ETA: 5s - loss: 0.6350 - ac - ETA: 5s - loss: 0 - ETA: 4s - loss: 0.6348 - a - ETA: 3s - loss: 0.6348 - acc: 0.669 - ETA: 3s - loss: 0.63 - ETA: 3s - loss: 0.6349 - acc: 0.6 - ETA: 2s - loss: 0.6350 - acc: 0 - ETA: 2s - loss: 0.6350 - acc: - ETA: 2s - loss: 0.6348 - acc: 0.669 - ETA: 2s - - ETA: 0s - loss: 0.6343 - acc: - ETA: 0s - loss: 0.6342 - acc: 0.67 - ETA: 0s - loss: 0.6343 - acc\n",
      "Epoch 8/10\n",
      "32843/32843 [==============================] - 55s 2ms/step - loss: 0.6346 - acc: 0.6700 - val_loss: 0.6347 - val_acc: 0.6697. - ETA: 45s - loss: 0.6409 - acc: 0.66 - ETA: 45s - loss: 0.6442 - acc:  - ETA: 44s - loss: 0.6431 - acc: 0.65 - ETA: 44s - loss: 0.6438 - - ETA: 44s - loss: 0.6432 - acc: 0.65 - ETA: 44s -  - E - ETA: 37s - loss: 0.6363 - acc: 0.66 - ETA: 37s - loss: 0.6366 - acc: 0. - ETA: 36s - loss:  - ETA: 36s - loss: 0.63 - ETA: 33s - loss: 0.6363 - acc: 0.66 - ETA: 33s - loss: 0. - ETA: 33s  - ETA: 30s - lo - ETA:  - ETA: 28s - loss: 0.6357 - acc:  - ETA: 28s - loss: 0.6361 - acc - ETA: 28s - loss:  - ETA: 27s - loss: 0.6355 - acc: 0.66 - ETA: 26s - loss: 0.6355 - acc: 0.66 - ETA: 26s - loss:  - ETA: 26s - loss: 0.6352 - acc: 0.66 - ETA: 26s - loss: 0.6353 - acc: 0.66 - E - ETA: 25s - loss: 0.6352 - acc - ETA: 25s - loss: 0.63 - ETA: 24s - loss: 0. - ETA: 24s - loss: 0.6343 - - ETA: 23s - loss: 0.6336 - - ETA: 23s - loss: 0. - ETA: 23s - loss: 0.63 - ETA: 22s - loss: 0. - ETA: 22s - loss: 0.6347 - acc: 0.66 - ETA: 22s - loss: 0.6346 - acc: 0.67 - ETA: 22s - loss: 0.6345 - acc: 0.67 - ETA: 22s - loss: 0.6345 - acc: 0. - ETA: 22s - loss:  - ETA: 20s - loss: 0.6343 - acc: 0.67 - ETA: 20s - loss - ETA: 19s - loss: 0.6343 - acc: 0.67 - ETA: 19s - loss: 0.6344 - - - ETA: 18s - loss: 0.6345 - acc: 0.67 - ETA: 18s - loss: 0.6346 - acc: 0.66 - ETA: 18s - loss: 0. - ETA: 18s - loss: 0.6344 - acc:  - E - ETA: 17s - loss: 0.6348 - acc: 0.66 - ETA: 17s - loss: 0.6348 - acc: 0.66 - ETA: 16s - loss: 0.6349 - acc: 0. - ETA: 16s - loss: 0. - E - ETA: 15s -  - ETA: 14s - loss: 0.6343 - a - ETA: 14s - loss: 0.6342 - - ETA: 14s -  - ETA: 13s - loss - ETA: 12s - loss: 0.6350 - a - ETA: 11s - loss: 0.6348 - acc: 0. - ETA: 11s - loss: 0.6345 - acc - ETA: 11s - loss: 0.6345 - acc: 0.67 - ETA: 11s - loss: 0.6346 - acc: 0.67 - ETA: 11s - loss:  - ETA: 10s - loss: 0.6346 - acc: 0.67 - ETA: 10s - loss:  - ETA: 10s - loss: 0.6342 - acc: 0. - ETA: 10s - loss: 0.6344 - acc: 0.67 - ETA: 10s - loss: 0.6343 - acc: 0. - ETA: 9s - loss: 0.6341 - acc: - ETA: 9s - loss: 0.6346 - acc: - ETA: 9s - loss: 0.6345 - - ETA: 8s - loss: 0.6344 - acc: 0 - ETA: 8s - loss: 0.6343 - acc: 0.670 - ETA: 8s - loss: 0.6344 - acc: 0.670 - ETA: 8s - loss: 0.6344 - acc: 0.6 - ETA: 8s - loss: 0.6343 - acc:  - ETA: 7s - loss: - ETA: 6s - loss: 0.6340 - acc: 0.670 - ETA: 6s - loss: 0.6340 - acc: 0.67 - ETA: 6s - loss: 0.6340 - ac - ETA: 6s - loss: 0.6338 - acc: 0. - ETA: 5s - loss: 0.6340 - acc: 0 - ETA: 5s - loss: 0.63 - ETA: 4s - - ETA: 1s -\n",
      "Epoch 9/10\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "32843/32843 [==============================] - 55s 2ms/step - loss: 0.6345 - acc: 0.6700 - val_loss: 0.6350 - val_acc: 0.6697c:  - ETA: 47s - loss: 0.6411 - acc: 0.66 - ETA: 47s - loss: 0.6423 - acc: 0. - ETA: 47s - loss:  - ETA: 46s - loss: 0.6385 - acc: 0.66 - ETA: 46s  - ETA: 45s - loss:  - ETA: 44s - loss: 0.6388 - acc: 0. - ETA: 44s - loss: 0.6373 - acc: 0.66 - ETA: 44s - loss: 0.6370 - acc: 0. - ETA: 44s - loss: 0.6386 - acc: 0.66 - ETA: 44s - loss: 0.6397 - acc: 0.66 - ETA: 44s - loss: 0.6401 - acc: 0. - ETA: 44s - loss: 0.6401 - acc: 0.66 - ETA: 44s - loss: 0.6400 - acc: 0. - ETA: 40s - loss: 0.6409 - - - ETA: 37s - loss: 0.6357 - acc: 0.66 - ETA: 37s - loss: 0.6356 - acc: 0.66 - ETA: 36s - loss: 0.6356 - acc: 0. - ETA: 36s - loss: 0.6360 - acc: 0. - ETA: 36 - ETA: 36s - loss: 0.6355 - ETA: 34s - loss: 0.6369 - acc: 0.66 - E - ETA - ETA: 28s -  - ETA: 28s - loss: 0.6358 - acc: 0.66 - ETA: 28s - loss: 0.6358 - acc: 0. - ETA: 28s - loss: 0.6358 - acc - ETA: 27s - lo - ETA: 27s - loss: 0.6365 - acc: 0. - ETA: 27s - loss: 0.6367 - acc - ETA: 26s - loss: 0.6365 - acc - ETA: 26s - loss: 0.6365 - acc: 0.66 - ETA: 26s - loss: 0. - ETA: 26s - loss: 0.63 - ETA: 24s - loss: 0.63 - ETA: 24s - loss: 0.6375 - acc: 0.66 - ETA: 24s - loss: 0.6376 - acc: 0.66 - ETA: 24s - loss: 0.6374 - acc: 0. - ETA: 24s - loss: 0.6374 - acc:  - ETA: 23s - loss: 0.6372 - acc - ETA: 23s  - ETA: 22s - loss: 0.6370 - acc:  - ETA: 22s - loss: 0.6373 - ETA: 22s - loss: 0.6370 - acc - ETA: 22s - loss: 0.6369 - acc: 0. - ETA: 22s - loss: 0.6367 - acc - ETA: 20s - loss: 0.6372 - acc: 0.66 - ETA: 20s - loss: 0.6372 - acc: 0. - ETA: 20 - ETA: 19s -  - ETA: 19s - loss: 0.6360 - acc: 0. - ETA: 19s - loss:  - E - ETA: 17s - loss: 0.6348 - acc: 0. - ETA: 17s - loss: 0.63 - ETA: 16s - loss: 0.6349 - acc: 0.66 - ETA: 16s - loss: 0.6348 - acc: 0. - ETA: 15s - loss: 0.6347 - acc: 0. - ETA: 15s - loss: 0.6346 - acc: 0.66 - ETA: 15s - loss: 0.63 - ETA: 14s - loss: 0. - ETA: 13s - loss: 0.6344 - acc: 0.67 - ETA: 13s - loss: 0.6344 - acc: 0.67 - ETA: 13s - loss: 0.6344 - a - ETA: 12s - loss: 0.6336 - - ETA: 12s - loss: 0.6337 - acc: 0.67 - ETA: 12s - loss: 0.6337 - acc: 0.67 - ETA: 12s - loss: 0.6337 - acc: 0.67 - ETA: 12s - loss: 0.6337 - acc: 0.67 - ETA: 12s - loss: 0.6336 - - ETA: 11s - loss: 0.6334 - acc: 0. - ETA: 11s  - ETA: 10s  - ETA: 8s - loss: 0.6338 - acc: 0.670 - ETA: 8s - loss: 0.6339 - acc: 0.67 - ETA: 7s - loss: 0.6340 - acc: - ETA: 7s - loss: 0.6341 - acc: 0.670 - ETA: 7s - loss: 0.6341 - acc: 0.6 - ETA: 7s - loss: 0.6339 - acc: 0.670 - ETA: 7s - loss: 0.6339 - acc: 0.670 - ETA: 7s  - ETA: 5s - loss: 0.6338 - acc: 0.67 - ETA: 5s - loss: 0.6339 - ETA: 5s - loss: 0.6337 - acc: 0.67 - ETA: 4s - loss: 0.6337 - acc: 0.671 - ETA: 4s - loss: 0.6338 - acc: 0.671 - ETA: 4s - loss: 0.6338 - ac - ETA: 4s - loss: 0.6339 -  - ETA: 3s - loss: 0.6341 - acc:  - ETA: 3s - loss: 0.6343 - acc: - ETA: 3s - loss: 0.6340 - ac - ETA: 2s - loss: 0.6341 - acc: 0.67 - ETA: 2s - loss: - ETA: 1s - loss: 0.6341 - acc: 0.670 - ETA: 1s - loss: 0.6341 - acc: 0.670 - ETA: 1s - loss: 0.6342 - acc - ETA: 0s - loss: 0.\n",
      "Epoch 10/10\n",
      "32843/32843 [==============================] - 55s 2ms/step - loss: 0.6345 - acc: 0.6700 - val_loss: 0.6346 - val_acc: 0.6697A: 46s - loss: 0.6305 - a - ETA: 45s - loss: 0.6246 - acc: 0.68 - ETA: 45s - loss: 0.6251 - acc: 0.68 - ETA: 45s - loss: 0.62 - ETA: 44s - loss: 0.6289 - acc: 0.67 - ETA: 43s - loss: 0.6292 - acc: 0.67 - ETA: 43s - loss: 0.62 - ETA:  - ETA: 42s - loss: 0.6317 - acc: 0.67 - ETA: 42s - loss: 0.6322 - acc:  - ETA: 42s - loss: 0.6321 - acc: 0.67 - ETA: 42s - loss: 0.6329 - acc: 0.67 - ETA: 42s - loss: 0.6329 - acc: 0. - ETA: 42s - loss:  - ETA: 41s - loss: 0.6343 - acc: 0.67 - ETA - ETA: 40s -  - ETA: 40s - loss: 0.6342 - acc: 0.67 - ETA: 39s - lo - ETA: 39s - loss: 0.6329 - a - ETA: 39s - loss:  - - ETA: 37s - loss: 0.6326 - acc: 0.67 - ETA: 37s - loss: 0.6323 - acc: 0.67 - ETA: 37s - loss:  - ETA - ETA: 35s - loss: 0.63 - ETA: 35s - loss: 0.6312 - acc: 0.67 - ETA: 35s - loss: 0.6314 - acc:  - E - ETA: 34s  - ETA: 32s - loss: 0.6324 - acc: 0. - ETA: 32s - loss: 0.6327 - acc - ETA: 32s - lo - ETA: 31s - loss: 0.6324 - acc - ETA: 31s - loss: 0.6329 - acc: 0.67 - ETA: 31s - loss: 0.6327 - acc: 0.67 - ETA: 31s - loss: 0. - ETA: 30s - loss: 0.6322 - acc: 0.67 - ETA: 30s - loss: 0.6322 - acc: 0.67 - ETA: 30s - loss: 0.6323 - acc: 0.67 - ETA: 30s - loss: 0.6324 - acc: 0.67 - ETA: 30s - loss: 0.6321 - acc: 0.67 - ETA - ETA: 29s - loss: 0.6326 - acc:  - ETA: 29s - loss: 0.6326 - acc: 0. - ETA: 29s - loss:  - ETA: 28s - loss: 0.6333 - acc: 0.67 - ETA: 27s - loss: 0.6333 - acc:  - ETA: 27s - loss: 0.63 - ETA: 27s - loss:  - ETA: 25s -  - ETA: 25s - loss: 0.6339 - acc - ETA: 24s - loss: 0.6335 - acc: 0. - ETA: 24s - loss: 0.6334 - - ETA: 21s - loss: 0.6335 - acc:  - ETA: 20s - lo - ETA: 18s - loss: 0.6326 - acc: 0.67 - ETA: 18s - loss: 0.6328 - - ETA: 18s - loss: 0.6332 - - E - ETA: 16s -  - ETA: 16 - ETA: 15s - loss: 0.6329 - acc: 0. - ETA: 15s - loss: 0.6328 - acc:  - ETA:  - ETA: 13s - loss: 0.6336 - acc: 0. - - ETA: 12s - loss: 0.6337 - acc: 0.67 - ETA: 12s - loss: 0.6339 - acc: 0.67 - ETA: 12s - loss: 0.63 - - ETA: 10s - loss - ETA: 10s - loss: 0.6338 - acc:  - ETA: 9s - loss: 0.6340 - acc:  - ETA: 9s - loss: 0.6342 - acc: 0.6 - ETA: 9s - loss: 0.6343 - acc: 0 - ETA: 9s - loss: 0. - ETA: 8s - loss: 0.6346 - - ETA: 7s - loss: 0.6345 - acc:  - ETA: 7s - loss: 0.6345 - acc: 0.670 - ETA: 7s - loss: 0.6345 - a - ETA: 6s - loss: 0.6346 - acc - ETA: 6s - loss: 0.6346 - acc: 0.669 - ETA: 6s - loss: 0.6347 -  - ETA: 5s - loss: 0.6351 - acc: 0.66 - ETA: 5s - loss:  - ETA:  - ETA: 2s - loss: 0.6349 - acc: 0. - ETA: 2s - loss: 0.6348 - acc: 0.6 - ETA: 2s - loss: 0.6348 - acc:  - ETA: 2s - loss: 0.6348 - acc: 0.66 - ETA: 2s - loss: 0.6347 - acc: 0.669 - ETA: 2s - loss: 0.63 - ETA: 1s - loss: 0.6347 - acc: 0.669 - ETA: 1s - loss: 0.6348 - a - ETA: 0s - loss: 0.6347 - acc: 0.669 - ETA: 0s - loss: 0.6346 - acc: 0.66 - ETA: 0s - loss: 0.6346 - acc: 0.670 - ETA: 0s - loss: 0.6346 - ac\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/anaconda3/lib/python3.6/site-packages/ipykernel_launcher.py:18: UserWarning: The `input_dim` and `input_length` arguments in recurrent layers are deprecated. Use `input_shape` instead.\n",
      "/anaconda3/lib/python3.6/site-packages/ipykernel_launcher.py:18: UserWarning: Update your `SimpleRNN` call to the Keras 2 API: `SimpleRNN(64, activation=\"relu\", input_shape=(None, 32))`\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 32843 samples, validate on 14076 samples\n",
      "Epoch 1/10\n",
      "32843/32843 [==============================] - 37s 1ms/step - loss: 0.1040 - acc: 0.6695 - val_loss: 0.1034 - val_acc: 0.6697ss: 0.1042 - a - ETA: 40s - loss: 0.1048 - acc: 0.66 - ETA: 40s - loss: 0.1054 - acc: 0.66 - ETA: 39s - loss: 0.1045 - a - ETA: 38s - loss: 0.1041 - ETA: 33s - loss: 0.1057 - acc: 0. - ETA: 32s - loss: 0.1056 - acc: 0. - ETA: 32s - loss:  - ETA: 30s - loss: 0.1045 - acc: 0.67 - ETA: 29s - loss: 0. - ETA: 25s - loss: 0. - ETA: 24s - loss: 0. - ETA: 24s - loss:  - ETA: 23s - loss: 0.1039 - acc: 0.67 - ETA: 23s - loss: 0.1039 - a - ETA: 23s - lo - ETA: 22s - loss: 0.1035 - acc: 0.67 - ETA: 22s - loss: 0.1033 - acc:  - ETA: 21s - loss: 0.1035 - acc: 0.67 - ETA: 21s - loss: 0.1036 - acc: 0.67 - ETA: 21s - loss: 0.1036 - acc: 0. - ETA: 21s - loss: 0.10 - ETA: 19s - loss: 0. - ETA: 19s - loss: 0.10 - ETA: 18s - loss: 0.1039 - acc: 0.67 - ETA: 18s - loss - ETA: 16s - loss: 0.1041 - acc: 0. - ETA: 15s - loss: 0.1041 - acc: 0.67 - ETA: 15s - loss: 0.1041 - acc: 0.67 - ETA: 15s - loss:  - ETA: 15s - loss: 0.1042 - acc: 0.67 - ETA: 15s - loss - ETA: 14s - loss: 0. - ETA: 13s - loss: 0.1042 - acc: 0.67 - ETA: 13s - loss: 0.1042 - a - ETA: 13s - loss: 0.1043 - acc - ETA: 13s - loss: 0.1042 - acc: 0.67 - ETA: 13s - loss: 0.1042 - acc: 0. - E - ETA: 12s - loss: 0.1042 - a - ETA: 12s - loss - ETA: 11s - loss: 0.1041 - acc:  - ETA: 11s  - ETA: 10s - loss: 0.1042 - acc: 0. - ETA: 10s - loss: 0.1042 - acc: 0.67 - ETA: 10s - loss: 0.1042 - acc: 0.67 - ETA: 10s - loss: 0.1042 - a - ETA: 10s - loss: 0.1041 - ETA: 9s - loss: 0.1042 - acc: 0.669 - ETA: 9s - loss: 0.1042 - acc: 0.669 - ETA: 9s - loss: 0.1042 - acc: 0.669 - ETA: 9s - loss: 0.1042 - acc: 0.669 - ETA: 9s - loss: 0.1042  - ETA: 8s - loss: 0.1041 - acc: 0.67 - ETA: 8s - loss: 0.1042 - acc - ETA: 8s - loss: 0.1042 - acc: 0.66 - ETA: 8s - loss: 0.1042 - acc: 0.669 - ETA: 7s - loss: 0.1042 - acc: 0.66 - ETA: 7s - loss: 0.1042 - acc: 0.669 - ETA: 7s - loss: 0.1042 - acc:  - ETA: 7s - loss: 0.1041 - acc: 0 - ETA: 7s - loss: 0.1041 - acc: 0.66 - ETA: 7s - loss: 0.1041 - acc: 0.669 - ETA: 7s - loss: 0.1041 - acc: 0. - ETA: 6s - loss: 0.104 - ETA: 6s - loss: 0.1041 - acc: 0. - ETA: 5s - loss: 0.1041 - acc: 0 - ETA: 5s - loss: 0.1042 - acc - ETA: 5s - loss: 0.1042 - acc - ETA: 4s - loss: 0.1043 - - ETA: 4s - loss: 0.1042 - acc: 0.66 - ETA: 4s - loss: 0.1042 - acc: 0.668 - ETA: 4s - loss: 0.1042 - acc: 0.66 - ETA: 3s - loss: 0.1042 - acc: 0.668 - ETA: 3s - loss: 0.1042 -  - ETA: 3s - loss: 0.1041 - acc: 0.66 - ETA: 3s - loss: 0.1041 - acc: 0.66 - ETA: 3s - loss: 0.1041 - acc: 0.669 - ETA: 3s - loss: 0.104 - ETA: 2s - loss: 0.1041 - acc: 0. - ETA: 2s - loss: 0.1040 - acc: - ETA: 1s - loss: 0.104 - ETA: 0s - loss: 0.1040 - acc:  - ETA: 0s - loss: 0.1040 - acc: 0.66 - ETA: 0s - loss: 0.1040 - acc: 0.6 - ETA: 0s - loss: 0.1040 - acc: 0.6 - ETA: 0s - loss: 0.1040 - acc: 0.\n",
      "Epoch 2/10\n",
      "32843/32843 [==============================] - 37s 1ms/step - loss: 0.1029 - acc: 0.6699 - val_loss: 0.1034 - val_acc: 0.6688A: 32s - loss - ETA: 31s - loss: 0.1038 - acc: 0.66 - - ETA: 29s - loss: 0.1043 - acc: 0.65 - ETA: 29s - loss: 0.1040 - acc: 0. - ETA: 29s - loss: 0.1042 - acc:  - ETA: 29s - loss:  - ETA: 29s  - ETA: 28s - loss: 0.1036 - acc:  - ETA: 28s - loss: 0.1034 - a - ETA: 27s - loss: 0.1034 - acc: 0. - ETA: 27s -  - E - ETA: 25s - loss: 0.1034 - a - ETA: 25s  - ETA: 24s - loss: 0.1038 - a - ETA: 24s - loss: 0.1041 - acc: 0.66 - ETA: 24s - loss: 0.1041 - acc: 0. - ETA: 24s - loss: 0.1042 - acc:  - ETA: 23s - loss:  - - ETA: 22s - loss: 0.1042 - acc: 0.66 - ETA: 22s - loss: 0. - ETA: 21s - loss - ETA: 20s - loss: 0.1036 - acc: 0.66 - ETA: 20s - loss: 0.1036 - acc: 0.66 - ETA: 20s - loss: 0.1036 - acc: 0.66 - ETA: 20s - loss: 0.1036 - acc: 0.66 - ETA: 20s - loss: 0.1036 - acc: 0.66 - ETA: 20s - loss: 0.1037 - - ETA: 19 - ETA: 19s - loss: 0.1035 - acc: 0.66 - ETA: 19 - ETA: 18s - loss: 0. - ETA: 17s - loss: 0.1037 - acc - ETA: 17s - loss: 0.1037 - acc - ETA: 17 - ETA: 16s  - ETA: 12s - loss: 0.1030 - acc - E - ETA: 10s - loss: 0.1030 - ETA: 10s - loss: 0.10 - ETA: 9s - loss: 0.10 - ETA: 8s - loss: 0.1031 - a - ETA: 8s - loss: 0.1031 - acc: 0.669 - ETA: 8s - loss: 0.1031 - acc: - ETA: 7s - loss: 0.1030 - acc: 0.67 - ETA: 7s - loss: 0.1029 - acc: - ETA: 7s - loss: 0.1030 - ac - ETA: 6s - loss: 0.1029 - acc:  - ETA: 6s - loss: 0.1030 - a - ETA: 6s - loss: 0.1030 - acc: 0.670 - ETA: 5s - loss: 0.1030 - acc:  - ETA: 5s - loss: 0 - ETA: 4s - loss: 0.1030 - acc: 0.6 - ETA: 4s - loss: 0.1030 - acc: 0. - ETA: 4s - loss: 0.1031 - acc: 0 - ETA: 4s - loss: 0.1031 - acc: 0.669 - ETA: 4s - loss: 0.1031 - acc: 0.669 - ETA: 3s - loss: 0.1031 - acc: 0.66 - ETA: 3s - loss: 0.1031 - acc: 0.6 - ETA: 3s  - ETA: 2s - los - ETA: 1s - loss: 0.1030 - acc: 0.67 - ETA: 1s - loss: 0.1030 - acc: 0.6 - ETA: 0s - loss: 0.1029 - acc: 0.6 - ETA: 0s - loss: 0.1029 - acc: 0.670 - ETA: 0s - loss: 0.1029 - acc - ETA: 0s - loss: 0.1029 - acc: 0.670 - ETA: 0s - loss: 0.1029 - acc: 0.\n",
      "Epoch 3/10\n",
      "32843/32843 [==============================] - 37s 1ms/step - loss: 0.1028 - acc: 0.6702 - val_loss: 0.1029 - val_acc: 0.6697 0.1019 - acc: 0.68 - ETA: 31s - loss: 0.1020 - acc: 0.67 - ETA: 31s - loss: 0.1021 - acc: 0.67 - ETA: 31s - loss: 0.1025 - acc: 0. - ETA: 31s - loss: 0.1008 - acc: 0. - ETA: 31s - loss: 0.0997 - ETA: 31s - loss: 0.1006 - acc: 0. - ETA: 31s - lo - E - ETA: 29s - lo - ETA: 28s - loss: 0.1012 - a - E - ETA: 26s - loss: 0.1015 - acc: 0.67 - ETA: 26s - loss: 0.1016 - acc: 0.67 - ETA: 26s - loss: 0.1015 - acc: 0.67 - ETA: 26s - loss: 0.1017 - acc: 0.67 - ETA: 26s - loss: 0.1016 - acc:  - ETA: 26s - loss: 0.1016 - - ETA: 25s - loss: 0.1020 - acc: 0. - ETA: 25s - loss - ETA: 24s  - ETA: 23s - loss: 0.1021 - acc: 0. - - ETA: 22s - loss: 0.1022 - acc: 0.67 - ETA: 22s - loss: 0.1023 - acc: 0. - ETA: 22s - loss:  - ETA: 20s -  - ETA: 20s - loss: 0.1025 - acc: 0.67 - ETA: 20s - loss: 0.10 - E - ETA: 18s - loss: 0.1020 - acc: 0.67 - ETA: 18s - loss: 0.1020 - acc: 0.67 - ETA: 18s - loss: 0.1020 - acc: 0. - ETA: 18s - loss: 0. - ETA: 18s - loss: 0.1022 - acc: 0. - ETA: 18s - loss: 0.1022 - acc: 0.67 - ETA: 18s - loss: 0. - ETA: 16s - loss: 0.1020 - acc: 0. - ETA: 16s - loss: 0.1020 - acc: 0. - ETA: 16s - loss: 0.1021 - acc: 0.67 - ETA: 16s - loss: 0.1022 - acc: 0.67 - ETA: 16s - loss: 0.1021 - - ETA: 16s - loss: 0.1020 - acc: 0.67 - ETA - ETA: 15s - loss: 0.1020 - acc: 0.67 - ETA: 15s - loss: 0.1021 - acc: 0.67 - ETA: 15s - loss: 0.1021 - acc:  - ETA: 14s - loss: 0. - ETA: 14s - loss: 0.1021 - acc: 0.67 - ETA: 14s - loss: 0.1021 - acc: 0. - ETA: 14s - loss: 0.1021 - acc: 0.67 - - ETA: 13s - loss: 0.1021 - acc - ETA: 13s - loss - ETA: 12s - loss: 0.1021 - acc: 0.67 - ETA: 12s - loss: 0.1021 - a - ETA: 12s - lo - ETA: 11s - loss - ETA: 9s - loss: 0.1025 - - ETA: 9s - loss: 0.1025 - acc: 0.671 - ETA: 9s - loss: 0.1025 - acc: 0.67 - ETA: 9s - loss: 0.1025 - acc: 0.6 - ETA: 8s - loss: 0.1025  - ETA: 8s - loss: 0.1026 - acc: 0.671 - ETA: 8s - loss: 0.1026 - acc: 0.671 - ETA: 8s - loss: 0.1026 - acc: 0.671 - ETA: 8s - loss: 0.1026 - acc: 0.671 - ET - ETA: 6s - loss: 0.1027 - acc: 0.67 - ETA: 6s -  - ETA: 4s - loss: 0.1028 - acc - ETA: 4s - loss: 0.1028  - ETA: 3s - loss: 0.1028 -  - ETA: 3s - loss: 0.1029 - acc: 0.66 - ETA: 3s - loss: 0.1029 - acc: 0.669 - ETA: 3s - loss: 0.1029 - a - ETA: 2s - loss: 0.1028 - acc: 0 - ETA: 2s - loss: 0.1028 - acc: 0.67 - ETA: 2s - loss: 0.1028 - acc: 0.67 - ETA: 2s - loss: 0.1029 - acc: 0.66 - ETA: 2s - loss: 0 - ETA: 1s - loss: 0.1028 - - ETA: 0s - loss: 0.1029 - acc\n",
      "Epoch 4/10\n",
      "32843/32843 [==============================] - 37s 1ms/step - loss: 0.1026 - acc: 0.6703 - val_loss: 0.1028 - val_acc: 0.6698 32s - loss: 0.1034 - acc: 0. - ETA: 31s - lo - ETA: 31s - loss: 0.1033 - acc: 0. - ETA: 31s - loss: 0.1035 - - ETA: 31s - loss: 0.1032 - acc: 0.66 - ETA: 30s - loss: 0.1033 - acc:  - ETA: 30s - loss: 0.1021 - acc: 0.67 - ETA: 30s - loss: 0.1023 - acc: 0.67 - ETA: 30s - loss: 0.1023 - a - ETA: 29s - loss: 0. - ETA: 28s - loss: 0.1036 - acc: 0.66 - ETA: 28s - loss: 0.1035 - acc: 0.66 - ETA: 28s - loss: 0.1035 - acc:  - ETA: 28s - loss: 0.1034 - acc: 0.66 - ETA: 28s - loss: 0.10 - ETA: 28s - lo - ETA: 27s - loss: 0.1028 - acc: 0.66 - ETA: 27s -  - ETA: 26s - loss: 0.1029 - acc: 0. - ETA: 25s - loss: 0.1026 - acc: 0.67 - ETA: 25s - loss: 0.1026 - acc - ETA: 25s - loss: 0.1023 - - ETA: 24s - loss: 0.1019 - acc: 0.67 - ETA: 24s - loss: 0.1020 - acc: 0.67 - ETA: 24s - loss - ETA: 24s - loss - ETA: 23s - loss: 0.1021 - acc: 0.67 - ETA: 23s - loss: 0.1021 - acc: 0.67 - ETA: 23 - ETA: 22s - loss - ETA: 22s - loss: 0.1026 - acc: 0.66 - ETA: 22s - loss: 0.1025 - - ETA: 21s - loss: 0.1025 - acc: 0.67 - ETA: 21s - loss: 0.1023 - acc - ETA: 21s - loss: 0.1019 - a - ETA: 21s - loss: 0.1019 - acc: 0. - E - ETA: 20s - loss: 0.1017 - acc - ETA: 20s - loss: 0.1017 - acc: 0. - ETA: 19s - lo - ETA: 19s - loss: 0.1015 - acc - ETA: 19s - loss: 0.1016 - acc - ETA - ETA: 18s - loss: 0.1020 - acc - ETA: 17s - loss: 0.1020 - acc: 0.67 - ETA: 17s - loss - ETA: 16s - loss: 0.1020 - acc: 0.67 - ETA: 16s - loss: 0.1020 - ETA: 15s -  - ETA: 14s - loss: 0.1023 - acc: 0.67 - ETA: 14s - loss: 0. - ETA: 13s - loss: 0.1025 - - ETA: 12s - loss: 0.1024 - acc: 0.67 - ETA: 12s - loss: 0.1024 - acc: 0. - ETA: 12s - loss: 0.1024 - ETA: 11s - loss: 0.1024 - acc: 0.67 - ETA: 11s - loss: 0.1024 - acc: 0.67 - ETA: 11s - loss: 0.1024 - acc: 0. - ETA: 10s - loss: 0.1026 - acc: 0.67 - ETA: 10s - los - ETA: 9s - loss: 0.1025 - acc: 0.6 - ETA: 9s - loss: 0.1025 -  - ETA: 9s - loss: 0.102 - ETA: 8s - loss: 0.10 - ETA: 7s - loss: 0.1026  - ETA: 6s - loss: 0.1027 - acc: 0. - ETA: 6s - loss: 0.1026 - acc: 0.670 - ETA: 6s - loss: 0.1026 - acc:  - ETA: 6s - loss: 0.1027 - acc: 0 - ETA: 6s - loss: 0.1027 - acc: 0.669 - ETA: 6s - loss: 0.1027 - acc: 0.669 - ETA: 6s - loss: 0.1028 - acc: 0.669 - ETA: 6s - loss: 0 - ETA: 5s - loss: 0.1026 - acc:  - ETA: 4s - loss: 0.1026 - acc: 0.67 - ETA: 4s - los - ETA: 3s - loss: 0.1027 - acc: 0.66 - ETA: - ETA: - ETA: 0s - loss: 0.1026 - acc: 0.6\n",
      "Epoch 5/10\n",
      "32843/32843 [==============================] - 37s 1ms/step - loss: 0.1025 - acc: 0.6707 - val_loss: 0.1028 - val_acc: 0.66976 - acc: 0.66 - ETA: 31s - loss: 0.1056 - acc: 0.65 - ETA: 31s - loss: 0.1052 - - ETA: 30s - loss: 0.1032 - acc: 0.66 - ETA: 30s - loss - ETA: 29s - loss: 0.1040 - acc: 0.66 - ETA: 29s - loss: 0.1036 - acc: 0.66 - ETA: 29s - loss: 0.1036 - acc: 0.66 - ETA: 29s - loss: 0.1039 - acc: 0.66 - E - ETA: 27s - loss: 0.1036 - acc: 0.66 - ETA:  - ETA: 26s - loss: 0. - ETA: 26s - loss: 0.1036 - acc - ETA: 26s - loss: 0.1033 - acc: 0. - ETA: 26s - loss: 0.1032 - acc: 0.66 - ETA: 25s -  - ETA: 25s - loss: 0.1034 - ETA: 23s - loss - ETA: 23s - loss: 0.1035 - a - ETA: 23s - loss - ETA: 22s - loss: 0.1037 - acc: 0. - ETA: 22s - loss: 0.1037 - acc: 0. - ETA: 22s - loss: 0.1036 - acc:  - ETA: 22s - loss: 0.1036 - acc:  - ETA: 22s - loss: 0.1035 - acc: 0. - ETA: 21s - loss - ETA: 21s - loss: 0.1038 - acc: 0.66 - ETA: 21s - loss: 0.1038 - acc: 0.66 - ETA: 21s - loss: 0.1038 - acc: 0. - ETA: 20s - loss: 0.1036 - acc: 0. - ETA: 20s - loss: 0.1036 - acc: 0. - ETA: 19s - loss - ETA: 19s - loss: 0.1034 - - ETA: 19s - loss: 0.1036 - acc: 0. - ETA: 19s - loss: 0.1036 - acc: 0.66 - ETA: 18s - loss: 0.1036 - acc: 0.66 - ETA: 18s - loss: 0.1036 - acc: 0.66 - ETA: 18s - loss: 0.1037 - acc: 0.66 - ETA: 18s - loss: 0.1036 - acc: 0.66 - ETA: 18s - loss: 0.1036 - acc: 0.66 - ETA: 18s - loss: 0.1036 - acc: 0.66 - ETA: 18s - loss: 0.1036 - a - ETA: 17s - loss: 0.1032 - acc: 0.66 - ETA: 17s - loss:  - E - ETA: 14s - loss: 0.1029 - acc: 0. - ETA: 14s - loss - ETA: 14s - loss: 0.1029 - acc: 0. - ETA: 14s - loss: 0.1029 - acc: 0.66 - ETA: 13s - loss: 0.1030 - acc - ETA: 13s - loss: 0.1030 - acc - ETA: 13s - loss: 0.1029 - a - ETA: 13s - loss: 0.1029 - ETA: 12s - loss: 0.1029 - acc:  - ETA: 12s - loss: 0.1029 - acc - ETA: 12s - loss: 0.1029 - acc: 0. - ETA: 12s - loss: 0.1028 - acc:  - ETA: 12 - ETA: 11s - - ETA: 9s - loss: 0.1029 - acc: 0.6 - ETA: 9s - loss: 0.1030 - acc: 0.667 - ETA: 9s - loss: 0.1030 - acc: 0.66 - ETA: 9s - loss: 0. - ETA: 8s - loss: 0.1030 - acc: 0.66 - ETA: 8s - loss: 0.1030 - - ETA: 7s - loss: 0.1 - ETA: 6s - loss: 0.102 - ETA: 6s - loss: 0.1028 - acc:  - ETA: 5s - loss: 0.1028 - acc:  - ETA: 5s - loss: 0.1027 - ac - ETA: 5s - loss: 0.1027 - ETA: 4s - loss: 0.1027 - ETA: 3s - loss: 0.1027 - ac - ETA: 3s - loss: 0.1027 - ac - ETA: 2s - loss: 0.1027 - acc: 0.6 - ETA: 2s - loss: 0.1027 - acc: 0 - ETA: 2s - loss: 0.1027  - ETA: 1s - loss: 0.1026 - acc: 0 - ETA: 1s - loss: 0.1026 - acc: - ETA: 1s - loss: 0.1025 - - ETA: 0s - loss: 0.1025 - acc: - ETA: 0s - loss: 0.1025 - acc: 0.670\n",
      "Epoch 6/10\n",
      "32843/32843 [==============================] - 37s 1ms/step - loss: 0.1024 - acc: 0.6705 - val_loss: 0.1036 - val_acc: 0.6697A: 31s - lo - ETA: 31s - loss: 0.1020 - acc: 0. - ETA: 31s - loss: 0.1015 - acc - ETA: 30s - loss: 0.1022 - acc: 0.66 - ETA: 30s - loss: 0. - ETA: 30s - loss: 0.1036 - acc: 0.66 - ETA: 30s - loss:  - ETA: 29s - loss - - ETA: 28s - loss: 0.1038 - a - ETA: 28s - loss: 0.1040 - ETA: 28s - loss: 0.1034 - acc:  - ETA: 27s - loss: 0.1032 - ETA: 26s - loss: 0.1030 - acc:  - ETA: 26s - lo - - ETA: 24s - loss: 0.1030 - a - ETA: 24s - loss: 0.1029 - acc: 0. - ETA: 24s - loss: 0.1028 - acc: 0.66 - ETA: 24s - loss: 0.1029 - acc: 0.66 - ETA: 24 - ETA: 23s - loss: 0.1026 - - ETA: 23s - loss - ETA: 22s - loss: 0.1025 - acc: 0.66 - ETA: 22s - loss: 0.1025 - acc: 0.66 - ETA: 22s - loss: 0.1024 - a - ETA: 21s - lo - ETA: 20s - loss: 0.1022 - acc: 0.67 - ETA: 20 - ETA: 19s - loss: 0.1020 - a - ETA: 19s - loss - ETA: 18s - loss: 0.1022 - acc: 0.67 - ETA: 18s - loss: 0.1022 - acc: 0.67 - E - ETA: 16s - loss: 0.1023 - ETA:  - ETA: 13s - loss: 0.1025 - acc: 0.67 - ETA: 13s - loss: 0. - ETA: 12s -  - ETA: 12s - loss: 0.1025 - - ETA: 11s - loss: 0.1026 - acc - ETA - ETA: 10 - ETA: 10s - loss: 0. - ETA: 9s - loss: 0.1022 - acc: 0. - ETA: 9s - loss: 0.1023 - a - ETA: 8s - loss - ETA: 7s - loss: 0.1023 - acc: 0.6 - ETA: 7s - loss: 0.1023 - acc: 0.67 - ETA: 7s - loss: 0.1023 - ETA: 6s - loss: 0. - ETA: 5s - loss: 0.1023 - acc: - ETA: 5s - loss: 0.1023 - acc: 0 - ETA: 5s - loss: 0.1024 - acc: - ETA: 4s - loss: 0.1024 - acc: 0. - ETA: 4s - loss: 0.1024 - acc: 0.670 - ETA: 4s - loss: 0.1023 - acc: 0.670 - ETA: 4s - loss: 0.1023 - acc:  - ETA: 4s - loss: 0.10 - ETA: 3s - loss: 0.1023 - acc: 0. - ETA: 3s - loss: 0.1023 - a - ETA: 2s - loss: 0.1023 - acc: 0.671 - ETA: 2s - loss: 0.1023 - acc: 0.671 - ETA: 2s - loss: 0.1023 - acc: 0.671 - ETA: 2s - loss: 0.1023 - acc: 0.671 - ETA: 2s  - ETA: 1s - loss: 0.1025 - acc: 0.67 - ETA: 0s - loss: 0.1025 - acc: 0.670 - ETA: 0s - loss: 0.1025 - acc - ETA: 0s - loss: 0.1025 - acc: 0 - ETA: 0s - loss: 0.1025 - acc: 0.67 - ETA: 0s - loss: 0.1024 - acc: 0.670 - ETA: 0s - loss: 0.1024 - acc: 0.67\n",
      "Epoch 7/10\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "32843/32843 [==============================] - 37s 1ms/step - loss: 0.1023 - acc: 0.6703 - val_loss: 0.1026 - val_acc: 0.6697A: 32s - loss:  - ETA: 32s -  - ETA: 31s - loss: 0.1045 - acc: 0.65 - ETA: 31s - loss: 0.1048 - acc: 0.65 - ETA: 30s  - ETA: 29s - loss: 0.1036 - a - ETA: 29s - loss: 0.1037 - acc: 0.66 - ETA: 29s  - ETA: 28s - loss: 0.1037 - acc: 0.66 - ETA: 27s - loss: 0.1039 - acc: 0.66 - ETA: 27s - loss: 0.1039 - acc:  - ETA: 27s - loss: 0.1039 - acc - ETA: 27s - lo - ETA: 26s - loss - ETA: 24s -  - ETA: 24s - loss: 0.1028 - acc: 0. - ETA: 24s - loss: 0.1026 - acc: 0. - ETA: 24s - loss: 0.1027 - acc: 0. - ETA: 24s - loss: 0.1024 - a - ETA: 23s - lo - ETA: 23s - loss: 0.1028 - acc: 0.66 - - ETA: 22s - loss: 0.1025 - acc: 0.66 - ETA: 22s - loss: 0.1024 - a - ETA: 21s - loss - ETA: 21s - loss: 0.1020 - - ETA: 21s - loss: 0.1019 - acc: 0.67 - ETA: 19s - loss - ETA: 19s - loss: 0.1016 - ETA: 18s  - ETA: 18s - loss: 0.1021 - acc: 0. - ETA: 18s - loss: 0.1020 - acc:  - ETA: 17 - ETA: 16s - loss: 0.1021 - acc: 0. - ETA: 16s - loss: 0.1021 - acc: 0.67 - E - ETA: 15s - loss: 0.1021 - acc: 0. - E - ETA: 14s - loss: 0.1022 - acc: 0.67 - ETA: 13 - ETA: 13s - loss: 0.1023 - acc - ETA: 12s - loss: 0.1022 - acc: 0.67 - ETA: 12s - loss: 0.1022 - acc: 0. - ETA: 12s - loss: 0.10 - ETA: 12s - loss: 0.1022 - acc: 0.67 - ETA: 12s - loss: 0.1022 - acc: 0. - ETA: 12s - loss: 0.1022 - acc - ETA: 12s - loss: 0.1021 - acc: 0.67 - ETA: 11 - ETA: 11s - lo - ETA: 10s - loss - ETA: 10s - loss: 0.1021  - ETA: 9s - loss: 0.1020 - acc: 0.67 - ETA: 9s - loss: 0.1020 - acc: 0.672 - ETA: 9s - loss: 0.1020 - acc: - ETA: 8s - loss: 0.1019 - acc: 0.6 - ETA: 8s - loss: 0.1020 - acc: 0.672 - ETA: 8s - loss: 0.1020 - acc: 0.672 - ETA: 8s - loss: 0.1019 - acc: 0.672 - ETA: 8s - loss: 0.1020 - acc: 0.672 - ETA: 8s - loss: 0.1020 - acc: 0.672 - ETA: 8s - loss: 0.1020 - acc: 0.672 - ETA: 8s - loss: 0.1020 - - ETA: 7s - loss: 0.1021 - acc: 0.67 - ETA: 7s - loss: 0.1021 - acc: 0.67 - ETA: 7s - loss: 0.1021 - acc - ETA: 7s - loss: 0.1022 - acc - ETA: 6s - loss: 0.1022 - acc: 0.67 - ETA: 6s - los - ETA: 5s - los - ETA: 4s - loss: 0.1024 - acc: 0.67 - ETA: 4s - loss: 0.1024 - acc: 0.670 - ETA: 4s - loss: 0.1024 - acc: 0.67 - ETA: 4s - loss: 0.1024 - acc: 0.670 - ETA: 3s - loss: 0.1024 - acc: 0. - ETA: 3s - loss: 0.1024 - acc - ETA: 3s - loss: 0.1023 - acc: 0. - ETA: 3s - loss: 0.1022 - - ETA: 2s - loss: 0.1023  - ETA: 1s - loss: 0.1023 - acc: 0.670 - ETA: 1s - loss: 0.1023 - acc: 0.67 - ETA: 1s - loss: 0.1022 - acc: 0.670 - ETA: 1s - loss: 0.1022 - acc: 0.670 - ETA: 1s - lo - ETA: 0s - loss: 0.1023 - acc: 0.670 - ETA: 0s - loss: 0.1023 - acc: \n",
      "Epoch 8/10\n",
      "32843/32843 [==============================] - 37s 1ms/step - loss: 0.1024 - acc: 0.6707 - val_loss: 0.1031 - val_acc: 0.6699 - loss: 0.1050 - acc:  - ETA: 31s - loss:  - ETA: 30s -  - ETA: 29s - loss: 0.1034 - acc: 0. - ETA: 29s - loss: 0.1031 - acc: 0.66 - ETA: 29s - loss: 0.1035 - acc - ETA: 29s - lo - ETA: 28s - loss: 0.1023 - acc: 0.67 - ETA: 28s - loss: 0.1024 - acc: 0.66 - ETA: 28s - loss: 0.1025 - acc: 0.66 - ETA: 28s - loss: 0.1025 - acc: 0. - ETA: 28s - loss: 0.1025 - acc:  - ETA: 28s - loss: 0.1027 - acc:  - E - ETA: 27s - loss - ETA: 27s - loss: 0.1028 - acc: 0.67 - ETA: 27s - loss: 0.1029 - acc:  - ETA: 26s - loss: 0.1031 - ETA: 25s - loss: 0.1029 - - ETA: 25s - loss: 0.1028 - acc: 0. - ETA: 25s - loss: 0.1027 - acc:  - ETA: 25s - loss: 0.1028 - acc: 0.66 - ETA: 25s - loss: 0.1028 - acc: 0.66 - ETA: 24s - loss - \n",
      "Epoch 9/10\n",
      "32843/32843 [==============================] - 37s 1ms/step - loss: 0.1132 - acc: 0.6699 - val_loss: 0.1433 - val_acc: 0.6697\n",
      "Epoch 10/10\n",
      "32843/32843 [==============================] - 37s 1ms/step - loss: 0.1431 - acc: 0.6700 - val_loss: 0.1433 - val_acc: 0.6697\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/anaconda3/lib/python3.6/site-packages/ipykernel_launcher.py:18: UserWarning: The `input_dim` and `input_length` arguments in recurrent layers are deprecated. Use `input_shape` instead.\n",
      "/anaconda3/lib/python3.6/site-packages/ipykernel_launcher.py:18: UserWarning: Update your `SimpleRNN` call to the Keras 2 API: `SimpleRNN(16, activation=\"relu\", input_shape=(None, 32))`\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 32843 samples, validate on 14076 samples\n",
      "Epoch 1/10\n",
      "32843/32843 [==============================] - 37s 1ms/step - loss: 0.1040 - acc: 0.6701 - val_loss: 0.1035 - val_acc: 0.6697\n",
      "Epoch 2/10\n",
      "32843/32843 [==============================] - 36s 1ms/step - loss: 0.1031 - acc: 0.6698 - val_loss: 0.1038 - val_acc: 0.6697\n",
      "Epoch 3/10\n",
      "32843/32843 [==============================] - 36s 1ms/step - loss: 0.1027 - acc: 0.6700 - val_loss: 0.1030 - val_acc: 0.6695\n",
      "Epoch 4/10\n",
      "32843/32843 [==============================] - 36s 1ms/step - loss: 0.1026 - acc: 0.6704 - val_loss: 0.1033 - val_acc: 0.6697\n",
      "Epoch 5/10\n",
      "32843/32843 [==============================] - 36s 1ms/step - loss: 0.1024 - acc: 0.6703 - val_loss: 0.1030 - val_acc: 0.6697\n",
      "Epoch 6/10\n",
      "32843/32843 [==============================] - 36s 1ms/step - loss: 0.1023 - acc: 0.6702 - val_loss: 0.1037 - val_acc: 0.6695\n",
      "Epoch 7/10\n",
      "32843/32843 [==============================] - 36s 1ms/step - loss: 0.1023 - acc: 0.6703 - val_loss: 0.1028 - val_acc: 0.6695\n",
      "Epoch 8/10\n",
      "32843/32843 [==============================] - 36s 1ms/step - loss: 0.1022 - acc: 0.6701 - val_loss: 0.1025 - val_acc: 0.6696\n",
      "Epoch 9/10\n",
      "32843/32843 [==============================] - 36s 1ms/step - loss: 0.1021 - acc: 0.6700 - val_loss: 0.1028 - val_acc: 0.6697\n",
      "Epoch 10/10\n",
      "32843/32843 [==============================] - 36s 1ms/step - loss: 0.1022 - acc: 0.6705 - val_loss: 0.1025 - val_acc: 0.6697\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/anaconda3/lib/python3.6/site-packages/ipykernel_launcher.py:18: UserWarning: The `input_dim` and `input_length` arguments in recurrent layers are deprecated. Use `input_shape` instead.\n",
      "/anaconda3/lib/python3.6/site-packages/ipykernel_launcher.py:18: UserWarning: Update your `SimpleRNN` call to the Keras 2 API: `SimpleRNN(64, activation=\"relu\", input_shape=(None, 32))`\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 32843 samples, validate on 14076 samples\n",
      "Epoch 1/10\n",
      "32843/32843 [==============================] - 56s 2ms/step - loss: 0.6334 - acc: 0.6695 - val_loss: 0.6301 - val_acc: 0.6695\n",
      "Epoch 2/10\n",
      "32843/32843 [==============================] - 55s 2ms/step - loss: 0.6285 - acc: 0.6703 - val_loss: 0.6264 - val_acc: 0.6693\n",
      "Epoch 3/10\n",
      "32843/32843 [==============================] - 55s 2ms/step - loss: 0.6273 - acc: 0.6701 - val_loss: 0.6270 - val_acc: 0.6700\n",
      "Epoch 4/10\n",
      "32843/32843 [==============================] - 55s 2ms/step - loss: 0.6267 - acc: 0.6701 - val_loss: 0.6252 - val_acc: 0.6699\n",
      "Epoch 5/10\n",
      "32843/32843 [==============================] - 55s 2ms/step - loss: 0.6271 - acc: 0.6699 - val_loss: 0.6252 - val_acc: 0.6699\n",
      "Epoch 6/10\n",
      "32843/32843 [==============================] - 55s 2ms/step - loss: 0.6268 - acc: 0.6700 - val_loss: 0.6248 - val_acc: 0.6704\n",
      "Epoch 7/10\n",
      "32843/32843 [==============================] - 55s 2ms/step - loss: 0.6265 - acc: 0.6700 - val_loss: 0.6265 - val_acc: 0.6697\n",
      "Epoch 8/10\n",
      "32843/32843 [==============================] - 55s 2ms/step - loss: 0.6266 - acc: 0.6704 - val_loss: 0.6297 - val_acc: 0.6696\n",
      "Epoch 9/10\n",
      "32843/32843 [==============================] - 55s 2ms/step - loss: 0.6256 - acc: 0.6698 - val_loss: 0.6287 - val_acc: 0.6697\n",
      "Epoch 10/10\n",
      "32843/32843 [==============================] - 55s 2ms/step - loss: 0.6271 - acc: 0.6699 - val_loss: 0.6262 - val_acc: 0.6702\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/anaconda3/lib/python3.6/site-packages/ipykernel_launcher.py:18: UserWarning: The `input_dim` and `input_length` arguments in recurrent layers are deprecated. Use `input_shape` instead.\n",
      "/anaconda3/lib/python3.6/site-packages/ipykernel_launcher.py:18: UserWarning: Update your `SimpleRNN` call to the Keras 2 API: `SimpleRNN(128, activation=\"relu\", input_shape=(None, 32))`\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 32843 samples, validate on 14076 samples\n",
      "Epoch 1/10\n",
      "32843/32843 [==============================] - 38s 1ms/step - loss: 0.1045 - acc: 0.6696 - val_loss: 0.1036 - val_acc: 0.6697\n",
      "Epoch 2/10\n",
      "32843/32843 [==============================] - 38s 1ms/step - loss: 0.1034 - acc: 0.6702 - val_loss: 0.1031 - val_acc: 0.6697\n",
      "Epoch 3/10\n",
      "32843/32843 [==============================] - 38s 1ms/step - loss: 0.1032 - acc: 0.6702 - val_loss: 0.1031 - val_acc: 0.6694\n",
      "Epoch 4/10\n",
      "32843/32843 [==============================] - 38s 1ms/step - loss: 0.1031 - acc: 0.6702 - val_loss: 0.1030 - val_acc: 0.6696\n",
      "Epoch 5/10\n",
      "32843/32843 [==============================] - 38s 1ms/step - loss: 0.1029 - acc: 0.6703 - val_loss: 0.1037 - val_acc: 0.6697\n",
      "Epoch 6/10\n",
      "32843/32843 [==============================] - 38s 1ms/step - loss: 0.1028 - acc: 0.6698 - val_loss: 0.1029 - val_acc: 0.6697\n",
      "Epoch 7/10\n",
      "32843/32843 [==============================] - 38s 1ms/step - loss: 0.1028 - acc: 0.6696 - val_loss: 0.1035 - val_acc: 0.6698\n",
      "Epoch 8/10\n",
      "32843/32843 [==============================] - 38s 1ms/step - loss: 0.1029 - acc: 0.6699 - val_loss: 0.1037 - val_acc: 0.6699\n",
      "Epoch 9/10\n",
      "32843/32843 [==============================] - 38s 1ms/step - loss: 0.1027 - acc: 0.6702 - val_loss: 0.1034 - val_acc: 0.6699\n",
      "Epoch 10/10\n",
      "32843/32843 [==============================] - 38s 1ms/step - loss: 0.1026 - acc: 0.6708 - val_loss: 0.1027 - val_acc: 0.6696\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/anaconda3/lib/python3.6/site-packages/ipykernel_launcher.py:18: UserWarning: The `input_dim` and `input_length` arguments in recurrent layers are deprecated. Use `input_shape` instead.\n",
      "/anaconda3/lib/python3.6/site-packages/ipykernel_launcher.py:18: UserWarning: Update your `SimpleRNN` call to the Keras 2 API: `SimpleRNN(32, activation=\"relu\", input_shape=(None, 32))`\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 32843 samples, validate on 14076 samples\n",
      "Epoch 1/10\n",
      "32843/32843 [==============================] - 28s 843us/step - loss: 0.1043 - acc: 0.6699 - val_loss: 0.1036 - val_acc: 0.6697\n",
      "Epoch 2/10\n",
      "32843/32843 [==============================] - 27s 833us/step - loss: 0.1036 - acc: 0.6698 - val_loss: 0.1035 - val_acc: 0.6698\n",
      "Epoch 3/10\n",
      "32843/32843 [==============================] - 27s 837us/step - loss: 0.1034 - acc: 0.6701 - val_loss: 0.1031 - val_acc: 0.6697\n",
      "Epoch 4/10\n",
      "32843/32843 [==============================] - 27s 834us/step - loss: 0.1031 - acc: 0.6705 - val_loss: 0.1031 - val_acc: 0.6697\n",
      "Epoch 5/10\n",
      "32843/32843 [==============================] - 27s 829us/step - loss: 0.1029 - acc: 0.6703 - val_loss: 0.1030 - val_acc: 0.6697\n",
      "Epoch 6/10\n",
      "32843/32843 [==============================] - 28s 837us/step - loss: 0.1031 - acc: 0.6701 - val_loss: 0.1028 - val_acc: 0.6698\n",
      "Epoch 7/10\n",
      "32843/32843 [==============================] - 27s 836us/step - loss: 0.1030 - acc: 0.6697 - val_loss: 0.1034 - val_acc: 0.6699\n",
      "Epoch 8/10\n",
      "32843/32843 [==============================] - 27s 831us/step - loss: 0.1029 - acc: 0.6709 - val_loss: 0.1030 - val_acc: 0.6704\n",
      "Epoch 9/10\n",
      "32843/32843 [==============================] - 27s 831us/step - loss: 0.1028 - acc: 0.6697 - val_loss: 0.1039 - val_acc: 0.6697\n",
      "Epoch 10/10\n",
      "32843/32843 [==============================] - 28s 840us/step - loss: 0.1028 - acc: 0.6704 - val_loss: 0.1029 - val_acc: 0.6704\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/anaconda3/lib/python3.6/site-packages/ipykernel_launcher.py:18: UserWarning: The `input_dim` and `input_length` arguments in recurrent layers are deprecated. Use `input_shape` instead.\n",
      "/anaconda3/lib/python3.6/site-packages/ipykernel_launcher.py:18: UserWarning: Update your `SimpleRNN` call to the Keras 2 API: `SimpleRNN(4, activation=\"relu\", input_shape=(None, 32))`\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 32843 samples, validate on 14076 samples\n",
      "Epoch 1/10\n",
      "32843/32843 [==============================] - 54s 2ms/step - loss: 0.1047 - acc: 0.6700 - val_loss: 0.1050 - val_acc: 0.6697\n",
      "Epoch 2/10\n",
      "32843/32843 [==============================] - 54s 2ms/step - loss: 0.1038 - acc: 0.6700 - val_loss: 0.1034 - val_acc: 0.6697\n",
      "Epoch 3/10\n",
      "32843/32843 [==============================] - 54s 2ms/step - loss: 0.1037 - acc: 0.6700 - val_loss: 0.1037 - val_acc: 0.6697\n",
      "Epoch 4/10\n",
      "32843/32843 [==============================] - 55s 2ms/step - loss: 0.1036 - acc: 0.6700 - val_loss: 0.1032 - val_acc: 0.6697\n",
      "Epoch 5/10\n",
      "32843/32843 [==============================] - 54s 2ms/step - loss: 0.1036 - acc: 0.6700 - val_loss: 0.1032 - val_acc: 0.6697\n",
      "Epoch 6/10\n",
      "32843/32843 [==============================] - 54s 2ms/step - loss: 0.1035 - acc: 0.6700 - val_loss: 0.1031 - val_acc: 0.6697\n",
      "Epoch 7/10\n",
      "32843/32843 [==============================] - 54s 2ms/step - loss: 0.1036 - acc: 0.6700 - val_loss: 0.1040 - val_acc: 0.6697\n",
      "Epoch 8/10\n",
      "32843/32843 [==============================] - 54s 2ms/step - loss: 0.1034 - acc: 0.6700 - val_loss: 0.1034 - val_acc: 0.6697\n",
      "Epoch 9/10\n",
      "32843/32843 [==============================] - 54s 2ms/step - loss: 0.1035 - acc: 0.6700 - val_loss: 0.1031 - val_acc: 0.6697\n",
      "Epoch 10/10\n",
      "32843/32843 [==============================] - 54s 2ms/step - loss: 0.1034 - acc: 0.6700 - val_loss: 0.1035 - val_acc: 0.6697\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/anaconda3/lib/python3.6/site-packages/ipykernel_launcher.py:18: UserWarning: The `input_dim` and `input_length` arguments in recurrent layers are deprecated. Use `input_shape` instead.\n",
      "/anaconda3/lib/python3.6/site-packages/ipykernel_launcher.py:18: UserWarning: Update your `SimpleRNN` call to the Keras 2 API: `SimpleRNN(16, activation=\"relu\", input_shape=(None, 32))`\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 32843 samples, validate on 14076 samples\n",
      "Epoch 1/10\n",
      "32843/32843 [==============================] - 55s 2ms/step - loss: 0.1040 - acc: 0.6695 - val_loss: 0.1035 - val_acc: 0.6697\n",
      "Epoch 2/10\n",
      "32843/32843 [==============================] - 54s 2ms/step - loss: 0.1031 - acc: 0.6700 - val_loss: 0.1032 - val_acc: 0.6697\n",
      "Epoch 3/10\n",
      "32843/32843 [==============================] - 55s 2ms/step - loss: 0.1028 - acc: 0.6700 - val_loss: 0.1032 - val_acc: 0.6697\n",
      "Epoch 4/10\n",
      "32843/32843 [==============================] - 54s 2ms/step - loss: 0.1026 - acc: 0.6700 - val_loss: 0.1029 - val_acc: 0.6697\n",
      "Epoch 5/10\n",
      "32843/32843 [==============================] - 55s 2ms/step - loss: 0.1025 - acc: 0.6700 - val_loss: 0.1028 - val_acc: 0.6697\n",
      "Epoch 6/10\n",
      "32843/32843 [==============================] - 54s 2ms/step - loss: 0.1024 - acc: 0.6700 - val_loss: 0.1031 - val_acc: 0.6697\n",
      "Epoch 7/10\n",
      "32843/32843 [==============================] - 54s 2ms/step - loss: 0.1023 - acc: 0.6700 - val_loss: 0.1028 - val_acc: 0.6697\n",
      "Epoch 8/10\n",
      "32843/32843 [==============================] - 55s 2ms/step - loss: 0.1022 - acc: 0.6700 - val_loss: 0.1045 - val_acc: 0.6697\n",
      "Epoch 9/10\n",
      "32843/32843 [==============================] - 54s 2ms/step - loss: 0.1023 - acc: 0.6699 - val_loss: 0.1028 - val_acc: 0.6697\n",
      "Epoch 10/10\n",
      "32843/32843 [==============================] - 55s 2ms/step - loss: 0.1022 - acc: 0.6699 - val_loss: 0.1028 - val_acc: 0.6697\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/anaconda3/lib/python3.6/site-packages/ipykernel_launcher.py:18: UserWarning: The `input_dim` and `input_length` arguments in recurrent layers are deprecated. Use `input_shape` instead.\n",
      "/anaconda3/lib/python3.6/site-packages/ipykernel_launcher.py:18: UserWarning: Update your `SimpleRNN` call to the Keras 2 API: `SimpleRNN(128, activation=\"relu\", input_shape=(None, 32))`\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 32843 samples, validate on 14076 samples\n",
      "Epoch 1/10\n",
      "32843/32843 [==============================] - 57s 2ms/step - loss: 0.6331 - acc: 0.6692 - val_loss: 0.6308 - val_acc: 0.6697\n",
      "Epoch 2/10\n",
      "32843/32843 [==============================] - 57s 2ms/step - loss: 0.6280 - acc: 0.6701 - val_loss: 0.6276 - val_acc: 0.6697\n",
      "Epoch 3/10\n",
      "32843/32843 [==============================] - 56s 2ms/step - loss: 0.6278 - acc: 0.6702 - val_loss: 0.6302 - val_acc: 0.6700\n",
      "Epoch 4/10\n",
      "32843/32843 [==============================] - 57s 2ms/step - loss: 0.6264 - acc: 0.6703 - val_loss: 0.6266 - val_acc: 0.6699\n",
      "Epoch 5/10\n",
      "32843/32843 [==============================] - 57s 2ms/step - loss: 0.6261 - acc: 0.6702 - val_loss: 0.6266 - val_acc: 0.6698\n",
      "Epoch 6/10\n",
      "32843/32843 [==============================] - 56s 2ms/step - loss: 0.6259 - acc: 0.6702 - val_loss: 0.6254 - val_acc: 0.6699\n",
      "Epoch 7/10\n",
      "32843/32843 [==============================] - 57s 2ms/step - loss: 0.6252 - acc: 0.6698 - val_loss: 0.6246 - val_acc: 0.6703\n",
      "Epoch 8/10\n",
      "32843/32843 [==============================] - 56s 2ms/step - loss: 0.6251 - acc: 0.6699 - val_loss: 0.6262 - val_acc: 0.6699\n",
      "Epoch 9/10\n",
      "32843/32843 [==============================] - 57s 2ms/step - loss: 0.6243 - acc: 0.6694 - val_loss: 0.6267 - val_acc: 0.6699\n",
      "Epoch 10/10\n",
      "32843/32843 [==============================] - 56s 2ms/step - loss: 0.6241 - acc: 0.6699 - val_loss: 0.6266 - val_acc: 0.6696\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/anaconda3/lib/python3.6/site-packages/ipykernel_launcher.py:18: UserWarning: The `input_dim` and `input_length` arguments in recurrent layers are deprecated. Use `input_shape` instead.\n",
      "/anaconda3/lib/python3.6/site-packages/ipykernel_launcher.py:18: UserWarning: Update your `SimpleRNN` call to the Keras 2 API: `SimpleRNN(4, activation=\"relu\", input_shape=(None, 32))`\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 32843 samples, validate on 14076 samples\n",
      "Epoch 1/10\n",
      "32843/32843 [==============================] - 36s 1ms/step - loss: 0.1050 - acc: 0.6699 - val_loss: 0.1048 - val_acc: 0.6697\n",
      "Epoch 2/10\n",
      "32843/32843 [==============================] - 36s 1ms/step - loss: 0.1047 - acc: 0.6700 - val_loss: 0.1049 - val_acc: 0.6697\n",
      "Epoch 3/10\n",
      "32843/32843 [==============================] - 36s 1ms/step - loss: 0.1047 - acc: 0.6700 - val_loss: 0.1048 - val_acc: 0.6697\n",
      "Epoch 4/10\n",
      "32843/32843 [==============================] - 36s 1ms/step - loss: 0.1047 - acc: 0.6700 - val_loss: 0.1048 - val_acc: 0.6697\n",
      "Epoch 5/10\n",
      "32843/32843 [==============================] - 36s 1ms/step - loss: 0.1047 - acc: 0.6700 - val_loss: 0.1048 - val_acc: 0.6697\n",
      "Epoch 6/10\n",
      "32843/32843 [==============================] - 36s 1ms/step - loss: 0.1047 - acc: 0.6700 - val_loss: 0.1048 - val_acc: 0.6697\n",
      "Epoch 7/10\n",
      "32843/32843 [==============================] - 36s 1ms/step - loss: 0.1048 - acc: 0.6700 - val_loss: 0.1049 - val_acc: 0.6697\n",
      "Epoch 8/10\n",
      "32843/32843 [==============================] - 36s 1ms/step - loss: 0.1047 - acc: 0.6700 - val_loss: 0.1048 - val_acc: 0.6697\n",
      "Epoch 9/10\n",
      "32843/32843 [==============================] - 36s 1ms/step - loss: 0.1047 - acc: 0.6700 - val_loss: 0.1048 - val_acc: 0.6697\n",
      "Epoch 10/10\n",
      "32843/32843 [==============================] - 36s 1ms/step - loss: 0.1047 - acc: 0.6700 - val_loss: 0.1048 - val_acc: 0.6697\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/anaconda3/lib/python3.6/site-packages/ipykernel_launcher.py:18: UserWarning: The `input_dim` and `input_length` arguments in recurrent layers are deprecated. Use `input_shape` instead.\n",
      "/anaconda3/lib/python3.6/site-packages/ipykernel_launcher.py:18: UserWarning: Update your `SimpleRNN` call to the Keras 2 API: `SimpleRNN(4, activation=\"relu\", input_shape=(None, 32))`\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 32843 samples, validate on 14076 samples\n",
      "Epoch 1/10\n",
      "32843/32843 [==============================] - 53s 2ms/step - loss: 0.6346 - acc: 0.6690 - val_loss: 0.6311 - val_acc: 0.6698\n",
      "Epoch 2/10\n",
      "32843/32843 [==============================] - 53s 2ms/step - loss: 0.6294 - acc: 0.6704 - val_loss: 0.6303 - val_acc: 0.6699\n",
      "Epoch 3/10\n",
      "32843/32843 [==============================] - 53s 2ms/step - loss: 0.6284 - acc: 0.6695 - val_loss: 0.6272 - val_acc: 0.6698\n",
      "Epoch 4/10\n",
      "32843/32843 [==============================] - 53s 2ms/step - loss: 0.6277 - acc: 0.6692 - val_loss: 0.6260 - val_acc: 0.6698\n",
      "Epoch 5/10\n",
      "32843/32843 [==============================] - 53s 2ms/step - loss: 0.6269 - acc: 0.6709 - val_loss: 0.6292 - val_acc: 0.6697\n",
      "Epoch 6/10\n",
      "32843/32843 [==============================] - 53s 2ms/step - loss: 0.6270 - acc: 0.6700 - val_loss: 0.6256 - val_acc: 0.6701\n",
      "Epoch 7/10\n",
      "32843/32843 [==============================] - 53s 2ms/step - loss: 0.6267 - acc: 0.6699 - val_loss: 0.6249 - val_acc: 0.6699\n",
      "Epoch 8/10\n",
      "32843/32843 [==============================] - 53s 2ms/step - loss: 0.6266 - acc: 0.6706 - val_loss: 0.6250 - val_acc: 0.6699\n",
      "Epoch 9/10\n",
      "32843/32843 [==============================] - 53s 2ms/step - loss: 0.6261 - acc: 0.6698 - val_loss: 0.6274 - val_acc: 0.6700\n",
      "Epoch 10/10\n",
      "32843/32843 [==============================] - 53s 2ms/step - loss: 0.6256 - acc: 0.6702 - val_loss: 0.6322 - val_acc: 0.6704\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/anaconda3/lib/python3.6/site-packages/ipykernel_launcher.py:18: UserWarning: The `input_dim` and `input_length` arguments in recurrent layers are deprecated. Use `input_shape` instead.\n",
      "/anaconda3/lib/python3.6/site-packages/ipykernel_launcher.py:18: UserWarning: Update your `SimpleRNN` call to the Keras 2 API: `SimpleRNN(8, activation=\"relu\", input_shape=(None, 32))`\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 32843 samples, validate on 14076 samples\n",
      "Epoch 1/10\n",
      "32843/32843 [==============================] - 36s 1ms/step - loss: 0.1041 - acc: 0.6704 - val_loss: 0.1036 - val_acc: 0.6696\n",
      "Epoch 2/10\n",
      "32843/32843 [==============================] - 35s 1ms/step - loss: 0.1030 - acc: 0.6695 - val_loss: 0.1040 - val_acc: 0.6697\n",
      "Epoch 3/10\n",
      "32843/32843 [==============================] - 35s 1ms/step - loss: 0.1028 - acc: 0.6702 - val_loss: 0.1034 - val_acc: 0.6701\n",
      "Epoch 4/10\n",
      "32843/32843 [==============================] - 35s 1ms/step - loss: 0.1025 - acc: 0.6699 - val_loss: 0.1033 - val_acc: 0.6657\n",
      "Epoch 5/10\n",
      "32843/32843 [==============================] - 36s 1ms/step - loss: 0.1025 - acc: 0.6699 - val_loss: 0.1030 - val_acc: 0.6693\n",
      "Epoch 6/10\n",
      "32843/32843 [==============================] - 36s 1ms/step - loss: 0.1023 - acc: 0.6700 - val_loss: 0.1028 - val_acc: 0.6695\n",
      "Epoch 7/10\n",
      "32843/32843 [==============================] - 35s 1ms/step - loss: 0.1022 - acc: 0.6701 - val_loss: 0.1028 - val_acc: 0.6701\n",
      "Epoch 8/10\n",
      "32843/32843 [==============================] - 36s 1ms/step - loss: 0.1022 - acc: 0.6710 - val_loss: 0.1029 - val_acc: 0.6682\n",
      "Epoch 9/10\n",
      "32843/32843 [==============================] - 35s 1ms/step - loss: 0.1021 - acc: 0.6705 - val_loss: 0.1030 - val_acc: 0.6697\n",
      "Epoch 10/10\n",
      "32843/32843 [==============================] - 35s 1ms/step - loss: 0.1021 - acc: 0.6702 - val_loss: 0.1026 - val_acc: 0.6697\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/anaconda3/lib/python3.6/site-packages/ipykernel_launcher.py:18: UserWarning: The `input_dim` and `input_length` arguments in recurrent layers are deprecated. Use `input_shape` instead.\n",
      "/anaconda3/lib/python3.6/site-packages/ipykernel_launcher.py:18: UserWarning: Update your `SimpleRNN` call to the Keras 2 API: `SimpleRNN(64, activation=\"relu\", input_shape=(None, 32))`\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 32843 samples, validate on 14076 samples\n",
      "Epoch 1/10\n",
      "32843/32843 [==============================] - 55s 2ms/step - loss: 0.6316 - acc: 0.6700 - val_loss: 0.6300 - val_acc: 0.6696\n",
      "Epoch 2/10\n",
      "32843/32843 [==============================] - 55s 2ms/step - loss: 0.6268 - acc: 0.6700 - val_loss: 0.6269 - val_acc: 0.6693\n",
      "Epoch 3/10\n",
      "32843/32843 [==============================] - 55s 2ms/step - loss: 0.6254 - acc: 0.6700 - val_loss: 0.6262 - val_acc: 0.6697\n",
      "Epoch 4/10\n",
      "32843/32843 [==============================] - 55s 2ms/step - loss: 0.6250 - acc: 0.6705 - val_loss: 0.6260 - val_acc: 0.6695\n",
      "Epoch 5/10\n",
      "32843/32843 [==============================] - 55s 2ms/step - loss: 0.6251 - acc: 0.6704 - val_loss: 0.6244 - val_acc: 0.6694\n",
      "Epoch 6/10\n",
      "32843/32843 [==============================] - 55s 2ms/step - loss: 0.6245 - acc: 0.6700 - val_loss: 0.6270 - val_acc: 0.6696\n",
      "Epoch 7/10\n",
      "32843/32843 [==============================] - 55s 2ms/step - loss: 0.6243 - acc: 0.6703 - val_loss: 0.6253 - val_acc: 0.6697\n",
      "Epoch 8/10\n",
      "32843/32843 [==============================] - 55s 2ms/step - loss: 0.6240 - acc: 0.6705 - val_loss: 0.6253 - val_acc: 0.6691\n",
      "Epoch 9/10\n",
      "32843/32843 [==============================] - 55s 2ms/step - loss: 0.6235 - acc: 0.6702 - val_loss: 0.6264 - val_acc: 0.6697\n",
      "Epoch 10/10\n",
      "32843/32843 [==============================] - 55s 2ms/step - loss: 0.6233 - acc: 0.6705 - val_loss: 0.6239 - val_acc: 0.6698\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/anaconda3/lib/python3.6/site-packages/ipykernel_launcher.py:18: UserWarning: The `input_dim` and `input_length` arguments in recurrent layers are deprecated. Use `input_shape` instead.\n",
      "/anaconda3/lib/python3.6/site-packages/ipykernel_launcher.py:18: UserWarning: Update your `SimpleRNN` call to the Keras 2 API: `SimpleRNN(16, activation=\"relu\", input_shape=(None, 32))`\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 32843 samples, validate on 14076 samples\n",
      "Epoch 1/10\n",
      "32843/32843 [==============================] - 55s 2ms/step - loss: 0.6327 - acc: 0.6694 - val_loss: 0.6298 - val_acc: 0.6697\n",
      "Epoch 2/10\n",
      "32843/32843 [==============================] - 54s 2ms/step - loss: 0.6288 - acc: 0.6700 - val_loss: 0.6301 - val_acc: 0.6697\n",
      "Epoch 3/10\n",
      "32843/32843 [==============================] - 54s 2ms/step - loss: 0.6275 - acc: 0.6700 - val_loss: 0.6272 - val_acc: 0.6697\n",
      "Epoch 4/10\n",
      "32843/32843 [==============================] - 55s 2ms/step - loss: 0.6270 - acc: 0.6700 - val_loss: 0.6257 - val_acc: 0.6697\n",
      "Epoch 5/10\n",
      "32843/32843 [==============================] - 54s 2ms/step - loss: 0.6272 - acc: 0.6700 - val_loss: 0.6255 - val_acc: 0.6697\n",
      "Epoch 6/10\n",
      "32843/32843 [==============================] - 55s 2ms/step - loss: 0.6260 - acc: 0.6700 - val_loss: 0.6261 - val_acc: 0.6697\n",
      "Epoch 7/10\n",
      "32843/32843 [==============================] - 54s 2ms/step - loss: 0.6257 - acc: 0.6700 - val_loss: 0.6289 - val_acc: 0.6697\n",
      "Epoch 8/10\n",
      "32843/32843 [==============================] - 55s 2ms/step - loss: 0.6261 - acc: 0.6700 - val_loss: 0.6249 - val_acc: 0.6697\n",
      "Epoch 9/10\n",
      "32843/32843 [==============================] - 54s 2ms/step - loss: 0.6255 - acc: 0.6700 - val_loss: 0.6275 - val_acc: 0.6697\n",
      "Epoch 10/10\n",
      "32843/32843 [==============================] - 55s 2ms/step - loss: 0.6255 - acc: 0.6700 - val_loss: 0.6271 - val_acc: 0.6697\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/anaconda3/lib/python3.6/site-packages/ipykernel_launcher.py:18: UserWarning: The `input_dim` and `input_length` arguments in recurrent layers are deprecated. Use `input_shape` instead.\n",
      "/anaconda3/lib/python3.6/site-packages/ipykernel_launcher.py:18: UserWarning: Update your `SimpleRNN` call to the Keras 2 API: `SimpleRNN(16, activation=\"relu\", input_shape=(None, 32))`\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 32843 samples, validate on 14076 samples\n",
      "Epoch 1/10\n",
      "32843/32843 [==============================] - 53s 2ms/step - loss: 0.6355 - acc: 0.6690 - val_loss: 0.6316 - val_acc: 0.6697\n",
      "Epoch 2/10\n",
      "32843/32843 [==============================] - 57s 2ms/step - loss: 0.6300 - acc: 0.6700 - val_loss: 0.6290 - val_acc: 0.6697\n",
      "Epoch 3/10\n",
      "32843/32843 [==============================] - 54s 2ms/step - loss: 0.6288 - acc: 0.6700 - val_loss: 0.6299 - val_acc: 0.6697\n",
      "Epoch 4/10\n",
      "32843/32843 [==============================] - 63s 2ms/step - loss: 0.6278 - acc: 0.6700 - val_loss: 0.6269 - val_acc: 0.6697\n",
      "Epoch 5/10\n",
      "32843/32843 [==============================] - 53s 2ms/step - loss: 0.6277 - acc: 0.6700 - val_loss: 0.6269 - val_acc: 0.6697\n",
      "Epoch 6/10\n",
      "32843/32843 [==============================] - 53s 2ms/step - loss: 0.6278 - acc: 0.6700 - val_loss: 0.6309 - val_acc: 0.6697\n",
      "Epoch 7/10\n",
      "32843/32843 [==============================] - 53s 2ms/step - loss: 0.6272 - acc: 0.6700 - val_loss: 0.6270 - val_acc: 0.6697\n",
      "Epoch 8/10\n",
      "32843/32843 [==============================] - 55s 2ms/step - loss: 0.6273 - acc: 0.6700 - val_loss: 0.6279 - val_acc: 0.6697\n",
      "Epoch 9/10\n",
      "32843/32843 [==============================] - 56s 2ms/step - loss: 0.6266 - acc: 0.6700 - val_loss: 0.6267 - val_acc: 0.6697\n",
      "Epoch 10/10\n",
      "32843/32843 [==============================] - 53s 2ms/step - loss: 0.6265 - acc: 0.6700 - val_loss: 0.6345 - val_acc: 0.6697\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/anaconda3/lib/python3.6/site-packages/ipykernel_launcher.py:18: UserWarning: The `input_dim` and `input_length` arguments in recurrent layers are deprecated. Use `input_shape` instead.\n",
      "/anaconda3/lib/python3.6/site-packages/ipykernel_launcher.py:18: UserWarning: Update your `SimpleRNN` call to the Keras 2 API: `SimpleRNN(32, activation=\"relu\", input_shape=(None, 32))`\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 32843 samples, validate on 14076 samples\n",
      "Epoch 1/10\n",
      "32843/32843 [==============================] - 54s 2ms/step - loss: 0.1041 - acc: 0.6693 - val_loss: 0.1035 - val_acc: 0.6697\n",
      "Epoch 2/10\n",
      "32843/32843 [==============================] - 54s 2ms/step - loss: 0.1033 - acc: 0.6702 - val_loss: 0.1042 - val_acc: 0.6697\n",
      "Epoch 3/10\n",
      "32843/32843 [==============================] - 54s 2ms/step - loss: 0.1030 - acc: 0.6699 - val_loss: 0.1030 - val_acc: 0.6698\n",
      "Epoch 4/10\n",
      "32843/32843 [==============================] - 56s 2ms/step - loss: 0.1028 - acc: 0.6700 - val_loss: 0.1031 - val_acc: 0.6699\n",
      "Epoch 5/10\n",
      "32843/32843 [==============================] - 55s 2ms/step - loss: 0.1027 - acc: 0.6702 - val_loss: 0.1029 - val_acc: 0.6700\n",
      "Epoch 6/10\n",
      "32843/32843 [==============================] - 54s 2ms/step - loss: 0.1027 - acc: 0.6702 - val_loss: 0.1030 - val_acc: 0.6697\n",
      "Epoch 7/10\n",
      "32843/32843 [==============================] - 54s 2ms/step - loss: 0.1025 - acc: 0.6698 - val_loss: 0.1029 - val_acc: 0.6702\n",
      "Epoch 8/10\n",
      "32843/32843 [==============================] - 54s 2ms/step - loss: 0.1026 - acc: 0.6695 - val_loss: 0.1027 - val_acc: 0.6702\n",
      "Epoch 9/10\n",
      "32843/32843 [==============================] - 54s 2ms/step - loss: 0.1025 - acc: 0.6704 - val_loss: 0.1029 - val_acc: 0.6699\n",
      "Epoch 10/10\n",
      "32843/32843 [==============================] - 54s 2ms/step - loss: 0.1025 - acc: 0.6707 - val_loss: 0.1036 - val_acc: 0.6697\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/anaconda3/lib/python3.6/site-packages/ipykernel_launcher.py:18: UserWarning: The `input_dim` and `input_length` arguments in recurrent layers are deprecated. Use `input_shape` instead.\n",
      "/anaconda3/lib/python3.6/site-packages/ipykernel_launcher.py:18: UserWarning: Update your `SimpleRNN` call to the Keras 2 API: `SimpleRNN(32, activation=\"relu\", input_shape=(None, 32))`\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 32843 samples, validate on 14076 samples\n",
      "Epoch 1/10\n",
      "32843/32843 [==============================] - 53s 2ms/step - loss: 0.6339 - acc: 0.6685 - val_loss: 0.6323 - val_acc: 0.6697\n",
      "Epoch 2/10\n",
      "32843/32843 [==============================] - 53s 2ms/step - loss: 0.6274 - acc: 0.6702 - val_loss: 0.6268 - val_acc: 0.6697\n",
      "Epoch 3/10\n",
      "32843/32843 [==============================] - 53s 2ms/step - loss: 0.6264 - acc: 0.6695 - val_loss: 0.6265 - val_acc: 0.6698\n",
      "Epoch 4/10\n",
      "32843/32843 [==============================] - 53s 2ms/step - loss: 0.6257 - acc: 0.6705 - val_loss: 0.6294 - val_acc: 0.6697\n",
      "Epoch 5/10\n",
      "32843/32843 [==============================] - 53s 2ms/step - loss: 0.6244 - acc: 0.6704 - val_loss: 0.6276 - val_acc: 0.6696\n",
      "Epoch 6/10\n",
      "32843/32843 [==============================] - 53s 2ms/step - loss: 0.6242 - acc: 0.6700 - val_loss: 0.6245 - val_acc: 0.6695\n",
      "Epoch 7/10\n",
      "32843/32843 [==============================] - 54s 2ms/step - loss: 0.6238 - acc: 0.6708 - val_loss: 0.6258 - val_acc: 0.6697\n",
      "Epoch 8/10\n",
      "32843/32843 [==============================] - 53s 2ms/step - loss: 0.6248 - acc: 0.6702 - val_loss: 0.6252 - val_acc: 0.6701\n",
      "Epoch 9/10\n",
      "32843/32843 [==============================] - 54s 2ms/step - loss: 0.6239 - acc: 0.6705 - val_loss: 0.6267 - val_acc: 0.6699\n",
      "Epoch 10/10\n",
      "32843/32843 [==============================] - 53s 2ms/step - loss: 0.6231 - acc: 0.6702 - val_loss: 0.6259 - val_acc: 0.6698\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/anaconda3/lib/python3.6/site-packages/ipykernel_launcher.py:18: UserWarning: The `input_dim` and `input_length` arguments in recurrent layers are deprecated. Use `input_shape` instead.\n",
      "/anaconda3/lib/python3.6/site-packages/ipykernel_launcher.py:18: UserWarning: Update your `SimpleRNN` call to the Keras 2 API: `SimpleRNN(128, activation=\"relu\", input_shape=(None, 32))`\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 32843 samples, validate on 14076 samples\n",
      "Epoch 1/10\n",
      "32843/32843 [==============================] - 37s 1ms/step - loss: 0.6308 - acc: 0.6706 - val_loss: 0.6345 - val_acc: 0.6693\n",
      "Epoch 2/10\n",
      "32843/32843 [==============================] - 37s 1ms/step - loss: 0.6266 - acc: 0.6698 - val_loss: 0.6265 - val_acc: 0.6697\n",
      "Epoch 3/10\n",
      "32843/32843 [==============================] - 37s 1ms/step - loss: 0.6251 - acc: 0.6705 - val_loss: 0.6281 - val_acc: 0.6697\n",
      "Epoch 4/10\n",
      "32843/32843 [==============================] - 37s 1ms/step - loss: 0.6242 - acc: 0.6702 - val_loss: 0.6413 - val_acc: 0.6701\n",
      "Epoch 5/10\n",
      "32843/32843 [==============================] - 37s 1ms/step - loss: 0.6240 - acc: 0.6699 - val_loss: 0.6252 - val_acc: 0.6696\n",
      "Epoch 6/10\n",
      "32843/32843 [==============================] - 37s 1ms/step - loss: 0.6236 - acc: 0.6704 - val_loss: 0.6257 - val_acc: 0.6697\n",
      "Epoch 7/10\n",
      "32843/32843 [==============================] - 37s 1ms/step - loss: 0.6230 - acc: 0.6708 - val_loss: 0.6290 - val_acc: 0.6699\n",
      "Epoch 8/10\n",
      "32843/32843 [==============================] - 37s 1ms/step - loss: 0.6243 - acc: 0.6704 - val_loss: 0.6261 - val_acc: 0.6695\n",
      "Epoch 9/10\n",
      "32843/32843 [==============================] - 37s 1ms/step - loss: 0.6232 - acc: 0.6702 - val_loss: 0.6256 - val_acc: 0.6699\n",
      "Epoch 10/10\n",
      "32843/32843 [==============================] - 37s 1ms/step - loss: 0.6228 - acc: 0.6708 - val_loss: 0.6246 - val_acc: 0.6699\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/anaconda3/lib/python3.6/site-packages/ipykernel_launcher.py:18: UserWarning: The `input_dim` and `input_length` arguments in recurrent layers are deprecated. Use `input_shape` instead.\n",
      "/anaconda3/lib/python3.6/site-packages/ipykernel_launcher.py:18: UserWarning: Update your `SimpleRNN` call to the Keras 2 API: `SimpleRNN(16, activation=\"relu\", input_shape=(None, 32))`\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 32843 samples, validate on 14076 samples\n",
      "Epoch 1/10\n",
      "32843/32843 [==============================] - 36s 1ms/step - loss: 0.1044 - acc: 0.6688 - val_loss: 0.1036 - val_acc: 0.6694\n",
      "Epoch 2/10\n",
      "32843/32843 [==============================] - 36s 1ms/step - loss: 0.1032 - acc: 0.6701 - val_loss: 0.1043 - val_acc: 0.6696\n",
      "Epoch 3/10\n",
      "32843/32843 [==============================] - 36s 1ms/step - loss: 0.1029 - acc: 0.6699 - val_loss: 0.1034 - val_acc: 0.6694\n",
      "Epoch 4/10\n",
      "32843/32843 [==============================] - 36s 1ms/step - loss: 0.1027 - acc: 0.6697 - val_loss: 0.1031 - val_acc: 0.6694\n",
      "Epoch 5/10\n",
      "32843/32843 [==============================] - 36s 1ms/step - loss: 0.1027 - acc: 0.6696 - val_loss: 0.1028 - val_acc: 0.6694\n",
      "Epoch 6/10\n",
      "32843/32843 [==============================] - 36s 1ms/step - loss: 0.1025 - acc: 0.6696 - val_loss: 0.1027 - val_acc: 0.6694\n",
      "Epoch 7/10\n",
      "32843/32843 [==============================] - 36s 1ms/step - loss: 0.1025 - acc: 0.6703 - val_loss: 0.1028 - val_acc: 0.6694\n",
      "Epoch 8/10\n",
      "32843/32843 [==============================] - 36s 1ms/step - loss: 0.1023 - acc: 0.6700 - val_loss: 0.1028 - val_acc: 0.6694\n",
      "Epoch 9/10\n",
      "32843/32843 [==============================] - 36s 1ms/step - loss: 0.1024 - acc: 0.6702 - val_loss: 0.1027 - val_acc: 0.6694\n",
      "Epoch 10/10\n",
      "32843/32843 [==============================] - 36s 1ms/step - loss: 0.1023 - acc: 0.6704 - val_loss: 0.1026 - val_acc: 0.6696\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/anaconda3/lib/python3.6/site-packages/ipykernel_launcher.py:18: UserWarning: The `input_dim` and `input_length` arguments in recurrent layers are deprecated. Use `input_shape` instead.\n",
      "/anaconda3/lib/python3.6/site-packages/ipykernel_launcher.py:18: UserWarning: Update your `SimpleRNN` call to the Keras 2 API: `SimpleRNN(32, activation=\"relu\", input_shape=(None, 32))`\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 32843 samples, validate on 14076 samples\n",
      "Epoch 1/10\n",
      "32843/32843 [==============================] - 36s 1ms/step - loss: 0.1047 - acc: 0.6695 - val_loss: 0.1039 - val_acc: 0.6697\n",
      "Epoch 2/10\n",
      "32843/32843 [==============================] - 36s 1ms/step - loss: 0.1034 - acc: 0.6703 - val_loss: 0.1033 - val_acc: 0.6695\n",
      "Epoch 3/10\n",
      "32843/32843 [==============================] - 36s 1ms/step - loss: 0.1030 - acc: 0.6702 - val_loss: 0.1031 - val_acc: 0.6695\n",
      "Epoch 4/10\n",
      "32843/32843 [==============================] - 36s 1ms/step - loss: 0.1029 - acc: 0.6703 - val_loss: 0.1048 - val_acc: 0.6697\n",
      "Epoch 5/10\n",
      "32843/32843 [==============================] - 36s 1ms/step - loss: 0.1028 - acc: 0.6708 - val_loss: 0.1035 - val_acc: 0.6697\n",
      "Epoch 6/10\n",
      "32843/32843 [==============================] - 36s 1ms/step - loss: 0.1028 - acc: 0.6701 - val_loss: 0.1028 - val_acc: 0.6696\n",
      "Epoch 7/10\n",
      "32843/32843 [==============================] - 36s 1ms/step - loss: 0.1026 - acc: 0.6704 - val_loss: 0.1028 - val_acc: 0.6699\n",
      "Epoch 8/10\n",
      "32843/32843 [==============================] - 36s 1ms/step - loss: 0.1026 - acc: 0.6697 - val_loss: 0.1028 - val_acc: 0.6696\n",
      "Epoch 9/10\n",
      "32843/32843 [==============================] - 36s 1ms/step - loss: 0.1024 - acc: 0.6702 - val_loss: 0.1027 - val_acc: 0.6698\n",
      "Epoch 10/10\n",
      "32843/32843 [==============================] - 36s 1ms/step - loss: 0.1025 - acc: 0.6709 - val_loss: 0.1030 - val_acc: 0.6696\n",
      "Scan Finished!\n"
     ]
    }
   ],
   "source": [
    "from keras.optimizers import Adam, Nadam\n",
    "from keras.activations import softmax\n",
    "from keras.losses import categorical_crossentropy, logcosh\n",
    "\n",
    "pickle_load_dir_path = \"./dataset_pickle_tuple_type/\"\n",
    "X = {}\n",
    "y = {}\n",
    "\n",
    "\n",
    "MODEL = [\"SimpleRNN\"]\n",
    "idx_time_unit = 10     # candle stick minutes\n",
    "idx_window_size = 10  # Unit: num. of candle sticks\n",
    "idx_gap = 1            # Unit: num. of candle sticks\n",
    "idx_margix_rate = 0.1  # Unit: percent\n",
    "\n",
    "key_name_X = \"X_\"\n",
    "key_name_y = \"y_\"\n",
    "\n",
    "\n",
    "key_name_X += str(idx_time_unit) + \"_\" + str(idx_window_size) + \"_\" + str(idx_gap) + \"_\" + str(idx_margix_rate)\n",
    "key_name_y += str(idx_time_unit) + \"_\" + str(idx_window_size) + \"_\" + str(idx_gap) + \"_\" + str(idx_margix_rate)\n",
    "\n",
    "X = Load_Dataset_X(pickle_load_dir_path, idx_time_unit, idx_window_size, idx_gap, idx_margix_rate)\n",
    "y = Load_Dataset_y(pickle_load_dir_path, idx_time_unit, idx_window_size, idx_gap, idx_margix_rate)\n",
    "\n",
    "y_single = {}\n",
    "y_single['BTC'] = y[:, 1]\n",
    "y_single['ETH'] = y[:, 2]\n",
    "y_single['XRP'] = y[:, 3]\n",
    "y_single['BCH'] = y[:, 4]\n",
    "y_single['LTC'] = y[:, 5]\n",
    "y_single['DASH'] = y[:, 6]\n",
    "y_single['ETC'] = y[:, 7]\n",
    "\n",
    "coin_list2 = [\"BTC\", \"ETH\", \"XRP\", \"BCH\", \"LTC\", \"DASH\", \"ETC\"]\n",
    "\n",
    "y2 = onehottify(y_single['BTC'], n=2) \n",
    "#                         for coin in coin_list2:\n",
    "#                             print(\"y_single[\"+coin+\"]\"+\".shape\")\n",
    "#                             print(y_single[coin].shape)\n",
    "#                             print()\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y2, test_size=0.1, random_state=42)\n",
    "#     print(\"X_train.shape\")\n",
    "#     print(X_train.shape)\n",
    "#     print(\"y_train.shape\")\n",
    "#     print(y_train.shape)\n",
    "#     print()\n",
    "#     print(\"X_test.shape\")\n",
    "#     print(X_test.shape)\n",
    "#     print(\"y_test.shape\")\n",
    "#     print(y_test.shape)\n",
    "#     print()\n",
    "\n",
    "n_coins = 8\n",
    "n_price = 4\n",
    "n_steps = idx_window_size # 원래 100이었음. reshape 문제 때문에 수정함\n",
    "\n",
    "X_train_2 = X_train.transpose([0, 2, 1, 3])\n",
    "X_test_2 = X_test.transpose([0, 2, 1, 3])\n",
    "#     print(\"X_train_2.shape\")\n",
    "#     print(X_train_2.shape)\n",
    "#     print(\"X_test_2.shape\")\n",
    "#     print(X_test_2.shape)\n",
    "#     print()\n",
    "\n",
    "X_train_3 = X_train_2.reshape([X_train.shape[0], n_steps, n_coins * n_price])\n",
    "X_test_3 = X_test_2.reshape([X_test.shape[0], n_steps, n_coins * n_price])\n",
    "#     print(\"X_train_3.shape\")\n",
    "#     print(X_train_3.shape)\n",
    "#     print(\"X_test_3.shape\")\n",
    "#     print(X_test_3.shape)\n",
    "#     print()\n",
    "\n",
    "X_train_reshape = X_train_2.reshape([X_train.shape[0], n_steps*n_coins * n_price])\n",
    "X_test_reshape = X_test_2.reshape([X_test.shape[0], n_steps*n_coins * n_price])\n",
    "#     print(\"X_train_reshape.shape\")\n",
    "#     print(X_train_reshape.shape)\n",
    "#     print(\"X_test_reshape.shape\")\n",
    "#     print(X_test_reshape.shape)\n",
    "#     print()\n",
    "\n",
    "scaler = MinMaxScaler()\n",
    "scaler.fit(X_train_reshape)\n",
    "X_train_scaled = scaler.transform(X_train_reshape)\n",
    "X_test_scaled = scaler.transform(X_test_reshape)\n",
    "\n",
    "X_train_scaled = X_train_scaled.reshape(-1, n_steps, n_coins * n_price)\n",
    "\n",
    "X_test_scaled = X_test_scaled.reshape(-1, n_steps, n_coins * n_price)\n",
    "\n",
    "X_train_scaled, X_test_scaled = input_reshape(X_train_scaled, X_test_scaled, n_steps, n_coins, n_price)\n",
    "\n",
    "                        \n",
    "p = {'lr': (0.1, 0.01, 0.001),\n",
    "     'first_neuron':[4, 8, 16, 32, 64, 128],\n",
    "     'batch_size': [64,128,256],\n",
    "     'epochs': [10],\n",
    "     'dropout': (0, 0.40, 10),\n",
    "     'optimizer': [RMSProp, Adam],\n",
    "     'kernel_initializer'=[glorot_uniform, uniform, he_uniform],\n",
    "     'recurrent_initializer'=[orthogonal], \n",
    "     'bias_initializer'=[zeros],\n",
    "     'loss': [categorical_crossentropy, logcosh],\n",
    "     'last_activation': [softmax],\n",
    "     'weight_regulizer':[None],\n",
    "     'emb_output_dims': [None]}\n",
    "\n",
    "import hyperio as hy\n",
    "h = hy.Hyperio(X_train_scaled, y_train, \n",
    "               params=p, \n",
    "               dataset_name='coin', \n",
    "               experiment_no='1', \n",
    "               model=create_model_SimpleRNN_non_GPU_test,\n",
    "               # create_model_SimpleRNN_non_GPU_test(x_train, y_train, x_val, y_val, params)\n",
    "               grid_downsample=.01)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ㅗ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<hyperio.main.Hyperio at 0x116242dd8>"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "h"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'lr': (2, 10, 30),\n",
       " 'first_neuron': [4, 8, 16, 32, 64, 128],\n",
       " 'batch_size': [2, 3, 4],\n",
       " 'epochs': [10],\n",
       " 'dropout': (0, 0.4, 10),\n",
       " 'optimizer': [keras.optimizers.Adam, keras.optimizers.Nadam],\n",
       " 'loss': [<function keras.losses.categorical_crossentropy(y_true, y_pred)>,\n",
       "  <function keras.losses.logcosh(y_true, y_pred)>],\n",
       " 'last_activation': [<function keras.activations.softmax(x, axis=-1)>],\n",
       " 'weight_regulizer': [None],\n",
       " 'emb_output_dims': [None]}"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "h.param_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[7, 32, 2, 10, 0.16, <class 'keras.optimizers.Adam'>,\n",
       "        <function categorical_crossentropy at 0xb1b3b6048>,\n",
       "        <function softmax at 0xb1b3dc9d8>, None, None, 0],\n",
       "       [7, 8, 4, 10, 0.28, <class 'keras.optimizers.Nadam'>,\n",
       "        <function categorical_crossentropy at 0xb1b3b6048>,\n",
       "        <function softmax at 0xb1b3dc9d8>, None, None, 1],\n",
       "       [8, 4, 4, 10, 0.24, <class 'keras.optimizers.Adam'>,\n",
       "        <function categorical_crossentropy at 0xb1b3b6048>,\n",
       "        <function softmax at 0xb1b3dc9d8>, None, None, 2],\n",
       "       [3, 16, 2, 10, 0.0, <class 'keras.optimizers.Nadam'>,\n",
       "        <function logcosh at 0xb1b3a4f28>,\n",
       "        <function softmax at 0xb1b3dc9d8>, None, None, 3],\n",
       "       [5, 8, 4, 10, 0.24, <class 'keras.optimizers.Nadam'>,\n",
       "        <function logcosh at 0xb1b3a4f28>,\n",
       "        <function softmax at 0xb1b3dc9d8>, None, None, 4],\n",
       "       [2, 64, 2, 10, 0.24, <class 'keras.optimizers.Nadam'>,\n",
       "        <function categorical_crossentropy at 0xb1b3b6048>,\n",
       "        <function softmax at 0xb1b3dc9d8>, None, None, 5],\n",
       "       [2, 64, 3, 10, 0.0, <class 'keras.optimizers.Nadam'>,\n",
       "        <function logcosh at 0xb1b3a4f28>,\n",
       "        <function softmax at 0xb1b3dc9d8>, None, None, 6],\n",
       "       [8, 128, 2, 10, 0.0, <class 'keras.optimizers.Nadam'>,\n",
       "        <function categorical_crossentropy at 0xb1b3b6048>,\n",
       "        <function softmax at 0xb1b3dc9d8>, None, None, 7],\n",
       "       [5, 16, 4, 10, 0.08, <class 'keras.optimizers.Adam'>,\n",
       "        <function logcosh at 0xb1b3a4f28>,\n",
       "        <function softmax at 0xb1b3dc9d8>, None, None, 8],\n",
       "       [4, 32, 2, 10, 0.08, <class 'keras.optimizers.Nadam'>,\n",
       "        <function logcosh at 0xb1b3a4f28>,\n",
       "        <function softmax at 0xb1b3dc9d8>, None, None, 9],\n",
       "       [2, 64, 3, 10, 0.04, <class 'keras.optimizers.Nadam'>,\n",
       "        <function logcosh at 0xb1b3a4f28>,\n",
       "        <function softmax at 0xb1b3dc9d8>, None, None, 10],\n",
       "       [2, 16, 3, 10, 0.08, <class 'keras.optimizers.Nadam'>,\n",
       "        <function categorical_crossentropy at 0xb1b3b6048>,\n",
       "        <function softmax at 0xb1b3dc9d8>, None, None, 11],\n",
       "       [6, 4, 3, 10, 0.32, <class 'keras.optimizers.Nadam'>,\n",
       "        <function logcosh at 0xb1b3a4f28>,\n",
       "        <function softmax at 0xb1b3dc9d8>, None, None, 12],\n",
       "       [2, 128, 4, 10, 0.12, <class 'keras.optimizers.Adam'>,\n",
       "        <function categorical_crossentropy at 0xb1b3b6048>,\n",
       "        <function softmax at 0xb1b3dc9d8>, None, None, 13],\n",
       "       [7, 32, 3, 10, 0.28, <class 'keras.optimizers.Adam'>,\n",
       "        <function logcosh at 0xb1b3a4f28>,\n",
       "        <function softmax at 0xb1b3dc9d8>, None, None, 14],\n",
       "       [5, 64, 3, 10, 0.28, <class 'keras.optimizers.Adam'>,\n",
       "        <function categorical_crossentropy at 0xb1b3b6048>,\n",
       "        <function softmax at 0xb1b3dc9d8>, None, None, 15],\n",
       "       [4, 32, 4, 10, 0.04, <class 'keras.optimizers.Nadam'>,\n",
       "        <function logcosh at 0xb1b3a4f28>,\n",
       "        <function softmax at 0xb1b3dc9d8>, None, None, 16],\n",
       "       [8, 128, 3, 10, 0.08, <class 'keras.optimizers.Adam'>,\n",
       "        <function categorical_crossentropy at 0xb1b3b6048>,\n",
       "        <function softmax at 0xb1b3dc9d8>, None, None, 17],\n",
       "       [6, 4, 2, 10, 0.32, <class 'keras.optimizers.Nadam'>,\n",
       "        <function categorical_crossentropy at 0xb1b3b6048>,\n",
       "        <function softmax at 0xb1b3dc9d8>, None, None, 18],\n",
       "       [3, 64, 4, 10, 0.0, <class 'keras.optimizers.Nadam'>,\n",
       "        <function logcosh at 0xb1b3a4f28>,\n",
       "        <function softmax at 0xb1b3dc9d8>, None, None, 19],\n",
       "       [2, 128, 3, 10, 0.24, <class 'keras.optimizers.Nadam'>,\n",
       "        <function logcosh at 0xb1b3a4f28>,\n",
       "        <function softmax at 0xb1b3dc9d8>, None, None, 20],\n",
       "       [4, 128, 2, 10, 0.2, <class 'keras.optimizers.Nadam'>,\n",
       "        <function categorical_crossentropy at 0xb1b3b6048>,\n",
       "        <function softmax at 0xb1b3dc9d8>, None, None, 21],\n",
       "       [5, 64, 2, 10, 0.12, <class 'keras.optimizers.Adam'>,\n",
       "        <function categorical_crossentropy at 0xb1b3b6048>,\n",
       "        <function softmax at 0xb1b3dc9d8>, None, None, 22],\n",
       "       [7, 4, 2, 10, 0.28, <class 'keras.optimizers.Adam'>,\n",
       "        <function categorical_crossentropy at 0xb1b3b6048>,\n",
       "        <function softmax at 0xb1b3dc9d8>, None, None, 23],\n",
       "       [8, 32, 3, 10, 0.36, <class 'keras.optimizers.Adam'>,\n",
       "        <function categorical_crossentropy at 0xb1b3b6048>,\n",
       "        <function softmax at 0xb1b3dc9d8>, None, None, 24],\n",
       "       [4, 16, 2, 10, 0.04, <class 'keras.optimizers.Adam'>,\n",
       "        <function categorical_crossentropy at 0xb1b3b6048>,\n",
       "        <function softmax at 0xb1b3dc9d8>, None, None, 25],\n",
       "       [3, 32, 4, 10, 0.12, <class 'keras.optimizers.Nadam'>,\n",
       "        <function logcosh at 0xb1b3a4f28>,\n",
       "        <function softmax at 0xb1b3dc9d8>, None, None, 26],\n",
       "       [4, 8, 4, 10, 0.0, <class 'keras.optimizers.Nadam'>,\n",
       "        <function categorical_crossentropy at 0xb1b3b6048>,\n",
       "        <function softmax at 0xb1b3dc9d8>, None, None, 27],\n",
       "       [8, 16, 2, 10, 0.36, <class 'keras.optimizers.Adam'>,\n",
       "        <function categorical_crossentropy at 0xb1b3b6048>,\n",
       "        <function softmax at 0xb1b3dc9d8>, None, None, 28],\n",
       "       [6, 16, 3, 10, 0.08, <class 'keras.optimizers.Adam'>,\n",
       "        <function logcosh at 0xb1b3a4f28>,\n",
       "        <function softmax at 0xb1b3dc9d8>, None, None, 29],\n",
       "       [9, 32, 4, 10, 0.32, <class 'keras.optimizers.Nadam'>,\n",
       "        <function logcosh at 0xb1b3a4f28>,\n",
       "        <function softmax at 0xb1b3dc9d8>, None, None, 30],\n",
       "       [7, 4, 3, 10, 0.2, <class 'keras.optimizers.Nadam'>,\n",
       "        <function logcosh at 0xb1b3a4f28>,\n",
       "        <function softmax at 0xb1b3dc9d8>, None, None, 31],\n",
       "       [7, 128, 2, 10, 0.16, <class 'keras.optimizers.Nadam'>,\n",
       "        <function categorical_crossentropy at 0xb1b3b6048>,\n",
       "        <function softmax at 0xb1b3dc9d8>, None, None, 32],\n",
       "       [5, 32, 4, 10, 0.24, <class 'keras.optimizers.Nadam'>,\n",
       "        <function categorical_crossentropy at 0xb1b3b6048>,\n",
       "        <function softmax at 0xb1b3dc9d8>, None, None, 33],\n",
       "       [8, 16, 2, 10, 0.16, <class 'keras.optimizers.Nadam'>,\n",
       "        <function categorical_crossentropy at 0xb1b3b6048>,\n",
       "        <function softmax at 0xb1b3dc9d8>, None, None, 34],\n",
       "       [8, 64, 2, 10, 0.08, <class 'keras.optimizers.Nadam'>,\n",
       "        <function categorical_crossentropy at 0xb1b3b6048>,\n",
       "        <function softmax at 0xb1b3dc9d8>, None, None, 35],\n",
       "       [6, 128, 3, 10, 0.24, <class 'keras.optimizers.Adam'>,\n",
       "        <function logcosh at 0xb1b3a4f28>,\n",
       "        <function softmax at 0xb1b3dc9d8>, None, None, 36],\n",
       "       [7, 8, 3, 10, 0.0, <class 'keras.optimizers.Adam'>,\n",
       "        <function logcosh at 0xb1b3a4f28>,\n",
       "        <function softmax at 0xb1b3dc9d8>, None, None, 37],\n",
       "       [8, 32, 4, 10, 0.32, <class 'keras.optimizers.Nadam'>,\n",
       "        <function logcosh at 0xb1b3a4f28>,\n",
       "        <function softmax at 0xb1b3dc9d8>, None, None, 38],\n",
       "       [3, 16, 3, 10, 0.0, <class 'keras.optimizers.Nadam'>,\n",
       "        <function logcosh at 0xb1b3a4f28>,\n",
       "        <function softmax at 0xb1b3dc9d8>, None, None, 39],\n",
       "       [7, 16, 3, 10, 0.08, <class 'keras.optimizers.Adam'>,\n",
       "        <function logcosh at 0xb1b3a4f28>,\n",
       "        <function softmax at 0xb1b3dc9d8>, None, None, 40],\n",
       "       [7, 128, 3, 10, 0.28, <class 'keras.optimizers.Adam'>,\n",
       "        <function categorical_crossentropy at 0xb1b3b6048>,\n",
       "        <function softmax at 0xb1b3dc9d8>, None, None, 41],\n",
       "       [3, 8, 4, 10, 0.0, <class 'keras.optimizers.Adam'>,\n",
       "        <function categorical_crossentropy at 0xb1b3b6048>,\n",
       "        <function softmax at 0xb1b3dc9d8>, None, None, 42],\n",
       "       [8, 16, 4, 10, 0.2, <class 'keras.optimizers.Nadam'>,\n",
       "        <function categorical_crossentropy at 0xb1b3b6048>,\n",
       "        <function softmax at 0xb1b3dc9d8>, None, None, 43],\n",
       "       [3, 4, 3, 10, 0.04, <class 'keras.optimizers.Adam'>,\n",
       "        <function categorical_crossentropy at 0xb1b3b6048>,\n",
       "        <function softmax at 0xb1b3dc9d8>, None, None, 44],\n",
       "       [3, 4, 2, 10, 0.24, <class 'keras.optimizers.Nadam'>,\n",
       "        <function logcosh at 0xb1b3a4f28>,\n",
       "        <function softmax at 0xb1b3dc9d8>, None, None, 45],\n",
       "       [2, 128, 3, 10, 0.04, <class 'keras.optimizers.Adam'>,\n",
       "        <function categorical_crossentropy at 0xb1b3b6048>,\n",
       "        <function softmax at 0xb1b3dc9d8>, None, None, 46],\n",
       "       [4, 8, 4, 10, 0.12, <class 'keras.optimizers.Adam'>,\n",
       "        <function categorical_crossentropy at 0xb1b3b6048>,\n",
       "        <function softmax at 0xb1b3dc9d8>, None, None, 47],\n",
       "       [7, 64, 3, 10, 0.16, <class 'keras.optimizers.Adam'>,\n",
       "        <function categorical_crossentropy at 0xb1b3b6048>,\n",
       "        <function softmax at 0xb1b3dc9d8>, None, None, 48],\n",
       "       [2, 8, 3, 10, 0.0, <class 'keras.optimizers.Adam'>,\n",
       "        <function logcosh at 0xb1b3a4f28>,\n",
       "        <function softmax at 0xb1b3dc9d8>, None, None, 49],\n",
       "       [8, 4, 2, 10, 0.0, <class 'keras.optimizers.Adam'>,\n",
       "        <function logcosh at 0xb1b3a4f28>,\n",
       "        <function softmax at 0xb1b3dc9d8>, None, None, 50],\n",
       "       [9, 16, 4, 10, 0.24, <class 'keras.optimizers.Nadam'>,\n",
       "        <function logcosh at 0xb1b3a4f28>,\n",
       "        <function softmax at 0xb1b3dc9d8>, None, None, 51],\n",
       "       [3, 64, 3, 10, 0.12, <class 'keras.optimizers.Adam'>,\n",
       "        <function categorical_crossentropy at 0xb1b3b6048>,\n",
       "        <function softmax at 0xb1b3dc9d8>, None, None, 52],\n",
       "       [9, 16, 3, 10, 0.08, <class 'keras.optimizers.Adam'>,\n",
       "        <function logcosh at 0xb1b3a4f28>,\n",
       "        <function softmax at 0xb1b3dc9d8>, None, None, 53],\n",
       "       [7, 64, 2, 10, 0.32, <class 'keras.optimizers.Nadam'>,\n",
       "        <function logcosh at 0xb1b3a4f28>,\n",
       "        <function softmax at 0xb1b3dc9d8>, None, None, 54],\n",
       "       [4, 4, 3, 10, 0.0, <class 'keras.optimizers.Nadam'>,\n",
       "        <function logcosh at 0xb1b3a4f28>,\n",
       "        <function softmax at 0xb1b3dc9d8>, None, None, 55],\n",
       "       [6, 16, 4, 10, 0.28, <class 'keras.optimizers.Nadam'>,\n",
       "        <function categorical_crossentropy at 0xb1b3b6048>,\n",
       "        <function softmax at 0xb1b3dc9d8>, None, None, 56]], dtype=object)"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "h.param_grid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'lr': 7,\n",
       " 'first_neuron': 32,\n",
       " 'batch_size': 3,\n",
       " 'epochs': 10,\n",
       " 'dropout': 0.28,\n",
       " 'optimizer': keras.optimizers.Adam,\n",
       " 'loss': <function keras.losses.logcosh(y_true, y_pred)>,\n",
       " 'last_activation': <function keras.activations.softmax(x, axis=-1)>,\n",
       " 'weight_regulizer': None,\n",
       " 'emb_output_dims': None}"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "h.params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>round_epochs</th>\n",
       "      <th>train_peak</th>\n",
       "      <th>val_peak</th>\n",
       "      <th>train_acc</th>\n",
       "      <th>val_acc</th>\n",
       "      <th>train_loss</th>\n",
       "      <th>val_loss</th>\n",
       "      <th>train_score</th>\n",
       "      <th>val_score</th>\n",
       "      <th>lr</th>\n",
       "      <th>first_neuron</th>\n",
       "      <th>batch_size</th>\n",
       "      <th>epochs</th>\n",
       "      <th>dropout</th>\n",
       "      <th>optimizer</th>\n",
       "      <th>loss</th>\n",
       "      <th>last_activation</th>\n",
       "      <th>weight_regulizer</th>\n",
       "      <th>emb_output_dims</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>10</td>\n",
       "      <td>8</td>\n",
       "      <td>5</td>\n",
       "      <td>0.671102</td>\n",
       "      <td>0.669793</td>\n",
       "      <td>0.102212</td>\n",
       "      <td>0.102655</td>\n",
       "      <td>0.56889</td>\n",
       "      <td>0.567137</td>\n",
       "      <td>2</td>\n",
       "      <td>64</td>\n",
       "      <td>3</td>\n",
       "      <td>10</td>\n",
       "      <td>0.04</td>\n",
       "      <td>&lt;class 'keras.optimizers.Nadam'&gt;</td>\n",
       "      <td>&lt;function logcosh at 0xb1b3a4f28&gt;</td>\n",
       "      <td>&lt;function softmax at 0xb1b3dc9d8&gt;</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>10</td>\n",
       "      <td>8</td>\n",
       "      <td>6</td>\n",
       "      <td>0.670767</td>\n",
       "      <td>0.670432</td>\n",
       "      <td>0.102477</td>\n",
       "      <td>0.102709</td>\n",
       "      <td>0.56829</td>\n",
       "      <td>0.567723</td>\n",
       "      <td>9</td>\n",
       "      <td>16</td>\n",
       "      <td>4</td>\n",
       "      <td>10</td>\n",
       "      <td>0.24</td>\n",
       "      <td>&lt;class 'keras.optimizers.Nadam'&gt;</td>\n",
       "      <td>&lt;function logcosh at 0xb1b3a4f28&gt;</td>\n",
       "      <td>&lt;function softmax at 0xb1b3dc9d8&gt;</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>10</td>\n",
       "      <td>9</td>\n",
       "      <td>8</td>\n",
       "      <td>0.67031</td>\n",
       "      <td>0.669864</td>\n",
       "      <td>0.622116</td>\n",
       "      <td>0.62357</td>\n",
       "      <td>0.0481947</td>\n",
       "      <td>0.0462941</td>\n",
       "      <td>2</td>\n",
       "      <td>128</td>\n",
       "      <td>3</td>\n",
       "      <td>10</td>\n",
       "      <td>0.04</td>\n",
       "      <td>&lt;class 'keras.optimizers.Adam'&gt;</td>\n",
       "      <td>&lt;function categorical_crossentropy at 0xb1b3b6...</td>\n",
       "      <td>&lt;function softmax at 0xb1b3dc9d8&gt;</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>10</td>\n",
       "      <td>7</td>\n",
       "      <td>9</td>\n",
       "      <td>0.671041</td>\n",
       "      <td>0.670148</td>\n",
       "      <td>0.102457</td>\n",
       "      <td>0.102643</td>\n",
       "      <td>0.568584</td>\n",
       "      <td>0.567505</td>\n",
       "      <td>6</td>\n",
       "      <td>128</td>\n",
       "      <td>3</td>\n",
       "      <td>10</td>\n",
       "      <td>0.24</td>\n",
       "      <td>&lt;class 'keras.optimizers.Adam'&gt;</td>\n",
       "      <td>&lt;function logcosh at 0xb1b3a4f28&gt;</td>\n",
       "      <td>&lt;function softmax at 0xb1b3dc9d8&gt;</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>10</td>\n",
       "      <td>8</td>\n",
       "      <td>9</td>\n",
       "      <td>0.670432</td>\n",
       "      <td>0.669793</td>\n",
       "      <td>0.102367</td>\n",
       "      <td>0.102836</td>\n",
       "      <td>0.568065</td>\n",
       "      <td>0.566957</td>\n",
       "      <td>9</td>\n",
       "      <td>16</td>\n",
       "      <td>3</td>\n",
       "      <td>10</td>\n",
       "      <td>0.08</td>\n",
       "      <td>&lt;class 'keras.optimizers.Adam'&gt;</td>\n",
       "      <td>&lt;function logcosh at 0xb1b3a4f28&gt;</td>\n",
       "      <td>&lt;function softmax at 0xb1b3dc9d8&gt;</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>10</td>\n",
       "      <td>7</td>\n",
       "      <td>9</td>\n",
       "      <td>0.670797</td>\n",
       "      <td>0.669864</td>\n",
       "      <td>0.623974</td>\n",
       "      <td>0.624579</td>\n",
       "      <td>0.0468236</td>\n",
       "      <td>0.0452846</td>\n",
       "      <td>7</td>\n",
       "      <td>64</td>\n",
       "      <td>3</td>\n",
       "      <td>10</td>\n",
       "      <td>0.16</td>\n",
       "      <td>&lt;class 'keras.optimizers.Adam'&gt;</td>\n",
       "      <td>&lt;function categorical_crossentropy at 0xb1b3b6...</td>\n",
       "      <td>&lt;function softmax at 0xb1b3dc9d8&gt;</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>10</td>\n",
       "      <td>9</td>\n",
       "      <td>9</td>\n",
       "      <td>0.670036</td>\n",
       "      <td>0.66965</td>\n",
       "      <td>0.62655</td>\n",
       "      <td>0.62682</td>\n",
       "      <td>0.0434862</td>\n",
       "      <td>0.0428309</td>\n",
       "      <td>8</td>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "      <td>10</td>\n",
       "      <td>0.24</td>\n",
       "      <td>&lt;class 'keras.optimizers.Adam'&gt;</td>\n",
       "      <td>&lt;function categorical_crossentropy at 0xb1b3b6...</td>\n",
       "      <td>&lt;function softmax at 0xb1b3dc9d8&gt;</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>10</td>\n",
       "      <td>9</td>\n",
       "      <td>6</td>\n",
       "      <td>0.670493</td>\n",
       "      <td>0.669793</td>\n",
       "      <td>0.624413</td>\n",
       "      <td>0.624419</td>\n",
       "      <td>0.0460796</td>\n",
       "      <td>0.0453735</td>\n",
       "      <td>5</td>\n",
       "      <td>32</td>\n",
       "      <td>4</td>\n",
       "      <td>10</td>\n",
       "      <td>0.24</td>\n",
       "      <td>&lt;class 'keras.optimizers.Nadam'&gt;</td>\n",
       "      <td>&lt;function categorical_crossentropy at 0xb1b3b6...</td>\n",
       "      <td>&lt;function softmax at 0xb1b3dc9d8&gt;</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>10</td>\n",
       "      <td>8</td>\n",
       "      <td>5</td>\n",
       "      <td>0.670554</td>\n",
       "      <td>0.669864</td>\n",
       "      <td>0.624511</td>\n",
       "      <td>0.624588</td>\n",
       "      <td>0.0460428</td>\n",
       "      <td>0.0452751</td>\n",
       "      <td>8</td>\n",
       "      <td>16</td>\n",
       "      <td>4</td>\n",
       "      <td>10</td>\n",
       "      <td>0.2</td>\n",
       "      <td>&lt;class 'keras.optimizers.Nadam'&gt;</td>\n",
       "      <td>&lt;function categorical_crossentropy at 0xb1b3b6...</td>\n",
       "      <td>&lt;function softmax at 0xb1b3dc9d8&gt;</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>10</td>\n",
       "      <td>9</td>\n",
       "      <td>9</td>\n",
       "      <td>0.670036</td>\n",
       "      <td>0.66965</td>\n",
       "      <td>0.622782</td>\n",
       "      <td>0.623409</td>\n",
       "      <td>0.0472542</td>\n",
       "      <td>0.0462416</td>\n",
       "      <td>3</td>\n",
       "      <td>4</td>\n",
       "      <td>3</td>\n",
       "      <td>10</td>\n",
       "      <td>0.04</td>\n",
       "      <td>&lt;class 'keras.optimizers.Adam'&gt;</td>\n",
       "      <td>&lt;function categorical_crossentropy at 0xb1b3b6...</td>\n",
       "      <td>&lt;function softmax at 0xb1b3dc9d8&gt;</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>10</td>\n",
       "      <td>8</td>\n",
       "      <td>7</td>\n",
       "      <td>0.670828</td>\n",
       "      <td>0.670148</td>\n",
       "      <td>0.62437</td>\n",
       "      <td>0.625117</td>\n",
       "      <td>0.0464578</td>\n",
       "      <td>0.0450305</td>\n",
       "      <td>7</td>\n",
       "      <td>8</td>\n",
       "      <td>4</td>\n",
       "      <td>10</td>\n",
       "      <td>0.28</td>\n",
       "      <td>&lt;class 'keras.optimizers.Nadam'&gt;</td>\n",
       "      <td>&lt;function categorical_crossentropy at 0xb1b3b6...</td>\n",
       "      <td>&lt;function softmax at 0xb1b3dc9d8&gt;</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>10</td>\n",
       "      <td>6</td>\n",
       "      <td>9</td>\n",
       "      <td>0.670889</td>\n",
       "      <td>0.669722</td>\n",
       "      <td>0.102449</td>\n",
       "      <td>0.102538</td>\n",
       "      <td>0.56844</td>\n",
       "      <td>0.567183</td>\n",
       "      <td>5</td>\n",
       "      <td>16</td>\n",
       "      <td>4</td>\n",
       "      <td>10</td>\n",
       "      <td>0.08</td>\n",
       "      <td>&lt;class 'keras.optimizers.Adam'&gt;</td>\n",
       "      <td>&lt;function logcosh at 0xb1b3a4f28&gt;</td>\n",
       "      <td>&lt;function softmax at 0xb1b3dc9d8&gt;</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>10</td>\n",
       "      <td>6</td>\n",
       "      <td>7</td>\n",
       "      <td>0.670036</td>\n",
       "      <td>0.670077</td>\n",
       "      <td>0.62393</td>\n",
       "      <td>0.62507</td>\n",
       "      <td>0.0461058</td>\n",
       "      <td>0.0450069</td>\n",
       "      <td>5</td>\n",
       "      <td>64</td>\n",
       "      <td>3</td>\n",
       "      <td>10</td>\n",
       "      <td>0.28</td>\n",
       "      <td>&lt;class 'keras.optimizers.Adam'&gt;</td>\n",
       "      <td>&lt;function categorical_crossentropy at 0xb1b3b6...</td>\n",
       "      <td>&lt;function softmax at 0xb1b3dc9d8&gt;</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>10</td>\n",
       "      <td>7</td>\n",
       "      <td>8</td>\n",
       "      <td>0.670676</td>\n",
       "      <td>0.670077</td>\n",
       "      <td>0.102353</td>\n",
       "      <td>0.103037</td>\n",
       "      <td>0.568323</td>\n",
       "      <td>0.567039</td>\n",
       "      <td>3</td>\n",
       "      <td>32</td>\n",
       "      <td>4</td>\n",
       "      <td>10</td>\n",
       "      <td>0.12</td>\n",
       "      <td>&lt;class 'keras.optimizers.Nadam'&gt;</td>\n",
       "      <td>&lt;function logcosh at 0xb1b3a4f28&gt;</td>\n",
       "      <td>&lt;function softmax at 0xb1b3dc9d8&gt;</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>10</td>\n",
       "      <td>9</td>\n",
       "      <td>3</td>\n",
       "      <td>0.670584</td>\n",
       "      <td>0.670077</td>\n",
       "      <td>0.102084</td>\n",
       "      <td>0.102807</td>\n",
       "      <td>0.5685</td>\n",
       "      <td>0.56727</td>\n",
       "      <td>3</td>\n",
       "      <td>64</td>\n",
       "      <td>4</td>\n",
       "      <td>10</td>\n",
       "      <td>0</td>\n",
       "      <td>&lt;class 'keras.optimizers.Nadam'&gt;</td>\n",
       "      <td>&lt;function logcosh at 0xb1b3a4f28&gt;</td>\n",
       "      <td>&lt;function softmax at 0xb1b3dc9d8&gt;</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>10</td>\n",
       "      <td>8</td>\n",
       "      <td>6</td>\n",
       "      <td>0.670919</td>\n",
       "      <td>0.67029</td>\n",
       "      <td>0.623055</td>\n",
       "      <td>0.624548</td>\n",
       "      <td>0.0478639</td>\n",
       "      <td>0.0457421</td>\n",
       "      <td>8</td>\n",
       "      <td>128</td>\n",
       "      <td>2</td>\n",
       "      <td>10</td>\n",
       "      <td>0</td>\n",
       "      <td>&lt;class 'keras.optimizers.Nadam'&gt;</td>\n",
       "      <td>&lt;function categorical_crossentropy at 0xb1b3b6...</td>\n",
       "      <td>&lt;function softmax at 0xb1b3dc9d8&gt;</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>10</td>\n",
       "      <td>7</td>\n",
       "      <td>8</td>\n",
       "      <td>0.670188</td>\n",
       "      <td>0.66965</td>\n",
       "      <td>0.102992</td>\n",
       "      <td>0.102904</td>\n",
       "      <td>0.567196</td>\n",
       "      <td>0.566747</td>\n",
       "      <td>5</td>\n",
       "      <td>8</td>\n",
       "      <td>4</td>\n",
       "      <td>10</td>\n",
       "      <td>0.24</td>\n",
       "      <td>&lt;class 'keras.optimizers.Nadam'&gt;</td>\n",
       "      <td>&lt;function logcosh at 0xb1b3a4f28&gt;</td>\n",
       "      <td>&lt;function softmax at 0xb1b3dc9d8&gt;</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>10</td>\n",
       "      <td>7</td>\n",
       "      <td>6</td>\n",
       "      <td>0.670706</td>\n",
       "      <td>0.670006</td>\n",
       "      <td>0.10232</td>\n",
       "      <td>0.102518</td>\n",
       "      <td>0.568386</td>\n",
       "      <td>0.567488</td>\n",
       "      <td>6</td>\n",
       "      <td>16</td>\n",
       "      <td>3</td>\n",
       "      <td>10</td>\n",
       "      <td>0.08</td>\n",
       "      <td>&lt;class 'keras.optimizers.Adam'&gt;</td>\n",
       "      <td>&lt;function logcosh at 0xb1b3a4f28&gt;</td>\n",
       "      <td>&lt;function softmax at 0xb1b3dc9d8&gt;</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>10</td>\n",
       "      <td>9</td>\n",
       "      <td>5</td>\n",
       "      <td>0.670036</td>\n",
       "      <td>0.66965</td>\n",
       "      <td>0.622998</td>\n",
       "      <td>0.624347</td>\n",
       "      <td>0.0470385</td>\n",
       "      <td>0.0453039</td>\n",
       "      <td>2</td>\n",
       "      <td>16</td>\n",
       "      <td>3</td>\n",
       "      <td>10</td>\n",
       "      <td>0.08</td>\n",
       "      <td>&lt;class 'keras.optimizers.Nadam'&gt;</td>\n",
       "      <td>&lt;function categorical_crossentropy at 0xb1b3b6...</td>\n",
       "      <td>&lt;function softmax at 0xb1b3dc9d8&gt;</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>10</td>\n",
       "      <td>9</td>\n",
       "      <td>6</td>\n",
       "      <td>0.67028</td>\n",
       "      <td>0.670858</td>\n",
       "      <td>0.622062</td>\n",
       "      <td>0.625124</td>\n",
       "      <td>0.0482177</td>\n",
       "      <td>0.0457342</td>\n",
       "      <td>3</td>\n",
       "      <td>8</td>\n",
       "      <td>4</td>\n",
       "      <td>10</td>\n",
       "      <td>0</td>\n",
       "      <td>&lt;class 'keras.optimizers.Adam'&gt;</td>\n",
       "      <td>&lt;function categorical_crossentropy at 0xb1b3b6...</td>\n",
       "      <td>&lt;function softmax at 0xb1b3dc9d8&gt;</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>10</td>\n",
       "      <td>9</td>\n",
       "      <td>8</td>\n",
       "      <td>0.670858</td>\n",
       "      <td>0.670574</td>\n",
       "      <td>0.101999</td>\n",
       "      <td>0.102648</td>\n",
       "      <td>0.56886</td>\n",
       "      <td>0.567926</td>\n",
       "      <td>7</td>\n",
       "      <td>8</td>\n",
       "      <td>3</td>\n",
       "      <td>10</td>\n",
       "      <td>0</td>\n",
       "      <td>&lt;class 'keras.optimizers.Adam'&gt;</td>\n",
       "      <td>&lt;function logcosh at 0xb1b3a4f28&gt;</td>\n",
       "      <td>&lt;function softmax at 0xb1b3dc9d8&gt;</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>10</td>\n",
       "      <td>9</td>\n",
       "      <td>6</td>\n",
       "      <td>0.670584</td>\n",
       "      <td>0.66965</td>\n",
       "      <td>0.622684</td>\n",
       "      <td>0.624057</td>\n",
       "      <td>0.0479003</td>\n",
       "      <td>0.0455933</td>\n",
       "      <td>2</td>\n",
       "      <td>128</td>\n",
       "      <td>4</td>\n",
       "      <td>10</td>\n",
       "      <td>0.12</td>\n",
       "      <td>&lt;class 'keras.optimizers.Adam'&gt;</td>\n",
       "      <td>&lt;function categorical_crossentropy at 0xb1b3b6...</td>\n",
       "      <td>&lt;function softmax at 0xb1b3dc9d8&gt;</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>10</td>\n",
       "      <td>7</td>\n",
       "      <td>8</td>\n",
       "      <td>0.670554</td>\n",
       "      <td>0.670219</td>\n",
       "      <td>0.623173</td>\n",
       "      <td>0.623874</td>\n",
       "      <td>0.047381</td>\n",
       "      <td>0.0463445</td>\n",
       "      <td>3</td>\n",
       "      <td>64</td>\n",
       "      <td>3</td>\n",
       "      <td>10</td>\n",
       "      <td>0.12</td>\n",
       "      <td>&lt;class 'keras.optimizers.Adam'&gt;</td>\n",
       "      <td>&lt;function categorical_crossentropy at 0xb1b3b6...</td>\n",
       "      <td>&lt;function softmax at 0xb1b3dc9d8&gt;</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>10</td>\n",
       "      <td>9</td>\n",
       "      <td>5</td>\n",
       "      <td>0.670493</td>\n",
       "      <td>0.669579</td>\n",
       "      <td>0.624345</td>\n",
       "      <td>0.624782</td>\n",
       "      <td>0.0461482</td>\n",
       "      <td>0.0447975</td>\n",
       "      <td>7</td>\n",
       "      <td>128</td>\n",
       "      <td>2</td>\n",
       "      <td>10</td>\n",
       "      <td>0.16</td>\n",
       "      <td>&lt;class 'keras.optimizers.Nadam'&gt;</td>\n",
       "      <td>&lt;function categorical_crossentropy at 0xb1b3b6...</td>\n",
       "      <td>&lt;function softmax at 0xb1b3dc9d8&gt;</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>10</td>\n",
       "      <td>8</td>\n",
       "      <td>7</td>\n",
       "      <td>0.670249</td>\n",
       "      <td>0.670148</td>\n",
       "      <td>0.102811</td>\n",
       "      <td>0.102702</td>\n",
       "      <td>0.567438</td>\n",
       "      <td>0.567445</td>\n",
       "      <td>7</td>\n",
       "      <td>64</td>\n",
       "      <td>2</td>\n",
       "      <td>10</td>\n",
       "      <td>0.32</td>\n",
       "      <td>&lt;class 'keras.optimizers.Nadam'&gt;</td>\n",
       "      <td>&lt;function logcosh at 0xb1b3a4f28&gt;</td>\n",
       "      <td>&lt;function softmax at 0xb1b3dc9d8&gt;</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>10</td>\n",
       "      <td>9</td>\n",
       "      <td>9</td>\n",
       "      <td>0.670158</td>\n",
       "      <td>0.669793</td>\n",
       "      <td>0.624623</td>\n",
       "      <td>0.624864</td>\n",
       "      <td>0.0455349</td>\n",
       "      <td>0.0449285</td>\n",
       "      <td>8</td>\n",
       "      <td>32</td>\n",
       "      <td>3</td>\n",
       "      <td>10</td>\n",
       "      <td>0.36</td>\n",
       "      <td>&lt;class 'keras.optimizers.Adam'&gt;</td>\n",
       "      <td>&lt;function categorical_crossentropy at 0xb1b3b6...</td>\n",
       "      <td>&lt;function softmax at 0xb1b3dc9d8&gt;</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>10</td>\n",
       "      <td>9</td>\n",
       "      <td>8</td>\n",
       "      <td>0.67028</td>\n",
       "      <td>0.670077</td>\n",
       "      <td>0.622529</td>\n",
       "      <td>0.624352</td>\n",
       "      <td>0.0477506</td>\n",
       "      <td>0.0457244</td>\n",
       "      <td>4</td>\n",
       "      <td>16</td>\n",
       "      <td>2</td>\n",
       "      <td>10</td>\n",
       "      <td>0.04</td>\n",
       "      <td>&lt;class 'keras.optimizers.Adam'&gt;</td>\n",
       "      <td>&lt;function categorical_crossentropy at 0xb1b3b6...</td>\n",
       "      <td>&lt;function softmax at 0xb1b3dc9d8&gt;</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>10</td>\n",
       "      <td>7</td>\n",
       "      <td>6</td>\n",
       "      <td>0.670554</td>\n",
       "      <td>0.670006</td>\n",
       "      <td>0.625212</td>\n",
       "      <td>0.624989</td>\n",
       "      <td>0.0453414</td>\n",
       "      <td>0.0450162</td>\n",
       "      <td>6</td>\n",
       "      <td>16</td>\n",
       "      <td>4</td>\n",
       "      <td>10</td>\n",
       "      <td>0.28</td>\n",
       "      <td>&lt;class 'keras.optimizers.Nadam'&gt;</td>\n",
       "      <td>&lt;function categorical_crossentropy at 0xb1b3b6...</td>\n",
       "      <td>&lt;function softmax at 0xb1b3dc9d8&gt;</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>10</td>\n",
       "      <td>8</td>\n",
       "      <td>9</td>\n",
       "      <td>0.671254</td>\n",
       "      <td>0.670219</td>\n",
       "      <td>0.623793</td>\n",
       "      <td>0.625346</td>\n",
       "      <td>0.0474615</td>\n",
       "      <td>0.0448728</td>\n",
       "      <td>4</td>\n",
       "      <td>8</td>\n",
       "      <td>4</td>\n",
       "      <td>10</td>\n",
       "      <td>0.12</td>\n",
       "      <td>&lt;class 'keras.optimizers.Adam'&gt;</td>\n",
       "      <td>&lt;function categorical_crossentropy at 0xb1b3b6...</td>\n",
       "      <td>&lt;function softmax at 0xb1b3dc9d8&gt;</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30</th>\n",
       "      <td>10</td>\n",
       "      <td>7</td>\n",
       "      <td>4</td>\n",
       "      <td>0.670158</td>\n",
       "      <td>0.66965</td>\n",
       "      <td>0.102175</td>\n",
       "      <td>0.102703</td>\n",
       "      <td>0.567983</td>\n",
       "      <td>0.566948</td>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "      <td>3</td>\n",
       "      <td>10</td>\n",
       "      <td>0</td>\n",
       "      <td>&lt;class 'keras.optimizers.Nadam'&gt;</td>\n",
       "      <td>&lt;function logcosh at 0xb1b3a4f28&gt;</td>\n",
       "      <td>&lt;function softmax at 0xb1b3dc9d8&gt;</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31</th>\n",
       "      <td>10</td>\n",
       "      <td>9</td>\n",
       "      <td>5</td>\n",
       "      <td>0.670036</td>\n",
       "      <td>0.66965</td>\n",
       "      <td>0.104719</td>\n",
       "      <td>0.104759</td>\n",
       "      <td>0.565317</td>\n",
       "      <td>0.564892</td>\n",
       "      <td>6</td>\n",
       "      <td>4</td>\n",
       "      <td>3</td>\n",
       "      <td>10</td>\n",
       "      <td>0.32</td>\n",
       "      <td>&lt;class 'keras.optimizers.Nadam'&gt;</td>\n",
       "      <td>&lt;function logcosh at 0xb1b3a4f28&gt;</td>\n",
       "      <td>&lt;function softmax at 0xb1b3dc9d8&gt;</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>32</th>\n",
       "      <td>10</td>\n",
       "      <td>9</td>\n",
       "      <td>9</td>\n",
       "      <td>0.67031</td>\n",
       "      <td>0.669437</td>\n",
       "      <td>0.621554</td>\n",
       "      <td>0.62353</td>\n",
       "      <td>0.0487565</td>\n",
       "      <td>0.0459078</td>\n",
       "      <td>4</td>\n",
       "      <td>8</td>\n",
       "      <td>4</td>\n",
       "      <td>10</td>\n",
       "      <td>0</td>\n",
       "      <td>&lt;class 'keras.optimizers.Nadam'&gt;</td>\n",
       "      <td>&lt;function categorical_crossentropy at 0xb1b3b6...</td>\n",
       "      <td>&lt;function softmax at 0xb1b3dc9d8&gt;</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>33</th>\n",
       "      <td>10</td>\n",
       "      <td>7</td>\n",
       "      <td>7</td>\n",
       "      <td>0.670341</td>\n",
       "      <td>0.669793</td>\n",
       "      <td>0.102802</td>\n",
       "      <td>0.102878</td>\n",
       "      <td>0.567539</td>\n",
       "      <td>0.566915</td>\n",
       "      <td>8</td>\n",
       "      <td>32</td>\n",
       "      <td>4</td>\n",
       "      <td>10</td>\n",
       "      <td>0.32</td>\n",
       "      <td>&lt;class 'keras.optimizers.Nadam'&gt;</td>\n",
       "      <td>&lt;function logcosh at 0xb1b3a4f28&gt;</td>\n",
       "      <td>&lt;function softmax at 0xb1b3dc9d8&gt;</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>34</th>\n",
       "      <td>10</td>\n",
       "      <td>8</td>\n",
       "      <td>7</td>\n",
       "      <td>0.670371</td>\n",
       "      <td>0.669864</td>\n",
       "      <td>0.624742</td>\n",
       "      <td>0.625978</td>\n",
       "      <td>0.0456295</td>\n",
       "      <td>0.0438859</td>\n",
       "      <td>7</td>\n",
       "      <td>128</td>\n",
       "      <td>3</td>\n",
       "      <td>10</td>\n",
       "      <td>0.28</td>\n",
       "      <td>&lt;class 'keras.optimizers.Adam'&gt;</td>\n",
       "      <td>&lt;function categorical_crossentropy at 0xb1b3b6...</td>\n",
       "      <td>&lt;function softmax at 0xb1b3dc9d8&gt;</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>35</th>\n",
       "      <td>10</td>\n",
       "      <td>8</td>\n",
       "      <td>9</td>\n",
       "      <td>0.670676</td>\n",
       "      <td>0.670077</td>\n",
       "      <td>0.102278</td>\n",
       "      <td>0.102513</td>\n",
       "      <td>0.568398</td>\n",
       "      <td>0.567564</td>\n",
       "      <td>4</td>\n",
       "      <td>32</td>\n",
       "      <td>4</td>\n",
       "      <td>10</td>\n",
       "      <td>0.04</td>\n",
       "      <td>&lt;class 'keras.optimizers.Nadam'&gt;</td>\n",
       "      <td>&lt;function logcosh at 0xb1b3a4f28&gt;</td>\n",
       "      <td>&lt;function softmax at 0xb1b3dc9d8&gt;</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>36</th>\n",
       "      <td>10</td>\n",
       "      <td>8</td>\n",
       "      <td>8</td>\n",
       "      <td>0.670341</td>\n",
       "      <td>0.66965</td>\n",
       "      <td>0.102138</td>\n",
       "      <td>0.102708</td>\n",
       "      <td>0.568202</td>\n",
       "      <td>0.566943</td>\n",
       "      <td>8</td>\n",
       "      <td>4</td>\n",
       "      <td>2</td>\n",
       "      <td>10</td>\n",
       "      <td>0</td>\n",
       "      <td>&lt;class 'keras.optimizers.Adam'&gt;</td>\n",
       "      <td>&lt;function logcosh at 0xb1b3a4f28&gt;</td>\n",
       "      <td>&lt;function softmax at 0xb1b3dc9d8&gt;</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>37</th>\n",
       "      <td>10</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0.670036</td>\n",
       "      <td>0.66965</td>\n",
       "      <td>0.634303</td>\n",
       "      <td>0.634431</td>\n",
       "      <td>0.0357337</td>\n",
       "      <td>0.0352191</td>\n",
       "      <td>6</td>\n",
       "      <td>4</td>\n",
       "      <td>2</td>\n",
       "      <td>10</td>\n",
       "      <td>0.32</td>\n",
       "      <td>&lt;class 'keras.optimizers.Nadam'&gt;</td>\n",
       "      <td>&lt;function categorical_crossentropy at 0xb1b3b6...</td>\n",
       "      <td>&lt;function softmax at 0xb1b3dc9d8&gt;</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>38</th>\n",
       "      <td>10</td>\n",
       "      <td>7</td>\n",
       "      <td>6</td>\n",
       "      <td>0.670706</td>\n",
       "      <td>0.66965</td>\n",
       "      <td>0.10235</td>\n",
       "      <td>0.102641</td>\n",
       "      <td>0.568356</td>\n",
       "      <td>0.567009</td>\n",
       "      <td>2</td>\n",
       "      <td>64</td>\n",
       "      <td>3</td>\n",
       "      <td>10</td>\n",
       "      <td>0</td>\n",
       "      <td>&lt;class 'keras.optimizers.Nadam'&gt;</td>\n",
       "      <td>&lt;function logcosh at 0xb1b3a4f28&gt;</td>\n",
       "      <td>&lt;function softmax at 0xb1b3dc9d8&gt;</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>39</th>\n",
       "      <td>10</td>\n",
       "      <td>9</td>\n",
       "      <td>9</td>\n",
       "      <td>0.670523</td>\n",
       "      <td>0.66965</td>\n",
       "      <td>0.102177</td>\n",
       "      <td>0.102515</td>\n",
       "      <td>0.568347</td>\n",
       "      <td>0.567135</td>\n",
       "      <td>3</td>\n",
       "      <td>16</td>\n",
       "      <td>3</td>\n",
       "      <td>10</td>\n",
       "      <td>0</td>\n",
       "      <td>&lt;class 'keras.optimizers.Nadam'&gt;</td>\n",
       "      <td>&lt;function logcosh at 0xb1b3a4f28&gt;</td>\n",
       "      <td>&lt;function softmax at 0xb1b3dc9d8&gt;</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>40</th>\n",
       "      <td>10</td>\n",
       "      <td>8</td>\n",
       "      <td>5</td>\n",
       "      <td>0.669823</td>\n",
       "      <td>0.670361</td>\n",
       "      <td>0.625648</td>\n",
       "      <td>0.624782</td>\n",
       "      <td>0.0441755</td>\n",
       "      <td>0.0455785</td>\n",
       "      <td>2</td>\n",
       "      <td>64</td>\n",
       "      <td>2</td>\n",
       "      <td>10</td>\n",
       "      <td>0.24</td>\n",
       "      <td>&lt;class 'keras.optimizers.Nadam'&gt;</td>\n",
       "      <td>&lt;function categorical_crossentropy at 0xb1b3b6...</td>\n",
       "      <td>&lt;function softmax at 0xb1b3dc9d8&gt;</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>41</th>\n",
       "      <td>10</td>\n",
       "      <td>9</td>\n",
       "      <td>9</td>\n",
       "      <td>0.670828</td>\n",
       "      <td>0.669579</td>\n",
       "      <td>0.102616</td>\n",
       "      <td>0.102661</td>\n",
       "      <td>0.568212</td>\n",
       "      <td>0.566919</td>\n",
       "      <td>2</td>\n",
       "      <td>128</td>\n",
       "      <td>3</td>\n",
       "      <td>10</td>\n",
       "      <td>0.24</td>\n",
       "      <td>&lt;class 'keras.optimizers.Nadam'&gt;</td>\n",
       "      <td>&lt;function logcosh at 0xb1b3a4f28&gt;</td>\n",
       "      <td>&lt;function softmax at 0xb1b3dc9d8&gt;</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>42</th>\n",
       "      <td>10</td>\n",
       "      <td>7</td>\n",
       "      <td>9</td>\n",
       "      <td>0.670919</td>\n",
       "      <td>0.670361</td>\n",
       "      <td>0.1029</td>\n",
       "      <td>0.102923</td>\n",
       "      <td>0.568019</td>\n",
       "      <td>0.567437</td>\n",
       "      <td>9</td>\n",
       "      <td>32</td>\n",
       "      <td>4</td>\n",
       "      <td>10</td>\n",
       "      <td>0.32</td>\n",
       "      <td>&lt;class 'keras.optimizers.Nadam'&gt;</td>\n",
       "      <td>&lt;function logcosh at 0xb1b3a4f28&gt;</td>\n",
       "      <td>&lt;function softmax at 0xb1b3dc9d8&gt;</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>43</th>\n",
       "      <td>10</td>\n",
       "      <td>7</td>\n",
       "      <td>8</td>\n",
       "      <td>0.670036</td>\n",
       "      <td>0.66965</td>\n",
       "      <td>0.103385</td>\n",
       "      <td>0.10307</td>\n",
       "      <td>0.566652</td>\n",
       "      <td>0.56658</td>\n",
       "      <td>3</td>\n",
       "      <td>4</td>\n",
       "      <td>2</td>\n",
       "      <td>10</td>\n",
       "      <td>0.24</td>\n",
       "      <td>&lt;class 'keras.optimizers.Nadam'&gt;</td>\n",
       "      <td>&lt;function logcosh at 0xb1b3a4f28&gt;</td>\n",
       "      <td>&lt;function softmax at 0xb1b3dc9d8&gt;</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>44</th>\n",
       "      <td>10</td>\n",
       "      <td>7</td>\n",
       "      <td>6</td>\n",
       "      <td>0.670006</td>\n",
       "      <td>0.66965</td>\n",
       "      <td>0.102242</td>\n",
       "      <td>0.102768</td>\n",
       "      <td>0.567763</td>\n",
       "      <td>0.566883</td>\n",
       "      <td>3</td>\n",
       "      <td>16</td>\n",
       "      <td>2</td>\n",
       "      <td>10</td>\n",
       "      <td>0</td>\n",
       "      <td>&lt;class 'keras.optimizers.Nadam'&gt;</td>\n",
       "      <td>&lt;function logcosh at 0xb1b3a4f28&gt;</td>\n",
       "      <td>&lt;function softmax at 0xb1b3dc9d8&gt;</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>45</th>\n",
       "      <td>10</td>\n",
       "      <td>9</td>\n",
       "      <td>6</td>\n",
       "      <td>0.669884</td>\n",
       "      <td>0.67029</td>\n",
       "      <td>0.624102</td>\n",
       "      <td>0.624645</td>\n",
       "      <td>0.0457823</td>\n",
       "      <td>0.0456446</td>\n",
       "      <td>4</td>\n",
       "      <td>128</td>\n",
       "      <td>2</td>\n",
       "      <td>10</td>\n",
       "      <td>0.2</td>\n",
       "      <td>&lt;class 'keras.optimizers.Nadam'&gt;</td>\n",
       "      <td>&lt;function categorical_crossentropy at 0xb1b3b6...</td>\n",
       "      <td>&lt;function softmax at 0xb1b3dc9d8&gt;</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>46</th>\n",
       "      <td>10</td>\n",
       "      <td>8</td>\n",
       "      <td>7</td>\n",
       "      <td>0.670036</td>\n",
       "      <td>0.66965</td>\n",
       "      <td>0.104719</td>\n",
       "      <td>0.104759</td>\n",
       "      <td>0.565317</td>\n",
       "      <td>0.564892</td>\n",
       "      <td>7</td>\n",
       "      <td>4</td>\n",
       "      <td>3</td>\n",
       "      <td>10</td>\n",
       "      <td>0.2</td>\n",
       "      <td>&lt;class 'keras.optimizers.Nadam'&gt;</td>\n",
       "      <td>&lt;function logcosh at 0xb1b3a4f28&gt;</td>\n",
       "      <td>&lt;function softmax at 0xb1b3dc9d8&gt;</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>47</th>\n",
       "      <td>10</td>\n",
       "      <td>9</td>\n",
       "      <td>6</td>\n",
       "      <td>0.670188</td>\n",
       "      <td>0.669864</td>\n",
       "      <td>0.625612</td>\n",
       "      <td>0.62485</td>\n",
       "      <td>0.0445762</td>\n",
       "      <td>0.0450132</td>\n",
       "      <td>7</td>\n",
       "      <td>4</td>\n",
       "      <td>2</td>\n",
       "      <td>10</td>\n",
       "      <td>0.28</td>\n",
       "      <td>&lt;class 'keras.optimizers.Adam'&gt;</td>\n",
       "      <td>&lt;function categorical_crossentropy at 0xb1b3b6...</td>\n",
       "      <td>&lt;function softmax at 0xb1b3dc9d8&gt;</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>48</th>\n",
       "      <td>10</td>\n",
       "      <td>7</td>\n",
       "      <td>6</td>\n",
       "      <td>0.67098</td>\n",
       "      <td>0.670148</td>\n",
       "      <td>0.102169</td>\n",
       "      <td>0.102827</td>\n",
       "      <td>0.568811</td>\n",
       "      <td>0.567321</td>\n",
       "      <td>2</td>\n",
       "      <td>8</td>\n",
       "      <td>3</td>\n",
       "      <td>10</td>\n",
       "      <td>0</td>\n",
       "      <td>&lt;class 'keras.optimizers.Adam'&gt;</td>\n",
       "      <td>&lt;function logcosh at 0xb1b3a4f28&gt;</td>\n",
       "      <td>&lt;function softmax at 0xb1b3dc9d8&gt;</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>49</th>\n",
       "      <td>10</td>\n",
       "      <td>9</td>\n",
       "      <td>9</td>\n",
       "      <td>0.670523</td>\n",
       "      <td>0.669793</td>\n",
       "      <td>0.623278</td>\n",
       "      <td>0.623884</td>\n",
       "      <td>0.0472451</td>\n",
       "      <td>0.0459082</td>\n",
       "      <td>8</td>\n",
       "      <td>64</td>\n",
       "      <td>2</td>\n",
       "      <td>10</td>\n",
       "      <td>0.08</td>\n",
       "      <td>&lt;class 'keras.optimizers.Nadam'&gt;</td>\n",
       "      <td>&lt;function categorical_crossentropy at 0xb1b3b6...</td>\n",
       "      <td>&lt;function softmax at 0xb1b3dc9d8&gt;</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50</th>\n",
       "      <td>10</td>\n",
       "      <td>8</td>\n",
       "      <td>7</td>\n",
       "      <td>0.670036</td>\n",
       "      <td>0.66965</td>\n",
       "      <td>0.62552</td>\n",
       "      <td>0.624926</td>\n",
       "      <td>0.044516</td>\n",
       "      <td>0.0447249</td>\n",
       "      <td>8</td>\n",
       "      <td>16</td>\n",
       "      <td>2</td>\n",
       "      <td>10</td>\n",
       "      <td>0.16</td>\n",
       "      <td>&lt;class 'keras.optimizers.Nadam'&gt;</td>\n",
       "      <td>&lt;function categorical_crossentropy at 0xb1b3b6...</td>\n",
       "      <td>&lt;function softmax at 0xb1b3dc9d8&gt;</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>51</th>\n",
       "      <td>10</td>\n",
       "      <td>9</td>\n",
       "      <td>8</td>\n",
       "      <td>0.670036</td>\n",
       "      <td>0.66965</td>\n",
       "      <td>0.626473</td>\n",
       "      <td>0.626713</td>\n",
       "      <td>0.043563</td>\n",
       "      <td>0.0429371</td>\n",
       "      <td>8</td>\n",
       "      <td>16</td>\n",
       "      <td>2</td>\n",
       "      <td>10</td>\n",
       "      <td>0.36</td>\n",
       "      <td>&lt;class 'keras.optimizers.Adam'&gt;</td>\n",
       "      <td>&lt;function categorical_crossentropy at 0xb1b3b6...</td>\n",
       "      <td>&lt;function softmax at 0xb1b3dc9d8&gt;</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>52</th>\n",
       "      <td>10</td>\n",
       "      <td>9</td>\n",
       "      <td>7</td>\n",
       "      <td>0.670706</td>\n",
       "      <td>0.670219</td>\n",
       "      <td>0.102462</td>\n",
       "      <td>0.102718</td>\n",
       "      <td>0.568245</td>\n",
       "      <td>0.567501</td>\n",
       "      <td>4</td>\n",
       "      <td>32</td>\n",
       "      <td>2</td>\n",
       "      <td>10</td>\n",
       "      <td>0.08</td>\n",
       "      <td>&lt;class 'keras.optimizers.Nadam'&gt;</td>\n",
       "      <td>&lt;function logcosh at 0xb1b3a4f28&gt;</td>\n",
       "      <td>&lt;function softmax at 0xb1b3dc9d8&gt;</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>53</th>\n",
       "      <td>10</td>\n",
       "      <td>9</td>\n",
       "      <td>5</td>\n",
       "      <td>0.670249</td>\n",
       "      <td>0.669508</td>\n",
       "      <td>0.623128</td>\n",
       "      <td>0.624469</td>\n",
       "      <td>0.0471216</td>\n",
       "      <td>0.045039</td>\n",
       "      <td>7</td>\n",
       "      <td>32</td>\n",
       "      <td>2</td>\n",
       "      <td>10</td>\n",
       "      <td>0.16</td>\n",
       "      <td>&lt;class 'keras.optimizers.Adam'&gt;</td>\n",
       "      <td>&lt;function categorical_crossentropy at 0xb1b3b6...</td>\n",
       "      <td>&lt;function softmax at 0xb1b3dc9d8&gt;</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>54</th>\n",
       "      <td>10</td>\n",
       "      <td>9</td>\n",
       "      <td>9</td>\n",
       "      <td>0.670767</td>\n",
       "      <td>0.669935</td>\n",
       "      <td>0.622755</td>\n",
       "      <td>0.624558</td>\n",
       "      <td>0.0480125</td>\n",
       "      <td>0.0453771</td>\n",
       "      <td>8</td>\n",
       "      <td>128</td>\n",
       "      <td>3</td>\n",
       "      <td>10</td>\n",
       "      <td>0.08</td>\n",
       "      <td>&lt;class 'keras.optimizers.Adam'&gt;</td>\n",
       "      <td>&lt;function categorical_crossentropy at 0xb1b3b6...</td>\n",
       "      <td>&lt;function softmax at 0xb1b3dc9d8&gt;</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>55</th>\n",
       "      <td>10</td>\n",
       "      <td>9</td>\n",
       "      <td>9</td>\n",
       "      <td>0.670371</td>\n",
       "      <td>0.669579</td>\n",
       "      <td>0.102254</td>\n",
       "      <td>0.102562</td>\n",
       "      <td>0.568117</td>\n",
       "      <td>0.567018</td>\n",
       "      <td>7</td>\n",
       "      <td>16</td>\n",
       "      <td>3</td>\n",
       "      <td>10</td>\n",
       "      <td>0.08</td>\n",
       "      <td>&lt;class 'keras.optimizers.Adam'&gt;</td>\n",
       "      <td>&lt;function logcosh at 0xb1b3a4f28&gt;</td>\n",
       "      <td>&lt;function softmax at 0xb1b3dc9d8&gt;</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>56</th>\n",
       "      <td>10</td>\n",
       "      <td>9</td>\n",
       "      <td>8</td>\n",
       "      <td>0.67095</td>\n",
       "      <td>0.669793</td>\n",
       "      <td>0.102528</td>\n",
       "      <td>0.102662</td>\n",
       "      <td>0.568422</td>\n",
       "      <td>0.56713</td>\n",
       "      <td>7</td>\n",
       "      <td>32</td>\n",
       "      <td>3</td>\n",
       "      <td>10</td>\n",
       "      <td>0.28</td>\n",
       "      <td>&lt;class 'keras.optimizers.Adam'&gt;</td>\n",
       "      <td>&lt;function logcosh at 0xb1b3a4f28&gt;</td>\n",
       "      <td>&lt;function softmax at 0xb1b3dc9d8&gt;</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "0  round_epochs train_peak val_peak train_acc   val_acc train_loss  val_loss  \\\n",
       "1            10          8        5  0.671102  0.669793   0.102212  0.102655   \n",
       "2            10          8        6  0.670767  0.670432   0.102477  0.102709   \n",
       "3            10          9        8   0.67031  0.669864   0.622116   0.62357   \n",
       "4            10          7        9  0.671041  0.670148   0.102457  0.102643   \n",
       "5            10          8        9  0.670432  0.669793   0.102367  0.102836   \n",
       "6            10          7        9  0.670797  0.669864   0.623974  0.624579   \n",
       "7            10          9        9  0.670036   0.66965    0.62655   0.62682   \n",
       "8            10          9        6  0.670493  0.669793   0.624413  0.624419   \n",
       "9            10          8        5  0.670554  0.669864   0.624511  0.624588   \n",
       "10           10          9        9  0.670036   0.66965   0.622782  0.623409   \n",
       "11           10          8        7  0.670828  0.670148    0.62437  0.625117   \n",
       "12           10          6        9  0.670889  0.669722   0.102449  0.102538   \n",
       "13           10          6        7  0.670036  0.670077    0.62393   0.62507   \n",
       "14           10          7        8  0.670676  0.670077   0.102353  0.103037   \n",
       "15           10          9        3  0.670584  0.670077   0.102084  0.102807   \n",
       "16           10          8        6  0.670919   0.67029   0.623055  0.624548   \n",
       "17           10          7        8  0.670188   0.66965   0.102992  0.102904   \n",
       "18           10          7        6  0.670706  0.670006    0.10232  0.102518   \n",
       "19           10          9        5  0.670036   0.66965   0.622998  0.624347   \n",
       "20           10          9        6   0.67028  0.670858   0.622062  0.625124   \n",
       "21           10          9        8  0.670858  0.670574   0.101999  0.102648   \n",
       "22           10          9        6  0.670584   0.66965   0.622684  0.624057   \n",
       "23           10          7        8  0.670554  0.670219   0.623173  0.623874   \n",
       "24           10          9        5  0.670493  0.669579   0.624345  0.624782   \n",
       "25           10          8        7  0.670249  0.670148   0.102811  0.102702   \n",
       "26           10          9        9  0.670158  0.669793   0.624623  0.624864   \n",
       "27           10          9        8   0.67028  0.670077   0.622529  0.624352   \n",
       "28           10          7        6  0.670554  0.670006   0.625212  0.624989   \n",
       "29           10          8        9  0.671254  0.670219   0.623793  0.625346   \n",
       "30           10          7        4  0.670158   0.66965   0.102175  0.102703   \n",
       "31           10          9        5  0.670036   0.66965   0.104719  0.104759   \n",
       "32           10          9        9   0.67031  0.669437   0.621554   0.62353   \n",
       "33           10          7        7  0.670341  0.669793   0.102802  0.102878   \n",
       "34           10          8        7  0.670371  0.669864   0.624742  0.625978   \n",
       "35           10          8        9  0.670676  0.670077   0.102278  0.102513   \n",
       "36           10          8        8  0.670341   0.66965   0.102138  0.102708   \n",
       "37           10          2        0  0.670036   0.66965   0.634303  0.634431   \n",
       "38           10          7        6  0.670706   0.66965    0.10235  0.102641   \n",
       "39           10          9        9  0.670523   0.66965   0.102177  0.102515   \n",
       "40           10          8        5  0.669823  0.670361   0.625648  0.624782   \n",
       "41           10          9        9  0.670828  0.669579   0.102616  0.102661   \n",
       "42           10          7        9  0.670919  0.670361     0.1029  0.102923   \n",
       "43           10          7        8  0.670036   0.66965   0.103385   0.10307   \n",
       "44           10          7        6  0.670006   0.66965   0.102242  0.102768   \n",
       "45           10          9        6  0.669884   0.67029   0.624102  0.624645   \n",
       "46           10          8        7  0.670036   0.66965   0.104719  0.104759   \n",
       "47           10          9        6  0.670188  0.669864   0.625612   0.62485   \n",
       "48           10          7        6   0.67098  0.670148   0.102169  0.102827   \n",
       "49           10          9        9  0.670523  0.669793   0.623278  0.623884   \n",
       "50           10          8        7  0.670036   0.66965    0.62552  0.624926   \n",
       "51           10          9        8  0.670036   0.66965   0.626473  0.626713   \n",
       "52           10          9        7  0.670706  0.670219   0.102462  0.102718   \n",
       "53           10          9        5  0.670249  0.669508   0.623128  0.624469   \n",
       "54           10          9        9  0.670767  0.669935   0.622755  0.624558   \n",
       "55           10          9        9  0.670371  0.669579   0.102254  0.102562   \n",
       "56           10          9        8   0.67095  0.669793   0.102528  0.102662   \n",
       "\n",
       "0  train_score  val_score lr first_neuron batch_size epochs dropout  \\\n",
       "1      0.56889   0.567137  2           64          3     10    0.04   \n",
       "2      0.56829   0.567723  9           16          4     10    0.24   \n",
       "3    0.0481947  0.0462941  2          128          3     10    0.04   \n",
       "4     0.568584   0.567505  6          128          3     10    0.24   \n",
       "5     0.568065   0.566957  9           16          3     10    0.08   \n",
       "6    0.0468236  0.0452846  7           64          3     10    0.16   \n",
       "7    0.0434862  0.0428309  8            4          4     10    0.24   \n",
       "8    0.0460796  0.0453735  5           32          4     10    0.24   \n",
       "9    0.0460428  0.0452751  8           16          4     10     0.2   \n",
       "10   0.0472542  0.0462416  3            4          3     10    0.04   \n",
       "11   0.0464578  0.0450305  7            8          4     10    0.28   \n",
       "12     0.56844   0.567183  5           16          4     10    0.08   \n",
       "13   0.0461058  0.0450069  5           64          3     10    0.28   \n",
       "14    0.568323   0.567039  3           32          4     10    0.12   \n",
       "15      0.5685    0.56727  3           64          4     10       0   \n",
       "16   0.0478639  0.0457421  8          128          2     10       0   \n",
       "17    0.567196   0.566747  5            8          4     10    0.24   \n",
       "18    0.568386   0.567488  6           16          3     10    0.08   \n",
       "19   0.0470385  0.0453039  2           16          3     10    0.08   \n",
       "20   0.0482177  0.0457342  3            8          4     10       0   \n",
       "21     0.56886   0.567926  7            8          3     10       0   \n",
       "22   0.0479003  0.0455933  2          128          4     10    0.12   \n",
       "23    0.047381  0.0463445  3           64          3     10    0.12   \n",
       "24   0.0461482  0.0447975  7          128          2     10    0.16   \n",
       "25    0.567438   0.567445  7           64          2     10    0.32   \n",
       "26   0.0455349  0.0449285  8           32          3     10    0.36   \n",
       "27   0.0477506  0.0457244  4           16          2     10    0.04   \n",
       "28   0.0453414  0.0450162  6           16          4     10    0.28   \n",
       "29   0.0474615  0.0448728  4            8          4     10    0.12   \n",
       "30    0.567983   0.566948  4            4          3     10       0   \n",
       "31    0.565317   0.564892  6            4          3     10    0.32   \n",
       "32   0.0487565  0.0459078  4            8          4     10       0   \n",
       "33    0.567539   0.566915  8           32          4     10    0.32   \n",
       "34   0.0456295  0.0438859  7          128          3     10    0.28   \n",
       "35    0.568398   0.567564  4           32          4     10    0.04   \n",
       "36    0.568202   0.566943  8            4          2     10       0   \n",
       "37   0.0357337  0.0352191  6            4          2     10    0.32   \n",
       "38    0.568356   0.567009  2           64          3     10       0   \n",
       "39    0.568347   0.567135  3           16          3     10       0   \n",
       "40   0.0441755  0.0455785  2           64          2     10    0.24   \n",
       "41    0.568212   0.566919  2          128          3     10    0.24   \n",
       "42    0.568019   0.567437  9           32          4     10    0.32   \n",
       "43    0.566652    0.56658  3            4          2     10    0.24   \n",
       "44    0.567763   0.566883  3           16          2     10       0   \n",
       "45   0.0457823  0.0456446  4          128          2     10     0.2   \n",
       "46    0.565317   0.564892  7            4          3     10     0.2   \n",
       "47   0.0445762  0.0450132  7            4          2     10    0.28   \n",
       "48    0.568811   0.567321  2            8          3     10       0   \n",
       "49   0.0472451  0.0459082  8           64          2     10    0.08   \n",
       "50    0.044516  0.0447249  8           16          2     10    0.16   \n",
       "51    0.043563  0.0429371  8           16          2     10    0.36   \n",
       "52    0.568245   0.567501  4           32          2     10    0.08   \n",
       "53   0.0471216   0.045039  7           32          2     10    0.16   \n",
       "54   0.0480125  0.0453771  8          128          3     10    0.08   \n",
       "55    0.568117   0.567018  7           16          3     10    0.08   \n",
       "56    0.568422    0.56713  7           32          3     10    0.28   \n",
       "\n",
       "0                          optimizer  \\\n",
       "1   <class 'keras.optimizers.Nadam'>   \n",
       "2   <class 'keras.optimizers.Nadam'>   \n",
       "3    <class 'keras.optimizers.Adam'>   \n",
       "4    <class 'keras.optimizers.Adam'>   \n",
       "5    <class 'keras.optimizers.Adam'>   \n",
       "6    <class 'keras.optimizers.Adam'>   \n",
       "7    <class 'keras.optimizers.Adam'>   \n",
       "8   <class 'keras.optimizers.Nadam'>   \n",
       "9   <class 'keras.optimizers.Nadam'>   \n",
       "10   <class 'keras.optimizers.Adam'>   \n",
       "11  <class 'keras.optimizers.Nadam'>   \n",
       "12   <class 'keras.optimizers.Adam'>   \n",
       "13   <class 'keras.optimizers.Adam'>   \n",
       "14  <class 'keras.optimizers.Nadam'>   \n",
       "15  <class 'keras.optimizers.Nadam'>   \n",
       "16  <class 'keras.optimizers.Nadam'>   \n",
       "17  <class 'keras.optimizers.Nadam'>   \n",
       "18   <class 'keras.optimizers.Adam'>   \n",
       "19  <class 'keras.optimizers.Nadam'>   \n",
       "20   <class 'keras.optimizers.Adam'>   \n",
       "21   <class 'keras.optimizers.Adam'>   \n",
       "22   <class 'keras.optimizers.Adam'>   \n",
       "23   <class 'keras.optimizers.Adam'>   \n",
       "24  <class 'keras.optimizers.Nadam'>   \n",
       "25  <class 'keras.optimizers.Nadam'>   \n",
       "26   <class 'keras.optimizers.Adam'>   \n",
       "27   <class 'keras.optimizers.Adam'>   \n",
       "28  <class 'keras.optimizers.Nadam'>   \n",
       "29   <class 'keras.optimizers.Adam'>   \n",
       "30  <class 'keras.optimizers.Nadam'>   \n",
       "31  <class 'keras.optimizers.Nadam'>   \n",
       "32  <class 'keras.optimizers.Nadam'>   \n",
       "33  <class 'keras.optimizers.Nadam'>   \n",
       "34   <class 'keras.optimizers.Adam'>   \n",
       "35  <class 'keras.optimizers.Nadam'>   \n",
       "36   <class 'keras.optimizers.Adam'>   \n",
       "37  <class 'keras.optimizers.Nadam'>   \n",
       "38  <class 'keras.optimizers.Nadam'>   \n",
       "39  <class 'keras.optimizers.Nadam'>   \n",
       "40  <class 'keras.optimizers.Nadam'>   \n",
       "41  <class 'keras.optimizers.Nadam'>   \n",
       "42  <class 'keras.optimizers.Nadam'>   \n",
       "43  <class 'keras.optimizers.Nadam'>   \n",
       "44  <class 'keras.optimizers.Nadam'>   \n",
       "45  <class 'keras.optimizers.Nadam'>   \n",
       "46  <class 'keras.optimizers.Nadam'>   \n",
       "47   <class 'keras.optimizers.Adam'>   \n",
       "48   <class 'keras.optimizers.Adam'>   \n",
       "49  <class 'keras.optimizers.Nadam'>   \n",
       "50  <class 'keras.optimizers.Nadam'>   \n",
       "51   <class 'keras.optimizers.Adam'>   \n",
       "52  <class 'keras.optimizers.Nadam'>   \n",
       "53   <class 'keras.optimizers.Adam'>   \n",
       "54   <class 'keras.optimizers.Adam'>   \n",
       "55   <class 'keras.optimizers.Adam'>   \n",
       "56   <class 'keras.optimizers.Adam'>   \n",
       "\n",
       "0                                                loss  \\\n",
       "1                   <function logcosh at 0xb1b3a4f28>   \n",
       "2                   <function logcosh at 0xb1b3a4f28>   \n",
       "3   <function categorical_crossentropy at 0xb1b3b6...   \n",
       "4                   <function logcosh at 0xb1b3a4f28>   \n",
       "5                   <function logcosh at 0xb1b3a4f28>   \n",
       "6   <function categorical_crossentropy at 0xb1b3b6...   \n",
       "7   <function categorical_crossentropy at 0xb1b3b6...   \n",
       "8   <function categorical_crossentropy at 0xb1b3b6...   \n",
       "9   <function categorical_crossentropy at 0xb1b3b6...   \n",
       "10  <function categorical_crossentropy at 0xb1b3b6...   \n",
       "11  <function categorical_crossentropy at 0xb1b3b6...   \n",
       "12                  <function logcosh at 0xb1b3a4f28>   \n",
       "13  <function categorical_crossentropy at 0xb1b3b6...   \n",
       "14                  <function logcosh at 0xb1b3a4f28>   \n",
       "15                  <function logcosh at 0xb1b3a4f28>   \n",
       "16  <function categorical_crossentropy at 0xb1b3b6...   \n",
       "17                  <function logcosh at 0xb1b3a4f28>   \n",
       "18                  <function logcosh at 0xb1b3a4f28>   \n",
       "19  <function categorical_crossentropy at 0xb1b3b6...   \n",
       "20  <function categorical_crossentropy at 0xb1b3b6...   \n",
       "21                  <function logcosh at 0xb1b3a4f28>   \n",
       "22  <function categorical_crossentropy at 0xb1b3b6...   \n",
       "23  <function categorical_crossentropy at 0xb1b3b6...   \n",
       "24  <function categorical_crossentropy at 0xb1b3b6...   \n",
       "25                  <function logcosh at 0xb1b3a4f28>   \n",
       "26  <function categorical_crossentropy at 0xb1b3b6...   \n",
       "27  <function categorical_crossentropy at 0xb1b3b6...   \n",
       "28  <function categorical_crossentropy at 0xb1b3b6...   \n",
       "29  <function categorical_crossentropy at 0xb1b3b6...   \n",
       "30                  <function logcosh at 0xb1b3a4f28>   \n",
       "31                  <function logcosh at 0xb1b3a4f28>   \n",
       "32  <function categorical_crossentropy at 0xb1b3b6...   \n",
       "33                  <function logcosh at 0xb1b3a4f28>   \n",
       "34  <function categorical_crossentropy at 0xb1b3b6...   \n",
       "35                  <function logcosh at 0xb1b3a4f28>   \n",
       "36                  <function logcosh at 0xb1b3a4f28>   \n",
       "37  <function categorical_crossentropy at 0xb1b3b6...   \n",
       "38                  <function logcosh at 0xb1b3a4f28>   \n",
       "39                  <function logcosh at 0xb1b3a4f28>   \n",
       "40  <function categorical_crossentropy at 0xb1b3b6...   \n",
       "41                  <function logcosh at 0xb1b3a4f28>   \n",
       "42                  <function logcosh at 0xb1b3a4f28>   \n",
       "43                  <function logcosh at 0xb1b3a4f28>   \n",
       "44                  <function logcosh at 0xb1b3a4f28>   \n",
       "45  <function categorical_crossentropy at 0xb1b3b6...   \n",
       "46                  <function logcosh at 0xb1b3a4f28>   \n",
       "47  <function categorical_crossentropy at 0xb1b3b6...   \n",
       "48                  <function logcosh at 0xb1b3a4f28>   \n",
       "49  <function categorical_crossentropy at 0xb1b3b6...   \n",
       "50  <function categorical_crossentropy at 0xb1b3b6...   \n",
       "51  <function categorical_crossentropy at 0xb1b3b6...   \n",
       "52                  <function logcosh at 0xb1b3a4f28>   \n",
       "53  <function categorical_crossentropy at 0xb1b3b6...   \n",
       "54  <function categorical_crossentropy at 0xb1b3b6...   \n",
       "55                  <function logcosh at 0xb1b3a4f28>   \n",
       "56                  <function logcosh at 0xb1b3a4f28>   \n",
       "\n",
       "0                     last_activation weight_regulizer emb_output_dims  \n",
       "1   <function softmax at 0xb1b3dc9d8>             None            None  \n",
       "2   <function softmax at 0xb1b3dc9d8>             None            None  \n",
       "3   <function softmax at 0xb1b3dc9d8>             None            None  \n",
       "4   <function softmax at 0xb1b3dc9d8>             None            None  \n",
       "5   <function softmax at 0xb1b3dc9d8>             None            None  \n",
       "6   <function softmax at 0xb1b3dc9d8>             None            None  \n",
       "7   <function softmax at 0xb1b3dc9d8>             None            None  \n",
       "8   <function softmax at 0xb1b3dc9d8>             None            None  \n",
       "9   <function softmax at 0xb1b3dc9d8>             None            None  \n",
       "10  <function softmax at 0xb1b3dc9d8>             None            None  \n",
       "11  <function softmax at 0xb1b3dc9d8>             None            None  \n",
       "12  <function softmax at 0xb1b3dc9d8>             None            None  \n",
       "13  <function softmax at 0xb1b3dc9d8>             None            None  \n",
       "14  <function softmax at 0xb1b3dc9d8>             None            None  \n",
       "15  <function softmax at 0xb1b3dc9d8>             None            None  \n",
       "16  <function softmax at 0xb1b3dc9d8>             None            None  \n",
       "17  <function softmax at 0xb1b3dc9d8>             None            None  \n",
       "18  <function softmax at 0xb1b3dc9d8>             None            None  \n",
       "19  <function softmax at 0xb1b3dc9d8>             None            None  \n",
       "20  <function softmax at 0xb1b3dc9d8>             None            None  \n",
       "21  <function softmax at 0xb1b3dc9d8>             None            None  \n",
       "22  <function softmax at 0xb1b3dc9d8>             None            None  \n",
       "23  <function softmax at 0xb1b3dc9d8>             None            None  \n",
       "24  <function softmax at 0xb1b3dc9d8>             None            None  \n",
       "25  <function softmax at 0xb1b3dc9d8>             None            None  \n",
       "26  <function softmax at 0xb1b3dc9d8>             None            None  \n",
       "27  <function softmax at 0xb1b3dc9d8>             None            None  \n",
       "28  <function softmax at 0xb1b3dc9d8>             None            None  \n",
       "29  <function softmax at 0xb1b3dc9d8>             None            None  \n",
       "30  <function softmax at 0xb1b3dc9d8>             None            None  \n",
       "31  <function softmax at 0xb1b3dc9d8>             None            None  \n",
       "32  <function softmax at 0xb1b3dc9d8>             None            None  \n",
       "33  <function softmax at 0xb1b3dc9d8>             None            None  \n",
       "34  <function softmax at 0xb1b3dc9d8>             None            None  \n",
       "35  <function softmax at 0xb1b3dc9d8>             None            None  \n",
       "36  <function softmax at 0xb1b3dc9d8>             None            None  \n",
       "37  <function softmax at 0xb1b3dc9d8>             None            None  \n",
       "38  <function softmax at 0xb1b3dc9d8>             None            None  \n",
       "39  <function softmax at 0xb1b3dc9d8>             None            None  \n",
       "40  <function softmax at 0xb1b3dc9d8>             None            None  \n",
       "41  <function softmax at 0xb1b3dc9d8>             None            None  \n",
       "42  <function softmax at 0xb1b3dc9d8>             None            None  \n",
       "43  <function softmax at 0xb1b3dc9d8>             None            None  \n",
       "44  <function softmax at 0xb1b3dc9d8>             None            None  \n",
       "45  <function softmax at 0xb1b3dc9d8>             None            None  \n",
       "46  <function softmax at 0xb1b3dc9d8>             None            None  \n",
       "47  <function softmax at 0xb1b3dc9d8>             None            None  \n",
       "48  <function softmax at 0xb1b3dc9d8>             None            None  \n",
       "49  <function softmax at 0xb1b3dc9d8>             None            None  \n",
       "50  <function softmax at 0xb1b3dc9d8>             None            None  \n",
       "51  <function softmax at 0xb1b3dc9d8>             None            None  \n",
       "52  <function softmax at 0xb1b3dc9d8>             None            None  \n",
       "53  <function softmax at 0xb1b3dc9d8>             None            None  \n",
       "54  <function softmax at 0xb1b3dc9d8>             None            None  \n",
       "55  <function softmax at 0xb1b3dc9d8>             None            None  \n",
       "56  <function softmax at 0xb1b3dc9d8>             None            None  "
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "h.result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5760"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "h.combinations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
