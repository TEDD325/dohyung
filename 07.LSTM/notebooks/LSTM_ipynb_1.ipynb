{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/link/anaconda3/lib/python3.6/site-packages/h5py/__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n",
      "Using TensorFlow backend.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using Keras version: 2.2.2 backend: tensorflow\n",
      "BTC_60_25_1_0.1_param.pickle FILE ALREADY EXIST.\n",
      "[INFO] X_train.shape : (41927, 8, 25, 4)\n",
      "[INFO] y_train.shape : (41927, 2)\n",
      "[INFO] X_test.shape : (10482, 8, 25, 4)\n",
      "[INFO] y_test.shape : (10482, 2)\n",
      "\n",
      "[INFO] X_train_2.shape: (41927, 25, 8, 4)\n",
      "[INFO] X_test_2.shape: (10482, 25, 8, 4)\n",
      "\n",
      "[INFO] X_train_3.shape: (41927, 25, 32)\n",
      "[INFO] X_test_3.shape: (10482, 25, 32)\n",
      "\n",
      "[INFO] X_train_reshape.shape: (41927, 800)\n",
      "[INFO] X_test_reshape.shape: (10482, 800)\n",
      "\n",
      "\n",
      "\n",
      "----------------------\n",
      "__ETH__time unit: 60  |  window_size :25  |  gap :1  |  margin_rate :0.1  started.\n",
      "Fitting 2 folds for each of 30 candidates, totalling 60 fits\n",
      "Train on 20963 samples, validate on 10482 samples\n",
      "Epoch 1/1\n",
      "11824/20963 [===============>..............] - ETA: 4:30 - loss: 1.9284 - _f1_score: 0.0822 - acc: 0.3988 - recall: 0.0822 - precision: 0.0822"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "LSTM\n",
    "순환 신경망\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "# import library\n",
    "import os\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"]=\"1\"\n",
    "import tensorflow as tf\n",
    "import keras\n",
    "from keras import __version__\n",
    "from keras import backend as K\n",
    "\n",
    "print('Using Keras version:', __version__, 'backend:', K.backend())\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, LSTM, Bidirectional, BatchNormalization, Embedding, CuDNNLSTM, Dropout\n",
    "from keras.wrappers.scikit_learn import KerasClassifier\n",
    "from keras.callbacks import EarlyStopping\n",
    "from sklearn.preprocessing import MinMaxScaler, StandardScaler\n",
    "from sklearn.model_selection import train_test_split, RandomizedSearchCV, GridSearchCV\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from datetime import datetime\n",
    "import sys\n",
    "import os.path\n",
    "import os\n",
    "os.environ['KMP_DUPLICATE_LIB_OK']='True'\n",
    "from os.path import isfile, join\n",
    "import boto3\n",
    "import pickle\n",
    "import time\n",
    "import warnings\n",
    "import smtplib\n",
    "from email.mime.text import MIMEText\n",
    "from keras.utils import multi_gpu_model\n",
    "# import email_info\n",
    "\n",
    "stopper = EarlyStopping(monitor='val_loss', patience=3, verbose=1, min_delta=0.001)\n",
    "sys.path.append(os.getcwd())\n",
    "warnings.filterwarnings('ignore')\n",
    "coins = {\n",
    "    0: 'KRW',\n",
    "    1: 'BTC',\n",
    "    2: 'ETH',\n",
    "    3: 'XRP',\n",
    "    4: 'BCH',\n",
    "    5: 'LTC',\n",
    "    6: 'DASH',\n",
    "    7: 'ETC'\n",
    "}\n",
    "# aws_client = boto3.client(\n",
    "#     's3',\n",
    "#     aws_access_key_id=LINK_AWSAccessKeyId,\n",
    "#     aws_secret_access_key=LINK_AWSSecretKey\n",
    "# )\n",
    "bucket = \"bithumb10\"\n",
    "cleanup_file_name = \"coin_{0}_{1}_cleanup.csv\"\n",
    "#######################################################\n",
    "def Load_Dataset_X(dir_path, time_unit, window_size, gap, margin_rate):\n",
    "    key_name_X = \"X_\" + \\\n",
    "                 str(time_unit) + \"_\" + \\\n",
    "                 str(window_size) + \"_\" + \\\n",
    "                 str(gap) + \"_\" + \\\n",
    "                 str(margin_rate)\n",
    "\n",
    "    with open(dir_path + key_name_X + \".pickle\", 'rb') as handle:\n",
    "        b_x = pickle.load(handle)\n",
    "    return b_x\n",
    "def Load_Dataset_y(dir_path, time_unit, window_size, gap, margin_rate):\n",
    "    key_name_y = \"y_\" + \\\n",
    "                 str(time_unit) + \"_\" + \\\n",
    "                 str(window_size) + \"_\" + \\\n",
    "                 str(gap) + \"_\" + \\\n",
    "                 str(margin_rate)\n",
    "\n",
    "    with open(dir_path + key_name_y + \".pickle\", 'rb') as handle:\n",
    "        b_y = pickle.load(handle)\n",
    "    return b_y\n",
    "def recall(y_true, y_pred):\n",
    "    K.set_epsilon(1e-05)\n",
    "    true_positives = K.sum(K.round(K.clip(y_true * y_pred, 0, 1)))\n",
    "    possible_positives = K.sum(K.round(K.clip(y_true, 0, 1)))\n",
    "    recall = true_positives / (possible_positives + K.epsilon())\n",
    "    return recall\n",
    "def precision(y_true, y_pred):\n",
    "    K.set_epsilon(1e-05)\n",
    "    true_positives = K.sum(K.round(K.clip(y_true * y_pred, 0, 1)))\n",
    "    predicted_positives = K.sum(K.round(K.clip(y_pred, 0, 1)))\n",
    "    precision = true_positives / (predicted_positives + K.epsilon())\n",
    "    return precision\n",
    "def _f1_score(y_true, y_pred):\n",
    "    def recall(y_true, y_pred):\n",
    "        K.set_epsilon(1e-05)\n",
    "        true_positives = K.sum(K.round(K.clip(y_true * y_pred, 0, 1)))\n",
    "        possible_positives = K.sum(K.round(K.clip(y_true, 0, 1)))\n",
    "        recall = true_positives / (possible_positives + K.epsilon())\n",
    "        return recall\n",
    "\n",
    "    def precision(y_true, y_pred):\n",
    "        K.set_epsilon(1e-05)\n",
    "        true_positives = K.sum(K.round(K.clip(y_true * y_pred, 0, 1)))\n",
    "        predicted_positives = K.sum(K.round(K.clip(y_pred, 0, 1)))\n",
    "        precision = true_positives / (predicted_positives + K.epsilon())\n",
    "        return precision\n",
    "\n",
    "    precision = precision(y_true, y_pred)\n",
    "    recall = recall(y_true, y_pred)\n",
    "    return 2 * ((precision * recall) / (precision + recall + K.epsilon()))\n",
    "def input_reshape(X_train_data, X_test_data, n_steps, n_coins, n_price):\n",
    "    X_train_reshape = X_train_data.reshape(\n",
    "        -1,\n",
    "        n_steps,\n",
    "        n_coins * n_price\n",
    "    )\n",
    "    X_test_reshape = X_test_data.reshape(\n",
    "        -1,\n",
    "        n_steps,\n",
    "        n_coins * n_price\n",
    "    )\n",
    "    return X_train_reshape, X_test_reshape\n",
    "def onehottify(x, n=None, dtype=np.int):\n",
    "    \"\"\"1-hot encode x with the max value n (computed from data if n is None).\"\"\"\n",
    "    x = np.asarray(x)\n",
    "    n = np.max(x) + 1 if n is None else n\n",
    "    return np.eye(n, dtype=dtype)[x]\n",
    "def sending_eamil(message):\n",
    "    f = open('../../../email_info.bin', 'rb')\n",
    "    a = pickle.load(f)\n",
    "\n",
    "    smtp = smtplib.SMTP_SSL('smtp.naver.com', 465)\n",
    "    smtp.ehlo()  # say Hello\n",
    "    # smtp.starttls()  # TLS 사용시 필요\n",
    "    smtp.login(a['email'], a['pw'])\n",
    "\n",
    "    msg = MIMEText(str(message)) \n",
    "    msg['Subject'] = '실험 완료'\n",
    "    msg['To'] = 'ulujo_dohk@naver.com'\n",
    "    smtp.sendmail(a['email'], 'ulujo_dohk@naver.com', msg.as_string())\n",
    "\n",
    "    smtp.quit()\n",
    "\n",
    "class SortedDisplayDict(dict):\n",
    "    def __str__(self):\n",
    "        return \"{\" + \", \".join(\"%r: %r\" % (key, self[key]) for key in sorted(self)) + \"}\"\n",
    "\n",
    "    def ordered_keys(self):\n",
    "        return sorted(self.keys())\n",
    "def search(dirname):\n",
    "    filenames = os.listdir(dirname)\n",
    "    fileList = []\n",
    "    for filename in filenames:\n",
    "        full_filename = os.path.join(dirname, filename)\n",
    "        fileList.append(full_filename)\n",
    "    return fileList\n",
    "def drawGraph(dir):\n",
    "    fileList = search(dir)\n",
    "    temp_list = []\n",
    "    cluster_coef_value_list_lstm = []\n",
    "    for file in fileList:\n",
    "        temp_list.append(pd.read_pickle(file))\n",
    "\n",
    "        cluster_coef_value_list_lstm = []\n",
    "    for i in range(len(temp_list)):\n",
    "        # print(temp_list[i])\n",
    "        # print()\n",
    "        # print(temp_list[i][list(temp_list[i].keys())[0]])\n",
    "        # print()\n",
    "        cluster_coef_value_list_lstm.append(temp_list[i][list(temp_list[i].keys())[0]]['Score'][0])\n",
    "        # print(score)\n",
    "        # print()\n",
    "\n",
    "    cluster_coef_value_list_gradientBoosting = [0.671, 0.616, 0.622, 0.672, 0.7, 0.69, 0.69]\n",
    "\n",
    "    first_legend_label = 'xgboost'\n",
    "    second_legend_label = 'lstm'\n",
    "    x_label = 'Cryptocurrency'\n",
    "    y_label = 'f1-score'\n",
    "    filename = str(datetime.now())\n",
    "\n",
    "    graph(cluster_coef_value_list_gradientBoosting,\n",
    "          cluster_coef_value_list_lstm[:7],\n",
    "          first_legend_label,\n",
    "          second_legend_label,\n",
    "          x_label,\n",
    "          y_label,\n",
    "          filename)\n",
    "def graph(cluster_coef_value_list_xgboost,\n",
    "          cluster_coef_value_list_lstm,\n",
    "          first_legend_label,\n",
    "          second_legend_label,\n",
    "          x_label,\n",
    "          y_label,\n",
    "          filename):\n",
    "    '''\n",
    "    < EXAMPLE >\n",
    "    f1_score = [0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9] # 9개\n",
    "    first_legend_label = 'xgboost'\n",
    "    second_legend_label = 'lstm'\n",
    "    x_label = 'Cryptocurrency'\n",
    "    y_label = 'f1-score'\n",
    "    filename = '_clustering_and_diameter'\n",
    "    '''\n",
    "    link_addition_ratio = f1_score\n",
    "    data_profile = {\n",
    "        \"Infocom05\": {\n",
    "            \"file_name\": \"data_infocom05.csv\",\n",
    "            \"num_nodes\": 41,\n",
    "            \"median\": 2684,\n",
    "            \"mean\": 9961,\n",
    "            \"std\": 26513,\n",
    "            \"contact_weight_map\": [((6, 9), 2187), ((12, 1), 4943), ((7, 12), 40849), ((1, 6), 768)],\n",
    "\n",
    "            \"graph\": {},\n",
    "            \"numberOfNodes\": {},\n",
    "            \"numberOfEdges\": {},\n",
    "            \"durationThreshold\": {},\n",
    "\n",
    "            \"density\": {},\n",
    "            \"clustering_coefficient\": {},\n",
    "            \"diameter_cc\": {},\n",
    "\n",
    "            \"global_bet\": {},\n",
    "            \"Brandes_ego_bet\": {},\n",
    "            \"Brandes_ego_elapsed_time\": {},\n",
    "            \"Brandes_xego_bet\": {},\n",
    "            \"Brandes_xego_elapsed_time\": {},\n",
    "\n",
    "            \"Proposed_ego_bet\": {},\n",
    "            \"Proposed_ego_elapsed_time\": {},\n",
    "            \"Proposed_xego_bet\": {},\n",
    "            \"Proposed_xego_elapsed_time\": {},\n",
    "\n",
    "            \"ego_global_pearson_corr\": {},\n",
    "            \"xego_global_pearson_corr\": {},\n",
    "\n",
    "            \"ego_global_spearman_corr\": {},\n",
    "            \"xego_global_spearman_corr\": {},\n",
    "\n",
    "            \"ego_node_coverage_in_connected_component\": {},\n",
    "            \"ego_edge_coverage_in_connected_component\": {},\n",
    "\n",
    "            \"xego_node_coverage_in_connected_component\": {},\n",
    "            \"xego_edge_coverage_in_connected_component\": {}\n",
    "        }\n",
    "    }\n",
    "    xticklabels = [r'$BTC$', r'$ETH$', r'$XRP$', r'$BCH$', r'$LTC$', r'$DASH$', r'$ETC$']\n",
    "    yticklabels = [r'$0.40$', r'$0.45$', r'$0.50$', r'$0.55$', r'$0.60$', r'$0.65$', r'$0.70$', r'$0.75$', r'$0.80$']\n",
    "    # yticklabels2 = [r'$0$', r'$2$', r'$4$', r'$6$', r'$8$', r'$10$', r'$12$']\n",
    "\n",
    "    ind = np.arange(len(xticklabels))\n",
    "\n",
    "    fig, axes = plt.subplots(1, 1, figsize=(8.719, 6.07))\n",
    "    subfigures = {}\n",
    "    data_name = 'Infocom05'\n",
    "    subfigures[data_name] = axes\n",
    "    subfigures[data_name].set_xticks(ind)\n",
    "    subfigures[data_name].set_xticklabels(xticklabels, fontsize=21)\n",
    "    subfigures[data_name].set_ylim([0.4, 0.8])\n",
    "    subfigures[data_name].set_yticklabels(yticklabels, fontsize=21)\n",
    "\n",
    "    cluster_coef_dic = SortedDisplayDict(data_profile[data_name]['clustering_coefficient'])\n",
    "    cluster_coef_value_list = [cluster_coef_dic[x] for x in cluster_coef_dic.ordered_keys()]\n",
    "\n",
    "    # cluster_coef_value_list_xgboost = [0.56, 0.67, 0.66, 0.78, 0.45, 0.47, 0.65]\n",
    "    # cluster_coef_value_list_lstm = [0.66, 0.77, 0.68, 0.68, 0.55, 0.57, 0.66]\n",
    "    cluster_coef_value_list_xgboost = cluster_coef_value_list_xgboost\n",
    "    cluster_coef_value_list_lstm = cluster_coef_value_list_lstm\n",
    "    subfigures[data_name].plot(ind, cluster_coef_value_list_xgboost,\n",
    "                               color='k', linestyle='-', marker='s', markersize=8,\n",
    "                               label=first_legend_label)\n",
    "    subfigures[data_name].plot(ind, cluster_coef_value_list_lstm,\n",
    "                               color='k', linestyle='--', marker='^', markersize=8,\n",
    "                               label=second_legend_label)\n",
    "    subfigures[data_name].set_xlabel(x_label, fontsize=21)\n",
    "    subfigures[data_name].set_ylabel(y_label, fontsize=21)\n",
    "\n",
    "    # subfigures[data_name].set_title(data_name, fontsize=21)\n",
    "    subfigures[data_name].grid(True)\n",
    "    if data_name == 'Infocom05':\n",
    "        subfigures[data_name].legend(loc=30, fontsize=18)\n",
    "\n",
    "    # subfigures[data_name] = subfigures[data_name].twinx()\n",
    "    # subfigures[data_name].set_ylim([0, 12])\n",
    "    # subfigures[data_name].set_yticklabels(yticklabels2, fontsize=21)\n",
    "\n",
    "    diameter_dic = SortedDisplayDict(data_profile[data_name]['diameter_cc'])\n",
    "    diameter_list = [diameter_dic[x] for x in diameter_dic.ordered_keys()]\n",
    "    # subfigures[data_name].bar(ind, diameter_list, barWidth, color='k', alpha=0.3, label='Diameter of Connected Component')\n",
    "    # subfigures[data_name].set_ylabel('Diameter of Connected Component', fontsize=21)\n",
    "    if data_name == 'Infocom05':\n",
    "        subfigures[data_name].legend(loc=4, fontsize=18)\n",
    "    subfigures[data_name].grid(True)\n",
    "\n",
    "    fig.savefig('./img/' + filename + '.pdf', format='pdf', bbox_inches='tight')\n",
    "    plt.show()\n",
    "\n",
    "def vanilla_LSTM(window_size, units_1, units_2, n_state_units=32, activation_1='softmax', activation_2='relu', optimizer='adam'):\n",
    "    #     global metrics\n",
    "    K.clear_session()\n",
    "    config = tf.ConfigProto()\n",
    "    config.gpu_options.per_process_gpu_memory_fraction = 0.4  # what portion of gpu to use\n",
    "    session = tf.Session(config=config)\n",
    "    K.set_session(session)\n",
    "    \n",
    "    model = Sequential()\n",
    "    model.add(\n",
    "        LSTM(units=n_state_units,\n",
    "             activation=activation_1,\n",
    "             input_shape=(window_size, 32)))\n",
    "    model.add(Dense(2))\n",
    "    # model = multi_gpu_model(model, gpus=2)\n",
    "    model.compile(loss=keras.losses.categorical_crossentropy,\n",
    "                  optimizer=optimizer,\n",
    "                  metrics=[_f1_score, 'accuracy', recall, precision])\n",
    "\n",
    "    return model\n",
    "def stacked_LSTM(window_size, n_state_units=32, activation_1='softmax', activation_2='relu', optimizer='adam'):\n",
    "    #     global metrics\n",
    "    K.clear_session()\n",
    "    model = Sequential()\n",
    "    model.add(\n",
    "        LSTM(units=n_state_units,\n",
    "             activation=activation_1,\n",
    "             return_sequences=True,\n",
    "             input_shape=(window_size, 32)))\n",
    "    model.add(\n",
    "        LSTM(units=n_state_units,\n",
    "             activation=activation_2))\n",
    "    model.add(Dense(2))\n",
    "    model.compile(loss=keras.losses.categorical_crossentropy,\n",
    "                  optimizer=optimizer,\n",
    "                  metrics=[_f1_score, 'accuracy', recall, precision])\n",
    "\n",
    "    return model\n",
    "def bidirectioanl_LSTM(window_size, units_1, units_2, n_state_units=32, activation_1='softmax', activation_2='relu', optimizer='adam'):\n",
    "    #     global metrics\n",
    "    K.clear_session()\n",
    "    model = Sequential()\n",
    "    model.add(\n",
    "        Bidirectional(\n",
    "            LSTM(units=n_state_units,\n",
    "                 activation=activation_1),\n",
    "            input_shape=(window_size, 32)))\n",
    "    model.add(Dense(2))\n",
    "    model.compile(loss=keras.losses.categorical_crossentropy,\n",
    "                  optimizer=optimizer,\n",
    "                  metrics=[_f1_score, 'accuracy', recall, precision])\n",
    "\n",
    "    return model\n",
    "def bidirectioanl_LSTM_with_BN(window_size, units_1, units_2, n_state_units=32, activation_1='softmax', activation_2='relu', optimizer='adam'):\n",
    "    # https://keras.io/layers/normalization/\n",
    "    #     global metrics\n",
    "    K.clear_session()\n",
    "    model = Sequential()\n",
    "    model.add(\n",
    "        Bidirectional(\n",
    "            LSTM(units=n_state_units,\n",
    "                 activation=activation_1),\n",
    "            input_shape=(window_size, 32)))\n",
    "    model.add(BatchNormalization())\n",
    "    model.add(Dense(2))\n",
    "    model.compile(loss=keras.losses.categorical_crossentropy,\n",
    "                  optimizer=optimizer,\n",
    "                  metrics=[_f1_score, 'accuracy', recall, precision])\n",
    "\n",
    "    return model\n",
    "def stacked_bidirectioanl_LSTM(window_size, units_1, units_2, n_state_units=32, activation_1='softmax', activation_2='relu', optimizer='adam'):\n",
    "    #     global metrics\n",
    "    K.clear_session()\n",
    "    model = Sequential()\n",
    "    model.add(\n",
    "        Bidirectional(\n",
    "            LSTM(units=n_state_units,\n",
    "                 activation=activation_1,\n",
    "                 return_sequences=True),\n",
    "            input_shape=(window_size, 32)))\n",
    "    model.add(\n",
    "        Bidirectional(\n",
    "            LSTM(units=n_state_units,\n",
    "                 activation=activation_2)))\n",
    "    model.add(Dense(2))\n",
    "    model.compile(loss=keras.losses.categorical_crossentropy,\n",
    "                  optimizer=optimizer,\n",
    "                  metrics=[_f1_score, 'accuracy', recall, precision])\n",
    "\n",
    "    return model\n",
    "def cuDNN_LSTM(window_size, units_1, units_2, n_state_units=32, activation_1='softmax', activation_2='relu', optimizer='adam'):\n",
    "    #     global metrics\n",
    "    K.clear_session()\n",
    "    model = Sequential()\n",
    "    model.add(\n",
    "        Bidirectional(\n",
    "            LSTM(units=n_state_units,\n",
    "                 activation=activation_1,\n",
    "                 return_sequences=True),\n",
    "            input_shape=(window_size, 32)))\n",
    "    model.add(\n",
    "        Bidirectional(\n",
    "            LSTM(units=n_state_units,\n",
    "                 activation=activation_2)))\n",
    "    model.add(Dense(2))\n",
    "    model.compile(loss=keras.losses.categorical_crossentropy,\n",
    "                  optimizer=optimizer,\n",
    "                  metrics=[_f1_score, 'accuracy', recall, precision])\n",
    "\n",
    "    return model\n",
    "def advanced_LSTM(window_size, activation, optimizer='adam', n_state_units=32):\n",
    "    # https://keras.io/layers/normalization/\n",
    "    #     global metrics\n",
    "    K.clear_session()\n",
    "    model = Sequential([\n",
    "        Bidirectional(LSTM(n_state_units, activation=activation, recurrent_activation='sigmoid')),\n",
    "        # BatchNormalization(), # BatchNormailization을 적용하면, nan값이 뜬다.\n",
    "        Dropout(0.2),\n",
    "        Dense(2)\n",
    "    ])\n",
    "    model.compile(loss=keras.losses.categorical_crossentropy,\n",
    "                  optimizer=optimizer,\n",
    "                  metrics=[_f1_score, 'accuracy', recall, precision])\n",
    "\n",
    "    return model\n",
    "\n",
    "\n",
    "def Start_Model():\n",
    "\n",
    "    pickle_load_dir_path = './data/RNN_coin/'\n",
    "    pickle_result_dir_path = './evaluate_result/'\n",
    "\n",
    "    idx_time_unit = 60 # 10, 30, 60\n",
    "    idx_window_size = 25 # 25, 50, 100\n",
    "    idx_gap = 1\n",
    "    idx_margin_rate = 0.1\n",
    "    epochs = 1\n",
    "    # _GPU = True\n",
    "    n_jobs = 1\n",
    "    cv = 2\n",
    "    n_iter = 30 # maximum 30\n",
    "    dataset_scale = -1 # [:10000] for test\n",
    "\n",
    "    # scaler = MinMaxScaler(feature_range=(-1, 1))\n",
    "    scaler = StandardScaler()\n",
    "\n",
    "    param_grid = {'window_size': [idx_window_size],\n",
    "                  'units_1': [32, 64],\n",
    "                  'units_2': [32, 64],\n",
    "                  'n_state_units': [64, 128],\n",
    "                  'activation_1': ['tanh', 'sigmoid', 'relu'],\n",
    "                  'activation_2': ['tanh', 'sigmoid', 'relu'],\n",
    "                  'optimizer': ['rmsprop', 'Adam', 'SGD']}\n",
    "\n",
    "    param_grid_test = {'window_size': [idx_window_size],\n",
    "                  'units_1': [16],\n",
    "                  'n_state_units': [32],\n",
    "                  'activation_1': ['relu'],\n",
    "                  'activation_2': ['relu'],\n",
    "                  'optimizer': ['Adam']}\n",
    "\n",
    "    param_grid_advanced_LSTM = {'window_size': [idx_window_size],\n",
    "                  'n_state_units': [32, 128],\n",
    "                  'activation': ['tanh', 'sigmoid', 'relu'],\n",
    "                  'optimizer': ['rmsprop', 'Adam', 'SGD']}\n",
    "\n",
    "    param_grid_stacked_LSTM = {'window_size': [idx_window_size],\n",
    "                                'n_state_units': [32],\n",
    "                                'activation_1': ['sigmoid', 'relu'],\n",
    "                                'activation_2': ['tanh', 'sigmoid', 'relu'],\n",
    "                                'optimizer': ['rmsprop', 'Adam', 'SGD']}\n",
    "\n",
    "    model = KerasClassifier(build_fn=vanilla_LSTM,\n",
    "                            epochs=epochs,\n",
    "                            batch_size=300,\n",
    "                            verbose=True)\n",
    "\n",
    "    grid = RandomizedSearchCV(estimator=model,\n",
    "                              param_distributions=param_grid,\n",
    "                              n_iter=n_iter,\n",
    "                              cv=cv,\n",
    "                              random_state=42,\n",
    "                              n_jobs=n_jobs,\n",
    "                              verbose=1)\n",
    "\n",
    "    key_name_X = \"X_\"\n",
    "    key_name_y = \"y_\"\n",
    "\n",
    "    key_name_X += str(idx_time_unit) + \"_\" + str(idx_window_size) + \"_\" + str(idx_gap) + \"_\" + str(idx_margin_rate)\n",
    "    key_name_y += str(idx_time_unit) + \"_\" + str(idx_window_size) + \"_\" + str(idx_gap) + \"_\" + str(idx_margin_rate)\n",
    "\n",
    "    # remove [:10000], when real training\n",
    "    X = Load_Dataset_X(pickle_load_dir_path, idx_time_unit, idx_window_size, idx_gap, idx_margin_rate)[0][\n",
    "        :dataset_scale]\n",
    "    y = Load_Dataset_y(pickle_load_dir_path, idx_time_unit, idx_window_size, idx_gap, idx_margin_rate)[1][\n",
    "        :dataset_scale]\n",
    "\n",
    "    y_single = {}\n",
    "    #     print(\"[INFO] y : {}\".format(y))\n",
    "    #     y = np.asarray(y[0])\n",
    "    #     print(\"[INFO] y.shape : {}\".format(y.shape))\n",
    "    #     print(\"[INFO] y : {}\".format(y))\n",
    "    y_single['BTC'] = y[:, 1]\n",
    "    y_single['ETH'] = y[:, 2]\n",
    "    y_single['XRP'] = y[:, 3]\n",
    "    y_single['BCH'] = y[:, 4]\n",
    "    y_single['LTC'] = y[:, 5]\n",
    "    y_single['DASH'] = y[:, 6]\n",
    "    y_single['ETC'] = y[:, 7]\n",
    "\n",
    "    coin_list2 = [\"BTC\", \"ETH\", \"XRP\", \"BCH\", \"LTC\", \"DASH\", \"ETC\"]\n",
    "\n",
    "    for coin in coin_list2:\n",
    "        if (os.path.isfile(pickle_result_dir_path + \\\n",
    "                           coin + \"_\" + \\\n",
    "                           str(idx_time_unit) + \"_\" + \\\n",
    "                           str(idx_window_size) + \"_\" + \\\n",
    "                           str(idx_gap) + \"_\" + \\\n",
    "                           str(idx_margin_rate) + \\\n",
    "                           \"_param.pickle\")) is True:\n",
    "            print(coin + \"_\" + \\\n",
    "                  str(idx_time_unit) + \"_\" + \\\n",
    "                  str(idx_window_size) + \"_\" + \\\n",
    "                  str(idx_gap) + \"_\" + \\\n",
    "                  str(idx_margin_rate) + \\\n",
    "                  \"_param.pickle FILE ALREADY EXIST.\")\n",
    "            continue\n",
    "        elif (os.path.isfile(pickle_result_dir_path + \\\n",
    "                             coin + \"_\" + \\\n",
    "                             str(idx_time_unit) + \"_\" + \\\n",
    "                             str(idx_window_size) + \"_\" + \\\n",
    "                             str(idx_gap) + \"_\" + \\\n",
    "                             str(idx_margin_rate) + \\\n",
    "                             \"_result.pickle\")) is True:\n",
    "            print(coin + \"_\" + \\\n",
    "                  str(idx_time_unit) + \"_\" + \\\n",
    "                  str(idx_window_size) + \"_\" + \\\n",
    "                  str(idx_gap) + \"_\" + \\\n",
    "                  str(idx_margin_rate) + \\\n",
    "                  \"_result.pickle FILE ALREADY EXIST.\")\n",
    "            continue\n",
    "        else:\n",
    "            y2 = onehottify(y_single[coin], n=2)\n",
    "\n",
    "            X_train, X_test, y_train, y_test = train_test_split(X,\n",
    "                                                                y2,\n",
    "                                                                test_size=0.2,\n",
    "                                                                random_state=42,\n",
    "                                                                shuffle=True)\n",
    "\n",
    "            print(\"[INFO] X_train.shape : {}\".format(X_train.shape))\n",
    "            print(\"[INFO] y_train.shape : {}\".format(y_train.shape))\n",
    "            print(\"[INFO] X_test.shape : {}\".format(X_test.shape))\n",
    "            print(\"[INFO] y_test.shape : {}\".format(y_test.shape))\n",
    "            print()\n",
    "\n",
    "            n_coins = 8\n",
    "            n_price = 4\n",
    "            n_steps = idx_window_size\n",
    "\n",
    "            X_train_2 = X_train.transpose([0, 2, 1, 3])\n",
    "            X_test_2 = X_test.transpose([0, 2, 1, 3])\n",
    "            print(\"[INFO] X_train_2.shape: {}\".format(X_train_2.shape))\n",
    "            print(\"[INFO] X_test_2.shape: {}\".format(X_test_2.shape))\n",
    "            print()\n",
    "\n",
    "            X_train_3 = X_train_2.reshape([X_train.shape[0], n_steps, n_coins * n_price])\n",
    "            X_test_3 = X_test_2.reshape([X_test.shape[0], n_steps, n_coins * n_price])\n",
    "            print(\"[INFO] X_train_3.shape: {}\".format(X_train_3.shape))\n",
    "            print(\"[INFO] X_test_3.shape: {}\".format(X_test_3.shape))\n",
    "            print()\n",
    "\n",
    "            X_train_reshape = X_train_2.reshape([X_train.shape[0], n_steps * n_coins * n_price])\n",
    "            X_test_reshape = X_test_2.reshape([X_test.shape[0], n_steps * n_coins * n_price])\n",
    "            print(\"[INFO] X_train_reshape.shape: {}\".format(X_train_reshape.shape))\n",
    "            print(\"[INFO] X_test_reshape.shape: {}\".format(X_test_reshape.shape))\n",
    "            print()\n",
    "            \n",
    "            scaler.fit(X_train_reshape)\n",
    "\n",
    "            # X_train_scaled = scaler.transform(X_train_reshape)\n",
    "            # X_test_scaled = scaler.transform(X_test_reshape)\n",
    "            X_train_scaled = scaler.fit_transform(X_train_reshape)\n",
    "            X_test_scaled = scaler.fit_transform(X_test_reshape)\n",
    "\n",
    "            X_train_scaled = X_train_scaled.reshape(-1,\n",
    "                                                    n_steps,\n",
    "                                                    n_coins * n_price)\n",
    "            X_test_scaled = X_test_scaled.reshape(-1,\n",
    "                                                  n_steps,\n",
    "                                                  n_coins * n_price)\n",
    "\n",
    "\n",
    "            # model = KerasClassifier(build_fn=stacked_LSTM,\n",
    "            #                         epochs=epochs,\n",
    "            #                         batch_size=300,\n",
    "            #                         verbose=True)\n",
    "\n",
    "\n",
    "            #             grid = GridSearchCV(estimator=model,\n",
    "            #                                 cv=cv,\n",
    "            #                                 n_jobs=n_jobs, # test\n",
    "            #                                 param_grid=param_grid,\n",
    "            #                                 verbose=1)\n",
    "\n",
    "            # grid = RandomizedSearchCV(estimator=model,\n",
    "            #                           param_distributions=param_grid,\n",
    "            #                           n_iter=n_iter,\n",
    "            #                           cv=cv,\n",
    "            #                           random_state=42,\n",
    "            #                           n_jobs=n_jobs,\n",
    "            #                           verbose=1)\n",
    "\n",
    "            X_train_scaled, X_test_scaled = input_reshape(X_train_scaled,\n",
    "                                                          X_test_scaled,\n",
    "                                                          n_steps,\n",
    "                                                          n_coins,\n",
    "                                                          n_price)\n",
    "\n",
    "            print()\n",
    "            print()\n",
    "            print(\"----------------------\")\n",
    "            print(\"__\" + coin + \"__\" + \\\n",
    "                  \"time unit: \" + str(idx_time_unit) + \"  |  \" + \\\n",
    "                  \"window_size :\" + str(idx_window_size) + \"  |  \" + \\\n",
    "                  \"gap :\" + str(idx_gap) + \"  |  \" + \\\n",
    "                  \"margin_rate :\" + str(idx_margin_rate) + \\\n",
    "                  \"  started.\")\n",
    "\n",
    "            #             fit_params = dict(callbacks=[stopper])\n",
    "\n",
    "            with K.tf.device('/gpu:0'):\n",
    "                grid_result = grid.fit(X_train_scaled,\n",
    "                                       y_train,\n",
    "                                       validation_data=(X_test_scaled,\n",
    "                                                        y_test),\n",
    "                                       batch_size=1,\n",
    "                                       callbacks=[stopper])\n",
    "            # grid_result = grid_result.reset_states()\n",
    "\n",
    "            print(\"----------------------\")\n",
    "            print(\"grid_result.score(X_test_scaled, y_test): \", grid_result.score(X_test_scaled, y_test))\n",
    "\n",
    "            evaluate_result = {}\n",
    "            test_score = grid_result.score(X_test_scaled, y_test)\n",
    "            evaluate_result[coin + \"_\" + \\\n",
    "                            str(idx_time_unit) + \"_\" + \\\n",
    "                            str(idx_window_size) + \"_\" + \\\n",
    "                            str(idx_gap) + \"_\" + \\\n",
    "                            str(idx_margin_rate)] = {\"Cryptocurrency\": coin, \\\n",
    "                                                     \"Score\": grid_result.cv_results_['mean_test_score'], \\\n",
    "                                                     \"Params\": grid_result.cv_results_['params'], \\\n",
    "                                                     \"test_score\": test_score}\n",
    "            #     print()\n",
    "            #     print(\"evaluate result dict: \", evaluate_result)\n",
    "            #     print()\n",
    "\n",
    "            # summarize results\n",
    "            print()\n",
    "            print(\"Best: %f using %s\" % (grid_result.best_score_, grid_result.best_params_))\n",
    "            print()\n",
    "            # for checking pickle file exist\n",
    "            print(\"---pickle saving..\")\n",
    "\n",
    "            X = {}\n",
    "            y = {}\n",
    "            key_name_X = \"X_\"\n",
    "            key_name_y = \"y_\"\n",
    "\n",
    "            key_name_X += str(idx_time_unit) + \"_\" + str(idx_window_size) + \"_\" + str(idx_gap) + \"_\" + str(\n",
    "                idx_margin_rate)\n",
    "            key_name_y += str(idx_time_unit) + \"_\" + str(idx_window_size) + \"_\" + str(idx_gap) + \"_\" + str(\n",
    "                idx_margin_rate)\n",
    "            if (os.path.isfile(pickle_result_dir_path + \\\n",
    "                               coin + \"_\" + \\\n",
    "                               str(idx_time_unit) + \"_\" + \\\n",
    "                               str(idx_window_size) + \"_\" + \\\n",
    "                               str(idx_gap) + \"_\" + \\\n",
    "                               str(idx_margin_rate) + \\\n",
    "                               \"_param.pickle\")) is not True:\n",
    "                with open(pickle_result_dir_path + \\\n",
    "                          coin + \"_\" + \\\n",
    "                          str(idx_time_unit) + \"_\" + \\\n",
    "                          str(idx_window_size) + \"_\" + \\\n",
    "                          str(idx_gap) + \"_\" + \\\n",
    "                          str(idx_margin_rate) + \\\n",
    "                          \"_param.pickle\", 'wb') as handle:\n",
    "                    pickle.dump(evaluate_result, handle, protocol=pickle.HIGHEST_PROTOCOL)\n",
    "                    sending_eamil(evaluate_result)\n",
    "\n",
    "            return grid_result\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    Start_Model()\n",
    "\n",
    "    # dir = \"./evaluate_result/\"\n",
    "    # dir = dir + model_info_2 + '/'\n",
    "    # time_unit_list = [10]\n",
    "    # window_size_list = [10, 50, 75, 100]\n",
    "    #\n",
    "    # for time_unit in time_unit_list:\n",
    "    #     # dir = dir + str(time_unit) + '/'\n",
    "    #     dir = \"./evaluate_result/\"\n",
    "    #     print(dir)\n",
    "    #     drawGraph(dir)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
