{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Using XGBoost in Python\n",
    "XGBoost(Extreme Gradient Boosting) is one of the most popular machine learning algorithm these days. Regardless of the type of prediction task at hand; regression or classification.<br>\n",
    "XGBoost is well known to provide better solutions than other machine learning algorithms.<br>\n",
    "In this tutorial, you’ll learn to build machine learning models using XGBoost in python. <br>\n",
    "<br>\n",
    "what makes XGBoost so popular?<br>\n",
    "- Speed and performance : Originally written in C++, it is comparatively faster than other ensemble classifiers.\n",
    "- Consistently outperforms other algorithm methods : It has shown better performance on a variety of machine learning benchmark datasets.\n",
    "- Wide variety of tuning parameters : XGBoost internally has parameters for cross-validation, regularization, user-defined objective functions, missing values, tree parameters, scikit-learn compatible API etc.\n",
    "It is an optimized distributed gradient boosting library. But wait, what is boosting? Well, keep on reading.<br>\n",
    "\n",
    "## Advantage\n",
    "- Regularization:\n",
    "  - Standard GBM implementation has no <a href=\"https://www.analyticsvidhya.com/blog/2015/02/avoid-over-fitting-regularization/\">regularization</a> like XGBoost, therefore it also helps to reduce overfitting.\n",
    "  - In fact, XGBoost is also known as ‘regularized boosting‘ technique.\n",
    "  \n",
    "- Parallel Processing:\n",
    "  - XGBoost implements parallel processing and is blazingly faster as compared to GBM.\n",
    "  - But hang on, we know that <a href=\"https://www.analyticsvidhya.com/blog/2015/11/quick-introduction-boosting-algorithms-machine-learning/\">boosting</a> is sequential process so how can it be parallelized? We know that each tree can be built only after the previous one, so what stops us from making a tree using all cores? I hope you get where I’m coming from. Check <a href=\"http://zhanpengfang.github.io/418home.html\">this link</a> out to explore further.\n",
    "  - XGBoost also supports implementation on Hadoop.\n",
    "\n",
    "- High Flexibility\n",
    "  - XGBoost allow users to define custom optimization objectives and evaluation criteria.\n",
    "  - This adds a whole new dimension to the model and there is no limit to what we can do.\n",
    "\n",
    "- Handling Missing Values\n",
    "  - XGBoost has an in-built routine to handle missing values.\n",
    "  - User is required to supply a different value than other observations and pass that as a parameter. XGBoost tries different things as it encounters a missing value on each node and learns which path to take for missing values in future.\n",
    "\n",
    "- Tree Pruning:\n",
    "  - A GBM would stop splitting a node when it encounters a negative loss in the split. Thus it is more of a greedy algorithm.\n",
    "  - XGBoost on the other hand make splits upto the max_depth specified and then start pruning the tree backwards and remove splits beyond which there is no positive gain.\n",
    "  - Another advantage is that sometimes a split of negative loss say -2 may be followed by a split of positive loss +10. GBM would stop as it encounters -2. But XGBoost will go deeper and it will see a combined effect of +8 of the split and keep both.\n",
    "\n",
    "- Built-in Cross-Validation\n",
    "  - XGBoost allows user to run a cross-validation at each iteration of the boosting process and thus it is easy to get the exact optimum number of boosting iterations in a single run.\n",
    "  - This is unlike GBM where we have to run a grid-search and only a limited values can be tested.\n",
    "\n",
    "- Continue on Existing Model\n",
    "  - User can start training an XGBoost model from its last iteration of previous run. This can be of significant advantage in certain specific applications.\n",
    "  - GBM implementation of sklearn also has this feature so they are even on this point.\n",
    "\n",
    "You can refer to following web-pages for a deeper understanding:\n",
    "<a href=\"https://xgboost.readthedocs.io/en/latest/model.html\">XGBoost Guide – Introduction to Boosted Trees</a>\n",
    "\n",
    "## Boosting\n",
    "Boosting is a sequential technique which works on the principle of an ensemble.  It combines a set of weak learners and delivers improved prediction accuracy. At any instant t, the model outcomes are weighed based on the outcomes of previous instant t-1. The outcomes predicted correctly are given a lower weight and the ones miss-classified are weighted higher. Note that a weak learner is one which is slightly better than random guessing. For example, a decision tree whose predictions are slightly better than 50%. Let's understand boosting in general with a simple illustration.<br>\n",
    "![](http://res.cloudinary.com/dyd911kmh/image/upload/f_auto,q_auto:best/v1528107577/output_1_0_ilvuyr.png)\n",
    "<br>\n",
    "Four classifiers (in 4 boxes), shown above, are trying to classify + and - classes as homogeneously as possible.\n",
    "\n",
    "- 1. Box 1: The first classifier (usually a decision stump) creates a vertical line (split) at D1. It says anything to the left of D1 is + and anything to the right of D1 is -. However, this classifier misclassifies three + points.\n",
    "a Decision Stump is a Decision Tree model that only splits off at one level, therefore the final prediction is based on only one feature.\n",
    "- 2. Box 2: The second classifier gives more weight to the three + misclassified points (see the bigger size of +) and creates a vertical line at D2. Again it says, anything to the right of D2 is - and left is +. Still, it makes mistakes by incorrectly classifying three - points.\n",
    "- 3. Box 3: Again, the third classifier gives more weight to the three - misclassified points and creates a horizontal line at D3. Still, this classifier fails to classify the points (in the circles) correctly.\n",
    "- 4. Box 4: This is a weighted combination of the weak classifiers (Box 1,2 and 3). As you can see, it does a good job at classifying all the points correctly.\n",
    "\n",
    "That's the basic idea behind boosting algorithms is building a weak model, making conclusions about the various feature importance and parameters, and then using those conclusions to build a new, stronger model and capitalize on the misclassification error of the previous model and try to reduce it. Now, let's come to XGBoost. The tree ensemble model is a set of classification and regression trees (CART). To begin with, you should know about the default base learners of XGBoost: tree ensembles. The tree ensemble model is a set of classification and regression trees (CART). Trees are grown one after another ,and attempts to reduce the misclassification rate are made in subsequent iterations. \n",
    "<br><br>\n",
    "If you check the image in Tree Ensemble section, you will notice each tree gives a different prediction score depending on the data it sees and the scores of each individual tree are summed up to get the final score.\n",
    "<br><br>\n",
    "In this tutorial, you will be using XGBoost to solve a regression problem. The dataset is taken from the UCI Machine Learning Repository and is also present in sklearn's datasets module. It has 14 explanatory variables describing various aspects of residential homes in Boston, the challenge is to predict the median value of owner-occupied homes per $1000s."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Using XGBoost in Python\n",
    "First of all, you are going to import the Boston Housing dataset and store it in a variable called boston. To import it from scikit-learn you will need to run this snippet."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import load_boston\n",
    "boston = load_boston()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The boston variable itself is a dictionary, so you can check for its keys using the .keys() method."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[6.3200e-03, 1.8000e+01, 2.3100e+00, ..., 1.5300e+01, 3.9690e+02,\n",
       "        4.9800e+00],\n",
       "       [2.7310e-02, 0.0000e+00, 7.0700e+00, ..., 1.7800e+01, 3.9690e+02,\n",
       "        9.1400e+00],\n",
       "       [2.7290e-02, 0.0000e+00, 7.0700e+00, ..., 1.7800e+01, 3.9283e+02,\n",
       "        4.0300e+00],\n",
       "       ...,\n",
       "       [6.0760e-02, 0.0000e+00, 1.1930e+01, ..., 2.1000e+01, 3.9690e+02,\n",
       "        5.6400e+00],\n",
       "       [1.0959e-01, 0.0000e+00, 1.1930e+01, ..., 2.1000e+01, 3.9345e+02,\n",
       "        6.4800e+00],\n",
       "       [4.7410e-02, 0.0000e+00, 1.1930e+01, ..., 2.1000e+01, 3.9690e+02,\n",
       "        7.8800e+00]])"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "boston['data']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dict_keys(['data', 'target', 'feature_names', 'DESCR'])\n"
     ]
    }
   ],
   "source": [
    "print(boston.keys())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can easily check for its shape by using the boston.data.shape attribute, which will return the size of the dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(506, 13)\n"
     ]
    }
   ],
   "source": [
    "print(boston.data.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As you can see it returned (506, 13), that means there are 506 rows of data with 13 columns. Now, if you want to know what the 13 columns are, you can simply use the .feature_names attribute and it will return the feature names."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['CRIM' 'ZN' 'INDUS' 'CHAS' 'NOX' 'RM' 'AGE' 'DIS' 'RAD' 'TAX' 'PTRATIO'\n",
      " 'B' 'LSTAT']\n"
     ]
    }
   ],
   "source": [
    "print(boston.feature_names)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The description of the dataset is available in the dataset itself. You can take a look at it using .DESCR."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Boston House Prices dataset\n",
      "===========================\n",
      "\n",
      "Notes\n",
      "------\n",
      "Data Set Characteristics:  \n",
      "\n",
      "    :Number of Instances: 506 \n",
      "\n",
      "    :Number of Attributes: 13 numeric/categorical predictive\n",
      "    \n",
      "    :Median Value (attribute 14) is usually the target\n",
      "\n",
      "    :Attribute Information (in order):\n",
      "        - CRIM     per capita crime rate by town\n",
      "        - ZN       proportion of residential land zoned for lots over 25,000 sq.ft.\n",
      "        - INDUS    proportion of non-retail business acres per town\n",
      "        - CHAS     Charles River dummy variable (= 1 if tract bounds river; 0 otherwise)\n",
      "        - NOX      nitric oxides concentration (parts per 10 million)\n",
      "        - RM       average number of rooms per dwelling\n",
      "        - AGE      proportion of owner-occupied units built prior to 1940\n",
      "        - DIS      weighted distances to five Boston employment centres\n",
      "        - RAD      index of accessibility to radial highways\n",
      "        - TAX      full-value property-tax rate per $10,000\n",
      "        - PTRATIO  pupil-teacher ratio by town\n",
      "        - B        1000(Bk - 0.63)^2 where Bk is the proportion of blacks by town\n",
      "        - LSTAT    % lower status of the population\n",
      "        - MEDV     Median value of owner-occupied homes in $1000's\n",
      "\n",
      "    :Missing Attribute Values: None\n",
      "\n",
      "    :Creator: Harrison, D. and Rubinfeld, D.L.\n",
      "\n",
      "This is a copy of UCI ML housing dataset.\n",
      "http://archive.ics.uci.edu/ml/datasets/Housing\n",
      "\n",
      "\n",
      "This dataset was taken from the StatLib library which is maintained at Carnegie Mellon University.\n",
      "\n",
      "The Boston house-price data of Harrison, D. and Rubinfeld, D.L. 'Hedonic\n",
      "prices and the demand for clean air', J. Environ. Economics & Management,\n",
      "vol.5, 81-102, 1978.   Used in Belsley, Kuh & Welsch, 'Regression diagnostics\n",
      "...', Wiley, 1980.   N.B. Various transformations are used in the table on\n",
      "pages 244-261 of the latter.\n",
      "\n",
      "The Boston house-price data has been used in many machine learning papers that address regression\n",
      "problems.   \n",
      "     \n",
      "**References**\n",
      "\n",
      "   - Belsley, Kuh & Welsch, 'Regression diagnostics: Identifying Influential Data and Sources of Collinearity', Wiley, 1980. 244-261.\n",
      "   - Quinlan,R. (1993). Combining Instance-Based and Model-Based Learning. In Proceedings on the Tenth International Conference of Machine Learning, 236-243, University of Massachusetts, Amherst. Morgan Kaufmann.\n",
      "   - many more! (see http://archive.ics.uci.edu/ml/datasets/Housing)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(boston.DESCR)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let’s convert it into a pandas DataFrame! For that you need to import the pandas library and call the DataFrame() function passing the argument boston.data. To label the names of the columns, use the .columnns attribute of the pandas DataFrame and assign it to boston.feature_names."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "data = pd.DataFrame(boston.data)\n",
    "data.columns = boston.feature_names"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Explore the top 5 rows of the dataset by using head() method on your pandas DataFrame."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>CRIM</th>\n",
       "      <th>ZN</th>\n",
       "      <th>INDUS</th>\n",
       "      <th>CHAS</th>\n",
       "      <th>NOX</th>\n",
       "      <th>RM</th>\n",
       "      <th>AGE</th>\n",
       "      <th>DIS</th>\n",
       "      <th>RAD</th>\n",
       "      <th>TAX</th>\n",
       "      <th>PTRATIO</th>\n",
       "      <th>B</th>\n",
       "      <th>LSTAT</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.00632</td>\n",
       "      <td>18.0</td>\n",
       "      <td>2.31</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.538</td>\n",
       "      <td>6.575</td>\n",
       "      <td>65.2</td>\n",
       "      <td>4.0900</td>\n",
       "      <td>1.0</td>\n",
       "      <td>296.0</td>\n",
       "      <td>15.3</td>\n",
       "      <td>396.90</td>\n",
       "      <td>4.98</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.02731</td>\n",
       "      <td>0.0</td>\n",
       "      <td>7.07</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.469</td>\n",
       "      <td>6.421</td>\n",
       "      <td>78.9</td>\n",
       "      <td>4.9671</td>\n",
       "      <td>2.0</td>\n",
       "      <td>242.0</td>\n",
       "      <td>17.8</td>\n",
       "      <td>396.90</td>\n",
       "      <td>9.14</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.02729</td>\n",
       "      <td>0.0</td>\n",
       "      <td>7.07</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.469</td>\n",
       "      <td>7.185</td>\n",
       "      <td>61.1</td>\n",
       "      <td>4.9671</td>\n",
       "      <td>2.0</td>\n",
       "      <td>242.0</td>\n",
       "      <td>17.8</td>\n",
       "      <td>392.83</td>\n",
       "      <td>4.03</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.03237</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2.18</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.458</td>\n",
       "      <td>6.998</td>\n",
       "      <td>45.8</td>\n",
       "      <td>6.0622</td>\n",
       "      <td>3.0</td>\n",
       "      <td>222.0</td>\n",
       "      <td>18.7</td>\n",
       "      <td>394.63</td>\n",
       "      <td>2.94</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.06905</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2.18</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.458</td>\n",
       "      <td>7.147</td>\n",
       "      <td>54.2</td>\n",
       "      <td>6.0622</td>\n",
       "      <td>3.0</td>\n",
       "      <td>222.0</td>\n",
       "      <td>18.7</td>\n",
       "      <td>396.90</td>\n",
       "      <td>5.33</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      CRIM    ZN  INDUS  CHAS    NOX     RM   AGE     DIS  RAD    TAX  \\\n",
       "0  0.00632  18.0   2.31   0.0  0.538  6.575  65.2  4.0900  1.0  296.0   \n",
       "1  0.02731   0.0   7.07   0.0  0.469  6.421  78.9  4.9671  2.0  242.0   \n",
       "2  0.02729   0.0   7.07   0.0  0.469  7.185  61.1  4.9671  2.0  242.0   \n",
       "3  0.03237   0.0   2.18   0.0  0.458  6.998  45.8  6.0622  3.0  222.0   \n",
       "4  0.06905   0.0   2.18   0.0  0.458  7.147  54.2  6.0622  3.0  222.0   \n",
       "\n",
       "   PTRATIO       B  LSTAT  \n",
       "0     15.3  396.90   4.98  \n",
       "1     17.8  396.90   9.14  \n",
       "2     17.8  392.83   4.03  \n",
       "3     18.7  394.63   2.94  \n",
       "4     18.7  396.90   5.33  "
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You'll notice that there is no column called PRICE in the DataFrame. This is because the target column is available in another attribute called boston.target. Append boston.target to your pandas DataFrame."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "data['PRICE'] = boston.target"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Run the .info() method on your DataFrame to get useful information about the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 506 entries, 0 to 505\n",
      "Data columns (total 14 columns):\n",
      "CRIM       506 non-null float64\n",
      "ZN         506 non-null float64\n",
      "INDUS      506 non-null float64\n",
      "CHAS       506 non-null float64\n",
      "NOX        506 non-null float64\n",
      "RM         506 non-null float64\n",
      "AGE        506 non-null float64\n",
      "DIS        506 non-null float64\n",
      "RAD        506 non-null float64\n",
      "TAX        506 non-null float64\n",
      "PTRATIO    506 non-null float64\n",
      "B          506 non-null float64\n",
      "LSTAT      506 non-null float64\n",
      "PRICE      506 non-null float64\n",
      "dtypes: float64(14)\n",
      "memory usage: 55.4 KB\n"
     ]
    }
   ],
   "source": [
    "data.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Turns out that this dataset has 14 columns (including the target variable PRICE) and 506 rows. Notice that the columns are of float data-type indicating the presence of only continuous features with no missing values in any of the columns. To get more summary statistics of the different features in the dataset you will use the describe() method on your DataFrame.<br>\n",
    "that describe() only gives summary statistics of columns which are continuous in nature and <U>not categorical</U>."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>CRIM</th>\n",
       "      <th>ZN</th>\n",
       "      <th>INDUS</th>\n",
       "      <th>CHAS</th>\n",
       "      <th>NOX</th>\n",
       "      <th>RM</th>\n",
       "      <th>AGE</th>\n",
       "      <th>DIS</th>\n",
       "      <th>RAD</th>\n",
       "      <th>TAX</th>\n",
       "      <th>PTRATIO</th>\n",
       "      <th>B</th>\n",
       "      <th>LSTAT</th>\n",
       "      <th>PRICE</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>506.000000</td>\n",
       "      <td>506.000000</td>\n",
       "      <td>506.000000</td>\n",
       "      <td>506.000000</td>\n",
       "      <td>506.000000</td>\n",
       "      <td>506.000000</td>\n",
       "      <td>506.000000</td>\n",
       "      <td>506.000000</td>\n",
       "      <td>506.000000</td>\n",
       "      <td>506.000000</td>\n",
       "      <td>506.000000</td>\n",
       "      <td>506.000000</td>\n",
       "      <td>506.000000</td>\n",
       "      <td>506.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>3.593761</td>\n",
       "      <td>11.363636</td>\n",
       "      <td>11.136779</td>\n",
       "      <td>0.069170</td>\n",
       "      <td>0.554695</td>\n",
       "      <td>6.284634</td>\n",
       "      <td>68.574901</td>\n",
       "      <td>3.795043</td>\n",
       "      <td>9.549407</td>\n",
       "      <td>408.237154</td>\n",
       "      <td>18.455534</td>\n",
       "      <td>356.674032</td>\n",
       "      <td>12.653063</td>\n",
       "      <td>22.532806</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>8.596783</td>\n",
       "      <td>23.322453</td>\n",
       "      <td>6.860353</td>\n",
       "      <td>0.253994</td>\n",
       "      <td>0.115878</td>\n",
       "      <td>0.702617</td>\n",
       "      <td>28.148861</td>\n",
       "      <td>2.105710</td>\n",
       "      <td>8.707259</td>\n",
       "      <td>168.537116</td>\n",
       "      <td>2.164946</td>\n",
       "      <td>91.294864</td>\n",
       "      <td>7.141062</td>\n",
       "      <td>9.197104</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>0.006320</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.460000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.385000</td>\n",
       "      <td>3.561000</td>\n",
       "      <td>2.900000</td>\n",
       "      <td>1.129600</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>187.000000</td>\n",
       "      <td>12.600000</td>\n",
       "      <td>0.320000</td>\n",
       "      <td>1.730000</td>\n",
       "      <td>5.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>0.082045</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>5.190000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.449000</td>\n",
       "      <td>5.885500</td>\n",
       "      <td>45.025000</td>\n",
       "      <td>2.100175</td>\n",
       "      <td>4.000000</td>\n",
       "      <td>279.000000</td>\n",
       "      <td>17.400000</td>\n",
       "      <td>375.377500</td>\n",
       "      <td>6.950000</td>\n",
       "      <td>17.025000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>0.256510</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>9.690000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.538000</td>\n",
       "      <td>6.208500</td>\n",
       "      <td>77.500000</td>\n",
       "      <td>3.207450</td>\n",
       "      <td>5.000000</td>\n",
       "      <td>330.000000</td>\n",
       "      <td>19.050000</td>\n",
       "      <td>391.440000</td>\n",
       "      <td>11.360000</td>\n",
       "      <td>21.200000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>3.647423</td>\n",
       "      <td>12.500000</td>\n",
       "      <td>18.100000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.624000</td>\n",
       "      <td>6.623500</td>\n",
       "      <td>94.075000</td>\n",
       "      <td>5.188425</td>\n",
       "      <td>24.000000</td>\n",
       "      <td>666.000000</td>\n",
       "      <td>20.200000</td>\n",
       "      <td>396.225000</td>\n",
       "      <td>16.955000</td>\n",
       "      <td>25.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>88.976200</td>\n",
       "      <td>100.000000</td>\n",
       "      <td>27.740000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.871000</td>\n",
       "      <td>8.780000</td>\n",
       "      <td>100.000000</td>\n",
       "      <td>12.126500</td>\n",
       "      <td>24.000000</td>\n",
       "      <td>711.000000</td>\n",
       "      <td>22.000000</td>\n",
       "      <td>396.900000</td>\n",
       "      <td>37.970000</td>\n",
       "      <td>50.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "             CRIM          ZN       INDUS        CHAS         NOX          RM  \\\n",
       "count  506.000000  506.000000  506.000000  506.000000  506.000000  506.000000   \n",
       "mean     3.593761   11.363636   11.136779    0.069170    0.554695    6.284634   \n",
       "std      8.596783   23.322453    6.860353    0.253994    0.115878    0.702617   \n",
       "min      0.006320    0.000000    0.460000    0.000000    0.385000    3.561000   \n",
       "25%      0.082045    0.000000    5.190000    0.000000    0.449000    5.885500   \n",
       "50%      0.256510    0.000000    9.690000    0.000000    0.538000    6.208500   \n",
       "75%      3.647423   12.500000   18.100000    0.000000    0.624000    6.623500   \n",
       "max     88.976200  100.000000   27.740000    1.000000    0.871000    8.780000   \n",
       "\n",
       "              AGE         DIS         RAD         TAX     PTRATIO           B  \\\n",
       "count  506.000000  506.000000  506.000000  506.000000  506.000000  506.000000   \n",
       "mean    68.574901    3.795043    9.549407  408.237154   18.455534  356.674032   \n",
       "std     28.148861    2.105710    8.707259  168.537116    2.164946   91.294864   \n",
       "min      2.900000    1.129600    1.000000  187.000000   12.600000    0.320000   \n",
       "25%     45.025000    2.100175    4.000000  279.000000   17.400000  375.377500   \n",
       "50%     77.500000    3.207450    5.000000  330.000000   19.050000  391.440000   \n",
       "75%     94.075000    5.188425   24.000000  666.000000   20.200000  396.225000   \n",
       "max    100.000000   12.126500   24.000000  711.000000   22.000000  396.900000   \n",
       "\n",
       "            LSTAT       PRICE  \n",
       "count  506.000000  506.000000  \n",
       "mean    12.653063   22.532806  \n",
       "std      7.141062    9.197104  \n",
       "min      1.730000    5.000000  \n",
       "25%      6.950000   17.025000  \n",
       "50%     11.360000   21.200000  \n",
       "75%     16.955000   25.000000  \n",
       "max     37.970000   50.000000  "
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you plan to use XGBoost on a dataset which has categorical features you may want to consider applying some encoding (like one-hot encoding) to such features before training the model. Also, if you have some missing values such as NA in the dataset you may or may not do a separate treatment for them, because XGBoost is capable of handling missing values internally. You can check out this <a href=\"https://github.com/dmlc/xgboost/issues/21\">link</a> if you wish to know more on this."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Without delving into more exploratory analysis and feature engineering, you will now focus on applying the algorithm to train the model on this data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You will build the model using Trees as base learners (which are the default base learners) using XGBoost's scikit-learn compatible API. Along the way, you will also learn some of the common tuning parameters which XGBoost provides in order to improve the model's performance, and using the root mean squared error (RMSE) performance metric to check the performance of the trained model on the test set. Root mean Squared error is the square root of the mean of the squared differences between the actual and the predicted values. As usual, you start by importing the library xgboost and other important libraries that you will be using for building the model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "you can install python libraries like xgboost on your system using pip install xgboost on cmd."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ! pip install xgboost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "import xgboost as xgb\n",
    "'''\n",
    "Note that I have imported 2 forms of XGBoost:\n",
    "\n",
    "xgb – this is the direct xgboost library. I will use a specific function “cv” from this library\n",
    "XGBClassifier – this is an sklearn wrapper for XGBoost. This allows us to use sklearn’s Grid Search with parallel processing in the same way we did for GBM\n",
    "'''\n",
    "from sklearn.metrics import mean_squared_error\n",
    "import pandas as pd\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Separate the target variable and rest of the variables using .iloc to subset the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "X, y = data.iloc[:,:-1],data.iloc[:,-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>CRIM</th>\n",
       "      <th>ZN</th>\n",
       "      <th>INDUS</th>\n",
       "      <th>CHAS</th>\n",
       "      <th>NOX</th>\n",
       "      <th>RM</th>\n",
       "      <th>AGE</th>\n",
       "      <th>DIS</th>\n",
       "      <th>RAD</th>\n",
       "      <th>TAX</th>\n",
       "      <th>PTRATIO</th>\n",
       "      <th>B</th>\n",
       "      <th>LSTAT</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.00632</td>\n",
       "      <td>18.0</td>\n",
       "      <td>2.31</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.538</td>\n",
       "      <td>6.575</td>\n",
       "      <td>65.2</td>\n",
       "      <td>4.0900</td>\n",
       "      <td>1.0</td>\n",
       "      <td>296.0</td>\n",
       "      <td>15.3</td>\n",
       "      <td>396.90</td>\n",
       "      <td>4.98</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.02731</td>\n",
       "      <td>0.0</td>\n",
       "      <td>7.07</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.469</td>\n",
       "      <td>6.421</td>\n",
       "      <td>78.9</td>\n",
       "      <td>4.9671</td>\n",
       "      <td>2.0</td>\n",
       "      <td>242.0</td>\n",
       "      <td>17.8</td>\n",
       "      <td>396.90</td>\n",
       "      <td>9.14</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.02729</td>\n",
       "      <td>0.0</td>\n",
       "      <td>7.07</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.469</td>\n",
       "      <td>7.185</td>\n",
       "      <td>61.1</td>\n",
       "      <td>4.9671</td>\n",
       "      <td>2.0</td>\n",
       "      <td>242.0</td>\n",
       "      <td>17.8</td>\n",
       "      <td>392.83</td>\n",
       "      <td>4.03</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.03237</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2.18</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.458</td>\n",
       "      <td>6.998</td>\n",
       "      <td>45.8</td>\n",
       "      <td>6.0622</td>\n",
       "      <td>3.0</td>\n",
       "      <td>222.0</td>\n",
       "      <td>18.7</td>\n",
       "      <td>394.63</td>\n",
       "      <td>2.94</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.06905</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2.18</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.458</td>\n",
       "      <td>7.147</td>\n",
       "      <td>54.2</td>\n",
       "      <td>6.0622</td>\n",
       "      <td>3.0</td>\n",
       "      <td>222.0</td>\n",
       "      <td>18.7</td>\n",
       "      <td>396.90</td>\n",
       "      <td>5.33</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>0.02985</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2.18</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.458</td>\n",
       "      <td>6.430</td>\n",
       "      <td>58.7</td>\n",
       "      <td>6.0622</td>\n",
       "      <td>3.0</td>\n",
       "      <td>222.0</td>\n",
       "      <td>18.7</td>\n",
       "      <td>394.12</td>\n",
       "      <td>5.21</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>0.08829</td>\n",
       "      <td>12.5</td>\n",
       "      <td>7.87</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.524</td>\n",
       "      <td>6.012</td>\n",
       "      <td>66.6</td>\n",
       "      <td>5.5605</td>\n",
       "      <td>5.0</td>\n",
       "      <td>311.0</td>\n",
       "      <td>15.2</td>\n",
       "      <td>395.60</td>\n",
       "      <td>12.43</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>0.14455</td>\n",
       "      <td>12.5</td>\n",
       "      <td>7.87</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.524</td>\n",
       "      <td>6.172</td>\n",
       "      <td>96.1</td>\n",
       "      <td>5.9505</td>\n",
       "      <td>5.0</td>\n",
       "      <td>311.0</td>\n",
       "      <td>15.2</td>\n",
       "      <td>396.90</td>\n",
       "      <td>19.15</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>0.21124</td>\n",
       "      <td>12.5</td>\n",
       "      <td>7.87</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.524</td>\n",
       "      <td>5.631</td>\n",
       "      <td>100.0</td>\n",
       "      <td>6.0821</td>\n",
       "      <td>5.0</td>\n",
       "      <td>311.0</td>\n",
       "      <td>15.2</td>\n",
       "      <td>386.63</td>\n",
       "      <td>29.93</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>0.17004</td>\n",
       "      <td>12.5</td>\n",
       "      <td>7.87</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.524</td>\n",
       "      <td>6.004</td>\n",
       "      <td>85.9</td>\n",
       "      <td>6.5921</td>\n",
       "      <td>5.0</td>\n",
       "      <td>311.0</td>\n",
       "      <td>15.2</td>\n",
       "      <td>386.71</td>\n",
       "      <td>17.10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>0.22489</td>\n",
       "      <td>12.5</td>\n",
       "      <td>7.87</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.524</td>\n",
       "      <td>6.377</td>\n",
       "      <td>94.3</td>\n",
       "      <td>6.3467</td>\n",
       "      <td>5.0</td>\n",
       "      <td>311.0</td>\n",
       "      <td>15.2</td>\n",
       "      <td>392.52</td>\n",
       "      <td>20.45</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>0.11747</td>\n",
       "      <td>12.5</td>\n",
       "      <td>7.87</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.524</td>\n",
       "      <td>6.009</td>\n",
       "      <td>82.9</td>\n",
       "      <td>6.2267</td>\n",
       "      <td>5.0</td>\n",
       "      <td>311.0</td>\n",
       "      <td>15.2</td>\n",
       "      <td>396.90</td>\n",
       "      <td>13.27</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>0.09378</td>\n",
       "      <td>12.5</td>\n",
       "      <td>7.87</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.524</td>\n",
       "      <td>5.889</td>\n",
       "      <td>39.0</td>\n",
       "      <td>5.4509</td>\n",
       "      <td>5.0</td>\n",
       "      <td>311.0</td>\n",
       "      <td>15.2</td>\n",
       "      <td>390.50</td>\n",
       "      <td>15.71</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>0.62976</td>\n",
       "      <td>0.0</td>\n",
       "      <td>8.14</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.538</td>\n",
       "      <td>5.949</td>\n",
       "      <td>61.8</td>\n",
       "      <td>4.7075</td>\n",
       "      <td>4.0</td>\n",
       "      <td>307.0</td>\n",
       "      <td>21.0</td>\n",
       "      <td>396.90</td>\n",
       "      <td>8.26</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>0.63796</td>\n",
       "      <td>0.0</td>\n",
       "      <td>8.14</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.538</td>\n",
       "      <td>6.096</td>\n",
       "      <td>84.5</td>\n",
       "      <td>4.4619</td>\n",
       "      <td>4.0</td>\n",
       "      <td>307.0</td>\n",
       "      <td>21.0</td>\n",
       "      <td>380.02</td>\n",
       "      <td>10.26</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>0.62739</td>\n",
       "      <td>0.0</td>\n",
       "      <td>8.14</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.538</td>\n",
       "      <td>5.834</td>\n",
       "      <td>56.5</td>\n",
       "      <td>4.4986</td>\n",
       "      <td>4.0</td>\n",
       "      <td>307.0</td>\n",
       "      <td>21.0</td>\n",
       "      <td>395.62</td>\n",
       "      <td>8.47</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>1.05393</td>\n",
       "      <td>0.0</td>\n",
       "      <td>8.14</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.538</td>\n",
       "      <td>5.935</td>\n",
       "      <td>29.3</td>\n",
       "      <td>4.4986</td>\n",
       "      <td>4.0</td>\n",
       "      <td>307.0</td>\n",
       "      <td>21.0</td>\n",
       "      <td>386.85</td>\n",
       "      <td>6.58</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>0.78420</td>\n",
       "      <td>0.0</td>\n",
       "      <td>8.14</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.538</td>\n",
       "      <td>5.990</td>\n",
       "      <td>81.7</td>\n",
       "      <td>4.2579</td>\n",
       "      <td>4.0</td>\n",
       "      <td>307.0</td>\n",
       "      <td>21.0</td>\n",
       "      <td>386.75</td>\n",
       "      <td>14.67</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>0.80271</td>\n",
       "      <td>0.0</td>\n",
       "      <td>8.14</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.538</td>\n",
       "      <td>5.456</td>\n",
       "      <td>36.6</td>\n",
       "      <td>3.7965</td>\n",
       "      <td>4.0</td>\n",
       "      <td>307.0</td>\n",
       "      <td>21.0</td>\n",
       "      <td>288.99</td>\n",
       "      <td>11.69</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>0.72580</td>\n",
       "      <td>0.0</td>\n",
       "      <td>8.14</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.538</td>\n",
       "      <td>5.727</td>\n",
       "      <td>69.5</td>\n",
       "      <td>3.7965</td>\n",
       "      <td>4.0</td>\n",
       "      <td>307.0</td>\n",
       "      <td>21.0</td>\n",
       "      <td>390.95</td>\n",
       "      <td>11.28</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>1.25179</td>\n",
       "      <td>0.0</td>\n",
       "      <td>8.14</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.538</td>\n",
       "      <td>5.570</td>\n",
       "      <td>98.1</td>\n",
       "      <td>3.7979</td>\n",
       "      <td>4.0</td>\n",
       "      <td>307.0</td>\n",
       "      <td>21.0</td>\n",
       "      <td>376.57</td>\n",
       "      <td>21.02</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>0.85204</td>\n",
       "      <td>0.0</td>\n",
       "      <td>8.14</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.538</td>\n",
       "      <td>5.965</td>\n",
       "      <td>89.2</td>\n",
       "      <td>4.0123</td>\n",
       "      <td>4.0</td>\n",
       "      <td>307.0</td>\n",
       "      <td>21.0</td>\n",
       "      <td>392.53</td>\n",
       "      <td>13.83</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>1.23247</td>\n",
       "      <td>0.0</td>\n",
       "      <td>8.14</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.538</td>\n",
       "      <td>6.142</td>\n",
       "      <td>91.7</td>\n",
       "      <td>3.9769</td>\n",
       "      <td>4.0</td>\n",
       "      <td>307.0</td>\n",
       "      <td>21.0</td>\n",
       "      <td>396.90</td>\n",
       "      <td>18.72</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>0.98843</td>\n",
       "      <td>0.0</td>\n",
       "      <td>8.14</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.538</td>\n",
       "      <td>5.813</td>\n",
       "      <td>100.0</td>\n",
       "      <td>4.0952</td>\n",
       "      <td>4.0</td>\n",
       "      <td>307.0</td>\n",
       "      <td>21.0</td>\n",
       "      <td>394.54</td>\n",
       "      <td>19.88</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>0.75026</td>\n",
       "      <td>0.0</td>\n",
       "      <td>8.14</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.538</td>\n",
       "      <td>5.924</td>\n",
       "      <td>94.1</td>\n",
       "      <td>4.3996</td>\n",
       "      <td>4.0</td>\n",
       "      <td>307.0</td>\n",
       "      <td>21.0</td>\n",
       "      <td>394.33</td>\n",
       "      <td>16.30</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>0.84054</td>\n",
       "      <td>0.0</td>\n",
       "      <td>8.14</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.538</td>\n",
       "      <td>5.599</td>\n",
       "      <td>85.7</td>\n",
       "      <td>4.4546</td>\n",
       "      <td>4.0</td>\n",
       "      <td>307.0</td>\n",
       "      <td>21.0</td>\n",
       "      <td>303.42</td>\n",
       "      <td>16.51</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>0.67191</td>\n",
       "      <td>0.0</td>\n",
       "      <td>8.14</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.538</td>\n",
       "      <td>5.813</td>\n",
       "      <td>90.3</td>\n",
       "      <td>4.6820</td>\n",
       "      <td>4.0</td>\n",
       "      <td>307.0</td>\n",
       "      <td>21.0</td>\n",
       "      <td>376.88</td>\n",
       "      <td>14.81</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>0.95577</td>\n",
       "      <td>0.0</td>\n",
       "      <td>8.14</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.538</td>\n",
       "      <td>6.047</td>\n",
       "      <td>88.8</td>\n",
       "      <td>4.4534</td>\n",
       "      <td>4.0</td>\n",
       "      <td>307.0</td>\n",
       "      <td>21.0</td>\n",
       "      <td>306.38</td>\n",
       "      <td>17.28</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>0.77299</td>\n",
       "      <td>0.0</td>\n",
       "      <td>8.14</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.538</td>\n",
       "      <td>6.495</td>\n",
       "      <td>94.4</td>\n",
       "      <td>4.4547</td>\n",
       "      <td>4.0</td>\n",
       "      <td>307.0</td>\n",
       "      <td>21.0</td>\n",
       "      <td>387.94</td>\n",
       "      <td>12.80</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>1.00245</td>\n",
       "      <td>0.0</td>\n",
       "      <td>8.14</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.538</td>\n",
       "      <td>6.674</td>\n",
       "      <td>87.3</td>\n",
       "      <td>4.2390</td>\n",
       "      <td>4.0</td>\n",
       "      <td>307.0</td>\n",
       "      <td>21.0</td>\n",
       "      <td>380.23</td>\n",
       "      <td>11.98</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>476</th>\n",
       "      <td>4.87141</td>\n",
       "      <td>0.0</td>\n",
       "      <td>18.10</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.614</td>\n",
       "      <td>6.484</td>\n",
       "      <td>93.6</td>\n",
       "      <td>2.3053</td>\n",
       "      <td>24.0</td>\n",
       "      <td>666.0</td>\n",
       "      <td>20.2</td>\n",
       "      <td>396.21</td>\n",
       "      <td>18.68</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>477</th>\n",
       "      <td>15.02340</td>\n",
       "      <td>0.0</td>\n",
       "      <td>18.10</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.614</td>\n",
       "      <td>5.304</td>\n",
       "      <td>97.3</td>\n",
       "      <td>2.1007</td>\n",
       "      <td>24.0</td>\n",
       "      <td>666.0</td>\n",
       "      <td>20.2</td>\n",
       "      <td>349.48</td>\n",
       "      <td>24.91</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>478</th>\n",
       "      <td>10.23300</td>\n",
       "      <td>0.0</td>\n",
       "      <td>18.10</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.614</td>\n",
       "      <td>6.185</td>\n",
       "      <td>96.7</td>\n",
       "      <td>2.1705</td>\n",
       "      <td>24.0</td>\n",
       "      <td>666.0</td>\n",
       "      <td>20.2</td>\n",
       "      <td>379.70</td>\n",
       "      <td>18.03</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>479</th>\n",
       "      <td>14.33370</td>\n",
       "      <td>0.0</td>\n",
       "      <td>18.10</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.614</td>\n",
       "      <td>6.229</td>\n",
       "      <td>88.0</td>\n",
       "      <td>1.9512</td>\n",
       "      <td>24.0</td>\n",
       "      <td>666.0</td>\n",
       "      <td>20.2</td>\n",
       "      <td>383.32</td>\n",
       "      <td>13.11</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>480</th>\n",
       "      <td>5.82401</td>\n",
       "      <td>0.0</td>\n",
       "      <td>18.10</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.532</td>\n",
       "      <td>6.242</td>\n",
       "      <td>64.7</td>\n",
       "      <td>3.4242</td>\n",
       "      <td>24.0</td>\n",
       "      <td>666.0</td>\n",
       "      <td>20.2</td>\n",
       "      <td>396.90</td>\n",
       "      <td>10.74</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>481</th>\n",
       "      <td>5.70818</td>\n",
       "      <td>0.0</td>\n",
       "      <td>18.10</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.532</td>\n",
       "      <td>6.750</td>\n",
       "      <td>74.9</td>\n",
       "      <td>3.3317</td>\n",
       "      <td>24.0</td>\n",
       "      <td>666.0</td>\n",
       "      <td>20.2</td>\n",
       "      <td>393.07</td>\n",
       "      <td>7.74</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>482</th>\n",
       "      <td>5.73116</td>\n",
       "      <td>0.0</td>\n",
       "      <td>18.10</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.532</td>\n",
       "      <td>7.061</td>\n",
       "      <td>77.0</td>\n",
       "      <td>3.4106</td>\n",
       "      <td>24.0</td>\n",
       "      <td>666.0</td>\n",
       "      <td>20.2</td>\n",
       "      <td>395.28</td>\n",
       "      <td>7.01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>483</th>\n",
       "      <td>2.81838</td>\n",
       "      <td>0.0</td>\n",
       "      <td>18.10</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.532</td>\n",
       "      <td>5.762</td>\n",
       "      <td>40.3</td>\n",
       "      <td>4.0983</td>\n",
       "      <td>24.0</td>\n",
       "      <td>666.0</td>\n",
       "      <td>20.2</td>\n",
       "      <td>392.92</td>\n",
       "      <td>10.42</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>484</th>\n",
       "      <td>2.37857</td>\n",
       "      <td>0.0</td>\n",
       "      <td>18.10</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.583</td>\n",
       "      <td>5.871</td>\n",
       "      <td>41.9</td>\n",
       "      <td>3.7240</td>\n",
       "      <td>24.0</td>\n",
       "      <td>666.0</td>\n",
       "      <td>20.2</td>\n",
       "      <td>370.73</td>\n",
       "      <td>13.34</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>485</th>\n",
       "      <td>3.67367</td>\n",
       "      <td>0.0</td>\n",
       "      <td>18.10</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.583</td>\n",
       "      <td>6.312</td>\n",
       "      <td>51.9</td>\n",
       "      <td>3.9917</td>\n",
       "      <td>24.0</td>\n",
       "      <td>666.0</td>\n",
       "      <td>20.2</td>\n",
       "      <td>388.62</td>\n",
       "      <td>10.58</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>486</th>\n",
       "      <td>5.69175</td>\n",
       "      <td>0.0</td>\n",
       "      <td>18.10</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.583</td>\n",
       "      <td>6.114</td>\n",
       "      <td>79.8</td>\n",
       "      <td>3.5459</td>\n",
       "      <td>24.0</td>\n",
       "      <td>666.0</td>\n",
       "      <td>20.2</td>\n",
       "      <td>392.68</td>\n",
       "      <td>14.98</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>487</th>\n",
       "      <td>4.83567</td>\n",
       "      <td>0.0</td>\n",
       "      <td>18.10</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.583</td>\n",
       "      <td>5.905</td>\n",
       "      <td>53.2</td>\n",
       "      <td>3.1523</td>\n",
       "      <td>24.0</td>\n",
       "      <td>666.0</td>\n",
       "      <td>20.2</td>\n",
       "      <td>388.22</td>\n",
       "      <td>11.45</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>488</th>\n",
       "      <td>0.15086</td>\n",
       "      <td>0.0</td>\n",
       "      <td>27.74</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.609</td>\n",
       "      <td>5.454</td>\n",
       "      <td>92.7</td>\n",
       "      <td>1.8209</td>\n",
       "      <td>4.0</td>\n",
       "      <td>711.0</td>\n",
       "      <td>20.1</td>\n",
       "      <td>395.09</td>\n",
       "      <td>18.06</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>489</th>\n",
       "      <td>0.18337</td>\n",
       "      <td>0.0</td>\n",
       "      <td>27.74</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.609</td>\n",
       "      <td>5.414</td>\n",
       "      <td>98.3</td>\n",
       "      <td>1.7554</td>\n",
       "      <td>4.0</td>\n",
       "      <td>711.0</td>\n",
       "      <td>20.1</td>\n",
       "      <td>344.05</td>\n",
       "      <td>23.97</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>490</th>\n",
       "      <td>0.20746</td>\n",
       "      <td>0.0</td>\n",
       "      <td>27.74</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.609</td>\n",
       "      <td>5.093</td>\n",
       "      <td>98.0</td>\n",
       "      <td>1.8226</td>\n",
       "      <td>4.0</td>\n",
       "      <td>711.0</td>\n",
       "      <td>20.1</td>\n",
       "      <td>318.43</td>\n",
       "      <td>29.68</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>491</th>\n",
       "      <td>0.10574</td>\n",
       "      <td>0.0</td>\n",
       "      <td>27.74</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.609</td>\n",
       "      <td>5.983</td>\n",
       "      <td>98.8</td>\n",
       "      <td>1.8681</td>\n",
       "      <td>4.0</td>\n",
       "      <td>711.0</td>\n",
       "      <td>20.1</td>\n",
       "      <td>390.11</td>\n",
       "      <td>18.07</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>492</th>\n",
       "      <td>0.11132</td>\n",
       "      <td>0.0</td>\n",
       "      <td>27.74</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.609</td>\n",
       "      <td>5.983</td>\n",
       "      <td>83.5</td>\n",
       "      <td>2.1099</td>\n",
       "      <td>4.0</td>\n",
       "      <td>711.0</td>\n",
       "      <td>20.1</td>\n",
       "      <td>396.90</td>\n",
       "      <td>13.35</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>493</th>\n",
       "      <td>0.17331</td>\n",
       "      <td>0.0</td>\n",
       "      <td>9.69</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.585</td>\n",
       "      <td>5.707</td>\n",
       "      <td>54.0</td>\n",
       "      <td>2.3817</td>\n",
       "      <td>6.0</td>\n",
       "      <td>391.0</td>\n",
       "      <td>19.2</td>\n",
       "      <td>396.90</td>\n",
       "      <td>12.01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>494</th>\n",
       "      <td>0.27957</td>\n",
       "      <td>0.0</td>\n",
       "      <td>9.69</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.585</td>\n",
       "      <td>5.926</td>\n",
       "      <td>42.6</td>\n",
       "      <td>2.3817</td>\n",
       "      <td>6.0</td>\n",
       "      <td>391.0</td>\n",
       "      <td>19.2</td>\n",
       "      <td>396.90</td>\n",
       "      <td>13.59</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>495</th>\n",
       "      <td>0.17899</td>\n",
       "      <td>0.0</td>\n",
       "      <td>9.69</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.585</td>\n",
       "      <td>5.670</td>\n",
       "      <td>28.8</td>\n",
       "      <td>2.7986</td>\n",
       "      <td>6.0</td>\n",
       "      <td>391.0</td>\n",
       "      <td>19.2</td>\n",
       "      <td>393.29</td>\n",
       "      <td>17.60</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>496</th>\n",
       "      <td>0.28960</td>\n",
       "      <td>0.0</td>\n",
       "      <td>9.69</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.585</td>\n",
       "      <td>5.390</td>\n",
       "      <td>72.9</td>\n",
       "      <td>2.7986</td>\n",
       "      <td>6.0</td>\n",
       "      <td>391.0</td>\n",
       "      <td>19.2</td>\n",
       "      <td>396.90</td>\n",
       "      <td>21.14</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>497</th>\n",
       "      <td>0.26838</td>\n",
       "      <td>0.0</td>\n",
       "      <td>9.69</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.585</td>\n",
       "      <td>5.794</td>\n",
       "      <td>70.6</td>\n",
       "      <td>2.8927</td>\n",
       "      <td>6.0</td>\n",
       "      <td>391.0</td>\n",
       "      <td>19.2</td>\n",
       "      <td>396.90</td>\n",
       "      <td>14.10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>498</th>\n",
       "      <td>0.23912</td>\n",
       "      <td>0.0</td>\n",
       "      <td>9.69</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.585</td>\n",
       "      <td>6.019</td>\n",
       "      <td>65.3</td>\n",
       "      <td>2.4091</td>\n",
       "      <td>6.0</td>\n",
       "      <td>391.0</td>\n",
       "      <td>19.2</td>\n",
       "      <td>396.90</td>\n",
       "      <td>12.92</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>499</th>\n",
       "      <td>0.17783</td>\n",
       "      <td>0.0</td>\n",
       "      <td>9.69</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.585</td>\n",
       "      <td>5.569</td>\n",
       "      <td>73.5</td>\n",
       "      <td>2.3999</td>\n",
       "      <td>6.0</td>\n",
       "      <td>391.0</td>\n",
       "      <td>19.2</td>\n",
       "      <td>395.77</td>\n",
       "      <td>15.10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>500</th>\n",
       "      <td>0.22438</td>\n",
       "      <td>0.0</td>\n",
       "      <td>9.69</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.585</td>\n",
       "      <td>6.027</td>\n",
       "      <td>79.7</td>\n",
       "      <td>2.4982</td>\n",
       "      <td>6.0</td>\n",
       "      <td>391.0</td>\n",
       "      <td>19.2</td>\n",
       "      <td>396.90</td>\n",
       "      <td>14.33</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>501</th>\n",
       "      <td>0.06263</td>\n",
       "      <td>0.0</td>\n",
       "      <td>11.93</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.573</td>\n",
       "      <td>6.593</td>\n",
       "      <td>69.1</td>\n",
       "      <td>2.4786</td>\n",
       "      <td>1.0</td>\n",
       "      <td>273.0</td>\n",
       "      <td>21.0</td>\n",
       "      <td>391.99</td>\n",
       "      <td>9.67</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>502</th>\n",
       "      <td>0.04527</td>\n",
       "      <td>0.0</td>\n",
       "      <td>11.93</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.573</td>\n",
       "      <td>6.120</td>\n",
       "      <td>76.7</td>\n",
       "      <td>2.2875</td>\n",
       "      <td>1.0</td>\n",
       "      <td>273.0</td>\n",
       "      <td>21.0</td>\n",
       "      <td>396.90</td>\n",
       "      <td>9.08</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>503</th>\n",
       "      <td>0.06076</td>\n",
       "      <td>0.0</td>\n",
       "      <td>11.93</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.573</td>\n",
       "      <td>6.976</td>\n",
       "      <td>91.0</td>\n",
       "      <td>2.1675</td>\n",
       "      <td>1.0</td>\n",
       "      <td>273.0</td>\n",
       "      <td>21.0</td>\n",
       "      <td>396.90</td>\n",
       "      <td>5.64</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>504</th>\n",
       "      <td>0.10959</td>\n",
       "      <td>0.0</td>\n",
       "      <td>11.93</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.573</td>\n",
       "      <td>6.794</td>\n",
       "      <td>89.3</td>\n",
       "      <td>2.3889</td>\n",
       "      <td>1.0</td>\n",
       "      <td>273.0</td>\n",
       "      <td>21.0</td>\n",
       "      <td>393.45</td>\n",
       "      <td>6.48</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>505</th>\n",
       "      <td>0.04741</td>\n",
       "      <td>0.0</td>\n",
       "      <td>11.93</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.573</td>\n",
       "      <td>6.030</td>\n",
       "      <td>80.8</td>\n",
       "      <td>2.5050</td>\n",
       "      <td>1.0</td>\n",
       "      <td>273.0</td>\n",
       "      <td>21.0</td>\n",
       "      <td>396.90</td>\n",
       "      <td>7.88</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>506 rows × 13 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "         CRIM    ZN  INDUS  CHAS    NOX     RM    AGE     DIS   RAD    TAX  \\\n",
       "0     0.00632  18.0   2.31   0.0  0.538  6.575   65.2  4.0900   1.0  296.0   \n",
       "1     0.02731   0.0   7.07   0.0  0.469  6.421   78.9  4.9671   2.0  242.0   \n",
       "2     0.02729   0.0   7.07   0.0  0.469  7.185   61.1  4.9671   2.0  242.0   \n",
       "3     0.03237   0.0   2.18   0.0  0.458  6.998   45.8  6.0622   3.0  222.0   \n",
       "4     0.06905   0.0   2.18   0.0  0.458  7.147   54.2  6.0622   3.0  222.0   \n",
       "5     0.02985   0.0   2.18   0.0  0.458  6.430   58.7  6.0622   3.0  222.0   \n",
       "6     0.08829  12.5   7.87   0.0  0.524  6.012   66.6  5.5605   5.0  311.0   \n",
       "7     0.14455  12.5   7.87   0.0  0.524  6.172   96.1  5.9505   5.0  311.0   \n",
       "8     0.21124  12.5   7.87   0.0  0.524  5.631  100.0  6.0821   5.0  311.0   \n",
       "9     0.17004  12.5   7.87   0.0  0.524  6.004   85.9  6.5921   5.0  311.0   \n",
       "10    0.22489  12.5   7.87   0.0  0.524  6.377   94.3  6.3467   5.0  311.0   \n",
       "11    0.11747  12.5   7.87   0.0  0.524  6.009   82.9  6.2267   5.0  311.0   \n",
       "12    0.09378  12.5   7.87   0.0  0.524  5.889   39.0  5.4509   5.0  311.0   \n",
       "13    0.62976   0.0   8.14   0.0  0.538  5.949   61.8  4.7075   4.0  307.0   \n",
       "14    0.63796   0.0   8.14   0.0  0.538  6.096   84.5  4.4619   4.0  307.0   \n",
       "15    0.62739   0.0   8.14   0.0  0.538  5.834   56.5  4.4986   4.0  307.0   \n",
       "16    1.05393   0.0   8.14   0.0  0.538  5.935   29.3  4.4986   4.0  307.0   \n",
       "17    0.78420   0.0   8.14   0.0  0.538  5.990   81.7  4.2579   4.0  307.0   \n",
       "18    0.80271   0.0   8.14   0.0  0.538  5.456   36.6  3.7965   4.0  307.0   \n",
       "19    0.72580   0.0   8.14   0.0  0.538  5.727   69.5  3.7965   4.0  307.0   \n",
       "20    1.25179   0.0   8.14   0.0  0.538  5.570   98.1  3.7979   4.0  307.0   \n",
       "21    0.85204   0.0   8.14   0.0  0.538  5.965   89.2  4.0123   4.0  307.0   \n",
       "22    1.23247   0.0   8.14   0.0  0.538  6.142   91.7  3.9769   4.0  307.0   \n",
       "23    0.98843   0.0   8.14   0.0  0.538  5.813  100.0  4.0952   4.0  307.0   \n",
       "24    0.75026   0.0   8.14   0.0  0.538  5.924   94.1  4.3996   4.0  307.0   \n",
       "25    0.84054   0.0   8.14   0.0  0.538  5.599   85.7  4.4546   4.0  307.0   \n",
       "26    0.67191   0.0   8.14   0.0  0.538  5.813   90.3  4.6820   4.0  307.0   \n",
       "27    0.95577   0.0   8.14   0.0  0.538  6.047   88.8  4.4534   4.0  307.0   \n",
       "28    0.77299   0.0   8.14   0.0  0.538  6.495   94.4  4.4547   4.0  307.0   \n",
       "29    1.00245   0.0   8.14   0.0  0.538  6.674   87.3  4.2390   4.0  307.0   \n",
       "..        ...   ...    ...   ...    ...    ...    ...     ...   ...    ...   \n",
       "476   4.87141   0.0  18.10   0.0  0.614  6.484   93.6  2.3053  24.0  666.0   \n",
       "477  15.02340   0.0  18.10   0.0  0.614  5.304   97.3  2.1007  24.0  666.0   \n",
       "478  10.23300   0.0  18.10   0.0  0.614  6.185   96.7  2.1705  24.0  666.0   \n",
       "479  14.33370   0.0  18.10   0.0  0.614  6.229   88.0  1.9512  24.0  666.0   \n",
       "480   5.82401   0.0  18.10   0.0  0.532  6.242   64.7  3.4242  24.0  666.0   \n",
       "481   5.70818   0.0  18.10   0.0  0.532  6.750   74.9  3.3317  24.0  666.0   \n",
       "482   5.73116   0.0  18.10   0.0  0.532  7.061   77.0  3.4106  24.0  666.0   \n",
       "483   2.81838   0.0  18.10   0.0  0.532  5.762   40.3  4.0983  24.0  666.0   \n",
       "484   2.37857   0.0  18.10   0.0  0.583  5.871   41.9  3.7240  24.0  666.0   \n",
       "485   3.67367   0.0  18.10   0.0  0.583  6.312   51.9  3.9917  24.0  666.0   \n",
       "486   5.69175   0.0  18.10   0.0  0.583  6.114   79.8  3.5459  24.0  666.0   \n",
       "487   4.83567   0.0  18.10   0.0  0.583  5.905   53.2  3.1523  24.0  666.0   \n",
       "488   0.15086   0.0  27.74   0.0  0.609  5.454   92.7  1.8209   4.0  711.0   \n",
       "489   0.18337   0.0  27.74   0.0  0.609  5.414   98.3  1.7554   4.0  711.0   \n",
       "490   0.20746   0.0  27.74   0.0  0.609  5.093   98.0  1.8226   4.0  711.0   \n",
       "491   0.10574   0.0  27.74   0.0  0.609  5.983   98.8  1.8681   4.0  711.0   \n",
       "492   0.11132   0.0  27.74   0.0  0.609  5.983   83.5  2.1099   4.0  711.0   \n",
       "493   0.17331   0.0   9.69   0.0  0.585  5.707   54.0  2.3817   6.0  391.0   \n",
       "494   0.27957   0.0   9.69   0.0  0.585  5.926   42.6  2.3817   6.0  391.0   \n",
       "495   0.17899   0.0   9.69   0.0  0.585  5.670   28.8  2.7986   6.0  391.0   \n",
       "496   0.28960   0.0   9.69   0.0  0.585  5.390   72.9  2.7986   6.0  391.0   \n",
       "497   0.26838   0.0   9.69   0.0  0.585  5.794   70.6  2.8927   6.0  391.0   \n",
       "498   0.23912   0.0   9.69   0.0  0.585  6.019   65.3  2.4091   6.0  391.0   \n",
       "499   0.17783   0.0   9.69   0.0  0.585  5.569   73.5  2.3999   6.0  391.0   \n",
       "500   0.22438   0.0   9.69   0.0  0.585  6.027   79.7  2.4982   6.0  391.0   \n",
       "501   0.06263   0.0  11.93   0.0  0.573  6.593   69.1  2.4786   1.0  273.0   \n",
       "502   0.04527   0.0  11.93   0.0  0.573  6.120   76.7  2.2875   1.0  273.0   \n",
       "503   0.06076   0.0  11.93   0.0  0.573  6.976   91.0  2.1675   1.0  273.0   \n",
       "504   0.10959   0.0  11.93   0.0  0.573  6.794   89.3  2.3889   1.0  273.0   \n",
       "505   0.04741   0.0  11.93   0.0  0.573  6.030   80.8  2.5050   1.0  273.0   \n",
       "\n",
       "     PTRATIO       B  LSTAT  \n",
       "0       15.3  396.90   4.98  \n",
       "1       17.8  396.90   9.14  \n",
       "2       17.8  392.83   4.03  \n",
       "3       18.7  394.63   2.94  \n",
       "4       18.7  396.90   5.33  \n",
       "5       18.7  394.12   5.21  \n",
       "6       15.2  395.60  12.43  \n",
       "7       15.2  396.90  19.15  \n",
       "8       15.2  386.63  29.93  \n",
       "9       15.2  386.71  17.10  \n",
       "10      15.2  392.52  20.45  \n",
       "11      15.2  396.90  13.27  \n",
       "12      15.2  390.50  15.71  \n",
       "13      21.0  396.90   8.26  \n",
       "14      21.0  380.02  10.26  \n",
       "15      21.0  395.62   8.47  \n",
       "16      21.0  386.85   6.58  \n",
       "17      21.0  386.75  14.67  \n",
       "18      21.0  288.99  11.69  \n",
       "19      21.0  390.95  11.28  \n",
       "20      21.0  376.57  21.02  \n",
       "21      21.0  392.53  13.83  \n",
       "22      21.0  396.90  18.72  \n",
       "23      21.0  394.54  19.88  \n",
       "24      21.0  394.33  16.30  \n",
       "25      21.0  303.42  16.51  \n",
       "26      21.0  376.88  14.81  \n",
       "27      21.0  306.38  17.28  \n",
       "28      21.0  387.94  12.80  \n",
       "29      21.0  380.23  11.98  \n",
       "..       ...     ...    ...  \n",
       "476     20.2  396.21  18.68  \n",
       "477     20.2  349.48  24.91  \n",
       "478     20.2  379.70  18.03  \n",
       "479     20.2  383.32  13.11  \n",
       "480     20.2  396.90  10.74  \n",
       "481     20.2  393.07   7.74  \n",
       "482     20.2  395.28   7.01  \n",
       "483     20.2  392.92  10.42  \n",
       "484     20.2  370.73  13.34  \n",
       "485     20.2  388.62  10.58  \n",
       "486     20.2  392.68  14.98  \n",
       "487     20.2  388.22  11.45  \n",
       "488     20.1  395.09  18.06  \n",
       "489     20.1  344.05  23.97  \n",
       "490     20.1  318.43  29.68  \n",
       "491     20.1  390.11  18.07  \n",
       "492     20.1  396.90  13.35  \n",
       "493     19.2  396.90  12.01  \n",
       "494     19.2  396.90  13.59  \n",
       "495     19.2  393.29  17.60  \n",
       "496     19.2  396.90  21.14  \n",
       "497     19.2  396.90  14.10  \n",
       "498     19.2  396.90  12.92  \n",
       "499     19.2  395.77  15.10  \n",
       "500     19.2  396.90  14.33  \n",
       "501     21.0  391.99   9.67  \n",
       "502     21.0  396.90   9.08  \n",
       "503     21.0  396.90   5.64  \n",
       "504     21.0  393.45   6.48  \n",
       "505     21.0  396.90   7.88  \n",
       "\n",
       "[506 rows x 13 columns]"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0      24.0\n",
       "1      21.6\n",
       "2      34.7\n",
       "3      33.4\n",
       "4      36.2\n",
       "5      28.7\n",
       "6      22.9\n",
       "7      27.1\n",
       "8      16.5\n",
       "9      18.9\n",
       "10     15.0\n",
       "11     18.9\n",
       "12     21.7\n",
       "13     20.4\n",
       "14     18.2\n",
       "15     19.9\n",
       "16     23.1\n",
       "17     17.5\n",
       "18     20.2\n",
       "19     18.2\n",
       "20     13.6\n",
       "21     19.6\n",
       "22     15.2\n",
       "23     14.5\n",
       "24     15.6\n",
       "25     13.9\n",
       "26     16.6\n",
       "27     14.8\n",
       "28     18.4\n",
       "29     21.0\n",
       "       ... \n",
       "476    16.7\n",
       "477    12.0\n",
       "478    14.6\n",
       "479    21.4\n",
       "480    23.0\n",
       "481    23.7\n",
       "482    25.0\n",
       "483    21.8\n",
       "484    20.6\n",
       "485    21.2\n",
       "486    19.1\n",
       "487    20.6\n",
       "488    15.2\n",
       "489     7.0\n",
       "490     8.1\n",
       "491    13.6\n",
       "492    20.1\n",
       "493    21.8\n",
       "494    24.5\n",
       "495    23.1\n",
       "496    19.7\n",
       "497    18.3\n",
       "498    21.2\n",
       "499    17.5\n",
       "500    16.8\n",
       "501    22.4\n",
       "502    20.6\n",
       "503    23.9\n",
       "504    22.0\n",
       "505    11.9\n",
       "Name: PRICE, Length: 506, dtype: float64"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now you will convert the dataset into an optimized data structure called Dmatrix that XGBoost supports and gives it acclaimed performance and efficiency gains. You will use this later in the tutorial."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0       0.00632\n",
       "1       0.02731\n",
       "2       0.02729\n",
       "3       0.03237\n",
       "4       0.06905\n",
       "5       0.02985\n",
       "6       0.08829\n",
       "7       0.14455\n",
       "8       0.21124\n",
       "9       0.17004\n",
       "10      0.22489\n",
       "11      0.11747\n",
       "12      0.09378\n",
       "13      0.62976\n",
       "14      0.63796\n",
       "15      0.62739\n",
       "16      1.05393\n",
       "17      0.78420\n",
       "18      0.80271\n",
       "19      0.72580\n",
       "20      1.25179\n",
       "21      0.85204\n",
       "22      1.23247\n",
       "23      0.98843\n",
       "24      0.75026\n",
       "25      0.84054\n",
       "26      0.67191\n",
       "27      0.95577\n",
       "28      0.77299\n",
       "29      1.00245\n",
       "         ...   \n",
       "476     4.87141\n",
       "477    15.02340\n",
       "478    10.23300\n",
       "479    14.33370\n",
       "480     5.82401\n",
       "481     5.70818\n",
       "482     5.73116\n",
       "483     2.81838\n",
       "484     2.37857\n",
       "485     3.67367\n",
       "486     5.69175\n",
       "487     4.83567\n",
       "488     0.15086\n",
       "489     0.18337\n",
       "490     0.20746\n",
       "491     0.10574\n",
       "492     0.11132\n",
       "493     0.17331\n",
       "494     0.27957\n",
       "495     0.17899\n",
       "496     0.28960\n",
       "497     0.26838\n",
       "498     0.23912\n",
       "499     0.17783\n",
       "500     0.22438\n",
       "501     0.06263\n",
       "502     0.04527\n",
       "503     0.06076\n",
       "504     0.10959\n",
       "505     0.04741\n",
       "Name: CRIM, Length: 506, dtype: float64"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X['CRIM']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_dmatrix = xgb.DMatrix(data=X,label=y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## XGBoost's hyperparameters\n",
    "The overall parameters have been divided into 3 categories by XGBoost authors:\n",
    "\n",
    "- <b>General Parameters: Guide the overall functioning\n",
    "- Booster Parameters: Guide the individual booster (tree/regression) at each step\n",
    "- Learning Task Parameters: Guide the optimization performed\n",
    "I will give analogies to GBM here and highly recommend to read <a href=\"https://www.analyticsvidhya.com/blog/2016/02/complete-guide-parameter-tuning-gradient-boosting-gbm-python/\">this article</a> to learn from the very basics.\n",
    "<br><br>\n",
    "    \n",
    "### General Parameters\n",
    "These define the overall functionality of XGBoost.\n",
    "\n",
    "- booster [default=gbtree]\n",
    "  - Select the type of model to run at each iteration. It has 2 options:\n",
    "    - <font color=\"red\">gbtree: tree-based models</font>\n",
    "    - gblinear: linear models\n",
    "- silent [default=0]:\n",
    "  - Silent mode is activated is set to 1, i.e. no running messages will be printed.\n",
    "  - It’s generally good to keep it 0 as the messages might help in understanding the model.\n",
    "- nthread [default to maximum number of threads available if not set]\n",
    "  - This is used for parallel processing and number of cores in the system should be entered\n",
    "  - If you wish to run on all cores, value should not be entered and algorithm will detect automatically\n",
    "There are 2 more parameters which are set automatically by XGBoost and you need not worry about them. Lets move on to Booster parameters.<br><br>\n",
    "\n",
    "### Booster Parameters\n",
    "Though there are 2 types of boosters, I’ll consider only tree booster here because it always outperforms the linear booster and thus the later is rarely used.\n",
    "- <font color=\"red\">eta [default=0.3]</font>\n",
    "  - Analogous to learning rate in GBM\n",
    "  - Makes the model more robust by shrinking the weights on each step\n",
    "  - Typical final values to be used: 0.01-0.2\n",
    "- <font color=\"red\">min_child_weight [default=1]</font>\n",
    "  - Defines the minimum sum of weights of all observations required in a child.\n",
    "  - This is similar to min_child_leaf in GBM but not exactly. This refers to min “sum of weights” of observations while GBM has min “number of observations”.\n",
    "  - <b>Used to control over-fitting. Higher values prevent a model from learning relations which might be highly specific to the particular sample selected for a tree.</b>\n",
    "  - Too high values can lead to under-fitting hence, it should be tuned using CV.\n",
    "- <font color=\"red\">max_depth [default=6]</font>\n",
    "  - The maximum depth of a tree, same as GBM.\n",
    "  - Used to control over-fitting as higher depth will allow model to learn relations very specific to a particular sample.\n",
    "  - Should be tuned using CV.\n",
    "  - Typical values: 3-10\n",
    "- max_leaf_nodes\n",
    "  - The maximum number of terminal nodes or leaves in a tree.\n",
    "  - Can be defined in place of max_depth. Since binary trees are created, a depth of ‘n’ would produce a maximum of 2^n leaves.\n",
    "  - If this is defined, GBM will ignore max_depth.\n",
    "- <font color=\"red\">gamma [default=0]</font>\n",
    "  - A node is split only when the resulting split gives a positive reduction in the loss function. Gamma specifies the minimum loss reduction required to make a split.\n",
    "  - Makes the algorithm conservative. The values can vary depending on the loss function and should be tuned.\n",
    "  \n",
    "- max_delta_step [default=0]\n",
    "  - In maximum delta step we allow each tree’s weight estimation to be. If the value is set to 0, it means there is no constraint. If it is set to a positive value, it can help making the update step more conservative.\n",
    "  - Usually this parameter is not needed, but it might help in logistic regression when class is extremely imbalanced.\n",
    "  - This is generally not used but you can explore further if you wish.\n",
    "  \n",
    "- <font color=\"red\">subsample [default=1]</font>\n",
    "  - Same as the subsample of GBM. Denotes the fraction of observations to be randomly samples for each tree.\n",
    "  - Lower values make the algorithm more conservative and prevents overfitting but too small values might lead to under-fitting.\n",
    "  - Typical values: 0.5-1\n",
    "  \n",
    "- colsample_bytree [default=1]\n",
    "  - Similar to max_features in GBM. Denotes the fraction of columns to be randomly samples for each tree.\n",
    "  - Typical values: 0.5-1\n",
    "  \n",
    "- colsample_bylevel [default=1]\n",
    "  - Denotes the subsample ratio of columns for each split, in each level.\n",
    "  - I don’t use this often because subsample and colsample_bytree will do the job for you. but you can explore further if you feel so.\n",
    "  \n",
    "- lambda [default=1]\n",
    "  - L2 regularization term on weights (analogous to Ridge regression)\n",
    "  - This used to handle the regularization part of XGBoost. Though many data scientists don’t use it often, it should be explored to reduce overfitting.\n",
    "  \n",
    "- alpha [default=0]\n",
    "  - L1 regularization term on weight (analogous to Lasso regression)\n",
    "  - Can be used in case of very high dimensionality so that the algorithm runs faster when implemented\n",
    "  \n",
    "- scale_pos_weight [default=1]\n",
    "  - A value greater than 0 should be used in case of high class imbalance as it helps in faster convergence.\n",
    "  \n",
    "### Learning Task Parameters\n",
    "These parameters are used to define the optimization objective the metric to be calculated at each step.\n",
    "- <font color=\"red\">objective [default=reg:linear]</font>\n",
    "  - This defines the loss function to be minimized. Mostly used values are:\n",
    "    - binary:logistic –logistic regression for binary classification, returns predicted probability (not class)\n",
    "    - multi:softmax –multiclass classification using the softmax objective, returns predicted class (not probabilities)\n",
    "      - you also need to set an additional num_class (number of classes) parameter defining the number of unique classes\n",
    "    - multi:softprob –same as softmax, but returns predicted probability of each data point belonging to each class.\n",
    "    \n",
    "- <font color=\"red\">eval_metric [ default according to objective ]</font>\n",
    "  - The metric to be used for validation data.\n",
    "  - The default values are rmse for regression and error for classification.\n",
    "  - Typical values are:\n",
    "    - rmse – root mean square error\n",
    "    - mae – mean absolute error\n",
    "    - logloss – negative log-likelihood\n",
    "    - error – Binary classification error rate (0.5 threshold)\n",
    "    - merror – Multiclass classification error rate\n",
    "    - mlogloss – Multiclass logloss\n",
    "    - auc: Area under the curve\n",
    "    \n",
    "- seed [default=0]\n",
    "  - The random number seed.\n",
    "  - Can be used for generating reproducible results and also for parameter tuning.\n",
    "  \n",
    "<br>\n",
    "If you’ve been using Scikit-Learn till now, these parameter names might not look familiar. A good news is that xgboost module in python has an sklearn wrapper called XGBClassifier. It uses sklearn style naming convention. The parameters names which will change are:\n",
    "<font color=\"red\">\n",
    "eta –> learning_rate<br></font>\n",
    "lambda –> reg_lambda<br>\n",
    "alpha –> reg_alpha<br>\n",
    "<br>\n",
    "I recommend you to go through the following parts of xgboost guide to better understand the parameters and codes:\n",
    "\n",
    "1. <a href=\"https://xgboost.readthedocs.io/en/latest/parameter.html#general-parameters\">XGBoost Parameters (official guide)</a>\n",
    "2. <a href=\"https://github.com/dmlc/xgboost/tree/master/demo/guide-python\">XGBoost Demo Codes (xgboost GitHub repository)</a>\n",
    "3. <a href=\"https://xgboost.readthedocs.io/en/latest/python/python_api.html\">Python API Reference (official guide)</a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "At this point, before building the model, you should be aware of the tuning parameters that XGBoost provides. Well, there are a plethora of tuning parameters for tree-based learners in XGBoost and you can read all about them <a href=\"https://xgboost.readthedocs.io/en/latest/parameter.html#general-parameters\">here</a>. But the most common ones that you should know are:\n",
    "- <code>learning_rate</code>: step size shrinkage used to prevent overfitting. Range is [0,1]\n",
    "- <code>max_depth</code>: determines how deeply each tree is allowed to grow during any boosting round.\n",
    "- <code>subsample</code>: percentage of samples used per tree. Low value can lead to underfitting.\n",
    "- <code>colsample_bytree</code>: percentage of features used per tree. High value can lead to overfitting.\n",
    "- <code>n_estimators</code>: number of trees you want to build.\n",
    "- <code>objective</code>: determines the loss function to be used like <code>reg:linear</code> for regression problems, <code>reg:logistic</code> for classification problems with only decision, <b>binary:logistic</b> for classification problems with probability.\n",
    "\n",
    "XGBoost also supports regularization parameters to penalize models as they become more complex and reduce them to simple (parsimonious) models.\n",
    "\n",
    "- <code>gamma</code>: controls whether a given node will split based on the expected reduction in loss after the split. A higher value leads to fewer splits. Supported only for tree-based learners.\n",
    "- <code>alpha</code>: L1 regularization on leaf weights. A large value leads to more regularization.\n",
    "- <code>lambda</code>: L2 regularization on leaf weights and is smoother than L1 regularization."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It's also worth mentioning that though you are using trees as your base learners, you can also use XGBoost's relatively less popular linear base learners and one other tree learner known as dart. All you have to do is set the <code>booster</code> parameter to either <code>gbtree</code>(default),<code>gblinear</code> or <code>dart</code>.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, you will create the train and test set for cross-validation of the results using the train_test_split function from sklearn's model_selection module with test_size size equal to 20% of the data. Also, to maintain reproducibility of the results, a random_state is also assigned."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=123)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The next step is to instantiate an XGBoost regressor object by calling the XGBRegressor() class from the XGBoost library with the hyper-parameters passed as arguments. For classification problems, you would have used the <code>XGBClassifier()</code> class."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>CRIM</th>\n",
       "      <th>ZN</th>\n",
       "      <th>INDUS</th>\n",
       "      <th>CHAS</th>\n",
       "      <th>NOX</th>\n",
       "      <th>RM</th>\n",
       "      <th>AGE</th>\n",
       "      <th>DIS</th>\n",
       "      <th>RAD</th>\n",
       "      <th>TAX</th>\n",
       "      <th>PTRATIO</th>\n",
       "      <th>B</th>\n",
       "      <th>LSTAT</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>203</th>\n",
       "      <td>0.03510</td>\n",
       "      <td>95.0</td>\n",
       "      <td>2.68</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.4161</td>\n",
       "      <td>7.853</td>\n",
       "      <td>33.2</td>\n",
       "      <td>5.1180</td>\n",
       "      <td>4.0</td>\n",
       "      <td>224.0</td>\n",
       "      <td>14.7</td>\n",
       "      <td>392.78</td>\n",
       "      <td>3.81</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>441</th>\n",
       "      <td>9.72418</td>\n",
       "      <td>0.0</td>\n",
       "      <td>18.10</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.7400</td>\n",
       "      <td>6.406</td>\n",
       "      <td>97.2</td>\n",
       "      <td>2.0651</td>\n",
       "      <td>24.0</td>\n",
       "      <td>666.0</td>\n",
       "      <td>20.2</td>\n",
       "      <td>385.96</td>\n",
       "      <td>19.52</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>172</th>\n",
       "      <td>0.13914</td>\n",
       "      <td>0.0</td>\n",
       "      <td>4.05</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.5100</td>\n",
       "      <td>5.572</td>\n",
       "      <td>88.5</td>\n",
       "      <td>2.5961</td>\n",
       "      <td>5.0</td>\n",
       "      <td>296.0</td>\n",
       "      <td>16.6</td>\n",
       "      <td>396.90</td>\n",
       "      <td>14.69</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>95</th>\n",
       "      <td>0.12204</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2.89</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.4450</td>\n",
       "      <td>6.625</td>\n",
       "      <td>57.8</td>\n",
       "      <td>3.4952</td>\n",
       "      <td>2.0</td>\n",
       "      <td>276.0</td>\n",
       "      <td>18.0</td>\n",
       "      <td>357.98</td>\n",
       "      <td>6.65</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>54</th>\n",
       "      <td>0.01360</td>\n",
       "      <td>75.0</td>\n",
       "      <td>4.00</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.4100</td>\n",
       "      <td>5.888</td>\n",
       "      <td>47.6</td>\n",
       "      <td>7.3197</td>\n",
       "      <td>3.0</td>\n",
       "      <td>469.0</td>\n",
       "      <td>21.1</td>\n",
       "      <td>396.90</td>\n",
       "      <td>14.80</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>449</th>\n",
       "      <td>7.52601</td>\n",
       "      <td>0.0</td>\n",
       "      <td>18.10</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.7130</td>\n",
       "      <td>6.417</td>\n",
       "      <td>98.3</td>\n",
       "      <td>2.1850</td>\n",
       "      <td>24.0</td>\n",
       "      <td>666.0</td>\n",
       "      <td>20.2</td>\n",
       "      <td>304.21</td>\n",
       "      <td>19.31</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>346</th>\n",
       "      <td>0.06162</td>\n",
       "      <td>0.0</td>\n",
       "      <td>4.39</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.4420</td>\n",
       "      <td>5.898</td>\n",
       "      <td>52.3</td>\n",
       "      <td>8.0136</td>\n",
       "      <td>3.0</td>\n",
       "      <td>352.0</td>\n",
       "      <td>18.8</td>\n",
       "      <td>364.61</td>\n",
       "      <td>12.67</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>351</th>\n",
       "      <td>0.07950</td>\n",
       "      <td>60.0</td>\n",
       "      <td>1.69</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.4110</td>\n",
       "      <td>6.579</td>\n",
       "      <td>35.9</td>\n",
       "      <td>10.7103</td>\n",
       "      <td>4.0</td>\n",
       "      <td>411.0</td>\n",
       "      <td>18.3</td>\n",
       "      <td>370.78</td>\n",
       "      <td>5.49</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>114</th>\n",
       "      <td>0.14231</td>\n",
       "      <td>0.0</td>\n",
       "      <td>10.01</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.5470</td>\n",
       "      <td>6.254</td>\n",
       "      <td>84.2</td>\n",
       "      <td>2.2565</td>\n",
       "      <td>6.0</td>\n",
       "      <td>432.0</td>\n",
       "      <td>17.8</td>\n",
       "      <td>388.74</td>\n",
       "      <td>10.45</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>483</th>\n",
       "      <td>2.81838</td>\n",
       "      <td>0.0</td>\n",
       "      <td>18.10</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.5320</td>\n",
       "      <td>5.762</td>\n",
       "      <td>40.3</td>\n",
       "      <td>4.0983</td>\n",
       "      <td>24.0</td>\n",
       "      <td>666.0</td>\n",
       "      <td>20.2</td>\n",
       "      <td>392.92</td>\n",
       "      <td>10.42</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>138</th>\n",
       "      <td>0.24980</td>\n",
       "      <td>0.0</td>\n",
       "      <td>21.89</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.6240</td>\n",
       "      <td>5.857</td>\n",
       "      <td>98.2</td>\n",
       "      <td>1.6686</td>\n",
       "      <td>4.0</td>\n",
       "      <td>437.0</td>\n",
       "      <td>21.2</td>\n",
       "      <td>392.04</td>\n",
       "      <td>21.32</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>132</th>\n",
       "      <td>0.59005</td>\n",
       "      <td>0.0</td>\n",
       "      <td>21.89</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.6240</td>\n",
       "      <td>6.372</td>\n",
       "      <td>97.9</td>\n",
       "      <td>2.3274</td>\n",
       "      <td>4.0</td>\n",
       "      <td>437.0</td>\n",
       "      <td>21.2</td>\n",
       "      <td>385.76</td>\n",
       "      <td>11.12</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>431</th>\n",
       "      <td>10.06230</td>\n",
       "      <td>0.0</td>\n",
       "      <td>18.10</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.5840</td>\n",
       "      <td>6.833</td>\n",
       "      <td>94.3</td>\n",
       "      <td>2.0882</td>\n",
       "      <td>24.0</td>\n",
       "      <td>666.0</td>\n",
       "      <td>20.2</td>\n",
       "      <td>81.33</td>\n",
       "      <td>19.69</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>343</th>\n",
       "      <td>0.02543</td>\n",
       "      <td>55.0</td>\n",
       "      <td>3.78</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.4840</td>\n",
       "      <td>6.696</td>\n",
       "      <td>56.4</td>\n",
       "      <td>5.7321</td>\n",
       "      <td>5.0</td>\n",
       "      <td>370.0</td>\n",
       "      <td>17.6</td>\n",
       "      <td>396.90</td>\n",
       "      <td>7.18</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.00632</td>\n",
       "      <td>18.0</td>\n",
       "      <td>2.31</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.5380</td>\n",
       "      <td>6.575</td>\n",
       "      <td>65.2</td>\n",
       "      <td>4.0900</td>\n",
       "      <td>1.0</td>\n",
       "      <td>296.0</td>\n",
       "      <td>15.3</td>\n",
       "      <td>396.90</td>\n",
       "      <td>4.98</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>408</th>\n",
       "      <td>7.40389</td>\n",
       "      <td>0.0</td>\n",
       "      <td>18.10</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.5970</td>\n",
       "      <td>5.617</td>\n",
       "      <td>97.9</td>\n",
       "      <td>1.4547</td>\n",
       "      <td>24.0</td>\n",
       "      <td>666.0</td>\n",
       "      <td>20.2</td>\n",
       "      <td>314.64</td>\n",
       "      <td>26.40</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>150</th>\n",
       "      <td>1.65660</td>\n",
       "      <td>0.0</td>\n",
       "      <td>19.58</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.8710</td>\n",
       "      <td>6.122</td>\n",
       "      <td>97.3</td>\n",
       "      <td>1.6180</td>\n",
       "      <td>5.0</td>\n",
       "      <td>403.0</td>\n",
       "      <td>14.7</td>\n",
       "      <td>372.80</td>\n",
       "      <td>14.10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>486</th>\n",
       "      <td>5.69175</td>\n",
       "      <td>0.0</td>\n",
       "      <td>18.10</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.5830</td>\n",
       "      <td>6.114</td>\n",
       "      <td>79.8</td>\n",
       "      <td>3.5459</td>\n",
       "      <td>24.0</td>\n",
       "      <td>666.0</td>\n",
       "      <td>20.2</td>\n",
       "      <td>392.68</td>\n",
       "      <td>14.98</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>362</th>\n",
       "      <td>3.67822</td>\n",
       "      <td>0.0</td>\n",
       "      <td>18.10</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.7700</td>\n",
       "      <td>5.362</td>\n",
       "      <td>96.2</td>\n",
       "      <td>2.1036</td>\n",
       "      <td>24.0</td>\n",
       "      <td>666.0</td>\n",
       "      <td>20.2</td>\n",
       "      <td>380.79</td>\n",
       "      <td>10.19</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>258</th>\n",
       "      <td>0.66351</td>\n",
       "      <td>20.0</td>\n",
       "      <td>3.97</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.6470</td>\n",
       "      <td>7.333</td>\n",
       "      <td>100.0</td>\n",
       "      <td>1.8946</td>\n",
       "      <td>5.0</td>\n",
       "      <td>264.0</td>\n",
       "      <td>13.0</td>\n",
       "      <td>383.29</td>\n",
       "      <td>7.79</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>286</th>\n",
       "      <td>0.01965</td>\n",
       "      <td>80.0</td>\n",
       "      <td>1.76</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.3850</td>\n",
       "      <td>6.230</td>\n",
       "      <td>31.5</td>\n",
       "      <td>9.0892</td>\n",
       "      <td>1.0</td>\n",
       "      <td>241.0</td>\n",
       "      <td>18.2</td>\n",
       "      <td>341.60</td>\n",
       "      <td>12.93</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>437</th>\n",
       "      <td>15.17720</td>\n",
       "      <td>0.0</td>\n",
       "      <td>18.10</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.7400</td>\n",
       "      <td>6.152</td>\n",
       "      <td>100.0</td>\n",
       "      <td>1.9142</td>\n",
       "      <td>24.0</td>\n",
       "      <td>666.0</td>\n",
       "      <td>20.2</td>\n",
       "      <td>9.32</td>\n",
       "      <td>26.45</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>1.25179</td>\n",
       "      <td>0.0</td>\n",
       "      <td>8.14</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.5380</td>\n",
       "      <td>5.570</td>\n",
       "      <td>98.1</td>\n",
       "      <td>3.7979</td>\n",
       "      <td>4.0</td>\n",
       "      <td>307.0</td>\n",
       "      <td>21.0</td>\n",
       "      <td>376.57</td>\n",
       "      <td>21.02</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>120</th>\n",
       "      <td>0.06899</td>\n",
       "      <td>0.0</td>\n",
       "      <td>25.65</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.5810</td>\n",
       "      <td>5.870</td>\n",
       "      <td>69.7</td>\n",
       "      <td>2.2577</td>\n",
       "      <td>2.0</td>\n",
       "      <td>188.0</td>\n",
       "      <td>19.1</td>\n",
       "      <td>389.15</td>\n",
       "      <td>14.37</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>242</th>\n",
       "      <td>0.10290</td>\n",
       "      <td>30.0</td>\n",
       "      <td>4.93</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.4280</td>\n",
       "      <td>6.358</td>\n",
       "      <td>52.9</td>\n",
       "      <td>7.0355</td>\n",
       "      <td>6.0</td>\n",
       "      <td>300.0</td>\n",
       "      <td>16.6</td>\n",
       "      <td>372.75</td>\n",
       "      <td>11.22</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>335</th>\n",
       "      <td>0.03961</td>\n",
       "      <td>0.0</td>\n",
       "      <td>5.19</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.5150</td>\n",
       "      <td>6.037</td>\n",
       "      <td>34.5</td>\n",
       "      <td>5.9853</td>\n",
       "      <td>5.0</td>\n",
       "      <td>224.0</td>\n",
       "      <td>20.2</td>\n",
       "      <td>396.90</td>\n",
       "      <td>8.01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>142</th>\n",
       "      <td>3.32105</td>\n",
       "      <td>0.0</td>\n",
       "      <td>19.58</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.8710</td>\n",
       "      <td>5.403</td>\n",
       "      <td>100.0</td>\n",
       "      <td>1.3216</td>\n",
       "      <td>5.0</td>\n",
       "      <td>403.0</td>\n",
       "      <td>14.7</td>\n",
       "      <td>396.90</td>\n",
       "      <td>26.82</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>170</th>\n",
       "      <td>1.20742</td>\n",
       "      <td>0.0</td>\n",
       "      <td>19.58</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.6050</td>\n",
       "      <td>5.875</td>\n",
       "      <td>94.6</td>\n",
       "      <td>2.4259</td>\n",
       "      <td>5.0</td>\n",
       "      <td>403.0</td>\n",
       "      <td>14.7</td>\n",
       "      <td>292.29</td>\n",
       "      <td>14.43</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>241</th>\n",
       "      <td>0.10612</td>\n",
       "      <td>30.0</td>\n",
       "      <td>4.93</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.4280</td>\n",
       "      <td>6.095</td>\n",
       "      <td>65.1</td>\n",
       "      <td>6.3361</td>\n",
       "      <td>6.0</td>\n",
       "      <td>300.0</td>\n",
       "      <td>16.6</td>\n",
       "      <td>394.62</td>\n",
       "      <td>12.40</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>379</th>\n",
       "      <td>17.86670</td>\n",
       "      <td>0.0</td>\n",
       "      <td>18.10</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.6710</td>\n",
       "      <td>6.223</td>\n",
       "      <td>100.0</td>\n",
       "      <td>1.3861</td>\n",
       "      <td>24.0</td>\n",
       "      <td>666.0</td>\n",
       "      <td>20.2</td>\n",
       "      <td>393.74</td>\n",
       "      <td>21.78</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>68</th>\n",
       "      <td>0.13554</td>\n",
       "      <td>12.5</td>\n",
       "      <td>6.07</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.4090</td>\n",
       "      <td>5.594</td>\n",
       "      <td>36.8</td>\n",
       "      <td>6.4980</td>\n",
       "      <td>4.0</td>\n",
       "      <td>345.0</td>\n",
       "      <td>18.9</td>\n",
       "      <td>396.90</td>\n",
       "      <td>13.09</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>208</th>\n",
       "      <td>0.13587</td>\n",
       "      <td>0.0</td>\n",
       "      <td>10.59</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.4890</td>\n",
       "      <td>6.064</td>\n",
       "      <td>59.1</td>\n",
       "      <td>4.2392</td>\n",
       "      <td>4.0</td>\n",
       "      <td>277.0</td>\n",
       "      <td>18.6</td>\n",
       "      <td>381.32</td>\n",
       "      <td>14.66</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>492</th>\n",
       "      <td>0.11132</td>\n",
       "      <td>0.0</td>\n",
       "      <td>27.74</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.6090</td>\n",
       "      <td>5.983</td>\n",
       "      <td>83.5</td>\n",
       "      <td>2.1099</td>\n",
       "      <td>4.0</td>\n",
       "      <td>711.0</td>\n",
       "      <td>20.1</td>\n",
       "      <td>396.90</td>\n",
       "      <td>13.35</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>420</th>\n",
       "      <td>11.08740</td>\n",
       "      <td>0.0</td>\n",
       "      <td>18.10</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.7180</td>\n",
       "      <td>6.411</td>\n",
       "      <td>100.0</td>\n",
       "      <td>1.8589</td>\n",
       "      <td>24.0</td>\n",
       "      <td>666.0</td>\n",
       "      <td>20.2</td>\n",
       "      <td>318.75</td>\n",
       "      <td>15.02</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>253</th>\n",
       "      <td>0.36894</td>\n",
       "      <td>22.0</td>\n",
       "      <td>5.86</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.4310</td>\n",
       "      <td>8.259</td>\n",
       "      <td>8.4</td>\n",
       "      <td>8.9067</td>\n",
       "      <td>7.0</td>\n",
       "      <td>330.0</td>\n",
       "      <td>19.1</td>\n",
       "      <td>396.90</td>\n",
       "      <td>3.54</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>334</th>\n",
       "      <td>0.03738</td>\n",
       "      <td>0.0</td>\n",
       "      <td>5.19</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.5150</td>\n",
       "      <td>6.310</td>\n",
       "      <td>38.5</td>\n",
       "      <td>6.4584</td>\n",
       "      <td>5.0</td>\n",
       "      <td>224.0</td>\n",
       "      <td>20.2</td>\n",
       "      <td>389.40</td>\n",
       "      <td>6.75</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>339</th>\n",
       "      <td>0.05497</td>\n",
       "      <td>0.0</td>\n",
       "      <td>5.19</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.5150</td>\n",
       "      <td>5.985</td>\n",
       "      <td>45.4</td>\n",
       "      <td>4.8122</td>\n",
       "      <td>5.0</td>\n",
       "      <td>224.0</td>\n",
       "      <td>20.2</td>\n",
       "      <td>396.90</td>\n",
       "      <td>9.74</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>409</th>\n",
       "      <td>14.43830</td>\n",
       "      <td>0.0</td>\n",
       "      <td>18.10</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.5970</td>\n",
       "      <td>6.852</td>\n",
       "      <td>100.0</td>\n",
       "      <td>1.4655</td>\n",
       "      <td>24.0</td>\n",
       "      <td>666.0</td>\n",
       "      <td>20.2</td>\n",
       "      <td>179.36</td>\n",
       "      <td>19.78</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>111</th>\n",
       "      <td>0.10084</td>\n",
       "      <td>0.0</td>\n",
       "      <td>10.01</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.5470</td>\n",
       "      <td>6.715</td>\n",
       "      <td>81.6</td>\n",
       "      <td>2.6775</td>\n",
       "      <td>6.0</td>\n",
       "      <td>432.0</td>\n",
       "      <td>17.8</td>\n",
       "      <td>395.59</td>\n",
       "      <td>10.16</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>224</th>\n",
       "      <td>0.31533</td>\n",
       "      <td>0.0</td>\n",
       "      <td>6.20</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.5040</td>\n",
       "      <td>8.266</td>\n",
       "      <td>78.3</td>\n",
       "      <td>2.8944</td>\n",
       "      <td>8.0</td>\n",
       "      <td>307.0</td>\n",
       "      <td>17.4</td>\n",
       "      <td>385.05</td>\n",
       "      <td>4.14</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>430</th>\n",
       "      <td>8.49213</td>\n",
       "      <td>0.0</td>\n",
       "      <td>18.10</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.5840</td>\n",
       "      <td>6.348</td>\n",
       "      <td>86.1</td>\n",
       "      <td>2.0527</td>\n",
       "      <td>24.0</td>\n",
       "      <td>666.0</td>\n",
       "      <td>20.2</td>\n",
       "      <td>83.45</td>\n",
       "      <td>17.64</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>32</th>\n",
       "      <td>1.38799</td>\n",
       "      <td>0.0</td>\n",
       "      <td>8.14</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.5380</td>\n",
       "      <td>5.950</td>\n",
       "      <td>82.0</td>\n",
       "      <td>3.9900</td>\n",
       "      <td>4.0</td>\n",
       "      <td>307.0</td>\n",
       "      <td>21.0</td>\n",
       "      <td>232.60</td>\n",
       "      <td>27.71</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>73</th>\n",
       "      <td>0.19539</td>\n",
       "      <td>0.0</td>\n",
       "      <td>10.81</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.4130</td>\n",
       "      <td>6.245</td>\n",
       "      <td>6.2</td>\n",
       "      <td>5.2873</td>\n",
       "      <td>4.0</td>\n",
       "      <td>305.0</td>\n",
       "      <td>19.2</td>\n",
       "      <td>377.17</td>\n",
       "      <td>7.54</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>47</th>\n",
       "      <td>0.22927</td>\n",
       "      <td>0.0</td>\n",
       "      <td>6.91</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.4480</td>\n",
       "      <td>6.030</td>\n",
       "      <td>85.5</td>\n",
       "      <td>5.6894</td>\n",
       "      <td>3.0</td>\n",
       "      <td>233.0</td>\n",
       "      <td>17.9</td>\n",
       "      <td>392.74</td>\n",
       "      <td>18.80</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>126</th>\n",
       "      <td>0.38735</td>\n",
       "      <td>0.0</td>\n",
       "      <td>25.65</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.5810</td>\n",
       "      <td>5.613</td>\n",
       "      <td>95.6</td>\n",
       "      <td>1.7572</td>\n",
       "      <td>2.0</td>\n",
       "      <td>188.0</td>\n",
       "      <td>19.1</td>\n",
       "      <td>359.29</td>\n",
       "      <td>27.26</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>113</th>\n",
       "      <td>0.22212</td>\n",
       "      <td>0.0</td>\n",
       "      <td>10.01</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.5470</td>\n",
       "      <td>6.092</td>\n",
       "      <td>95.4</td>\n",
       "      <td>2.5480</td>\n",
       "      <td>6.0</td>\n",
       "      <td>432.0</td>\n",
       "      <td>17.8</td>\n",
       "      <td>396.90</td>\n",
       "      <td>17.09</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>96</th>\n",
       "      <td>0.11504</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2.89</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.4450</td>\n",
       "      <td>6.163</td>\n",
       "      <td>69.6</td>\n",
       "      <td>3.4952</td>\n",
       "      <td>2.0</td>\n",
       "      <td>276.0</td>\n",
       "      <td>18.0</td>\n",
       "      <td>391.83</td>\n",
       "      <td>11.34</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>225</th>\n",
       "      <td>0.52693</td>\n",
       "      <td>0.0</td>\n",
       "      <td>6.20</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.5040</td>\n",
       "      <td>8.725</td>\n",
       "      <td>83.0</td>\n",
       "      <td>2.8944</td>\n",
       "      <td>8.0</td>\n",
       "      <td>307.0</td>\n",
       "      <td>17.4</td>\n",
       "      <td>382.00</td>\n",
       "      <td>4.63</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>214</th>\n",
       "      <td>0.28955</td>\n",
       "      <td>0.0</td>\n",
       "      <td>10.59</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.4890</td>\n",
       "      <td>5.412</td>\n",
       "      <td>9.8</td>\n",
       "      <td>3.5875</td>\n",
       "      <td>4.0</td>\n",
       "      <td>277.0</td>\n",
       "      <td>18.6</td>\n",
       "      <td>348.93</td>\n",
       "      <td>29.55</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>57</th>\n",
       "      <td>0.01432</td>\n",
       "      <td>100.0</td>\n",
       "      <td>1.32</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.4110</td>\n",
       "      <td>6.816</td>\n",
       "      <td>40.5</td>\n",
       "      <td>8.3248</td>\n",
       "      <td>5.0</td>\n",
       "      <td>256.0</td>\n",
       "      <td>15.1</td>\n",
       "      <td>392.90</td>\n",
       "      <td>3.95</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>123</th>\n",
       "      <td>0.15038</td>\n",
       "      <td>0.0</td>\n",
       "      <td>25.65</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.5810</td>\n",
       "      <td>5.856</td>\n",
       "      <td>97.0</td>\n",
       "      <td>1.9444</td>\n",
       "      <td>2.0</td>\n",
       "      <td>188.0</td>\n",
       "      <td>19.1</td>\n",
       "      <td>370.31</td>\n",
       "      <td>25.41</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>106</th>\n",
       "      <td>0.17120</td>\n",
       "      <td>0.0</td>\n",
       "      <td>8.56</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.5200</td>\n",
       "      <td>5.836</td>\n",
       "      <td>91.9</td>\n",
       "      <td>2.2110</td>\n",
       "      <td>5.0</td>\n",
       "      <td>384.0</td>\n",
       "      <td>20.9</td>\n",
       "      <td>395.67</td>\n",
       "      <td>18.66</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>83</th>\n",
       "      <td>0.03551</td>\n",
       "      <td>25.0</td>\n",
       "      <td>4.86</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.4260</td>\n",
       "      <td>6.167</td>\n",
       "      <td>46.7</td>\n",
       "      <td>5.4007</td>\n",
       "      <td>4.0</td>\n",
       "      <td>281.0</td>\n",
       "      <td>19.0</td>\n",
       "      <td>390.64</td>\n",
       "      <td>7.51</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>0.78420</td>\n",
       "      <td>0.0</td>\n",
       "      <td>8.14</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.5380</td>\n",
       "      <td>5.990</td>\n",
       "      <td>81.7</td>\n",
       "      <td>4.2579</td>\n",
       "      <td>4.0</td>\n",
       "      <td>307.0</td>\n",
       "      <td>21.0</td>\n",
       "      <td>386.75</td>\n",
       "      <td>14.67</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>230</th>\n",
       "      <td>0.53700</td>\n",
       "      <td>0.0</td>\n",
       "      <td>6.20</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.5040</td>\n",
       "      <td>5.981</td>\n",
       "      <td>68.1</td>\n",
       "      <td>3.6715</td>\n",
       "      <td>8.0</td>\n",
       "      <td>307.0</td>\n",
       "      <td>17.4</td>\n",
       "      <td>378.35</td>\n",
       "      <td>11.65</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>98</th>\n",
       "      <td>0.08187</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2.89</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.4450</td>\n",
       "      <td>7.820</td>\n",
       "      <td>36.9</td>\n",
       "      <td>3.4952</td>\n",
       "      <td>2.0</td>\n",
       "      <td>276.0</td>\n",
       "      <td>18.0</td>\n",
       "      <td>393.53</td>\n",
       "      <td>3.57</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>476</th>\n",
       "      <td>4.87141</td>\n",
       "      <td>0.0</td>\n",
       "      <td>18.10</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.6140</td>\n",
       "      <td>6.484</td>\n",
       "      <td>93.6</td>\n",
       "      <td>2.3053</td>\n",
       "      <td>24.0</td>\n",
       "      <td>666.0</td>\n",
       "      <td>20.2</td>\n",
       "      <td>396.21</td>\n",
       "      <td>18.68</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>322</th>\n",
       "      <td>0.35114</td>\n",
       "      <td>0.0</td>\n",
       "      <td>7.38</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.4930</td>\n",
       "      <td>6.041</td>\n",
       "      <td>49.9</td>\n",
       "      <td>4.7211</td>\n",
       "      <td>5.0</td>\n",
       "      <td>287.0</td>\n",
       "      <td>19.6</td>\n",
       "      <td>396.90</td>\n",
       "      <td>7.70</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>382</th>\n",
       "      <td>9.18702</td>\n",
       "      <td>0.0</td>\n",
       "      <td>18.10</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.7000</td>\n",
       "      <td>5.536</td>\n",
       "      <td>100.0</td>\n",
       "      <td>1.5804</td>\n",
       "      <td>24.0</td>\n",
       "      <td>666.0</td>\n",
       "      <td>20.2</td>\n",
       "      <td>396.90</td>\n",
       "      <td>23.60</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>365</th>\n",
       "      <td>4.55587</td>\n",
       "      <td>0.0</td>\n",
       "      <td>18.10</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.7180</td>\n",
       "      <td>3.561</td>\n",
       "      <td>87.9</td>\n",
       "      <td>1.6132</td>\n",
       "      <td>24.0</td>\n",
       "      <td>666.0</td>\n",
       "      <td>20.2</td>\n",
       "      <td>354.70</td>\n",
       "      <td>7.12</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>404 rows × 13 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "         CRIM     ZN  INDUS  CHAS     NOX     RM    AGE      DIS   RAD    TAX  \\\n",
       "203   0.03510   95.0   2.68   0.0  0.4161  7.853   33.2   5.1180   4.0  224.0   \n",
       "441   9.72418    0.0  18.10   0.0  0.7400  6.406   97.2   2.0651  24.0  666.0   \n",
       "172   0.13914    0.0   4.05   0.0  0.5100  5.572   88.5   2.5961   5.0  296.0   \n",
       "95    0.12204    0.0   2.89   0.0  0.4450  6.625   57.8   3.4952   2.0  276.0   \n",
       "54    0.01360   75.0   4.00   0.0  0.4100  5.888   47.6   7.3197   3.0  469.0   \n",
       "449   7.52601    0.0  18.10   0.0  0.7130  6.417   98.3   2.1850  24.0  666.0   \n",
       "346   0.06162    0.0   4.39   0.0  0.4420  5.898   52.3   8.0136   3.0  352.0   \n",
       "351   0.07950   60.0   1.69   0.0  0.4110  6.579   35.9  10.7103   4.0  411.0   \n",
       "114   0.14231    0.0  10.01   0.0  0.5470  6.254   84.2   2.2565   6.0  432.0   \n",
       "483   2.81838    0.0  18.10   0.0  0.5320  5.762   40.3   4.0983  24.0  666.0   \n",
       "138   0.24980    0.0  21.89   0.0  0.6240  5.857   98.2   1.6686   4.0  437.0   \n",
       "132   0.59005    0.0  21.89   0.0  0.6240  6.372   97.9   2.3274   4.0  437.0   \n",
       "431  10.06230    0.0  18.10   0.0  0.5840  6.833   94.3   2.0882  24.0  666.0   \n",
       "343   0.02543   55.0   3.78   0.0  0.4840  6.696   56.4   5.7321   5.0  370.0   \n",
       "0     0.00632   18.0   2.31   0.0  0.5380  6.575   65.2   4.0900   1.0  296.0   \n",
       "408   7.40389    0.0  18.10   0.0  0.5970  5.617   97.9   1.4547  24.0  666.0   \n",
       "150   1.65660    0.0  19.58   0.0  0.8710  6.122   97.3   1.6180   5.0  403.0   \n",
       "486   5.69175    0.0  18.10   0.0  0.5830  6.114   79.8   3.5459  24.0  666.0   \n",
       "362   3.67822    0.0  18.10   0.0  0.7700  5.362   96.2   2.1036  24.0  666.0   \n",
       "258   0.66351   20.0   3.97   0.0  0.6470  7.333  100.0   1.8946   5.0  264.0   \n",
       "286   0.01965   80.0   1.76   0.0  0.3850  6.230   31.5   9.0892   1.0  241.0   \n",
       "437  15.17720    0.0  18.10   0.0  0.7400  6.152  100.0   1.9142  24.0  666.0   \n",
       "20    1.25179    0.0   8.14   0.0  0.5380  5.570   98.1   3.7979   4.0  307.0   \n",
       "120   0.06899    0.0  25.65   0.0  0.5810  5.870   69.7   2.2577   2.0  188.0   \n",
       "242   0.10290   30.0   4.93   0.0  0.4280  6.358   52.9   7.0355   6.0  300.0   \n",
       "335   0.03961    0.0   5.19   0.0  0.5150  6.037   34.5   5.9853   5.0  224.0   \n",
       "142   3.32105    0.0  19.58   1.0  0.8710  5.403  100.0   1.3216   5.0  403.0   \n",
       "170   1.20742    0.0  19.58   0.0  0.6050  5.875   94.6   2.4259   5.0  403.0   \n",
       "241   0.10612   30.0   4.93   0.0  0.4280  6.095   65.1   6.3361   6.0  300.0   \n",
       "379  17.86670    0.0  18.10   0.0  0.6710  6.223  100.0   1.3861  24.0  666.0   \n",
       "..        ...    ...    ...   ...     ...    ...    ...      ...   ...    ...   \n",
       "68    0.13554   12.5   6.07   0.0  0.4090  5.594   36.8   6.4980   4.0  345.0   \n",
       "208   0.13587    0.0  10.59   1.0  0.4890  6.064   59.1   4.2392   4.0  277.0   \n",
       "492   0.11132    0.0  27.74   0.0  0.6090  5.983   83.5   2.1099   4.0  711.0   \n",
       "420  11.08740    0.0  18.10   0.0  0.7180  6.411  100.0   1.8589  24.0  666.0   \n",
       "253   0.36894   22.0   5.86   0.0  0.4310  8.259    8.4   8.9067   7.0  330.0   \n",
       "334   0.03738    0.0   5.19   0.0  0.5150  6.310   38.5   6.4584   5.0  224.0   \n",
       "339   0.05497    0.0   5.19   0.0  0.5150  5.985   45.4   4.8122   5.0  224.0   \n",
       "409  14.43830    0.0  18.10   0.0  0.5970  6.852  100.0   1.4655  24.0  666.0   \n",
       "111   0.10084    0.0  10.01   0.0  0.5470  6.715   81.6   2.6775   6.0  432.0   \n",
       "224   0.31533    0.0   6.20   0.0  0.5040  8.266   78.3   2.8944   8.0  307.0   \n",
       "430   8.49213    0.0  18.10   0.0  0.5840  6.348   86.1   2.0527  24.0  666.0   \n",
       "32    1.38799    0.0   8.14   0.0  0.5380  5.950   82.0   3.9900   4.0  307.0   \n",
       "73    0.19539    0.0  10.81   0.0  0.4130  6.245    6.2   5.2873   4.0  305.0   \n",
       "47    0.22927    0.0   6.91   0.0  0.4480  6.030   85.5   5.6894   3.0  233.0   \n",
       "126   0.38735    0.0  25.65   0.0  0.5810  5.613   95.6   1.7572   2.0  188.0   \n",
       "113   0.22212    0.0  10.01   0.0  0.5470  6.092   95.4   2.5480   6.0  432.0   \n",
       "96    0.11504    0.0   2.89   0.0  0.4450  6.163   69.6   3.4952   2.0  276.0   \n",
       "225   0.52693    0.0   6.20   0.0  0.5040  8.725   83.0   2.8944   8.0  307.0   \n",
       "214   0.28955    0.0  10.59   0.0  0.4890  5.412    9.8   3.5875   4.0  277.0   \n",
       "57    0.01432  100.0   1.32   0.0  0.4110  6.816   40.5   8.3248   5.0  256.0   \n",
       "123   0.15038    0.0  25.65   0.0  0.5810  5.856   97.0   1.9444   2.0  188.0   \n",
       "106   0.17120    0.0   8.56   0.0  0.5200  5.836   91.9   2.2110   5.0  384.0   \n",
       "83    0.03551   25.0   4.86   0.0  0.4260  6.167   46.7   5.4007   4.0  281.0   \n",
       "17    0.78420    0.0   8.14   0.0  0.5380  5.990   81.7   4.2579   4.0  307.0   \n",
       "230   0.53700    0.0   6.20   0.0  0.5040  5.981   68.1   3.6715   8.0  307.0   \n",
       "98    0.08187    0.0   2.89   0.0  0.4450  7.820   36.9   3.4952   2.0  276.0   \n",
       "476   4.87141    0.0  18.10   0.0  0.6140  6.484   93.6   2.3053  24.0  666.0   \n",
       "322   0.35114    0.0   7.38   0.0  0.4930  6.041   49.9   4.7211   5.0  287.0   \n",
       "382   9.18702    0.0  18.10   0.0  0.7000  5.536  100.0   1.5804  24.0  666.0   \n",
       "365   4.55587    0.0  18.10   0.0  0.7180  3.561   87.9   1.6132  24.0  666.0   \n",
       "\n",
       "     PTRATIO       B  LSTAT  \n",
       "203     14.7  392.78   3.81  \n",
       "441     20.2  385.96  19.52  \n",
       "172     16.6  396.90  14.69  \n",
       "95      18.0  357.98   6.65  \n",
       "54      21.1  396.90  14.80  \n",
       "449     20.2  304.21  19.31  \n",
       "346     18.8  364.61  12.67  \n",
       "351     18.3  370.78   5.49  \n",
       "114     17.8  388.74  10.45  \n",
       "483     20.2  392.92  10.42  \n",
       "138     21.2  392.04  21.32  \n",
       "132     21.2  385.76  11.12  \n",
       "431     20.2   81.33  19.69  \n",
       "343     17.6  396.90   7.18  \n",
       "0       15.3  396.90   4.98  \n",
       "408     20.2  314.64  26.40  \n",
       "150     14.7  372.80  14.10  \n",
       "486     20.2  392.68  14.98  \n",
       "362     20.2  380.79  10.19  \n",
       "258     13.0  383.29   7.79  \n",
       "286     18.2  341.60  12.93  \n",
       "437     20.2    9.32  26.45  \n",
       "20      21.0  376.57  21.02  \n",
       "120     19.1  389.15  14.37  \n",
       "242     16.6  372.75  11.22  \n",
       "335     20.2  396.90   8.01  \n",
       "142     14.7  396.90  26.82  \n",
       "170     14.7  292.29  14.43  \n",
       "241     16.6  394.62  12.40  \n",
       "379     20.2  393.74  21.78  \n",
       "..       ...     ...    ...  \n",
       "68      18.9  396.90  13.09  \n",
       "208     18.6  381.32  14.66  \n",
       "492     20.1  396.90  13.35  \n",
       "420     20.2  318.75  15.02  \n",
       "253     19.1  396.90   3.54  \n",
       "334     20.2  389.40   6.75  \n",
       "339     20.2  396.90   9.74  \n",
       "409     20.2  179.36  19.78  \n",
       "111     17.8  395.59  10.16  \n",
       "224     17.4  385.05   4.14  \n",
       "430     20.2   83.45  17.64  \n",
       "32      21.0  232.60  27.71  \n",
       "73      19.2  377.17   7.54  \n",
       "47      17.9  392.74  18.80  \n",
       "126     19.1  359.29  27.26  \n",
       "113     17.8  396.90  17.09  \n",
       "96      18.0  391.83  11.34  \n",
       "225     17.4  382.00   4.63  \n",
       "214     18.6  348.93  29.55  \n",
       "57      15.1  392.90   3.95  \n",
       "123     19.1  370.31  25.41  \n",
       "106     20.9  395.67  18.66  \n",
       "83      19.0  390.64   7.51  \n",
       "17      21.0  386.75  14.67  \n",
       "230     17.4  378.35  11.65  \n",
       "98      18.0  393.53   3.57  \n",
       "476     20.2  396.21  18.68  \n",
       "322     19.6  396.90   7.70  \n",
       "382     20.2  396.90  23.60  \n",
       "365     20.2  354.70   7.12  \n",
       "\n",
       "[404 rows x 13 columns]"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "xg_reg = xgb.XGBRegressor(objective ='reg:linear', \n",
    "                          colsample_bytree = 0.3, \n",
    "                          learning_rate = 0.1,\n",
    "                          max_depth = 5, \n",
    "                          alpha = 10, \n",
    "                          n_estimators = 10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Fit the regressor to the training set and make predictions on the test set using the <code>.fit()</code> and <code>.predict()</code> methods."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "xg_reg.fit(X_train,y_train)\n",
    "\n",
    "preds = xg_reg.predict(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Compute the rmse by invoking the <code>mean_sqaured_error</code> function from sklearn's <code>metrics</code> module."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RMSE: 10.569356\n"
     ]
    }
   ],
   "source": [
    "rmse = np.sqrt(mean_squared_error(y_test, preds))\n",
    "print(\"RMSE: %f\" % (rmse))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Well, you can see that your RMSE for the price prediction came out to be around 10.8 per 1000$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## k-fold Cross Validation using XGBoost\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In order to build more robust models, it is common to do a k-fold cross validation where all the entries in the original training dataset are used for both training as well as validation. Also, each entry is used for validation just once. XGBoost supports k-fold cross validation via the <code>cv()</code> method. All you have to do is specify the <code>nfolds</code> parameter, which is the number of cross validation sets you want to build. Also, it supports many other parameters (check out this <a href=\"https://xgboost.readthedocs.io/en/latest/python/python_api.html\">link</a>) like:\n",
    "- <code>num_boost_round</code>: denotes the number of trees you build (analogous to n_estimators)\n",
    "- <code>metrics</code>: tells the evaluation metrics to be watched during CV\n",
    "- <code>as_pandas</code>: to return the results in a pandas DataFrame.\n",
    "- <code>early_stopping_rounds</code>: finishes training of the model early if the hold-out metric (\"rmse\" in our case) does not improve for a given number of rounds.\n",
    "- <code>seed</code>: for reproducibility of results.\n",
    "\n",
    "This time you will create a hyper-parameter dictionary params which holds all the hyper-parameters and their values as key-value pairs but will exclude the <code>n_estimators</code> from the hyper-parameter dictionary because you will use <code>num_boost_rounds</code> instead."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You will use these parameters to build a 3-fold cross validation model by invoking XGBoost's <code>cv()</code> method and store the results in a <code>cv_results</code> DataFrame. Note that here you are using the Dmatrix object you created before."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[19:33:01] src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 6 extra nodes, 0 pruned nodes, max_depth=2\n",
      "[19:33:01] src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 6 extra nodes, 0 pruned nodes, max_depth=2\n",
      "[19:33:01] src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 6 extra nodes, 0 pruned nodes, max_depth=2\n",
      "[19:33:01] src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 4 extra nodes, 0 pruned nodes, max_depth=2\n",
      "[19:33:01] src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 2 extra nodes, 0 pruned nodes, max_depth=1\n",
      "[19:33:01] src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 6 extra nodes, 0 pruned nodes, max_depth=3\n",
      "[19:33:01] src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 8 extra nodes, 0 pruned nodes, max_depth=3\n",
      "[19:33:01] src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 6 extra nodes, 0 pruned nodes, max_depth=2\n",
      "[19:33:01] src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 6 extra nodes, 0 pruned nodes, max_depth=3\n",
      "[19:33:01] src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 4 extra nodes, 0 pruned nodes, max_depth=2\n",
      "[19:33:01] src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 6 extra nodes, 0 pruned nodes, max_depth=3\n",
      "[19:33:01] src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 2 extra nodes, 0 pruned nodes, max_depth=1\n",
      "[19:33:01] src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 10 extra nodes, 0 pruned nodes, max_depth=3\n",
      "[19:33:01] src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 6 extra nodes, 0 pruned nodes, max_depth=2\n",
      "[19:33:01] src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 8 extra nodes, 0 pruned nodes, max_depth=3\n",
      "[19:33:01] src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 8 extra nodes, 0 pruned nodes, max_depth=3\n",
      "[19:33:01] src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 6 extra nodes, 0 pruned nodes, max_depth=2\n",
      "[19:33:01] src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 4 extra nodes, 0 pruned nodes, max_depth=2\n",
      "[19:33:01] src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 6 extra nodes, 0 pruned nodes, max_depth=3\n",
      "[19:33:01] src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 10 extra nodes, 0 pruned nodes, max_depth=4\n",
      "[19:33:01] src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 6 extra nodes, 0 pruned nodes, max_depth=2\n",
      "[19:33:01] src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 10 extra nodes, 0 pruned nodes, max_depth=3\n",
      "[19:33:01] src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 10 extra nodes, 0 pruned nodes, max_depth=4\n",
      "[19:33:01] src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 10 extra nodes, 0 pruned nodes, max_depth=4\n",
      "[19:33:01] src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 8 extra nodes, 0 pruned nodes, max_depth=3\n",
      "[19:33:01] src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 6 extra nodes, 0 pruned nodes, max_depth=3\n",
      "[19:33:01] src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 10 extra nodes, 0 pruned nodes, max_depth=4\n",
      "[19:33:01] src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 12 extra nodes, 0 pruned nodes, max_depth=4\n",
      "[19:33:01] src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 12 extra nodes, 0 pruned nodes, max_depth=3\n",
      "[19:33:01] src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 12 extra nodes, 0 pruned nodes, max_depth=4\n",
      "[19:33:01] src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 10 extra nodes, 0 pruned nodes, max_depth=4\n",
      "[19:33:01] src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 12 extra nodes, 0 pruned nodes, max_depth=4\n",
      "[19:33:01] src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 12 extra nodes, 0 pruned nodes, max_depth=3\n",
      "[19:33:01] src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 12 extra nodes, 0 pruned nodes, max_depth=4\n",
      "[19:33:01] src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 6 extra nodes, 0 pruned nodes, max_depth=2\n",
      "[19:33:01] src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 14 extra nodes, 0 pruned nodes, max_depth=5\n",
      "[19:33:01] src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 10 extra nodes, 0 pruned nodes, max_depth=4\n",
      "[19:33:01] src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 4 extra nodes, 0 pruned nodes, max_depth=2\n",
      "[19:33:01] src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 6 extra nodes, 0 pruned nodes, max_depth=2\n",
      "[19:33:01] src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 6 extra nodes, 0 pruned nodes, max_depth=2\n",
      "[19:33:01] src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 10 extra nodes, 0 pruned nodes, max_depth=3\n",
      "[19:33:01] src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 14 extra nodes, 0 pruned nodes, max_depth=5\n",
      "[19:33:01] src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 10 extra nodes, 0 pruned nodes, max_depth=3\n",
      "[19:33:01] src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 20 extra nodes, 0 pruned nodes, max_depth=5\n",
      "[19:33:01] src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 16 extra nodes, 0 pruned nodes, max_depth=5\n",
      "[19:33:01] src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 10 extra nodes, 0 pruned nodes, max_depth=4\n",
      "[19:33:01] src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 20 extra nodes, 0 pruned nodes, max_depth=5\n",
      "[19:33:01] src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 8 extra nodes, 0 pruned nodes, max_depth=3\n",
      "[19:33:01] src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 18 extra nodes, 0 pruned nodes, max_depth=5\n",
      "[19:33:01] src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 18 extra nodes, 0 pruned nodes, max_depth=5\n",
      "[19:33:01] src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 20 extra nodes, 0 pruned nodes, max_depth=5\n",
      "[19:33:01] src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 14 extra nodes, 0 pruned nodes, max_depth=5\n",
      "[19:33:01] src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 6 extra nodes, 0 pruned nodes, max_depth=2\n",
      "[19:33:01] src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 10 extra nodes, 0 pruned nodes, max_depth=4\n",
      "[19:33:01] src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 14 extra nodes, 0 pruned nodes, max_depth=5\n",
      "[19:33:01] src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 18 extra nodes, 0 pruned nodes, max_depth=5\n",
      "[19:33:01] src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 16 extra nodes, 0 pruned nodes, max_depth=4\n",
      "[19:33:01] src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 16 extra nodes, 0 pruned nodes, max_depth=5\n",
      "[19:33:01] src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 22 extra nodes, 0 pruned nodes, max_depth=5\n",
      "[19:33:01] src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 16 extra nodes, 0 pruned nodes, max_depth=5\n",
      "[19:33:01] src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 18 extra nodes, 0 pruned nodes, max_depth=5\n",
      "[19:33:01] src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 16 extra nodes, 0 pruned nodes, max_depth=5\n",
      "[19:33:01] src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 18 extra nodes, 0 pruned nodes, max_depth=5\n",
      "[19:33:01] src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 10 extra nodes, 0 pruned nodes, max_depth=4\n",
      "[19:33:01] src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 20 extra nodes, 0 pruned nodes, max_depth=5\n",
      "[19:33:01] src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 10 extra nodes, 0 pruned nodes, max_depth=4\n",
      "[19:33:01] src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 22 extra nodes, 0 pruned nodes, max_depth=5\n",
      "[19:33:01] src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 22 extra nodes, 0 pruned nodes, max_depth=5\n",
      "[19:33:01] src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 20 extra nodes, 0 pruned nodes, max_depth=5\n",
      "[19:33:01] src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 16 extra nodes, 0 pruned nodes, max_depth=4\n",
      "[19:33:01] src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 16 extra nodes, 0 pruned nodes, max_depth=5\n",
      "[19:33:01] src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 28 extra nodes, 0 pruned nodes, max_depth=5\n",
      "[19:33:01] src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 14 extra nodes, 0 pruned nodes, max_depth=5\n",
      "[19:33:01] src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 10 extra nodes, 0 pruned nodes, max_depth=4\n",
      "[19:33:01] src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 16 extra nodes, 0 pruned nodes, max_depth=5\n",
      "[19:33:01] src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 20 extra nodes, 0 pruned nodes, max_depth=5\n",
      "[19:33:01] src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 14 extra nodes, 0 pruned nodes, max_depth=5\n",
      "[19:33:01] src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 26 extra nodes, 0 pruned nodes, max_depth=5\n",
      "[19:33:01] src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 20 extra nodes, 0 pruned nodes, max_depth=5\n",
      "[19:33:01] src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 8 extra nodes, 0 pruned nodes, max_depth=4\n",
      "[19:33:01] src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 30 extra nodes, 0 pruned nodes, max_depth=5\n",
      "[19:33:01] src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 22 extra nodes, 0 pruned nodes, max_depth=5\n",
      "[19:33:01] src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 32 extra nodes, 0 pruned nodes, max_depth=5\n",
      "[19:33:01] src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 20 extra nodes, 0 pruned nodes, max_depth=5\n",
      "[19:33:01] src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 14 extra nodes, 0 pruned nodes, max_depth=5\n",
      "[19:33:01] src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 20 extra nodes, 0 pruned nodes, max_depth=5\n",
      "[19:33:01] src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 22 extra nodes, 0 pruned nodes, max_depth=5\n",
      "[19:33:01] src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 12 extra nodes, 0 pruned nodes, max_depth=5\n",
      "[19:33:01] src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 10 extra nodes, 0 pruned nodes, max_depth=5\n",
      "[19:33:01] src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 24 extra nodes, 0 pruned nodes, max_depth=5\n",
      "[19:33:01] src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 22 extra nodes, 0 pruned nodes, max_depth=5\n",
      "[19:33:01] src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 24 extra nodes, 0 pruned nodes, max_depth=5\n",
      "[19:33:01] src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 26 extra nodes, 0 pruned nodes, max_depth=5\n",
      "[19:33:01] src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 18 extra nodes, 0 pruned nodes, max_depth=5\n",
      "[19:33:01] src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 16 extra nodes, 0 pruned nodes, max_depth=5\n",
      "[19:33:01] src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 16 extra nodes, 0 pruned nodes, max_depth=5\n",
      "[19:33:01] src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 20 extra nodes, 0 pruned nodes, max_depth=5\n",
      "[19:33:01] src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 20 extra nodes, 0 pruned nodes, max_depth=5\n",
      "[19:33:01] src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 24 extra nodes, 0 pruned nodes, max_depth=5\n",
      "[19:33:01] src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 28 extra nodes, 0 pruned nodes, max_depth=5\n",
      "[19:33:01] src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 14 extra nodes, 0 pruned nodes, max_depth=5\n",
      "[19:33:01] src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 12 extra nodes, 0 pruned nodes, max_depth=5\n",
      "[19:33:01] src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 18 extra nodes, 0 pruned nodes, max_depth=5\n",
      "[19:33:01] src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 20 extra nodes, 0 pruned nodes, max_depth=5\n",
      "[19:33:01] src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 18 extra nodes, 0 pruned nodes, max_depth=5\n",
      "[19:33:01] src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 22 extra nodes, 0 pruned nodes, max_depth=5\n",
      "[19:33:01] src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 24 extra nodes, 0 pruned nodes, max_depth=5\n",
      "[19:33:01] src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 16 extra nodes, 0 pruned nodes, max_depth=5\n",
      "[19:33:01] src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 22 extra nodes, 0 pruned nodes, max_depth=5\n",
      "[19:33:01] src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 16 extra nodes, 0 pruned nodes, max_depth=5\n",
      "[19:33:01] src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 12 extra nodes, 0 pruned nodes, max_depth=5\n",
      "[19:33:01] src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 20 extra nodes, 0 pruned nodes, max_depth=5\n",
      "[19:33:01] src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 26 extra nodes, 0 pruned nodes, max_depth=5\n",
      "[19:33:01] src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 24 extra nodes, 0 pruned nodes, max_depth=5\n",
      "[19:33:01] src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 28 extra nodes, 0 pruned nodes, max_depth=5\n",
      "[19:33:01] src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 30 extra nodes, 0 pruned nodes, max_depth=5\n",
      "[19:33:01] src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 22 extra nodes, 0 pruned nodes, max_depth=5\n",
      "[19:33:01] src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 20 extra nodes, 0 pruned nodes, max_depth=5\n",
      "[19:33:01] src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 22 extra nodes, 0 pruned nodes, max_depth=5\n",
      "[19:33:01] src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 16 extra nodes, 0 pruned nodes, max_depth=5\n",
      "[19:33:01] src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 14 extra nodes, 0 pruned nodes, max_depth=5\n",
      "[19:33:01] src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 20 extra nodes, 0 pruned nodes, max_depth=5\n",
      "[19:33:01] src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 24 extra nodes, 0 pruned nodes, max_depth=5\n",
      "[19:33:01] src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 4 extra nodes, 0 pruned nodes, max_depth=2\n",
      "[19:33:01] src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 28 extra nodes, 0 pruned nodes, max_depth=5\n",
      "[19:33:01] src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 20 extra nodes, 0 pruned nodes, max_depth=5\n",
      "[19:33:01] src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 22 extra nodes, 0 pruned nodes, max_depth=5\n",
      "[19:33:01] src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 20 extra nodes, 0 pruned nodes, max_depth=5\n",
      "[19:33:01] src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 12 extra nodes, 0 pruned nodes, max_depth=5\n",
      "[19:33:01] src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 10 extra nodes, 0 pruned nodes, max_depth=4\n",
      "[19:33:01] src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 26 extra nodes, 0 pruned nodes, max_depth=5\n",
      "[19:33:01] src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 18 extra nodes, 0 pruned nodes, max_depth=5\n",
      "[19:33:01] src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 18 extra nodes, 0 pruned nodes, max_depth=5\n",
      "[19:33:01] src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 18 extra nodes, 0 pruned nodes, max_depth=5\n",
      "[19:33:01] src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 12 extra nodes, 0 pruned nodes, max_depth=5\n",
      "[19:33:01] src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 22 extra nodes, 0 pruned nodes, max_depth=5\n",
      "[19:33:01] src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 24 extra nodes, 0 pruned nodes, max_depth=5\n",
      "[19:33:01] src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 18 extra nodes, 0 pruned nodes, max_depth=5\n",
      "[19:33:01] src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 26 extra nodes, 0 pruned nodes, max_depth=5\n",
      "[19:33:01] src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 18 extra nodes, 0 pruned nodes, max_depth=5\n",
      "[19:33:01] src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 22 extra nodes, 0 pruned nodes, max_depth=5\n",
      "[19:33:01] src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 16 extra nodes, 0 pruned nodes, max_depth=5\n",
      "[19:33:01] src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 18 extra nodes, 0 pruned nodes, max_depth=5\n",
      "[19:33:01] src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 26 extra nodes, 0 pruned nodes, max_depth=5\n",
      "[19:33:01] src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 18 extra nodes, 0 pruned nodes, max_depth=5\n",
      "[19:33:01] src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 18 extra nodes, 0 pruned nodes, max_depth=5\n",
      "[19:33:01] src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 18 extra nodes, 0 pruned nodes, max_depth=5\n",
      "[19:33:01] src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 16 extra nodes, 0 pruned nodes, max_depth=5\n",
      "[19:33:01] src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 18 extra nodes, 0 pruned nodes, max_depth=5\n",
      "[19:33:01] src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 18 extra nodes, 0 pruned nodes, max_depth=5\n"
     ]
    }
   ],
   "source": [
    "params = {\"objective\":\"reg:linear\",\n",
    "          'colsample_bytree': 0.3,\n",
    "          'learning_rate': 0.1,\n",
    "          'max_depth': 5,\n",
    "          'alpha': 10}\n",
    "\n",
    "cv_results = xgb.cv(dtrain=data_dmatrix,\n",
    "                    params=params,\n",
    "                    nfold=3,\n",
    "                    num_boost_round=50,\n",
    "                    early_stopping_rounds=10,\n",
    "                    metrics=\"rmse\",\n",
    "                    as_pandas=True,\n",
    "                    seed=123)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<code>cv_results</code> contains train and test RMSE metrics for each boosting round."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>train-rmse-mean</th>\n",
       "      <th>train-rmse-std</th>\n",
       "      <th>test-rmse-mean</th>\n",
       "      <th>test-rmse-std</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>21.652973</td>\n",
       "      <td>0.038276</td>\n",
       "      <td>21.667104</td>\n",
       "      <td>0.071389</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>19.743088</td>\n",
       "      <td>0.092474</td>\n",
       "      <td>19.772359</td>\n",
       "      <td>0.027169</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>17.990501</td>\n",
       "      <td>0.113422</td>\n",
       "      <td>18.073048</td>\n",
       "      <td>0.076570</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>16.491336</td>\n",
       "      <td>0.112586</td>\n",
       "      <td>16.593016</td>\n",
       "      <td>0.106891</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>15.014008</td>\n",
       "      <td>0.102990</td>\n",
       "      <td>15.168027</td>\n",
       "      <td>0.114311</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   train-rmse-mean  train-rmse-std  test-rmse-mean  test-rmse-std\n",
       "0        21.652973        0.038276       21.667104       0.071389\n",
       "1        19.743088        0.092474       19.772359       0.027169\n",
       "2        17.990501        0.113422       18.073048       0.076570\n",
       "3        16.491336        0.112586       16.593016       0.106891\n",
       "4        15.014008        0.102990       15.168027       0.114311"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cv_results.head() # num_boost_round 매 라운드 마다의 결과"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Extract and print the final boosting round metric."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "49    3.848362\n",
      "Name: test-rmse-mean, dtype: float64\n"
     ]
    }
   ],
   "source": [
    "print((cv_results[\"test-rmse-mean\"]).tail(1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can see that your RMSE for the price prediction has reduced as compared to last time and came out to be around 4.03 per 1000$. You can reach an even lower RMSE for a different set of hyper-parameters. You may consider applying techniques like Grid Search, Random Search and Bayesian Optimization to reach the optimal set of hyper-parameters."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visualize Boosting Trees and Feature Importance"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can also visualize individual trees from the fully boosted model that XGBoost creates using the entire housing dataset. XGBoost has a <code>plot_tree()</code> function that makes this type of visualization easy. Once you train a model using the XGBoost learning API, you can pass it to the <code>plot_tree()</code> function along with the number of trees you want to plot using the num_trees argument."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[19:39:43] src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 6 extra nodes, 0 pruned nodes, max_depth=2\n",
      "[19:39:43] src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 8 extra nodes, 0 pruned nodes, max_depth=3\n",
      "[19:39:43] src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 4 extra nodes, 0 pruned nodes, max_depth=2\n",
      "[19:39:43] src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 12 extra nodes, 0 pruned nodes, max_depth=3\n",
      "[19:39:43] src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 14 extra nodes, 0 pruned nodes, max_depth=4\n",
      "[19:39:43] src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 8 extra nodes, 0 pruned nodes, max_depth=3\n",
      "[19:39:43] src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 8 extra nodes, 0 pruned nodes, max_depth=3\n",
      "[19:39:43] src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 12 extra nodes, 0 pruned nodes, max_depth=5\n",
      "[19:39:43] src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 8 extra nodes, 0 pruned nodes, max_depth=3\n",
      "[19:39:43] src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 14 extra nodes, 0 pruned nodes, max_depth=5\n"
     ]
    }
   ],
   "source": [
    "xg_reg = xgb.train(params=params, \n",
    "                   dtrain=data_dmatrix, \n",
    "                   num_boost_round=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting graphviz\n",
      "  Downloading https://files.pythonhosted.org/packages/1f/e2/ef2581b5b86625657afd32030f90cf2717456c1d2b711ba074bf007c0f1a/graphviz-0.10.1-py2.py3-none-any.whl\n",
      "\u001b[31mdistributed 1.21.8 requires msgpack, which is not installed.\u001b[0m\n",
      "Installing collected packages: graphviz\n",
      "Successfully installed graphviz-0.10.1\n",
      "\u001b[33mYou are using pip version 10.0.1, however version 18.1 is available.\n",
      "You should consider upgrading via the 'pip install --upgrade pip' command.\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "#! pip install graphviz"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAnUAAAJCCAYAAACmiDFBAAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADl0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uIDIuMi4yLCBodHRwOi8vbWF0cGxvdGxpYi5vcmcvhp/UCwAAIABJREFUeJzs3XlYVGXjPvB7AAcQZBEUFClxQVHBXTZJc0XFyg3NvRRNSF/TlrfeTLT6mltmJpi4VFZuuf5UILdcCBdUBLRcAUUkFkVBlmHg+f3B67yRG+rAMzPcn+vikjnnzJl7ppSbc87zHIUQAkRERESk34xkByAiIiKi58dSR0RERGQAWOqIiIiIDABLHREREZEBYKkjIiIiMgAsdUREREQGgKWOiIiIyACw1BEREREZAJY6IiIiIgNgIjvAf/G2FkREREQPCgUwpzIb8kgdERERkQFgqSMiIiIyALpy+pWISGeo1WrcunUL2dnZyMnJQXZ2Nu7cuYO8vDwUFxdDpVKhuLgYRkZGUCqVqFOnDpRKJczMzGBraws7OzvY29ujWbNmMDLi785EVD1Y6oioxrp06RIuXLiA5ORkpKSkaP6Mj4+HEBUv9a1duzasrKygVCphamoKpVIJIQRUKpWm7BUXF6OwsFDzHCMjI9jb26Nx48ZwcXHR/Nm/f384OztX99slIgOn+Oc/XJLoRAgiMlwFBQU4efIkzp49i4SEBCQkJODkyZMAAHt7+wqly9PTE3Z2dpojbnZ2dqhVq1alXqeoqEhzdO/ChQvIyMioUBiTk5Nx584d2NjYwN3dHR4eHnB3d0f79u3RpUuXqvwIiEg/haKSAyVY6ojIYN28eRMxMTFYsGABzpw5A7VaDXt7e02ZGjVqFNzc3GBpaVmtuY4ePaoplgkJCUhKSkJeXh66desGX19f+Pr6wtvbG7a2ttWai4h0UihY6oioplGpVDh8+DCioqIQGRmJ8+fPw9jYGFOmTNGUJV087SmEwMWLF7FgwQLExMTgwoULMDIyQqdOndCvXz988sknvDaPqOYKBUsdEdUUOTk52LZtG6ZPn4579+7Bzc0N/fr1Q9++feHt7Y06derIjvhUsrOzceTIEURFRSEqKgr37t1Dnz598NprryEgIAC1a9eWHZGIqk8oWOqIyBCVlpZiyJAhiIyMhEKhgL+/PwIDA/HKK69U+2nU6paVlYUtW7Zg48aNOHz4MMaOHYugoCD4+PjIjkZEVScULHVEZEhSU1OxevVqrF27Fs2bN8f48ePx2muvwcrKSnY0KTIyMtC/f3+cOXMGrVq1wsSJEzF27FjY2dnJjkZE2hUK3lGCiAzBwYMHERAQgCZNmmDlypUYPXo0Dhw4gLFjx9bYQgcAjo6OOH36NOLi4uDn54fZs2fDyckJY8eOlR2NiCRhqSMinaRWq9GhQwf06NED+fn52Lx5M9LS0jBv3jzZ0XRKx44dsWLFCty8eRPLly/HmTNn0Lt3b0RHR8uORkTVjKWOiHRKUVERli5diiZNmsDV1RUnT57Eb7/9hsGDB8PEhPOlP4qFhQUmTJiAxMREAIC/vz88PDywfv16lJWVSU5HRNWBpY6IdIJKpUJ4eDiaNWuGDz/8EIMHD8aGDRvQqVMn2dH0zt69exEfHw8PDw+MHj0aHh4e2LJli+xYRFTFWOqISKq9e/dCqVRi6tSpCAgIQFpaGgoKCvDVV1/JjqbX2rZtix9//BGlpaVISkqCq6srFAoFunfvjrNnz8qOR0RVgKWOiKS4dOkSXnnlFfTp0wfnz5/Ht99+q5MTAxsKd3d3HDt2DEVFRejQoQMmT56MrKws2bGISItY6oioWt27dw/vvfceWrdujeTkZOzduxfNmjWTHatG8PT0RGxsLL7//nvs3r0bzZs3x5IlS2THIiItYakjomoTHR2N1q1bY/Xq1fjqq68QHx+PXr16yY5VoygUCowePRoXLlzA1KlT8eGHHyI+Pl52LCLSApY6IqoW2dnZ8Pf3h6enJ/744w8EBwfD2NhYdqway8LCAp9++ini4+PRuXNnfPjhhygqKpIdi4ieA0sdEVW5zZs3w83NDTt37sTGjRvh4OAgOxL9V8uWLfH1118jLCwM7u7uiImJkR2JiJ4RSx0RVYljx46hWbNmqF+/PszMzJCVlYWBAwfKjkUPMWXKFNy5cweXLl1CfHw8ateuDW9vb9mxiOgpsdQRkdZ99tln8PPzQ/PmzZGQkMAyp0dCQkIQFxeHwsJC/Pjjj7LjENFTUAghZGcAAJ0IQUTPLzc3Fw4ODliwYAGmTZsGhUIhOxI9A5VKBVNTU4SEhGDJkiWoVauW7EhENVUogDmV2ZBH6ohIaxISEtCpUyccOnQI//rXv1jo9JhSqcTmzZvx/fffo3v37khPT5cdiYiegKWOiLTi559/hre3Nxo1agQvLy/ZcUgLhg4dihMnTiAnJwcdOnTAkSNHZEciosdgqSOi5/bJJ59g1KhRmDx5Mvbt2yc7DmmRm5sbTpw4AR8fH/Ts2RM//PCD7EhE9AgsdUT0zLp37w5ra2v4+flBCIEvv/wSJiYmsmORlllZWWHr1q1QqVQoLi7GuHHjUFJSIjsWEf0DB0oQ0TNJS0uDj48P9uzZgzZt2siOQ9XIysoKnTt3xo4dO2BpaSk7DpGhCwUHShBRVbl69Sr8/Pxw/PhxFroa6MiRIzh37hx69+6N3Nxc2XGI6L9Y6ojoqfzxxx/w8/ODnZ0dGjRoIDsOSdC2bVscPnwYaWlpePnll5GVlSU7EhGBpY6InkJ8fDy6desGFxcXHDhwQHYcksjV1RVHjhzB3bt30a1bN055QqQDWOqIqFISEhLQo0cPtG3bFr/++iusrKxkRyLJGjdurJnmpHv37vjrr78kJyKq2ThMjYieyNnZGc7Ozrh27RovjKcKGjZsiPPnz+P69evw9PREQkICCz+RJDxSR0SPdePGDdSrVw979uxhoaNHcnZ2xv79+zFgwAAUFBTIjkNUI7HUEdEj5eXloX///vj1119hY2MjOw7puKZNm+LPP//EiBEjUFZWJjsOUY3DUkdED6VWqzFs2DBkZmbC3t5edhzSEzt37sTevXsxffp02VGIahxeU0dEDxUcHIyjR4/i0KFDsqOQHvH29sa6desQGBiIJk2asNwRVSMeqSOiB3zxxRdYvXo11q9fj44dO8qOQ3pm6NChmD9/PmbOnInt27fLjkNUY7DUEZHG5cuXYWtri0uXLqG0tBQDBw6UHYn01HvvvYeioiIsXrwYrq6usuMQ1Qi89ysRaXh4eECpVOLo0aMwMzOTHYcMQEZGBjp06IAbN25AoVDIjkOkj0LBe78S0dNKT0/Hli1bWOhIaxwdHbF582b83//9n+woRAaPR+qICACwYsUKNG3aFL1795YdhQyQsbEx9u/fj+7du8uOQqRvQlHJI3UsdUSEP//8Ex07dsS9e/dkRyEDNXjwYMTFxeHs2bOwtbWVHYdIn4SCp1+JqDJUKhVGjhyJNm3ayI5CBiwiIgKlpaV46623ZEchMlgsdUQ12G+//QYzMzO8/fbbOH78uOw4ZMDs7Oxw48YNODs7w8rKCikpKbIjERkcnn4lqqEKCwvh4eGBVq1aYceOHbLjUA2hUqnQvn17NGrUCNHR0bLjEOmDUPD0KxE9zpw5c5CZmYmwsDDZUagGUSqVWLVqFfbt24cffvhBdhwig8JSR1QDxcfHY/HixZg/fz6cnJxkx6EaxtvbG8HBwZgxY4bsKEQGhadfiWqgjh07wsLCAocOHeKEsCRFXl4eWrdujWvXrsmOQqTrQsHTr0T0KElJSYiIiGChI2nq1KmDsLAwHDx4UHYUIoPBUkdUw0ybNg1ZWVlo0aKF7ChUwwUEBKBv375YsWKF7ChEBoGljqgGOX/+PMLDw2FlZSU7ChEA4O2338asWbOQm5srOwqR3mOpI6pBZsyYAXd3d9kxiDQ++eQTAMCnn34qOQmR/mOpI6ohDh8+jOjoaCxevFh2FCINGxsbzJ49G8uXL8eNGzdkxyHSaxz9SlRDdOvWDSYmJti/f7/sKEQVqFQqNG/eHP3790d4eLjsOES6JhQc/UpE9+3duxeHDx/mKS7SSUqlEh9//DFWr17N24cRPQcT2QGIqGpFRUWhX79+OHnyJDp16iQ7DtFDBQUFQalUonnz5rhw4QKaNGkiOxKR3uHpVyID16NHDyiVSkRFRcmOQvRYpaWlaNasGQICArBs2TLZcYh0RSh4+pWITp8+jYMHD+Ldd9+VHYXoiYyNjTF9+nSsXbtWdhQivcQjdUQGbNSoUTh37hzi4+NlRyGqlPz8fDg7O+P27duyoxDpilBU8kgdr6kjMlDXr1/Hpk2bsGbNGtlRiCrN0tISkydPRnFxMUxNTWXHIdIrPP1KZKC++eYbODg4YMSIEbKjED2VadOmYcOGDbJjEOkdnn4lMlDGxsZISUmBs7Oz7ChET02hUODcuXNo1aqV7ChEsoWCAyWIajZ/f38WOtJbLi4uiIiIkB2DSK+w1BEZoJSUFAQFBcmOQfTMJk6ciHXr1qG4uFh2FCK9wVJHZIDWrl2LgIAA2TGIntn48eORm5uLHTt2yI5CpDdY6ogM0MaNG2FiwsHtpL8aNmyInj17csAE0VNgqSMyMGfPnsWFCxdkxyB6boGBgYiMjEReXp7sKER6gaWOyMAMGTKEd5AggzBhwgS8/PLLGDVqlOwoRHqBpY7IgJw+fRpXrlzB8OHDZUch0orhw4cjOjoad+/elR2FSOex1BEZkN27d8PJyQmdOnWSHYVIKwYOHAi1Wo19+/bJjkKk81jqiAxIVFQU/P39Zccg0pq6deuiS5cuiIyMlB2FSOex1BEZiNu3b+P48eMsdWRw+vXrh6ioKNkxiHQeSx2Rgfj111+hUCjQu3dv2VGItMrf3x9paWlISkqSHYVIp7HUERmIqKgoeHt7w9raWnYUIq3q1KkT7O3teQqW6AlY6ogMQHp6Or777jvMnj1bdhQirTMyMsKsWbMwb9482VGIdBpLHZEBOHr0KIyNjeHp6Sk7ClGV6Nq1K27fvi07BpFOY6kjMgAxMTFo27YtLC0tZUchqhJt27aFhYWF7BhEOo2ljsgAxMTEwNfXV3YMoipjbGwMLy8v2TGIdBpLHZGeKy4uxtmzZ+Hj4yM7ClGV8vb2lh2BSKex1BHpufPnz0OtVqNt27ayoxBVqbZt26KwsFB2DCKdxVJHpOd++ukndOjQAW5ubrKjEFWpwYMHY/369bJjEOksljoiPZeQkAB3d3fZMYiqnJGRERITE2XHINJZLHVEei4xMREeHh6yYxBVC5Y6okdjqSPScxkZGXB1dZUdg6haXL58WXYEIp3FUkdkABo3biw7AlG1SEtLg1qtlh2DSCex1BEZABcXF9kRiKpFaWkprl+/LjsGkU5iqSPSc507d+ZM+1RjWFhY4ODBg7JjEOkkljoiPffCCy/IjkBUbZydnXmkjugRWOqI9Fy9evVkRyCqNvXq1UN2drbsGEQ6iaWOSM/Z2dnJjkBUbezs7FjqiB6BpY5Iz9nb28uOQFRt7O3tkZOTIzsGkU5iqSPSczxSZ7jKyspkR9A5dnZ2LHVEj8BSR6THMjIypMxRl5aWhpEjR0KhUGDTpk0oKCh46HaJiYkwMzODt7c3du7ciS5dukChUGDq1KkAgOjoaCgUCnz66adYtmwZzMzMoFAo8J///Afz58+HUqnEzJkzH9ivpaUlcnNzKyzz9vbGv/71LyxevBiDBw+GQqFA+/btsXTpUrRu3RoKhQJ5eXmVen/h4eGoXbs2IiMjkZubi5kzZ6J27dqP3N7U1BQKheKBr507dwIA1q5dC4VCgRdeeAEmJiYwMTHBr7/+WmEfZWVlDzz//ijPffv2wcTEBF988QVSUlLwxhtvoGHDhprn/nN9w4YNkZaWplmvVqvRq1cvnD59GpGRkahXrx4++eQTzfqjR4/C1NQUgwYNgre3NxQKBerUqaNZHx0dDWNjY6xfvx7Z2dkwNjbG3LlzK/VZalvjxo2RkpIi5bWJdJ4QQhe+iOgZpKamiuPHj0t57e3btwsHB4fHbvP6668LAOLo0aNCCCHUarUYOHCgGDx4sBBCiG3btgk/Pz/N9o6OjgKASEpKEkIIMXv2bBEcHFxhn9euXRMAxIIFCyosb9u2reb7FStWCABi6NChQgghbt26JWxsbERmZmal3puFhYUYOXKk5nFZWZlwcnIShYWFj3yfFy5cEMXFxZqvhQsXisLCQnH79m3h4OAgDh06VCF/hw4dKuxj27ZtIjU1VfOVlpamWWdra1th+zt37og6deoIIYS4efPmA+vr1KkjvL29NY8//PBDUf7PfbnZs2cLACIqKkoIIURAQIC4fv26Zv2kSZMqbN+yZUvRrVs3zeMePXqI+vXrP+YTrDpr1qwRlpaWUl6bSJLZopJ9ikfqiPSYSqWCUqmU8tp16tR54vx492/plJycDAAwNjbGokWLkJ6eDgAwMzPD5MmTH/n8MWPGwMnJqcKy8PBwAMA333yD0tJSzXJ3d/dH7sfW1hYTJ06EsbHxI7dRq9X47rvv8NVXX+HevXto3769Zp1CoUCXLl3w888/P/S5a9asgaurK5RKpeZr586dMDMzw9atWzFjxgy89NJLAMqn5ACApKSkCvtYuHAhUlNT4eDggBdeeKHC+759+zaCgoI0j62srBAYGIizZ89i48aND6wPDAxEbGwszp49i9LSUkRERKBBgwaa9RMnTgQArFixAgDw6aefolGjRpr1w4cP13wvhEBmZibOnj2rOSJbUFAAIyM5Pz6USiWKi4ulvDaRrmOpI9JjxcXFMDU1lR3jkQYMGAAAmDJlCn755RcAgKurKxYsWAAA8Pf3x6hRox75/KZNm+Kjjz7SPC4uLsaqVavQqlUrXLt2DVu2bNGsW7du3WOzLFy4EHXr1n1guUqlQkREBFxdXfHGG29oTlvm5+dX2K5z5844c+bMQ/dtZmZW4fHNmzcRExMDAOjbty+mTZtWYb2dnR0cHR01j2NjY/H777/jpZdegouLC3788ccHXuOfpbV169bYsWMH9uzZ88D61q1bAwB27NiBEydOIDs7W7MMABo1agRra2vs2bMHarUa7dq1q7Dvffv2oWPHjgDKC+3QoUORm5uLCRMmoLCwEMePH8c777zz0M+iqpmamqKkpARCCCmvT6TLWOqI9JjMI3WVERQUhLp16yI/Px/Dhg1DRkYGAMDPz++Z9rdhwwa4uroiJCQEAPDVV189V76wsDA0a9YMkyZNQu3atbFx40ZN4Tx58mSFbV1cXDRHGJ9ky5Ytmv8uTk5OD5S+W7duwd/fX/PYwcEBCxcuRP369XHz5k2MGTPmgWsJ/36k7f5zUlNTce3atQfWOzg4AMAj1wNA/fr1oVKpcPPmTc2y69evY+LEiZg3b16F0jZz5kzUr18fGzZsQLt27RAREYH333+/Up+Ftt3/XFUqlZTXJ9JlLHVEekylUqFWrVqyYzxSw4YNERcXBy8vLwCAm5sboqOjn3l/y5YtQ3BwMMaOHQsrKyvExsbixIkTT72fwsJCLF26FCEhIbC2tsamTZuQmJiIwMBAGBkZQaFQIDY2FpmZmZrnXL58Gc2aNavU/n/55Rf07dv3keutrKwwe/ZszeMmTZrg3XffRUpKCiZNmgQAWLJkCU6dOqXZxtzcvMI+zM3NkZmZqcn49/X3v8/MzERWVtYjn39/GwC4dOkShg0bpjlKOHr0aM22rq6u2Lt3LwDg4sWLDxTE6nS/1JWUlEjLQKSrTGQHIKJnZ2Fh8ciRp7rCxcUFsbGxAICOHTvC398fjo6OuHHjxlNdlzVq1CicPn1acz3Y/R/qvr6+SE5OrnBN2KMUFxfDx8cHp0+fBgDk5ubC2tr6ge3Kysqwe/duLFq0CEII+Pv747vvvsOBAwee+BphYWH4/fffKxTC++7duwdfX98HRu7eZ25ujm+//RbBwcFo164dvv32W6xcuRIAcPfu3Qpl6tatW+jduzfy8/Px22+/VVh/69YtAEDv3r01p2Xv3r1b4bVu3boFW1tbzWnW5s2b49ixY5r1Pj4+uHDhAlq0aIGpU6di+/btyM/Ph4WFBYyMjODs7IyUlBQoFIonfibadO/ePQB47GhkopqKR+qI9JhSqdTJ01C//fYbfvrpJ0yfPr3C8vuDHDIyMiochXqSmzdvYuvWrUhNTUVBQQEKCgpQUlICT09PqNVqfPPNN5Xaj6mpKU6dOoVdu3bB09MTjRs3xpw5c3Dnzp0Hth0wYAAWLFiAhQsXIjMzE1euXKnUfXY3b96Mnj17wsbGpsLy0tJSjBo1CiNGjHjiPu5fb/f36+CuXr1aYZsbN26gdevWcHNze2D9jRs3NM93dXWFkZFRhfUlJSXIysqqsP9/8vPzg5OTE0pKSrBhwwZ069ZNMzAmICAA165de6ajpM9LpVLBxMRE2kANIl3GvxVEeszU1FTaSMDHXag+fPhwNGnSBGFhYbh06ZJmuYeHh2Zgxz9LD/C/yXb/ue9ly5bh5Zdf1owcve/+AISVK1dWOGL5qP3cN2DAABw7dgytW7dGaGgoGjdujLlz5z603F2+fBlvvfVWhXnh7r+GWq2usCwzMxNHjhzB4MGDKywXQmDSpElo1aoV/v3vfwMoL1aPGvm7efNmKJVK9OvXT7MsKiqqwjYnT56El5cXpkyZ8sD6kydPwtnZGV5eXmjQoAFee+01nD17VrM+ISEBxcXFCAwMfOjrA8CJEydgaWmJoqIi3Lp1q8LccH369AFQPiq3uhUXF+v0daREUlV27pMq/iKiZ5Cenq6ZA666LV26VNjb24t79+4JIcrncrtz547Yu3ev8PT0FEVFRcLIyEi4urpq5p1bunSpACB69OjxwP4KCgpErVq1BADx66+/apZnZGQIKysr8fXXXz/wnMLCQs1z/j5v3ccffywACB8fnye+jwMHDoju3bsLAMLGxkb8/PPPmnWxsbGiRYsWolevXiI5OVmzvKSkRLi5uYlGjRqJvLw8zfLw8HBhbGz8wHx4M2bMEADE6NGjxejRo0VgYKBo2bKlePfdd0VZWZno2bOnZk699PR00bJlS7F06VLN84cPHy7Mzc3FjRs3hBBCnDt3TiiVykeuVyqVYvfu3Zr1p06dEubm5prHo0ePFp6enqK0tFQIIUTv3r3F0qVLRXZ2thBCiH379glHR0fN9r179xYKhUKcO3dOCCHEwIEDRcOGDUV+fv4TP19t+/bbb4WtrW21vy6RRLNFJfuU7DLHUkf0HG7fvl2hAFWXoKAgYW1tLQAIExMTYWFhIYyMjAQAAUDMnj1bCCGEi4uL8Pf3F3Xr1hXt2rUTAMTrr7+uKQ/3hYeHi7Zt22qe37BhQzFhwgRx4sQJYWdnJwCIF154QWzYsEHznJs3b4rXXntN8xylUinS09NFYGCgMDc31yzv06dPpd7TkSNHRO/evcWUKVPEtm3bxMiRI4W9vb1YtWrVA9sWFhaKunXrCjMzM5GTk6NZ3qNHD9G9e/cK20ZERGiy/PPrwoULQgiheR9ubm7Cy8tLMynwfSqVSkyYMEHUq1dPDBgwQLz88ssVSt8/1/993d/fX7t27UTfvn3FiBEjxMWLFzXr6tevr/kMfX19xeTJk0VWVpZmfVZWlhg6dKgwMTER/v7+olOnTiI+Pr5Sn6u2LVmyRDRs2FDKaxNJMltUsk8phG7M9aMTIYj0jRACGzdurNR1WjJcvHgRrq6uEEIgOTkZdevWfehpV11SWFiIn3/+GR4eHujQocMjJyy+ffs2SktLYW9vr1l24cIFWFtbV5iDrjLufz4ODg6PndBZpVLhr7/+euA0dGXXp6enw9bW9oGRsGq1Gjk5OTA3N4eVldUjXz8vLw8pKSmPnei5qs2aNQs7duxAQkKCtAxE1SwUwJzKbMhSR6Tn5syZU2F6DCJDFhgYCCEENm/eLDsKUXUJRSVLHQdKEOm57Oxs2RGIqk1OTg7s7OxkxyDSSSx1RHouJydHdgSiapOdnV3hlDcR/Q9LHZGeu3+vUqKaIC0t7YHpZYioHEsdkZ5LTk6WHYGo2ty6dQuNGzeWHYNIJ7HUEem59PR0nbyrBFFVcXFxkR2BSCex1BHpubKyMhw+fFh2DKJq4eDgoLk1GhFVxFJHpOesrKxw+fJl2TGIqkWTJk1kRyDSWSx1RHquTZs2SExMlB2DqFq0adNGdgQincVSR6TnPDw8WOqoxpB5NwsiXcdSR6Tn3N3dWeqoRrhx4wY8PDxkxyDSWSx1RHrOw8MDubm5uHbtmuwoRFUqMTGRR+qIHoOljkjPde3aFc2bN0dERITsKERVauXKlahbt67sGEQ6i6WOyAD4+voiJiZGdgyiKvX777/LjkCk01jqiAyAr68vjh8/DrVaLTsKUZW4fPky/vrrL9kxiHQaSx2RAfD19UVBQQHi4+NlRyGqEjExMTA1NZUdg0insdQRGYCWLVuibt26PAVLBismJgadO3eWHYNIp7HUERkAhUIBPz8/HDx4UHYUoipx8OBB+Pn5yY5BpNNY6ogMxFdffYUdO3bgwIEDsqMQadWyZcuQmZmJuXPnyo5CpNNY6ogMROPGjdGyZUtERUXJjkKkVVFRUejVqxdMTExkRyHSaSx1RAbE398fkZGRsmMQaU1RUREOHjyIfv36yY5CpPNY6ogMSP/+/ZGUlISUlBTZUYi04sCBAygsLIS/v7/sKEQ6j6WOyIB0794ddevWxaZNm2RHIdKKTZs2wcvLC40aNZIdhUjnsdQRGZBatWph8ODBLHVkEFQqFbZv347hw4fLjkKkFxRCCNkZAEAnQhAZgtLSUjRo0ACZmZmyoxA9l9GjRyM1NRVHjhyRHYVIplAAcyqzIY/UERkYY2NjDBkyRHYMoudy79497Ny5k0fpiJ4CSx2RARo/fjyOHz8uOwbRM9u0aROKi4sxYsQI2VGI9AZLHZEB8vT0REREhOwYRM8sIiICgwYNgr29vewoRHqDpY7IQG3cuBF5eXmyYxA9k9jYWAQFBcmOQaRXWOqIDJT26pW5AAAgAElEQVRarcb69etlxyB6Jk2bNkWPHj1kxyDSK7znCpGBOnz4MLp06QJXV1d0795ddhyiSlu+fDmOHTsGhUIhOwqRXuGUJkQG7KWXXoKVlRV27dolOwpRpZSVlcHV1RWXL1+WHYVIV4SCU5oQ0bvvvos9e/bgjz/+kB2FqFK2b9+O5ORk2TGI9BJLHZEBGzhwIFxdXfHll1/KjkJUKYsWLcIrr7wiOwaRXmKpIzJgCoUCM2bMwLp163Djxg3ZcYge69ChQ4iNjcW7774rOwqRXuI1dUQGTqVSoXnz5hgwYADCwsJkxyF6pG7dusHExAT79++XHYVIl4SC19QREQAolUpcuXIF0dHRmDBhguw4RA/l7e2N2rVrs9ARPQeWOqIawMTEBJ988gl++OEHjioknbNnzx4cO3YMc+fOlR2FSK/x9CtRDVFaWoo2bdqgTZs22Lx5s+w4RADK/79s164dmjZtiu3bt8uOQ6SLQlHJ06+cfJiohjA2NsbixYsxYMAA2VGINFauXIkLFy5g69atsqMQ6T2efiWqQfr37w9/f3+UlZXJjkIEAJg1axamTZuG5s2by45CpPdY6ohqmCVLlmD16tWyYxABAIyMjDBr1izZMYgMAk+/EtUwLVu2RPfu3XHixAlERETIjkM1lBACfn5+yMjIgJERjy8QaQP/JhHVQMuWLcPq1avx22+/yY5CNVR4eDiOHz/OQkekRfzbRFQDDRs2DAMHDkRQUBCKiopkx6EaJi0tDf/+97/x3nvvyY5CZFBY6ohqqLCwMGRmZiI0NFR2FKphgoOD4ejoiE8++UR2FCKDwlJHVEM5OTlh/vz5WLRoEY4dOyY7DtUQ33//PXbt2oWIiAiYmZnJjkNkUDj5MFEN179/f1y4cAHx8fGoU6eO7DhkwK5evYp27dph0qRJWLRokew4RPoiFJx8mIgqY8+ePdi1axesrKygI7/kkQGaPXs2Fi5ciLi4OLRq1Up2HCKDxNOvRISAgAAEBwfj6tWrsqOQgfr888+xaNEiFjqiKsRSR0QAgEWLFmHw4MEoLCyUHYUMzI0bN/Daa68hODhYdhQig8ZSR0QAAHNzc1y/fh2TJ0+WHYUMiEqlwtChQ7F27VrZUYgMHksdEWn89NNP+Omnn/DNN9/IjkIGYvr06Th37hwH4RBVA5Y6ItLw9/fHnDlzMGPGDBw5ckR2HNJza9asQXh4OL777jvZUYhqBJY6Iqrg448/xp07d/Dee++hWbNmyM7Olh2J9My5c+dgY2ODffv2oaysDIMHD5YdiahGYKkjogeYm5tj586dKCsrwyuvvCI7DumRjIwM9O/fHx4eHli7di0UCoXsSEQ1BksdET1U/fr1sWfPHvz5558oLS2VHYf0REBAAMzMzLB9+3aYmprKjkNUo7DUEdEjtWzZEjt27MCECRM4MTE9UUFBAdLT0xEZGYm6devKjkNU4/COEkT0WH5+fujVqxcsLCywfPly2XFIRxUXF2PQoEHYu3cvmjRpIjsOUY3EUkdET7RhwwYEBgbC0tIS8+fPlx2HdIxarcbw4cNx/PhxtG7dWnYcohqLp1+J6IkGDRqEkpISuLm5wdjYGO+8847sSKQDiouLMXDgQNja2uKdd95Bbm6u7EhENRqP1BFRpY0fPx7m5uYYM2YM7t27h5UrV8qORJLcu3cPr776Kk6dOoV9+/bB09NTdiSiGk+hIxc/60QIIqqc//f//h+GDRuG/Px8mJjwd8Oa5s6dOxgwYAAuXryIX3/9Fe3atZMdiciQhQKYU5kNefqViJ7awIEDsWvXLgQEBCAvL092HKpmfn5+SElJwaFDh1joiHQISx0RPZNevXrh7Nmz6Nq1K9LS0mTHoWpy5swZCCEQGxsLNzc32XGI6G9Y6ojomR07dgxqtRpeXl6Ij4+XHYeq2O7du+Hn54eYmBg4OzvLjkNE/8BSR0TP7MUXX8S5c+eQnJyMsLAwGBkZ4T//+Y/sWKRFBQUFGDlyJIyNjZGUlIT8/HxYWVnJjkVED8GBEkSkNatXr0ZISAjS09N5RwEDcPnyZQwePBjp6elYv349evfuLTsSUU0UCg6UIKLqNmHCBBw9ehQdO3ZEXFyc7Dj0nDp37gylUolTp06x0BHpAZY6ItKqTp06oXnz5vDx8cEXX3yBsrIy2ZHoKRUWFmLKlCkYMmQIjh49ihdffFF2JCKqBE4wRURaFx0djS+//BIfffQRoqKisG7dOl5YryfOnDmDkSNH4q+//sKtW7dkxyGip8AjdUSkdQqFAjNnzsTx48eRmZkJDw8PbNiwQXYseozS0lIsWLAAXl5ecHR0REJCguxIRPSUWOqIqMq0a9cO58+fx+3bt+Ho6Ahra2uEh4dDRwZoEYCioiIolUq89NJLCAgIQHFxMQ4ePIhGjRrJjkZET4mljoiqRffu3REcHIxp06bBz88PiYmJsiPVePv370fbtm2xZMkSHDlyBK1atZIdiYieA0sdEVWbefPmIS4uDiUlJWjfvj2mTJmC7Oxs2bFqnCtXrmDQoEHo1asXWrZsiZCQEBgZ8ccBkb7j32IiqlZt27bFsWPHsHr1auzYsQPNmzdHSUmJ7Fg1Ql5eHj744AO0atUKFy9eRHR0NHbs2CE7FhFpCUsdEVU7hUKBcePG4eLFiwgODkabNm2wfv16Tn9ShYqLi9GsWTOsWrUKixcvxtmzZ9GnTx/ZsYhIi1jqiEgaS0tLfP755+jSpQtGjx4NDw8PbNmyhQMptKikpATh4eFo2rQpxowZg0uXLuHtt9+GiQlntCIyNCx1RCTdunXrUFpaiqSkJLi6umLIkCEwMjLCihUrUFhYKDue3vn888/h6OgIU1NTvP322wgICEBaWhoWLVrE27cRGTDe+5WIdNKff/6J9u3bw9LSEiEhIZgyZQocHBxkx9Jply5dwtKlS/Hzzz9j8uTJmDZtGho0aCA7FhE9n1BU8t6vLHVEpLMyMzOxfPlyhIWFITe3EV55pQmCgoLg7+8vO5rOKC4uxrZt2xAREYFzBw7A6YUXcPj8eVhYWMiORkTaEYpKljqefiUinVW/fn3MmTMHv/ySBrX6DFJSrNGvXz/MnTsXqampsuNJ984778DJyQljxoxBHUtLXK1fHyeHD2ehI6qhWOqISOetXm2K9u2BU6fW4M8//8Q333yDxo0bw9vbG1999RVu3LghO2K1+fPPPzFnzhy0atUKO3bswIwZM3Dt2jVs37EDtUNCYLR2reyIRCQJT78SkU7LyQGcnIBly4CgoPJlJSUl2Lt3LzZt2oTt27fj7t278PX1xapVq9CiRQu5gavAqVOnsGfPHmzevBmJiYlo0KABhg4diqVLl0KhUPxvw5s3gRdfBFQqeWGJSNtCwWvqiEjfCQE0awYMGAB8/fWTt4+JiUFUVBQiIyNx+vRpGBsbo3379vjggw/g4+Oj84MG1q1bh5iYGMTExODcuXMwMTGBj48Pli5dirZt21ZuJ4cPAxYWQMeOVRuWiKpLKFjqiEjfRUUB/foB588Dbm5P99zMzEwcOHAAR48exYoVK1BaWgoXFxd06NABHh4e8PDwwKuvvlrxSFc1UavVuHjxIhISEpCQkIDExETExcXh9u3b6NixI3x9fdG1a1d0794dVlZWT/8Cb74JrFmj/eBEJEMoWOqISN+98gqQlwccPPh8+8nLy8OxY8cQGxuL+Ph4JCQk4OrVq7CwsEDLli3RuHFjuLi4aP709PSEjY3Nc98PNTMzExkZGUhOTkZKSormz6ioKBQXF8PExAQtWrSAh4cH2rdvj2nTpsHU1PT53iwA1K4NpKUBnJOOyBCEgqWOiPSdsTGwYQMwbJj2933v3j38/PPPuHDhQoXCdevWLQCAkZERbG1tYW9vDzs7O9jY2MDKygpKpRKmpqZQKpUQQkClUiEvLw/FxcUoKirC7du3kZOTg+zsbOTm5mpez8HBQVMa+/fvDw8PD7i5uUGpVGr/zVlbA598Asycqf19E1F1CwVLHRHpu4YNgdRUoFat6nvNu3fv4uTJk5pilpOTg5ycHOTm5iIvLw8qlQrFxcVQqVRQKBRQKpWoU6cOlEolzMzMYGtrCzs7O9jb26NFixZwcHDAiy++CHNz8+p7E1OnApGRwKVLgITTy0SkVaGoZKnjzf+ISCeVlAATJ1ZvoQMAKysr9OzZs3pfVNuCg4FvvgGiowFO1ExUY7DUEZFO6tULOHRIdgo95eYG/PJL+bDhK1eAxo1lJyKiasDJh4lI55w7Vz4zBz2HV18FGjQAwsNlJyGiasJSR0Q6JzwcaN5cdgo9Z2ICTJ5cPrVJcbHsNERUDVjqiEin5OcDP/wAvPWW7CQGICgIuHMH2LhRdhIiqgYsdUSkU378sXyQxPjxspMYAEdHYPBgYPly2UmIqBqw1BGRTgkPB0aM4Ly5WhMSApw4AcTFyU5CRFWMpY6IdMbo0eVTmKxdKzuJAfHzK2/Kfn6ykxBRFWOpIyKdkJVVPgtHcLDsJAZo9GigKu5cQUQ6haWOiHTCmjWAuXn5qVfSMktLYNw4QDfuIEREVYSljoikKysDVqwoHxxRu7bsNAYqOBiIipKdgoiqEEsdEUkXGQmkpHAakyrVsiVHwRIZOJY6IpIuPBzo2RNo0UJ2EgMXGQkkJ8tOQURVhKWOiKSLjASmTJGdogZo2JC3DSMyYCayAxBRzaZSAbNmAUOGyE5SA8TFAc7OQOvW5QMniMig8EgdEUn1yy/ApEmyU9QQDg7l7ZnX1hEZJJY6IpIqLKz8rCBVk5AQ4OTJ8i8iMigsdUQkTUICEBMjO0UN07Ur4OFR3qaJyKCw1BGRNOHhHPEqRXAwsGEDkJMjOwkRaRFLHRFJkZcH/Pgj56aTYvRowNS0/DYeRGQwOPqViKTw8QECA4Hp02UnqYEsLIBjxwA3t/KRsP37y05ERFrAI3VEVO2OHAGSksrPApIkLVuWz/jMkbBEBoOljoiqXVgY0Lkz0LGj7CSVJ4TAgAEDnnsfPXr00FIiLQgJ4f1giQwISx0RVavMTGDrVv07SldWVob4+HiUlZU91z4SExOfax9a9cornE+GyICw1BFRtVq1CrC0BIYPl53k6RgbG+OPP/6AkdGz/7NpbGyMS5cuPdc+tMrYGJg8GSgqkp2EiLRAR/5lIaKaoKwMWLkSGD8eMDeXnebpWVlZPfc+bGxstJBEi4KCyqc3ISK9x1JHRNUmIABwdQUWL/7fsuzsbCgUCuzZs0ezTKVSoVatWgCA5ORkNGvWDN9++y1+//13vPzyy1jz36k4YmNjYWdnh5ycHBQVFWHJkiWVyjF//nzY2Nigffv2GDx4MN5++20YGxvD2dkZAwYMQL9+/aBQKODo6IiysjJkZWVh3LhxUCgUUKvVmv289NJLmDNnDqKjo1G3bt3HLr+/j1q1akGtVuP8+fPw8fGBQqGAk5MT3N3dYWtrC2NjY5z8290eYmJisHHjRuTm5sLX1xcKhQLOzs4Yp617tzo4AG+8UeEOE/ez3c81duzYB7KFh4fD1NQUa9euRWpqKj744AMoFArtZCKiZyOE0IUvIqoBFAohtm17cDkAMWnSJM3j6Oho0blzZyGEEIMGDRITJkzQrIuNjRUNGjQQQgjx1ltvCXt7e3H79m0hhBBffPFFpbPMmDFDLFmyRPN49OjRAoD4+uuvhRBCjBs3TgAQqampFXKWlJQIIYRISUkRAMRnn30mhBBiwIABj11+X5cuXTT7uHz5sgAgWrduLfLz80VYWJgAIL788kvN9gEBAUKtVgshhNixY8cDn5VWeHgIMW5chUWXL1/W5BJCVMh2+fJlYWZmJt58880Kz3nppZfE2rVrtZuNiGaLSvYpHqkjomrj5AQMHPjg8nr16uGHH35AVlYWAODHH3/EG2+8gby8PGzbtg1btmxBp06d0KlTJ0yZMgWWlpYoKiqCl5cXsrOz0b59e+zcuRMhISGVzmJtbQ13d3fN4xb/vbWFh4cHAKBly5YAgNTU1Ic+/4UXXoCLiws+/vhjZGZmYv369Y9dfp+pqanme/P/noN2dnaGhYUFWrVqBQDIzMzUbJOVlYUrV64AALy8vAAAt27dqvT7rJSQEGDjxgp3mDA3N9fkAlAh2/bt2zWf/9/5+/tj69at2s1GRJXGUkdE1aK4GJg0qfza/H+aNm0aioqKsHz5chQUFCAyMhKvv/46Ll68CACYNWsW4uLiEBcXhzNnzuDixYswMzPD8OHDMX78eKSkpODVV19FYWHhM+czMzOr8Pj+6d+SkpKHbq9QKLBmzRo4OjqiefPm2LFjx2OXV4bxQz6cXr16afbxxx9/ACgvT1o1alT5HSZWr65UtsuXLwPAAwM+unbtqvlvRkTVj6WOiKrFpk3AxIkPXxccHAwLCwuEhYVh/fr16NWrF2xsbFC7dm0AqHCNGQAUFBTgr7/+glqtxtq1a7FhwwY4OTlh2LBhVf02NEpLS+Hl5YWzZ8/i7t27GDduHA4cOPDI5c/qo48+wkcffYQpU6Zg+vTpeP/99/Hmm29q8Z2g/A4T48YBK1aUj2Z5gnr16gEAjh49WmG5k5MTHBwctJuNiCqNpY6IqkV4ONCgwcPX1a1bFxMnTkRWVhamTJmC8ePHAwCaNm0KS0tLbN68GWfOnNFsP3bsWFy7dg0hISHIyMjA8OHDkZCQgEOHDlU4NalWqyt99E4I8VTvJzk5GTNnzkT9+vWxevVqlJWVYfv27Y9c/qwKCwuxdetWvP3224iJicH8+fOrZkBCcDCQkgJERj5xU09PTwDA4cOHKyyPi4uDj4+P9rMRUeVU9uK7Kv4iIgM2aZIQbm5P3q53797io48+qrBMpVKJCRMmCCMjIwFAKBQKUVBQIIQQYuTIkcLKykoMGjRIvPnmmyImJkbzvLy8PFG3bl1R/s9cRV9++aWwtrYW7u7uYvfu3WLevHnC2tpaABDOzs5i2bJlwt7eXgAQrq6uYuXKlWLIkCECgPDy8hJxcXHi0qVLwsbGRri4uAh/f3/xzjvviLKyskcuT01NFUOGDBEKhUJ4eXmJ7du3i86dOwsAwsjISLz33nvCxcVFABDW1tZi9erVQgghVqxYIQBU+LKxsRGHDh16jv8ij7BtmxAKhbhy4IDo3LmzJldMTEyFbEIIcfv2bTFixAhhbm4uxo0bJ15//XXRunVr7Wciotmikn1KIZ7yt9MqohMhiEj77t4tv2nBvHnA1KmP3k6lUqFhw4Y4duwYmjVr9sD6/Px8XL58GU2bNkWdOnUAAEVFRTAzM0N6ejqUSiXs7e0rPKewsBALFizA7NmztfqegPK7Q6jVapSWlsLU1FRzfdmjlj+rd955B5MnT0ZWVhYKCgqQn5+P1atXo169evj++++18Vb+p7QUcHEpnxl64cJKPSUnJwdJSUmaASJEpHWhAOZUZkOWOiKqUsuWAR9+CNy4AVhbP3q78PBw7N69G7t27dLaa8fGxmL69Ok4fvy41vZZncLDwxEcHPzAqeEDBw7g119/xRdffKH9F/38c+DLL4G0NP2cIZrI8ISikqXOpGpzEFFNt2IFMHLkowvdihUrEBYWhsuXLz9wjdbziouLQ5Qe37C+6L+37/rhhx/w4osv4urVqzh+/Dj+/PPPB6ZK0ZqgIGDu3PK7TLzxRtW8BhFVCZY6Iqoyv/0GnD8P/Pjjo7dxcnLClStXsHDhQnTq1Emrrz/1ced79UBwcDByc3Px5ptvwsTEBG5ubhg7diwWL16smT9O6+rXB4YOBZYvZ6kj0jM8/UpEVWb4cODaNSA29vHbqdVqmJjwd8xHKSoqgqmpafXdhismBujaFdCNnw9ENV0oePqViGT67DNg//7yS7OehIXu8f45MXKV8/UFVq4EsrOBfww+ISLdxXnqiEjrSkvLO8EbbwDV3UdIS0aNeuwdJohI97DUEZHW7dpVfoTurbdkJ6FnVrt2pe8wQUS6gaWOiLQuLAzo2xdo2lR2EnouqanAnj2yUxBRJbHUEZHW7d0LTJkiOwU9t169ykfBEpFeYKkjIq1zdgYGDJCdgp5bSAgQHQ1cuSI7CRFVAksdEWnV998DJ04Axsayk9Bze/XV8mHMnTsDhYWy0xDRE7DUEZFWhYUBDg6yU5DWTJwI3LsHVNUdLIhIa1jqiEhrTp8uP0pHBqR+fWDYsPK2TkQ6jaWOiLQmLAxo3Vp2CtK64GDg1Cng+HHZSYjoMVjqiEgrcnPLz9Bx1KsB8vEB2rXjSFgiHcdSR0Ra8f33gJERMGaM7CRUJUJCgE2bym8dRkQ6iaWOiJ7bgQPA9OnA0aOAlZXsNFQlJk4sPwzbuXP545wcYOFCwMUFKC6Wm42IAAAKIYTsDACgEyGI6NkMGwakpwMxMbKTUJW6dAlo0QIYPRrYsAFQqwEhgL/+Kh9QQURVIRTAnMpsyCN1RPRc0tOB7dvLr6UnA1ZUVN7aXVzKC11JSXmhA4A7d+RmIyIAgInsAESk3yIiABsbYOhQ2UmoSly9CqxYUf517155kfvnGR6WOiKdwFJHRM9MrS4vdRMmAKamstOQ1mVkAB07lg9tfhyWOiKdwNOvRFRpa9eWT1uiUpU/3rkTuHkTmDxZbi6qIo6O5efWTZ7w+//du9WTh4gei0fqiKjSdu0Ctm4t/97EBDAzA2bNAl58UW4uqkLdupVfP7d8OfD22w+ur1ULuHZNqy9ZWlqKX375BRcuXEBKSgqSk5ORkpKC69evo7S0FACgVCphZ2cHOzs72NjYoE6dOjA1NYVSqYSpqSmEECguLkZeXh5UKhWKiopw+/ZtZGdn49atW5r9mJqa4oUXXoCLiwsaN26M/v37w8PDA40bN4ZCodDq+yKqaix1RFRpt27973u1uvza+TlzgNWrgalTgTffBOzt5eWjKhQSUj4qZt68itfUKRTPdfq1pKQEp0+fRmxsLOLj45GQkIA//vgDarUaLi4ucHFxQYsWLdC3b1+4uLigS5cusLe3R506dZ75NYUQuHLlCjIyMjSFMTk5GZcuXcKgQYMghECdOnXQpk0beHh4oH379ggKCoKREU9ukW7jlCZEVGnt2wPx8Q9fZ2xcPvnwggXlc9aRgRo3Dvjhh/89VirLj+AtXlzpXeTl5eHIkSOIiYnBkiVLUFhYCHt7e7Rv3x4eHh5wd3fHiBEjYCrhQs38/HwkJSUhISFB8xUfHw8jIyN4e3vD19cXXbt2hY+PD5RKZbXnoxopFJWc0oSljogqrWnT8sGQD2NiArzwAnDiBGBnV725qBqp1YC5efmfQHmbHz8eWLXqsU9LTExEVFQUIiMjcfToUZSUlKBFixZ4//334evrixYtWlR99mdUWlqKFStW4OjRo4iJicH169dhYWGBHj16YOnSpXBxcZEdkQxbKCpZ6nj6lYgqLS/v4cuNjQFra2D/fhY6g2diAri7A4mJ5cWutPSRo2PLyspw5MgRbNy4EeHh4ahXrx769OmDtWvXok+fPqhXr141h382xsbGCAkJQUhICAAgJSUF0dHRiIyMRJMmTdCiRQsMGjQIgYGBaN++veS0VJPxSB2RBKWlpTh69ChSU1ORlZWFnJwcZGdnIycnB6mpqVCpVFCpVCguLoZKpUJpaSksLS0rXAhua2sLOzs7tGjRAvb29mjQoIHmYm9ra+sqyW1hARQUVFymUJQPmIiJKT89SzVAdjbQpQtw/Xp5seveHTh4EED59WoxMTHYtGkTfvnlF9y8eRPu7u5YvXo1OnbsaHDXpR08eBB79uzBL7/8gpSUFDRv3hyBgYH47LPPZEcjwxEKnn4lqn6lpaU4e/YsYmJiEBMTg5MnT+Lq1auwtLSEu7s7WrdujSZNmqBx48Z4+eWX4ejoqLXXLiwsRGpqqubC7+joaCQmJiI5OVlz4beHhwd8fHzw3nvvPfVRkry8h9/Xdd268rtGUQ0UHw/h64sCS0uM6NIFkZGRmD59OsaPH482bdrITieNEAITJ07Exo0bUVZWhqFDhyIoKAh+fn6yo5F+CgVLHVH1EELgzJkz+OCDD3Ds2DHk5+fDxsYG3t7e8PLywujRo+Hi4iJteoT8/HwkJiYiMTERp0+fRkxMDJKSkuDq6oquXbuib9++6N27N2xtbR+7nxs3gEaNKi774gvggw+qMDzprLt37yIiIgJx8+fji6wsTOzVC0FBQQgMDJQdTWfk5eXh559/RkREBE6dOoU2bdrg1KlTHGBBTysULHVEVSc3N1dz0Xd0dDT++usvjBkzBr6+vvD19UXr1q11eo6ryMhIxMTE4NChQzh27BiEEPD09MSyZcvQoUOHhz7njz+AVq3+99jIqPxyKqp53n33XUREREAIgQkTJuCDli3hyBmoH+vMmTNYsmQJ9u3bh2nTpmHy5MlP/EWK6L9CwVJHpH15eXkYNWoUoqOjUVZWBh8fH/j7+6Nfv35o166d7HjPJDc3F/v27UNkZCTWrFmDJk2aIDAw8IGLvo8fB7y8yr83MQEGDCi/2QDVHFevXsXcuXMrFJOqun7TUH3wwQf49ttvoVarMWXKFHzwwQew5+SO9HihYKkj0g61Wo1du3bh+++/R1RUFHr06IHhw4fj1VdfNbgfaKdOncKmTZuwadMmzUXfr7/+OubMmYO9e4E+fcpHunbuXH5dvJmZ7MRUHa5du4ZPP/0U3333HVxcXHDu3DnUqlVLdiy9lZeXh5UrV2LBggUoKCjAtGnT8Pnnn8uORborFCx1RM/nypUrWLVqFb777jtkZmaiZ8+eGDFiBN58803Z0arF8ePHsWnTJqxfvx7u7u5wdw/F4sXeaN68/KgdzxwZvoyMDHz22WeIiIiAk5MTZs2ahbFjx8LY2Fh2NINw7949fPPNN1i4cCGmTZuGGTNmwNLSUnYs0j2hYKkjejpFRUVYt24dFi9ejIsXL2LnzqP8CEoAACAASURBVJ3o168ff4D91+LFObh8eRO2b5+LW7duYeTIkVi7dq3sWFQFtm/fjpkzZ2Ls2LF4//33YW5uLjuSwcvOzsasWbMQERGBDh06YOnSpfD29pYdi3RDKCpZ6gxrwiCiZ5CXl4fPP/8cL774IqZOnYquXbsiKSkJAQEBLHR/8847dggPn4KUlBSEhYXhxIkT8Pf3R0xMjOxopCWXLl1Cr169MGjQIHh5eWH27NksdNXE3t4e4eHhOH36NOrUqQNfX1+MGTNGdizSMyx1VKMVFBTAxcUFCxcuRFBQEFJTU7Fq1Sq0+vswTwJQPtoVAExNTTFhwgQkJSWhsLAQXbt2hb+/P06cOCE3ID0ztVqNefPmwcPDAzk5Ofj999/x008/yY5VI3l4eGD//v3YsmULDh06hHXr1smORHqEpY5qpOLiYnz11VdwcXHB5MmTkZycjM8++wwODg6yo+kNhUKBQ4cOYd++fbh79y48PT3xyiuv4OzZs7Kj0VPq1KkT5s6di9mzZ+PkyZM87acDBg0ahHPnzmH8+PHo27cvUlJSZEciPcBSRzXOtm3b0KpVK3z00UcYO3YsPv/8c84X9Rx69uyJ33//Hbt370Z6ejo6dOiAzMxM2bGoEkpKSvDhhx/C1tYWCQkJ+Pe//43/z96dx9Wc/X8Af932UkmlbKWyU8mWErJFZMaaJYZBMSaDn22GQc2M7xg7Yxtjp6JkHVImu2SXyJJUlqlRqSTSre75/dF0R9OidO89d3k/H48eM+69nfOq7rn3fc/nc85HQ4MuCS4vDAwMcOnSJbx48QK2trbYvHkz70hEzlFRR1RGbGwsevXqhWHDhsHZ2Rnx8fFYsWIF71hKY8CAAbh+/Tr27t2LZs2aYcWKFRAKhbxjkQrEx8fD2dkZ69evx5kzZ9CsWTPekUg5nJ2dcfv2bcycORPffPMNPv/8c6Snp/OOReQUFXVE6R07dgwWFhaIiYnB6dOnIRKJEBAQgEb/veYVqTGBQAAvLy+8fv0aI0eOxOjRoyEQCDBhwgRkZmbyjqfycnJyMGzYMKirq2PXrl2Ijo5Gbm6uXF/9hABaWlpYsmQJCgsLsXfvXsycOROzZs1CYWEh72hEztCWJkRpvXz5EtOnT0dISAjGjh1LJxxzcvjwYUybNg2FhYV4+fIl7zgqrUWLFsjJycH+/fvh6urKOw6pgVq1aqFjx44ICQmhc4GVnz9oSxOiyg4dOoTWrVvj2rVrCA8Pp4KOoyFDhuD+/fsYPHgwhg0bRjN2nISEhKBu3bq4desWFXRK4MqVK+JzWKOjo3nHIXKCijqidLy9vTFs2DAMGzYM9+7dQ79+/XhHUnm1a9fGli1bcO3aNfGWDUQ2RCIR5syZg5EjR+Ls2bOoX78+70hEAuzs7HD9+nW0b98erq6utIiCAKCijiiZ69ev48iRIzh8+DB+//131KpVi3ck8oHY2Fg4OzvDzc0Nc+fOpYUUUvbu3TsMGTIEGzduxN69e+l6rUrGyMgIx44dw8KFC+Hr64v/+7//4x2JcEbn1BGlsWHDBsyaNQvJyclo0KAB7zikEjt37sT06dPRunVrhIaGwsLCgnckpfP333/js88+Q3JyMo4dO0Z7zym54OBgjB8/HpmZmdDT0+Mdh0iWP+jar0QVJCYmYujQoXj+/DmCgoLoUKuCefv2LSZOnIiOHTti7ty5vOMojTp16sDW1hZHjx6FsbEx7zhEhpo1awaRSITIyEhYW1vzjkMkwx+0UIIou/Pnz6Njx45QU1PDzZs3qaBTQLVq1UJwcDAWLFgALy8v5Ofn846k8M6cOQNXV1dERkZSQaeCoqOjUbt2bXTr1g2PHj3iHYfIGBV1RGH17dsXvXv3RlRUFKysrHjHITVw8uRJhIWFwc3NjVbH1sCJEyfg4eGB0NBQaGtr845DODA1NcXZs2dhaWmJ7t27IzY2lnckIkNU1BGF9NNPP2HGjBkICQmBrq4u7zikhvr06YOoqCg8ffoUzs7OePLkCe9ICic0NBRDhgzBmDFj6FJfKq527dr4888/YWtrix49euDatWu8IxEZoXPqiEIpKirC5MmTsXv3btpNXQn9/fffGDhwIJ49e4YTJ06gU6dOvCMphCNHjsDT0xNff/011q5dS1eIIACA9+/fY9iwYbh8+TKysrJ4xyGfzh90Th1RNgUFBRg9ejT27duHo0eP8o5DpKBevXricyX79OnDO45CiIiIwMiRI+Hj44N169ZRQUfEdHR0cOjQIXTq1An379/nHYfIAM3UEbn39u1bDBo0CLdu3cLJkyfRuXNn3pGIDBQVFYkXUgwaNIh3HLlkaGiIfv36Yf/+/VBXV+cdh8ixTp06IT09HZcuXaLrXisef9BMHVEWn332GWJjY3H27Fkq6FSIuro6JkyYAE9PTxw/fpx3HLmTlJSErl27IigoiAo68lERERHQ19fHgAEDkJOTwzsOkRIq6ohcKygowJ07dxAZGYm2bdvyjkNkbNOmTRg/fjw8PT3p0mIfyMrKwoABAxASEkJXiSBVYmxsjLCwMKSnp8PT05POSVZSVNQRuVVUVAQvLy+Eh4fD3t6edxzCgUAgwJYtWzB06FAMGjQIly9f5h2JO6FQiKFDh+Lt27fQ19fnHYcoEEtLSxw/fhxRUVGYOnUq7zhECqioI3Jr0qRJCAsLoxWQKk5NTQ27d++Gm5sbBgwYgJiYGN6RuPL29satW7dw4sQJ3lGIAurQoQP27duHnTt34pdffuEdh0gYFXVELi1atAiBgYE4dOgQ7yhEDmhoaCA4OBidOnWCh4cH7zjcrF69GkFBQThw4ADs7Ox4xyEK6rPPPsPq1avx/fff845CJIyKOiJXIiMjoaWlBcYYCgoK6NJfRExLSwt//vknoqOj0bdvX5U6J+ivv/5CvXr1cPnyZRQWFqJv3768IxEFN336dOTm5qJdu3bIy8vjHYdICBV1RG48fPgQw4cPh6enJ3766SfecYicsrS0xOXLl+Hr68s7iswMHz4cderUwc6dO3lHIUpEV1cXz549w5QpU3hHIRJCRR2RG0OGDEGrVq2wY8cO2kCVVCogIABbt27Ftm3beEeRiXv37uHw4cMwMDDgHYUomcDAQAQEBNAHBiVBRR2RG5mZmXQhclIlgwcPxoIFCzBt2jTcuHGDdxypOn36NNavX4+WLVvyjkKUkLu7O2bPno3p06fTNZeVAF1RgsiF5cuXo3PnznB1deUdhSgIkUiE/v374+HDh7h58yZMTU15R5K4zMxM2Nvb48WLF7yjECUmFArRuXNnaGtr49KlS9DQ0OAdiZTmD7qiBFEU58+fx4IFC6igI9WipqaGffv2QU1NDV5eXpCTD6gS5ePjQ6ciEKnT0tJCUFAQYmNj8cMPVaodiJyioo5wNXLkSHh5eeHly5e8oxAFZGxsjKSkJPTq1Qs6Ojq840hMWloaTExMYGFhgefPn/OOQ1RAq1at8O7dO1y5coW2y1FgdPiVcLN7925MmDABp06dQp8+fXjHIQpMJBKhV69eOHnyJHR1dXnHqbFRo0bhypUruHfvHl01gshUYmIi7Ozs8PbtW95RyL/8QYdfiTxLSkrCN998g1mzZlFBR2pMTU0Ne/fuxbx583hHqbHjx48jODgYmzdvpoKOyJyNjQ1+/PFHPHz4kHcU8glopo5w0bdvX6SkpODWrVvQ0tLiHYcoCTU1NVy4cAFdu3blHeWTvHnzBq1bt4arqysCAgJ4xyEqqqioCK6urrh48SKd0ykf/EEzdURe7dmzB6dPn8a2bduooCMS5e7uDh8fH+Tn5/OO8kmWLFmCd+/eYe3atbyjEBWmrq6OK1euYO/evbyjkGqioo7I3KxZs+Dr6wsnJyfeUYiS+e233/DixQv873//4x3lk6xbtw7+/v5KuT0LUSze3t6YP38+nVunYOjwK5Gp8+fP482bNxg4cCDvKERJiUQiODk5QV9fH2fOnOEdp8rOnDlD1zsmcmXNmjVYsGABHjx4ACsrK95xVJk/qnj4lYo6IjMikQgdOnTA7du3eUchSu7KlSvo0qULQkNDMXToUN5xPooxBgcHB9y5c4d3FELECgoKYGtri3bt2mH//v2846gyf9A5dUTebN++Hffu3eMdg6gAJycneHl5Ye7cuQpxfl1wcDCNDSJ3NDU1sXTpUoSEhNAHDgVBRR2RiZycHCxcuBC+vr68oxAVsWzZMvz9999Ys2YN7yiVKioqgr+/P0aPHs07CiFlDBkyBO3atYOfnx/vKKQKqKgjMrF06VIUFRXRCwORmYYNG+K7777Dzz//zDtKpQIDA/HkyRMaG0QuCQQC/Pjjjzh69Chu3LjBOw75CDqnjkhdWloabGxs4O/vjzlz5vCOQ1TI27dvYWNjI7eXoSssLETLli3h6uqK7du3845DSIWcnJxgbGyMsLAw3lFUkT/onDoiD1JTU2FlZYWff/6ZCjoic7Vq1cLTp08xc+ZM3lHK1a5dO7i4uFBBR+TelStXkJaWxjsG+Qgq6ohU/fzzzzAxMcGUKVN4RyEqSkdHB7/99hv++usv3lFKCQ8Px7179zB79mzeUQipktmzZyM5OZl3DFIJKuqI1GRkZGD79u347rvvoK2tzTsOUWEmJiZyd5WGVatWwc3NDfb29ryjEFIlnp6ecr/wSNVRUUekZuPGjdDT08OECRN4RyEqbvr06fj999+Rk5PDOwoAICYmBpGRkXRKAlEoGhoa2LFjB7KysnhHIRWgoo5IzcaNGzF16lTo6enxjkJU3JQpUyASibB161beUQAAv/76K2xtbdG3b1/eUQipFjU1NezYsYN3DFIBKuqI1OTk5GDatGm8YxACIyMjeHt7Y926dSgsLOQdB8HBwfjqq694xyCk2saMGSM3H45IWbSlCZGKCxcuwNDQEA4ODryjECI2ePBg5OTkcL8m7Pjx47F7926uGQj5VB07dkTr1q2xZ88e3lFUhT9oSxPC09atW6mgI3LHx8cH586dQ0JCArcMt27dgo+PD7f+CakpHx8fhIaGIjs7m3cU8h9U1BGJy8rKQmhoKO8YhJTh7u6Ohg0bYtu2bdwy7Ny5E127duXWPyE15eXlBYFAgP379/OOQv6DijoicQEBAVBXV+cdg5Ay1NXVMXHiROzatYvLuXUikYg+8BCFZ2BggIEDByI4OJh3FPIfVNQRidu1axdGjhzJOwYh5Zo4cSLS0tJw8uRJmfd94cIF/P333zLvlxBJGzlyJD2f5RAVdUSiEhIScOvWLXh5efGOQki5GjduDBcXFy6zDMHBwbTZMFEKAwYMgJ6eHs08yxla/Uokqm3btnBxccGmTZt4RyGkQqmpqWjUqBGKiopk1ufZs2fRq1cvPH78GE2bNpVZv4RIy9GjRzFkyBCkpKSgXr16vOMoM3/Q6lciaw8fPkRsbCwdeiVyr379+ujevbtM+wwLC0PLli2poCNKw83NDdra2ggPD+cdhfyDijoiMYcPH4a5uTm6devGOwohHzV8+HAIhUKZ9Xfy5En0799fZv0RIm16enro3r07l/NTSfmoqCMSc/LkSbi7u0NNjZ5WRP55eHjg0qVLMunr+fPniIuLg7u7u0z6I0RW+vfvjz///JN3DPIPevclEvH69WtER0fTTARRGFZWVjI7bBQeHg49PT24urrKpD9CZMXd3R1ZWVm8Y5B/UFFHJCIyMhKMMbi5ufGOQkiVyeqwUXh4OHr27AltbW2Z9EeIrLRs2RJWVla8Y5B/UFFHJGLOnDlYtGgRjI2NeUchpMoSEhKkfg3W+Ph4HDp0CIsWLZJqP4TwsmjRIhQUFPCOQUBFHZGAlJQUJCcn06WPiMLp2LEjoqKipNpHVFQUdHV10b59e6n2QwgvLi4uuH37Nu8YBFTUEQmIioqCuro6OnfuzDsKIdXi4uIik6KuU6dO0NTUlGo/hPDSokULqY8jUjVU1JEai4qKQtu2baGvr887CiHV4uLiggcPHiAzM1NqfURFRcHFxUVq7RMiD6iokw9U1JEaozctoqi6dOkCAIiOjpZK+2/evMGjR4/g5OQklfYJkRfXr1/nHYGAijoiATExMXB2duYdg5BqMzExQbNmzXD16lWptH/37l0wxuh6r0TpPXv2DNnZ2bxjqDwq6kiNqaurw9PTk3cMQj7JF198gYCAAKm0HRgYiC5dutCWD0TpGRkZISgoiHcMlUdFHamx1q1bQ0NDg3cMQj6Jvb09kpOT8ebNG4m3HRsbS7N0RCXY2dkhNjaWdwyVR0UdqTF60yKKzN7eHowx3Lt3T+Jt37t3D3Z2dhJvlxB5Y2dnJ5UxRKqHijpSY7a2trwjEPLJGjduDH19fam8IWVnZ6Np06YSb5cQedO0aVM8efKEdwyVR0UdqZH379+jSZMmvGMQ8skEAgFsbGyQlJQklfatra2l0i4h8sTa2hovX77E+/fveUdRaVTUkRp5+vQpvWkRhWdtbS2Vok4gEMDS0lLi7RIib6ysrMAYQ3JyMu8oKo2KOlIjZ8+ehYODA+8YhNRIz549cfbsWYm326VLF2hra0u8XULkjYODA4yMjKQyjkjVUVFHaoQ+lRFlYGVlJZVDRxYWFhJtjxB5ZmFhgZSUFN4xVBoVdaRG0tLSeEcgpMbq1asHQPLPZxMTE4m2R4g8MzU1RUZGBu8YKo2KOlIjr1694h2BkBorKb4k/Xw2NTWVaHuEyDMTExN6T+CMijpSI/SpjCiDkqJO0s9nmqkjqoRm6vijoo7UCH0qU0yMMd4R5IqRkRHU1dVppo4oBcYYlzFOM3X8UVFHakRae3tV5MWLF/Dy8oJAIEBISAjevXtX7uPu3r0LHR0dODs749ixY9i0aRMEAgG++eYbAEBERAQEAgF69+6N9evXQ0dHBwKBAN9//z2WLVuGL7/8ErNnzy7T7vDhw8tctNrZ2RkmJiZYtWoVhg4dCoFAgHbt2mHdunVYtGgRBALBJ12CKjs7G5aWlhAIBCgsLKz0sSdOnEDTpk3h5OSEBw8elLovICAAAoGgzBcAFBYWok+fPnBwcMCtW7dQt25dLF68uNT3R0ZG4pdffkFycjImTJiABg0a4MWLF+L7NDQ0ytxfoirtA8Ur5wQCAYYNG1bt35MkCAQCWFlZSfT5nJqaymWhxIsXL8R/46qOkU2bNsHR0bHMGOnduzd++umnMmPkyy+/hJaWVrljRF9fv9wxMmPGjHLHSJs2bao9Ro4ePVrhc2bz5s3Q09PDyZMnkZ2dDT09PYSFhVXY1s6dO8Vbz2hoaEBDQwOnTp0S3//mzRvY2trC3Nxc/Hvt0aMHRCJRuf3Nnj0benp6Vc6TnZ2NVq1aITs7Gzdu3ED9+vXh7u4uXrTzsfaBsmPc1dVVPMZlydLSEs+fP5d5v+QDJRU95y+ioIqfQrJ15MiRj/Y7evRoBoBdunRJfNtnn33Ghg4dyhhj7PDhw6xbt27i++rVq8cAsHv37olv+/rrr0u1+ezZM6aurs6WL19e6va2bduyyMhIxhhjv/32GwPAhg8fLr7fyMiIpaWlVfOnZGzkyJEMAAPACgoKKnzcgQMHGADWu3dvVlhYWOb+tm3bsn379rGnT5+Kv0rMnz+fAWDHjh1jjDHm5+fHALDw8HDGGGOpqamsTp064se/fv2aGRgYMGdnZ8YYY3Xq1GHt27cvc39V22eMsaKiIvbHH38wAOK/Dw+tW7dm/v7+EmsvOTmZXbt2TWLtVQcAZm5uXulj/jtGCgsLy4yRD/13jPj5+ZU7RgCUO0ZK/HeMZGZmVmuMFBUVsbCwsHKfMykpKaxWrVrMy8tLfFvDhg2Zubk5y8vLK9NWVlYWMzc3Z+fPny+V/8Pn9Lfffsu8vLxYYWEhO3LkCNPU1Cz1nP5vfyKRiDVs2JDl5eVVKc+sWbNKvZ79/vvvDABbvXr1R9v/8Pf74Rh/8+ZNlX6XkrZz506mp6fHpW8l58eqWE/RTB35ZEKhkEu/BgYGH31MQkICgNIziStXrhQvt9fR0cGUKVMqbaNhw4al/r1582Y0b94cGzZsQFFRkfh2Ozs79OrVq8J2vL29oa6uXuH9hYWF2LVrF9auXSu+bc+ePXj16lWVZq4mTZoEMzMzBAYGlunnzz//RHx8PAQCARhjsLS0FG+GW1RUhK1bt6J+/frw8PAQZwWA3377DQAQHByMrKwscXuGhoYYMWIEoqOjcefOHWRlZcHHx6fM/Xfu3KlS+wCgpqaG1q1bf/TnlDYtLS3k5+dLrL38/HxoaWlJrL3qqlWrVqX3/3eMqKurlxkjlfniiy/KHSMAyh0jFalTp061xoiamhr69+9f7nPm1KlTePv2Ldq1aye+zdHRES9fvkRQUFCZxx86dAizZs1C9+7dAfy7BU3JJeMYYzh9+jS2bNkCdXV1DBo0CO7u7gCKZzoBlOlPIBDA0dERQUFBVcpz6NChUn+rzp07AwDWrFnz0faB8se4vr5+hb9LadLW1ub2vkCKUVFHPpk8D96SImLq1KkIDQ0FADRv3hzLly8HALi7u2PMmDGVtrFgwQLx/+fn52Pbtm3Yt28fnj17hoMHD4rv27t3b6WHOlasWAFjY+MytwuFQmzduhXNmzfHhAkTxIc0ExMT4efnh927d0NN7eNDNCcnB76+vhAIBLh9+zby8vLE961cuRJ5eXkYNWoUrKysMGrUKPz1118AgGvXriEjIwNt2rQR99OoUSPUrl0bYWFhKCwsLPewVZs2bQAUHwIDyr5ht2nTBkePHq1S+/JE0m9IQqFQrjce/nCMlPjvGKlMkyZNyh0jrVu3LneMVKa6Y6Qijx8/BgDk5uaKb+vUqRMA4Pbt22Ue369fP0yfPr3UbSYmJuItboDiDzYfFkl9+/YFANSvX19824f9lfR5+/btj+YRCoV49uwZCgoKxPe3adMGenp6eP78ufj8tIraB8qO8ZLxzYOWlhYKCwvFh6aJ7FFRRz6ZJGc1JM3HxwfGxsbIzc2Fp6enuIDr1q3bJ7W3f/9+NG/eHG3btgWAUrNqnyI/Px9NmzbF5MmToaenh+DgYPGb6dixY7Fy5cpS56ZVJCcnB0DxDEnr1q3Rvn17GBkZYcWKFQCKZ8amT58OR0dHqKmpITg4GLa2tgCAZ8+eASj95gQAZmZmEAqFSE1NFT/mQ+bm5gCKLxFX3vebm5vj6dOnVWpfnijbTN3HfDhGxowZg7///htAzceIr68vgJqPkU2bNlU4RipiZGQEALh+/br4tpLLGJa3KW7Dhg3LzEhmZmaKC9qS6wJ/KDExEVpaWqVm5z/sr6TPlJSUj+bR1NSEnp5eqQ8T6urq4hnDkswVtQ+UHeO2trbczmsreb7L8wd+ZUdFHflk8jbT8qEGDRrgxo0bcHJyAgAEBQUhIiLik9tbv349vv76awDFhxijo6Nx7dq1areTl5eHdevWwcbGBrVr10ZISAju3r2LESNGiGezWrVqVeUFAzdu3EDjxo3Fs2LXrl2DtrY25s2bBwDw9PTEunXrcPXqVZw5cwaNGjUSn8Senp4OANDV1S3VZsm/09LSyt2M98P7K/r+tLS0KrUvTzQ0NCT6nC4sLISGhobE2pO0D8dIUFAQWrVqJZExMm7cOImMEV9f3wrHSEV69uwJgUCA6Oho8fOr5DBz06ZNq9S/oaEh/Pz8yr0vJycHAQEBmDFjBho3bgwAZfor6bNp06YfzSMQCNCzZ89Sfbx+/Rrp6elQU1ODjY1Npe0DZcd4dnY2pk2bVqWfVdI0NTUByPd7g7Kjoo58spIBLK+sra0RFRWFjRs3wsDAAB4eHti+fXu124mOjsbTp08xZMgQiEQijB07FsC/57xUhUgkwsqVK2FtbY2ZM2fC1NQUsbGx8PT0LHXo9vLly1i3bl2V23348CH69u0r/mTfqVOncleXAoCrqytu374NPT09iEQi1K5dW5ztQyWHgho2bCieaajofqDs9igFBQVo2LBhldqXJwUFBRKdWdPS0ip1WE0elYwRAwMDZGdnS2SM6OnpSWSMHDhwoNwxUpkOHTpgxowZyMrKQvfu3TF79mwsWrQIAMSz7JV59uwZQkJCKpwlX7x4Mezt7fHzzz+Lb/tvfwMHDsSiRYvQtm3bKuVZsWIFzM3NMW7cOPj6+sLBwQGZmZlo0qQJatWqVWn7/+Xq6go9PT2cPn2ayyHQkhk6eX9vUGpVXVEh5S+igIRCIZfVr6dPn66034CAADZjxowyt+OflaTlrUgsb/UrY8Wr6XR0dNizZ89K3d65c2cGgH377belbi9v9euHjh8/Lv5ef39/lp2dXer+9u3bs4YNG4q/dHV1GQDWsGHDctu7fv26eCVqicTExEp/P1OmTGEikYilpKQwNTU15ujoKL5PKBQybW1t1rVrV8YYY1OnTi3T1g8//MAAsMjISAaAhYWFlbk/MjKySu2XePLkCffVr23btmULFiyQWHvx8fHs9u3bEmuvOgAwGxubcu87e/ZsuWPk6tWrcjVGjIyMyh0jJar6nAHA5s6dW+ljCgsL2aBBg9jSpUsrfEyrVq3YX3/9VWk7jDEWFBRU6firLM/79++Zvb09MzQ0ZKmpqZ/U/pQpU5ixsTETiUQfzSppgYGBTFNTU+b9qgA/VsV6incxR0WdghMIBDLvs6SYKE9GRga7fPky09TUZPHx8aXu09bWZgDK3M4YY2ZmZgwAu3v3bqnb58+fz/r371/m8YGBgQwAq1OnDnv79q349k2bNjEAbNiwYZX+DBEREQwAMzIyYj/88EOFb1wl2058uKVJUVGR+N9FRUVMW1ubCYVC8f2VbflSWFhYaquLoUOHMm1tbfEbyI0bNxgA9uuvvzLGGIuNjS3T1sCBA5mFhQXLzc1lANj06dPL3J+bm1ul9kvIQ1HXqlUr9sMPP0isvadPn7KrV69KrL3qqKioZooaGwAAIABJREFUy8jIYGZmZuWOkby8PLkaIy4uLpWOkao8Zx4/fswaNGhQ6RYfIpGITZw4kc2fP198m1AoZJMnTxb/OyYmhiUmJpb6vhMnTrDQ0NAy/RkaGrIGDRp8Up6SD1GrVq2q8Psra58xxszNzdmXX35Z4f3StGPHDqavr8+lbyXnx6ioI7Kgra0t8z7XrVvHAIjfKEQiEXv9+jX7888/mZ2dHXv//j1TU1NjzZs3LzWrAID16tWrTHvv3r0T7z116tQp8e1///03MzQ0LFOAMFb8BljyPR/uybVw4UIGgHXp0uWjP8eZM2dYjx49xG9cQUFBZR7z36KuoKCAtWrVijVq1Ej8xqCpqcmWLFki/p4vv/ySmZubs4MHD7J27dqxFStWiN8QV69ezaytrcWPvXnzJtPV1RXP2owdO5Z17tyZFRUViR8zcuRI8QxFXFwc09LSYidOnBDfp6urW+b+6rTPWPHsEQDm6ur60d+btNjY2FQ6U1Ndqamp7MKFCxJrrzoAMFNT0zJjxM7OjnXu3LncMVIyrhRljHzsORMdHc1atGjBkpKSyty3a9cu8T5wJfvEjR07lo0dO5aNGDGCtWzZks2ZM4cxVlxI1alTR3z/2LFjmZubGzM2NmavXr0q01+fPn3K7bOyPIwVF8cGBgZs8+bN5c6yldd+eWPc2tqaZWVllduHtP3222/M2NiYS99Kzo9RUUdkwczMTKb9+fj4sNq1azMATENDg9WqVYupqamJDxuVzCpZW1szd3d3ZmxszBwcHFiTJk3Y6NGjWUZGRpk227ZtK/7eBg0asEmTJjHGGDMxMWEAmKWlJdu/f7/48ampqWzw4MHi79HS0mLfffcdGzFihPhwKQDWt29fdvLkyY/+TBcvXmRubm5s6tSpZe77b1GXl5fHjI2NmY6OjvgN5dy5c8zGxoZ1796d9e7dmzk4OLD4+HgWHh4uzmNgYMA6dOjApk6dWmbG4+LFi6xp06bMwcGBjRo1qswsjVAoZHXr1mUeHh6sZ8+ebN26daXumzRpUoX3V6V9xhhr1qwZA8AEAgEbN24cu3Llykd/b5JmYmLCNm3aJLH2cnJyyhyalgUfHx/xc7C8MeLn58cYKz1GmjRpwgCUO0Y2b95cZowwxti1a9eqNUZSUlLKHSNV8d8x8u2335Z5zjBWPBN9+PBh5uXlxUxNTdm2bdvKbW/58uVs7ty5bOvWraVeOz78evToERMKhczKyqrc+0s2BK6sv6rkefLkCVu2bBnz8PAocwj7Y+2XN8YrmvWXhRUrVjALCwtu/SsxP1bFekrAWOmTnDmRixCk+tq0aYO4uDjeMcqIj49H8+bNwRhDUlISXr16Jd4fSl7l5eWVWSlanqysLBQVFZW5rmhycjJ0dXXFW44Axav1kpOTYWhoiMaNG1d6wnlKSkqFJ4gLhUK8fPmywstefez+j7XPm0gkgqamJvbv3w9PT0+JtRsQECBeNCBvPhwjN27cQLNmzcpdGCNPPjZGCgsLsXv3btjb26N9+/aVbmjMGJPIpbS2b99eYX9VyRMVFYX09HQMHjy42u0D1Rvj0jZ//nxERETg1q1b3DIoKX8AP1TlgVTUkRrp3r07Lly4wDsGITXy6tUrmJqa4vTp05VeHaS61q5di5kzZ0qsPULk2eTJk5GUlIQ///yTdxRl448qFnW0pQmpkRYtWvCOQEiNPXr0CIDkn8/x8fESbY8Qefbo0SN6T+CMijpSIx9eTocQRZWSkgI1NTXUrVtXou2WbMBMiCrIyMgoc1oIkS0q6kiNWFlZ8Y5ASI0lJyejQYMGEr+sF6/LNRHCw7Nnz9CoUSPeMVQaFXWkRqysrGg2gii8pKQkqXxASUpKknibhMijjIwM5Obmiq9tS/igoo7UiLW1NZKTk3nHIKRGkpOTpfJmlJaWhnfv3km8XULkTcn7AB294YuKOlIjFhYWNBtBFJ60ZuoA0IceohKSkpKgpqYGS0tL3lFUGhV1pEY0NTVx+PBh3jEI+WSPHj3CgwcPMGjQIIm3bWZmhlOnTkm8XULkzalTp+Dq6gpNTU3eUVQaFXWkxmJjY3lHIOSTxcbGQl1dHW3atJF423Z2drh7967E2yVE3ty9exe2tra8Y6g8KupIjcXHxyM/P593DEI+yd27d9GsWTPo6OhIvG07Ozv60ENUwr1792Bvb887hsqjoo7UWGFhIR48eMA7BiGfJDY2FnZ2dlJp297eHnFxcRCJRFJpnxB58fbtWyrq5AAVdaTGdHR0EBMTwzsGIZ8kJiYGbdu2lUrb9vb2yMvLQ0JCglTaJ0ReqKmpSeUUBlI9VNSRGuvYsSMuX77MOwYh1ZaSkoKnT5/CyclJKu23bt0a6urq9KGHKD0bGxvUqlWLdwyVR0UdqbF58+Zh27ZtyMzM5B2FkGpZvnw52rdvj969e0ulfV1dXXz11VdYuXKlVNonRB6IRCLMmjWLdwwCKuqIBHTp0gUAEB0dzTkJIdUTFRUFFxcXqfbh4uKCmJgY2oSYKK27d+9KfRyRqqGijtSYiYkJWrRogQsXLvCOQki1xMTEoGvXrlLtw8XFBQUFBbh27ZpU+yGEl6ioKNrORE5QUUckws3NDREREbxjEFItIpEIvXr1kmoflpaWaNSoEaKioqTaDyG8XLp0CWpqVE7IA/orEIno378/7ty5g5SUFN5RCKmyjh07wtTUVOr99OjRA6dPn5Z6P4TIGmMMZ86c4R2D/IOKOiIRPXr0gI6ODsLDw3lHIaTK+vfvL5N+3N3dcenSJbx580Ym/REiK7dv38bLly95xyD/oKKOSISuri6WLVuGWbNmQSgU8o5DyEctX74cixcvlklfY8aMQbt27TBnzhyZ9EeIrMyYMQNffvkl7xjkH1TUEYkZMmQIcnJy6ALmRCGEhITI9Dwgd3d3mskmSiU7OxvR0dEym/EmH0dFHZEYCwsLODs7IyQkhHcUQir15MkT3Lx5U6Z9enh44NmzZ7QRMVEaJ06cgJqaGvr27cs7CvkHFXVEokaOHImjR48iPz+fdxRCKhQSEiKTBRIfcnR0ROPGjREcHCzTfgmRlpCQEPTp0wdGRka8o5B/UFFHJGr48OHIzc2lw0xEroWEhGDYsGEy73fEiBE0k02UwuvXrxEREYGRI0fyjkI+QEUdkagGDRqgW7duCAgI4B2FkHLFxcUhJiYGo0aNknnfI0aMQGJiosz7JUTSjh49CsYYBg0axDsK+YAG7wBE+Zw8eRL169fnHYOQMvLz89G9e3f8/PPP6NGjh8z779ixI+bPnw+RSESbtRKFlZaWBm9vb+zcuZMOvcoZelUhEqerq4sxY8bwjkFIGYcOHUJOTg7XLRgmTJhApycQhbZ7927UqlULQ4cO5R2F/AcVdUQqfHx86FqwRO5s3boVHh4eXGeSmzVrhq1bt3Lrn5Ca2rZtG8aMGQNdXV3eUch/UFFHpMLBwQG//vor7xiEiN29exdnz57FV199xTsKjh8/jtTUVN4xCKm28+fPIz4+Hj4+PryjkHJQUUek5vDhw3RSOJEbq1atQuvWrdGvXz/eUWBoaIhdu3bxjkFItW3duhWdOnVC27ZteUch5aCijkiNpaUl1qxZwzsGIUhJScG+ffswe/ZsCAQC3nEwbtw4/P777ygqKuIdhZBqCQ0NxeTJk3nHIBWg1a9Eau7evQsLCwuYm5tj4cKFvOMQFdamTRssXrwYEydO5B0FALBmzRq8ePECzs7OuHbtGu84hFRJXl4evvvuO3h7e/OOQipAM3VEavT19TF9+nSsXr0ar1+/5h2HqDDGGL7++mveMUqZPXs2rl+/TguKiMLYtWsXfH19eccglaCijkjVrFmzAACrV6/mnISoquzsbMyaNQt16tThHaUUJycnuLi4YOXKlbyjEPJRIpEIa9asQd26dXlHIZWgoo5IVe3atTF79mysXbsWmZmZvOMQFbRy5UrMnDmTd4xyzZ07F8ePH0dsbCzvKIRUKiQkBE+ePOEdg3yEgDHGOwMAyEUIIh25ubmwtraGt7c3li5dyjsOUSEZGRmwtrbGmzdveEcpF2MMHTp0gKWlJY4cOcI7DiHlKioqgq2tLTp06ECXgOTDH8APVXkgzdQRqdPX18e3336LX3/9FS9evOAdh6iQH3/8EbVq1eIdo0ICgQA//fQTjh49ihs3bvCOQ0i5AgMD8fjxY/j5+fGOQj6CVr8SmZgzZw6aNGkCCwsLyMnsMFFy7u7uSEtLQ0pKCu8olfLw8MCpU6fQqVMnGhtE7mzbtg1ff/01EhISYGVlxTsO+Qgq6ojMDBkyBL169QJjTC72CiPK68SJE4iIiMD58+ehpib/ByTc3NzQo0cPGhtEruTm5mLx4sXw8fGhgk5B0Dl1RKZiY2Nx48YNudkvjCif9+/fw97eHg4ODggJCeEdp8piYmIQFxeHMWPG8I5CCABgwYIF2Lx5Mx4/fgxTU1PecVSZP6p4Th0VdUTmjI2N8eDBA5ibm/OOQpTQ999/j/Xr1yMuLg4WFha841RLo0aNEB8fDz09Pd5RiIpLSkpCq1at8Msvv8jt6nEV4g9aKEHklaGhIaZPn847BlFCsbGxWL58OZYuXapwBR0AvHnzBr/88gvvGIRg7ty5sLa2ps2GFQzN1BGZCw8PR//+/XHs2DF89tlnvOMQJeLo6AhNTU1cvHhRIc6l+6+VK1di8eLFuHfvHmxsbHjHISpMIBAgLCwM/fv35x2F0EwdkWfu7u5IT0/H5MmT4eXlxTsOURKxsbEYNGgQoqKiFLKgA4pXiZ85cwbNmjXD7t27ecchKmrevHn466+/qKBTQDRTR7iJiIhA//79sWfPHowdO5Z3HKLA3r9/j06dOiEmJgbq6uq849TYjBkzEBAQgAcPHsDMzIx3HKJCbt26BUdHRxQWFvKOQv7lD5qpI/KuX79+mD59Onx9fZGcnMw7DlFg8+bNw/Pnz5WioAOA//3vf9DX16dzT4lMFRYWwtvbGy4uLryjkE9ERR3hatmyZWjcuDFt40A+WVhYGNavX49NmzbxjiIx+vr62LJlC4KDg7F//37ecYiK+OGHH/Dw4UNs3bqVdxTyiejwK+Hu3r176Ny5M96+fcs7ClEwSUlJ6NixIwYOHKiU56BNmzYNgYGByMrK4h2FKLlLly6hR48eWL9+PaZOnco7DinNH7RPHVEkQUFBUFdXx8iRI3lHIQoiLy8PLi4uEIlEiI6Ohq6uLu9IEpeXl4eOHTvi7t27Crv4g8i/169fw8HBAba2tvjjjz94xyFl+YPOqSOKxMvLCxs2bEDz5s1pVoJ81MaNG6Gvr48lS5YgJiZGKQs6ANDV1UVcXBy0tbVx6dIl3nGIEtqxYwfq1KmDzZs3U0GnBKioI3LjwIEDyMvLw9ChQyEUCnnHIXJsxowZ+PHHHzFgwADeUWTCw8MDnp6eSE1N5R2FKBlfX1989913cHd35x2FSAAVdURu1KtXDydOnMCtW7fg7e3NOw6RUzExMfjiiy/w/fff844iM3v27IGhoSE8PT1RUFDAOw5REq9evUK3bt2wZMkS3lGIhFBRR+SKvb09Dhw4gH379sHPz493HCJnXrx4gYEDB+L333/nHUWmDA0NcejQIdy5cwezZs3iHYcoAZFIhNGjR2Pfvn10vqYSob8kkTt9+/bF5s2b8eOPP2Lbtm284xA5kZWVhQEDBsDIyAiampq848hcmzZtsH37dmzYsAE7d+7kHYcouHnz5uHixYswMTHhHYVIkAbvAISUx9vbGy9evMCUKVPoUCzBmzdv4O7ujuzsbJVeMDBixAjExsZiypQpsLS0RO/evXlHIgpo8+bNWL16Nfbu3cs7CpEw2tKEyD0dHR388ccfcHNz4x2FyJhQKMSgQYNw/fp1nDt3Dra2trwjyYX8/Hy4ubkhJCQE9erV4x2HKIjbt2+je/fu8PT0xI4dO3jHIVXnD9rShCgLT09PDBo0COHh4byjEBkbPHgwLl++jIiICCroPqCtrY0jR45gwIABeP36Ne84RAEkJibCw8MDTk5O2LJlC+84REqoqCNyb9euXRg5ciQGDRqEw4cP845DZCQ3NxdXr15FZGQkOnTowDuO3DE2NkZaWhoGDBhAV2MhH9W7d280aNAABw8eVMlzUlUFFXVE7qmrq2PHjh3w8fGBp6cnAgMDeUciUpadnQ03NzecO3cOnTp14h1HbkVGRiIhIQGff/453r9/zzsOkVNpaWmoVasWIiIiYGhoyDsOkSIq6ohCEAgE2LBhA2bPno1x48Zh48aNvCMRKUlNTUXPnj3x4sUL2NnZ8Y4j11q2bIlTp07h9u3btIcdKVdmZibc3NwQGRlJK11VABV1RKEsW7YMS5YswTfffAM5WeRDJOjevXtwcnJCXl4eLl68yDuOQmjbti1OnjyJ8+fPw9PTE/n5+bwjETmRkZGBPn36IDs7mxbUqAha/UoUlomJCZo3b44jR47A3NycdxxSA9u3b8fUqVPh6emJnTt3QktLi3ckhXTr1i3069cPT548ocNsKuzp06fo3bs3NDU1ERkZiYYNG/KORGrGH7T6lSi7y5cvIz09HR06dMCVK1d4xyGfSCgUwtvbG99++y0CAwOpoKuB9u3b4/z58+jTpw9evXrFOw7hID4+Ht26dYO+vj7Onz9PBZ2KoaKOKKwWLVrgxo0bcHBwgKurKzZv3sw7Eqmmv/76C66urjh48CB++ukn3nGUQuvWrZGeng5nZ2ckJCTwjkNkzNnZGQ0bNsTZs2dhZmbGOw6RMSrqiEIzMjLCH3/8gV69juDrr9MxYcIE3pFIFZ07dw7t27dHdnY2hg4dyjuOUrly5QqMjIzg7OyMy5cv845DZGT//v1wdXXFmTNnUKdOHd5xCAdU1BGFd+WKAKdP98f48SNx9OhRxMTE8I5EKlFYWIiFCxeid+/e6NatG65du8Y7ktIxNzfHuXPn4OLigt69eyMkJIR3JCJlS5cuhZeXF0JDQ6Grq8s7DuGEijqi0NLTgREjgH79gJ07W+DOnTvo3LkzVq1aRatj5VBCQgK6dOmCNWvWYNOmTQgNDYWBgQHvWEpJT08Phw4dwpQpUzBq1Cj88EOVzrMmCiYvLw/jx4/HokWLsH79eqip0du6KqO/PlFYixcDFhbAkSPAH38AAgFgYWGB/Px8uLq6olWrVjA0NMSmTZuowOMsMTERffr0wdatW3Hp0iW8ffsWU6ZM4R1L6ampqWHt2rUQiUTo168fmjZtitjYWN6xiASEh4fDxMQEzs7O8PPzQ2FhIXx9fXnHIpxRUUcU1pIlwK+/AuVdQapjx46IiYnBtGnTMGPGDHTr1k32AQmKioqwcuVK2NraIi0tDcuWLaPVrZw4OTmhUaNGcHZ2xs6dO3nHITVQVFQEDw8PeHh4IDo6GjY2NrwjETlBRR1RSE+fAmPHApMnV/wYHR0d/Pzzz7hx4wby8/OxePFivHv3TnYhVVx0dDQcHR2xcOFCLFiwADdv3uQdSeVFRkbC19cXkyZNwogRI5CVlcU7EqmmpKQkdO/eHevXr8eePXvo/DlSCm0+TBSOUAh07QqcOwfo6VXte4qKimBsbAxDQ0P88ssv8PLygkAgkGpOVTZ27FgEBgaiV69e2LhxI1q2bMk7EvnA6dOnMW7cOKipqWHv3r3o0aMH70ikCgICAuDr64vGjRvTYXTV4g/afJgoq//7P+Dhw6oXdACgrq6Ox48fw93dHePGjUOXLl1o1aUU5OXl4ccff0R0dDQOHTqE06dPU0Enh3r37o3Y2Fg4Ojqid+/eeP/+Pe9IpBIZGRkYPXo0xo0bh4kTJ+L69eu8IxE5RUUdUShBQcCmTcCOHdX/XjMzM2zduhU3b96ElpYWOnfuTPujSUh+fj7Wr18PGxsbrFy5Evfv38eQIUN4xyKVMDExwcGDB/H777/D3t4e586d4x2JlCMwMBCtWrVCwvnzuLJ8OdasWQNtbW3esYicoqKOKITcXKBVK2DjxuLDr8OHf3pbDg4OOH/+PBhjOHjwIGxtbaGuro5Ro0bhxo0bkgut5N6+fQtra2toa2tj1qxZGDZsGFJTU5GTk0NvOgpk0qRJiI+PR3p6OurVq4f69esjICCAVoxzJBQKsWLFChgYGMDKygrp6em4/ugRHNevBzw9eccjcoyKOqIQvL2BV6+AkBBAU1Ny7QoEAsTGxiIwMBDx8fHo1KkTevTogePHj0uuEyWTmpqKBQsWwMLCAh4eHnj8+DE2btyIBg0a8I5GasDT0xMPHjzA559/jvHjx6NLly68I6mkY8eOoU2bNvDz88OcOXPg4uJSfIeBAbB9OxAaWvxCSEg5qKgjcm/9euDAAWDfPkAa16ZWU1PDqFGjcOvWLZw+fRp6enr4/PPPsXbtWmRmZkq+QwV18eJFfPHFF7CyssKOHTswa9YsbNiwAZaWlryjEQmpU6cOtmzZgps3b0JbWxvjxo1DSkoK71gqIS4uDm5ubhg0aBA6dOiAhw8fws/Pr/SD+vQBpkwBaD86UgFa/UrknpZW8UbDCxfKrs/79+/DyckJQqEQQ4cOhY+PD3r06KGSK2bT09PRrVs3PHr0CB06dMDUqVMxduxYOsSqAmxsbPDy5Uv4+vpi3rx5MDU15R1JKSUmJqJ58+ZwcHDAunXr/p2dK8+bN4CdHZCcLLN8hDt/0OpXogwyMgA3N+D772Xbb+vWrZGSkoKNGzciMTERvXr1QtOmTfG9rINwkpubi6CgIAwaNAiNGjVCr169cOvWLdy4cQOTJk2igk5F3L9/H0uWLMHu3bthbW2NhQsX0t52EvTs2TNMnjwZLVq0wO7du3H9+vXKCzrg38Ow+/fLJiRRKDRTR+SWSAT071982NXYmG+Wu3fvYs+ePQgJCYGuri5GjBiB4cOHw97enm8wCQsODkZISAjCwsJQVFSE3r17i7dSIKrr7du32LBhA1asWIHCwkLEx8fDzMyMdyyF9eTJE6xatQrbt29HgwYNsHjxYkyYMKF6jZiaAnFxgLm5dEISeeKPKs7UUVFH5FJEBDBgALBhAzB1Ku80ZTHGcPPmTZw8eRInT57EtWvX4OjoiK5du8LFxQVdunRB3bp1eccsV1FRETZt2oSoqChcuHABqampMDMzQ79+/bBmzRqYmJjwjkjk3G+//YbVq1cjISEBAwcOxIwZM9CrVy+VPD2hKgoKCnDkyBGsXbsWhYWFmDNnDoYOHQp1dfVPb/TsWaB3byAgAPDyklxYIo/8QUUdUVTPnwPt2gHu7sWvV4ogMzMTCxYsQFRUFOLi4sAYQ/PmzeHk5ISxY8fCzs4O9erVk3kuoVCIhw8fIjY2FrGxsbh16xauXr0KDQ0NODs7o1u3bujbty/at29Pb8ikWkQiEY4dO4ZVq1bh0qVLsLGxgbe3N+bPn887mtyIj4/Htm3bsHv3bmRkZOCzzz7DkSNHJNfBtGnFhzLi4gAOry9EZvxBRR1RREIh0L178b50V68CtWrxTlR92dnZiI6ORlRUFK5fv45Tp04BAOrWrQtbW1tYW1vD2toaPXv2ROPGjWFmZlaji9wzxvD69Wv89ddfSEpKQnJyMpKSknDq1Ck8evQIBQUF0NLSQqtWreDg4ABnZ2dMnjyZijgiMXFxcdi6dSv27t2L7t2748svv4S7u7vKnnuZk5ODzz//HOfPn4elpSUmTpyIiRMnwsLCQrIdvX0L2NsDtrbA0aOSbZvIE39QUUcU0TffALt3A9evAy1a8E4jGRkZGeKZsgcPHiApKQlJSUlISEgQP8bAwAAmJiYwNTVF48aNoaWlBW1tbfF/1dTUkJubC6FQiPz8fLx//x5ZWVl49eoV4uPjUVhYKG7L1NQUVlZWcHNzg52dHezt7dGiRQtoaGjw+PGJCsnPz0f//v1x/vx56OvrY9CgQRg5ciTc3Nxq9MFFEeTm5uLYsWMICQlBeHg4BgwYAB8fH/Tr1w9qalJck3j+PNCzJ7BnDzB2rPT6ITz5g4o6omj27wdGjwaCg4ERI3inkb4XL17g2bNnSEtLw6tXr/Dq1StkZGTg2bNnyM/PFxdwQqEQRUVFMDAwEBd52traMDIygqmpKWbu2IFr27ahQYMGsLKygr6+Pu8fjai4lJQUHDhwACEhIYiOjkbt2rWxadMm9O3bV+nO2dy6dSvCwsIQHh6OwsJC9OnTByNHjsSXX34puxDTpxefqxIXB9SvL7t+iaz4g4o6omgMDICJE4F163gnUTACAfD6NWBoyDsJIWU8f/4cBw4cwLfffguRSARHR0e4u7ujX79+6NChAzQleYkYGXj37h2io6MRHh6O8PBwJCYmomfPnhg8eDCGDh0KYx5L9d+9Kz4M26oV8Mcfsu+fSJs/qKgjisTLC1i+HGjUiHcSBSQQAFFRAF3WiSiQR48eISoqCsuXL8ejR48gEAhgbW0tPmVgzJgxaNq0ac1WiH6CklMlYmNjcffuXcTGxiIlJQXt2rWDi4sLXFxc0LVrVzSSxxermTOBefMAumSfsvEHFXVEUWzcWHz0oKiIdxIFJRAAhw8DgwfzTkLIJ8nIyEB0dDTu3LkjLqgePUqEpiZgaWkJKysrWFtbw8rKCp07dxaff2piYgI9Pb1q9fXmzRtkZGQgPj4eqamp4oVFJf99/vy5eGGRvb097Ozs0K5dO/Tp00dKP70EvXtXfO4KXbta2fiDijqiCK5dA7p1K75ixOLFvNMoKCOj4mnOyZN5JyFEIgoLgaVLk9CkSXSpgis5ORlPnjwp9VhdXV0YGRmVOudUS0sLjDEIhUK8efMGQqFQvLhIKBSW+t7GjRuLC0Zra2sMGDBAsRcWqasXX3FCluf0EWnzRxWLOgV91hJl8OoV4OkJ9OoFLFrEO40CMzMD0tJ4pyBEYhISgIEDrdGunXWZ+0pWfWdkZIipuFLMAAAgAElEQVT/+/r1a7x586bUAiOBQABtbW1xsaejoyNeXGRiYoIWLVrAUBnPQ50xo/gwrJsb0LAh7zRExqioI1yIRP+uvg8IKD6CSD4RFXVEydy/X3yJwPLUqVMHderUQdOmTWUbSlH873/AiROAjw8QFsY7DZExKW6eQ0jFliwBzpwBDhwAlGyHA9mjoo4ombg4QFeXdwoFpasL7NxZfK3FHTt4pyEyRkUdkbm6dYGHD4H8fMDRkXcaJWBrC9y7xzsFIRITGso7gYLr0qX4ahPLlxdfb5GoDCrqiEwVFBRPLG3dyjuJEjE3p5k6ojSKioBHj3inUAI6OsUzdn/+CWzbxjsNkREq6ohMzZlT/ClcEa/pKrfMzIpXndCeMEQJPHlSPItPJMDZGZg1C5g9m3cSIiNU1BGZCQkBfv21eNNzIkFmZsUrT1694p2EkBqLi6OFUxL100+0GbEKoaKOyMTDh8CkScC0abyTKCEzs+L/0iFYogTu3wesrHinUCI6OsCuXcDvv/NOQmSAijoidW/fAsOHA23aAKtW8U6jhKioI0rk/n2gdWveKZRM587F5748fco7CZEyKuqIVG3eXHyd+VWrgCtXAC0t3omUkIkJUK8erYAlCk8kAg4dKt6UnEhYZCTQpAnw22+8kxApoqKOSM3168Ubmy9aBPTrxzuNkjMzA16+5J2CkBp58gR4/754Vp9ImKMjMHdu8VdyMu80REqoqCNS4+kJ9OhB13SVCdqAmCiB+/eLF0nQYiop8fcHGjcuPsFZPq77TiSMijoiFYwVH0oJDATU6FkmfVTUESVw/z5gaUlbHkmNtnbxookLF4rPjSFKh95uiVT873/FW5iYmvJOoiKoqCNKIC6ODr1KXceOwLx5xV9JSbzTEAmjoo5I3OnTgJ8f4OTEO4kKoaKOKAFa+Sojfn6AtTUwcSLvJETCqKgjElNQALi4FO9F9/o17zQqxtYWSEwE8vJ4JyHkkzx5Aty+TStfZUJLC7h7F+jatfh1gygNKuqIxMybB9y5Axw8COjr806jYmivOqLg4uKK/0uLJGRo0SJgwgRaNKFEqKgjEhEaCqxdW7xpOR0+4YCKOqLgShZJGBjwTqJCtLSAy5eB9et5JyESQkUdqbH4+OJTM77+GvDy4p1GRVFRRxQcnU/HyYIFwPz5QEIC7yREAqioIzU2bBjQsiWwZg3vJCqsVi1AT4+KOqKwaOUrJwsXAs2aFX8yF4l4pyE1REUdqbGUFODAAboEGHfm5lTUEYXEGPDwIc3UcaGpWbx33ZUrwK+/8k5DaoiKOvLJbt4EdHSAV6+KNyknnNna0vVfiUI6cQJ49w4YPpx3EhXl4AC8fQvs2VO8IpZm7BQWFXXkk2RlFb8Ad+vGO0nV9erVCx4eHjVqgzFW4zakhvaqIwrq/n2gUSPA0JB3kqo7duwYrKysJNLGqVOnJBOqJkpm7K5dK171RhQSFXWk2hgDvvgCKCwEgoJ4p6m6u3fvIiYmBqIafAoViUQ1bkNqqKgjCkoRz6dLSkrC06dPJdJGkrxc2cHevnibk4ULeSchn4iKOlJtS5cCp04VXwasbl3eaaru8ePHePDgAdRqcDFadXX1GrchNVTUEQWliCtfZ8yYUeOirqSNKVOmSCiVBMyfX7xZoDx+cCUfJYfvTESenTkDLF4MrFgBODvzTlM9RkZGMJTA8R1JtCEVZmZAejptJEoUCmPAgweKV9QBgKWlpVy0IVEaGsWHYWk7A4WkwTsAURwpKcDo0cDQocCMGf/ePnXqVACAhoYGvvvuOzRs2BAbN25EQkIC1vzzwpCeno5169YhJiYG06ZNg7u7u/j7Dx06hAMHDsDY2Biff/45+vXr99EsjDF89dVXmDt3Lq5evYo//vgDZmZmmD59OmJiYnD48GFoaGhgxowZaN++PQAgNDQUgYGBCA0Nhbq6OgDg+fPnmDx5MvT19dG4cWOsXLmy0tvz8vLg5eUlbuPmzZs4ePAg9PX18fz5czx69AgTJkzAF198Ic569epVrF27FsnJyWjZsiUcHR3Rtm1bdOnSpQZ/jXKYmRVfqy07GzcTExEREYEBAwZgy5YtZXK9f/8eGzZswJUrV5CXlwd7e3t88803aNCggWQzEfIRT58Wn6Pfpg0QGxuLzZs3AwA6deqEiRMnIjExEatWrYJIJMLw4cNhb28vfi0xMDDA+PHjxa8nz58/x7Zt23Dt2rUyY7cymZmZ2L9/P7KysvD69Ws8fPgQI0aMwOjRo5GcnIygoCDx65tAIAAAPHz4EIGBgfjpp5/E7ZS8lmVnZ2PmzJni17KKbi9pw9LSEp6enjh+/DgOHTqEdevW4ciRI/jjjz+wZ88e1KtXT9xHfn4+li1bhkuXLsHR0RGdOnWChYUFLC0tYWpqKpk/ip0d4OgIDBwItGhR6q7s7Owq5Sx5jTl79iy9vsgSY0wevogC0NNj7N69srdv2rSJAWCTJ08W3xYRESH+/yFDhrBJkyaJ/w2A1a9fnzHG2FdffcVMTU1ZVlYWu3jxIvvll1+qnAcAW7NmjfjfY8eOZcVP6WLjx49nANjTp08ZY4w5OjoyAKygoIAxxlhycnKpx3t4eFR6+4f9lrSRkJDAALA2bdqU+l2sXr1a/Hh1dXVWWFjIGGPs6NGjZX5XEpOayhjA2PnzLCEhgbVp04bl5uaWm0sgEIh/L4wxdubMGQaA5efnSz4XIZVYvpyxhg3//bdQKGSNGjUq9fx0dnZmJ06cYIyxUq8l0dHR4teTknG7ZMkS8f3/HbsVycnJYQCYtbU1Y+zfcd2kSROWmZnJGCse9xMmTBB/T1hYWKnXiQ9fyxhj4teyim7/sI3vv/+eMfbva9iuXbvEjzE0NBT//40bNxgA5ufnxxhjrF27dgwAS0lJqdLPWS337jGmpVX8B/qPj+X09vYu9RpT8vrSrFkzeo35NH6sivUUHX4lVXLoELBlS/knM/9/e3ceH9O9/gH8M0lkkcSSiwgVkViiiqjYaylNUVrUdlFqKSnaWlq0VEtd0lJuN1pU3Rv7L6q6WGorckUrQySKiEgqQUMQWWSRzDy/P45zMmfWk5jJJON5v17zImfO8p1znvPMM3O+5zvjx49H3bp1ERkZiczMTADA5s2bAQC5ubn44Ycf8P333yM0NBShoaEICgqCl5cXCgsL0blzZ9y+fRvt2rVDt27dMH369DK1q3Xr1tL/W+h9ogwODgYAqd+Lm5ub7Hl/f380adIEr7/+Om7duoVt27aZnW6Mh4cHAKBRo0YAgCcfXkO6pdO3TaPR4MqVKwCAzp07AxC+GbC6OnUAlQq4dQseHh5o1KgRPD09DdpFRPDz85Nd9unevTs8PT3xJw+JwiqYfn+6atWqYebMmfji4ZhpV65cwdWrV9G3b1/k5ubKcsnUqVOlfCKet++//76ic1eXt7c3VCoVGjZsCAAICgqCi4sLGjVqhNq1awMA3N3dZX3o9POJbi776aefpFxmarqxdYj5RMxdAJCTk4OioiIAkPJrUlISAKBTp04AbJRPWrUCFi0S+ttcvKi4nUSEHTt2yHKMmF8uX77MOcbGuKhjFl2+LPzm8yuvGH/ew8MDb731FgoLC7F69Wrk5+dj3759AEqTz8KFC6FWq6FWq5GcnIykpCS4u7tj5MiRGD9+PP766y/069cPBQUF5W6nu7u77O9q1aoBAIqLi43Or1Kp8N1332Ht2rVo1qwZfvzxR7PTlRAv6+oT13HxYXLUvfxsNS4ugI+P0ZsldNt169Ytgxs9XFxc0KlTJ+l4MVZRzp837E83ZcoUrF+/Hjk5OdiyZQvGjh0LZ2dnJCUlyXJJXFyclE/E87Z+/frlOnf16RdcLi4uJnMJAFkuGzRokJTLTE0vj+7du8PNzQ2HDh0CIFy+DQgIMPhAazVz5wqXYsePBzQaRYvcunULubm5shwj5hcAnGNsjIs6ZlZBgTAeXfPm5uebNm0aPD09sWbNGmzbtg3PPfccAKB69eoAgNjYWNn8+fn5uHnzJkpKSrBx40Zs374dBw4cwPDhw23yOozRaDTo3LkzRo8ejZycHLz66qs4cuSIyenlNWTIEMyfPx9Tp07FzJkzMXfuXEycONGKr0SHgjtga9eujWvXrhncudewYUP4+vrapl2MmXDxouEVAG9vb+Tk5GDdunXYsmULxo8fD0DIJ/q5BBDyiXjexsfHy87diqKbyxo2bCjlMlPTy8PT0xN79+5FXl4ewsPDcevWLan/sE04Ows3TcTHAwr6JgJCfnFxcTHIMeK3oJxjbIuLOmbW1KlAerrwM2Dm+Pj44LXXXkNmZiamTp0qJWHx0khUVBTi4uIACGO9jRs3DmlpaZg+fToyMjIwcuRIhISE4NixY7JLCSUlJYrbSmW86zM1NRVvv/02tmzZgg0bNkCr1WL37t0mp5fXnTt3sGvXLrzxxhs4ceIEPvnkE6mztdUpKOpcH/6e2/Hjx2XT1Wo1QkJCbNMuxkzIyzN+56ubmxvmz58PHx8f6TJfUFCQLJcApflEPG/r1asnO3dFJSUlZfqWrKz5RDeXJSQkSLnM1PTySklJwcsvv4z3338f58+ft/05++STwOLFwIcfCtfKLXB1dZXapJtj1Go1PDw8OMfYGBd1zKT164Vfjdm0CVAycPrs2bPh4uICX19fhIWFARBO8LfeegsajQY9e/bEokWL0L17dzRp0gQdOnSARqPBV199BUDoH9KqVSvUqlULgHCnaWBgIKKjo01uMzs7W/r/nTt3ZM/dvn0bAJCVlQVA6PMBAGlpadI8O3bsAADpVyK6detmdrpIXEdGRoasHeInU92knZCQgPj4eJw4cQJRUVHYt2+fQVut5mFRl5GRIds3+u1SqVT48ssvUVhYKD2fmJgo9R9irCIZK+peffVVFBcXY8KECdI0V1dXWS5ZtWqVlE8A4bwVz0ndX34Rc0mDBg2M5pObN2+CiKRzpqCgAPn5+UhPT5etQ8wlQGk+EenmMh8fHymXmZquuw5T+UQk5ovU1FSEh4cjJycHBw4cwI4dO6BWq43sUSt75x3hp8Qefli31M5Vq1ZJOQYozS/vvfce5xgbU5X104iNVIpGMMG9e0D79kCTJsIgw0rH2X3w4AEaNGiA33//HU2bNjV4Pi8vD8nJybJPaoWFhXB3d8eNGzeM3u5eUFCAjRs3Ytq0aeV+PaZotVqUlJQgLS0NgYGBUh8QU9PL691338X48eORmZmJ/Px85OXlYcOGDdi7d681XobcRx8JP/ORmGhxVo1Gg4sXLyI3Nxdt2rSRbqpgrCKFhQEHDxpO//TTT5GQkIDIyEiD58RcEhQUBG9vbwCl561Go8H169cNzt2CggIsX74cdevWtUk+0c1lrq6u0vAipqaXx8WLFzFq1ChERkbi9u3bKCgoQGpqKubOnYvz589Lxa3NrFgBvPCC4p//0Gg0OHXqFOeXR7cIwGJFcyq9TdbGD1aJvPiiMMTAzZvK5tdoNEQkDJuhdAgBpWJiYqShAKqiNWvWUGZmpsH0w4cP22aDX39NVLu2bdbNmA289Zb8b61WS1qtlgICAig2NtZq24mJiaGOHTtW2XwiDvUSHh4um67Vaum5556j7Oxs2zeipISofXuih0M6sQrzISmsp3jwYWZg3z7g6FHhSp4SHh4eaNGiBZKTkw36aT0qtVqNLlXtpyt0FBYWYsqUKRg8eDAaN26MlJQU/PHHH0hMTETv3r2tv8F69YCsLGEQ4od3/zJWmeleei0qKkLLli2Rk5ODrl27IjQ01GrbUavV2L9/v3Tps6rRaDQoLi7G999/jyZNmqBt27aIjY3FoUOHMHz48Ir5pRtnZ+F25U8+ARYssP32WJnx5Vcmc/QoEBcHzJqlfJnq1atDpVJh+fLlZR5nztEVFRWhZ8+eUKvVcHFxQcuWLTFu3DhMmTLFNpcj/vc/oHt34Pp1gEdvZ5VcerrwixLPPCP8TUTo2bMnoqOjcePGDfj5+dm3gZVMdHQ01qxZI92NHxYWhjfeeEMa/7JCrFwJzJ8PqNXCcCesIiyCwsuvXNQxyd9/A+3aAQ/7wCpWVFQEZ2dn291W7wAKCwvh5uZmu7teRUlJws/6xMUJHZsZq8R+/RXo0EEYXlHX/fv3uQ+WGURk+1xiilYrfHAsLAT++EMYH5PZ2iIoLOr47lcGACgpAUaOBMrzDb6bmxsXdBa4u7tXTBIWx4CyMKwJY5XB+fOGBR0ALugssFtBBwh3zm3cKAwwGBFhv3Ywo7ioYwCA994DTp8Gvv/e3i1hj6RmTcDVlYs6ViUoGPaMVUbNmwNLlwJLlgAJCfZuDdPBRd1jLjkZqFULyMkB7t/nLhIO4amnAP59RVYF7Nxp7xawcps1S/iqtXNnYXBiVilwUfcYE38CLCgIePjb2cwRKPhVCcbs7fp1QG/sWlbVNGsmXIJdutTeLWEPcUeox9j06UBamnDZVe+3q1lVxkUdqwLOn7d3C5hVvPmm0G+Hh1GqFPibusfYf/4j/AyYrQchZxWsXj3g5k17t4Ixsy5cUD4WJqvExBsn/vUve7eEgYu6x9bZs8C77wIDB9q7Jczq+Js6VgklJABFRaV/X7hg/DdfWRUUFAQsWyYMpaQvN7fi2/MY48uvDq64WOi3ovtzg9nZQl+6S5fs1y5mRRkZQhEnPpKShOvpAwYAN24I09q2BWzxW7OMKdS2LaBSAf7+wv/j4oDgYGG4M3d3e7eOPbJu3YBXXxUGJXZ1FaalpQGbNwuDFbMKwYMPO7hJk4D//lf4/6pVwifl+fOBI0eE8SOZA5g8Wbj8odGYnufYMaBHj4prE2N6LA2tFhgofNDkIS+rsNWrgRkzgNBQ4NQpgEjoZ5ebyx23H80i8ODDrKREGDJAoxEeM2YIBd3ixVzQOZS33zZf0Lm5CcMOMGZHlr6NW7mSC7oqz81N+H3Y06eFgg4QLhfFxNi3XY8RLuoc2NGjwvhzuoiA777jO88cSnAw0Lev8XdElQro2rX0cghjdtKypfHpzs5Ap07A4MEV2x5mZWFhwlWDBw+EbxRErq7CpSFWIbioc2D/93+Gd5hrNMIPaLdvb582MRuZN0+eSEUuLkKyZczO2rc3/rlDowE++6zi28OsaONG4VsEYx48APbvr9DmPM64qHNQGg0QFSV8862vpEToWzd9euk35KyKe/ZZ4edAnPRO6eJioHdv+7SJMR1PPWW8X92QIdw7oMojMv9mcuYMkJdXce15jHFR56COHQPu3TP9vIsLMHSo5c7LrAp5913DxOrhwV/LskqhdWvjHzI/+aTi28KsbOJEICDAdKdIrRaIjq7QJj2uuKhzQFotMHy44XRnZ2E4oRMn+AschzR6NLB8uXCgASHBLlnCvc9ZpdC7t/A707o8PIRfmmIOIDlZeGP59luhH51+3pkzxz7tesxwUeeAoqOBu3dL/3Z2Fq7KzZ0r3CDRtav92sZsbPLk0psiSkq4cmeVSuvWpf93dgYWLLBfW5iNTJokjFXXuLG8sLtwQf7GxGyCizoHFBUlv0EiOFg4x5Yt46GCHF7NmsDrrwvJ1NtbGOWVsUri6adLc5OPDzBrln3bw2ykdWvhZ4t0LxkRmb6ZglkNF3UORqsFtm8XvgV3cRE+DZ89C7RrZ++WsQozY4Zwp8yzzxreOMGYHT31lBCaKhUQEQFUr27vFjGb8fICtm4Vrhw4OwsPHtrE5jjjO5iYGODOHeH/bdsKv7fIXaoeM40bC3fB8FAmrJJp3Vr44NmsGTB+vL1bwypEbKyQkzQaYN8+e7fG8RFRZXhIpk2bRhB+Nowf5Xp8TkA+AbMIcKoE7Sn7o2fPnmQtj2s8dQCoVSVoR2V4WDOeysvPz8/u+6FyPLwI0BLwQiVoS8U97t69a9V4qlu3rt1fU1keXgBtgzDwiV8laE9VeNy6dUv3kH9ICuupSvkdTq9evfDbb7/ZuxlVjlYrXNYQhilZ9fBRtUyfPh3nrfxzFxxPjy9bxFN5ffbZZ5gxY4a9m2F39+8Dnp577N2MCnHo0CGE2egbc6qig4zeKCnhy0dmHDt2DL169Sr38rxnHQh3n2KMVXaenvZuAbMrLuhsissAxhhjjDEHwEUdY4wxxpgD4KKOMcYYY8wBcFHHGGOMMeYAuKhjjDHGGHMAXNQxxhhjjDkALuoYY4wxxhwAF3WMMcYYYw6AizrGGGOMMQfARR1jjDHGmAPgoo4xxhhjzAFwUecgquqPOzPb0Gq19m4CY2YRkdm8xTHMWNlV6aIuLi4O7dq1w0cffWSV9Z05cwYvv/wyAgMD0aVLFxw8eFDRcjk5OZg/fz46d+6MrVu3oqCgwOz8eXl5WLZsGQYOHIj//Oc/yM7ONlhfWFgY+vXrh4iICBQVFcmeLykpwTvvvIMZM2ZIj969e8uWX7JkibQO/eXz8vKwaNEiDBs2DJ9//jmKi4tNtvX48ePo16+fLMFaWr9IrVZj+vTpGDhwIKZPn252n9hbXFwcFi5caJN4KkssAZDtW2PHX9/+/ftlsTB16lS8/vrrAEqP9QsvvGDyeB8/fhxjxozBM888g9jYWJPbISJMnjwZmzdvtrh9XUeOHMGgQYPQrl07XLhwwWC9eXl5Ztsn0o0nR2XtGASARYsWoVu3bvD394dGoynTsm+++SZSU1MtzmcppwEwmrMSExOl5/XjSIxhQJ5zTJ0TRISdO3ciIiLCaBuvX79u8vUoOQcKCwvx2WefYfjw4Rg2bBgOHz5scb/YU15eHjZt2oSRI0eidu3aVluvGEsTJ04sUzxZOj5lmV833l588UWj8QaYjl/d4/3qq68aHPPc3Fxs3LgRo0aNwoQJE0x++DCVE0+cOIHx48ejR48emDNnDtRqtaLXbBXipyU7PyTTpk2jXr16kRLHjx8nALRo0SJF85uTlZVFjRs3ps2bN1NiYiJ5e3vT0KFDFS0bFhZGgYGBNHr0aAJACxcuNDmvVqulrl27Ut++feny5cvUu3dvql+/Pt28eZOIiHJycqhVq1ZUr149AkAAqGfPnqTRaKR1bNq0SXpOfHTv3l22vKenp7QO3eWzsrIoODiYBgwYQLGxsVS/fn3q27cvFRQUGN0njRo1IgBUXFysaP2iO3fuUPXq1WnFihX04MEDRfuRSDj+PXv2VDy/kvUpiafjx4/T0KFDbRJPZYmlnJwc2b41tX91NW/e3CAeDh06JDvWWVlZsuMt+u677wgANWrUiJydncnZ2Zl+/fVXo9tZuXIlAaAFCxZI0zQajdHti6Kjo8nV1ZUGDx5MnTt3Ji8vL8rOzpbtp+DgYIP26cbjnTt3KDw8vFLEU3n5+fnRZ599ZnE+a8YgEdGqVauoXbt2lJOTQ5MnT6Zz584pWi4jI4NmzJhBACghIcHsvJZymshUziIyHkeHDh0iIuU5p23btgSAXn75ZaOvxd3d3ejrUXoOBAUFUadOnejChQuWd+BDBw8eJAB09+5dxcsoUbduXYvzXL9+ndavX0/e3t6yc/JRrFq1SoolAIrjaffu3SaPT1nn1483tVptEG+W4lf3eAOQHfPi4mLq06cPtW3blk6fPk179+41+Z5uLCcSETk5OdHWrVspMzOT1q1bR05OTrR48WKLr5uI6OjRowSAbt26pTv5Q1JYT9m7mHukoo6IrJYAIyMjCQD9/fffRCScELm5uRaX27ZtGy1dulRKMC4uLlSvXj3SarVG59++fTsBoN9++42ISgvTNWvWEBHRvHnzaPTo0VRSUkK7d++matWqEQD66aefpHW0bduWtm3bRlevXpUeYlvF5QsKCqR16C4/e/ZsAkBJSUlERLRu3ToCQKtWrTJo68iRI6UEKxZ1ltZPRJSfn08hISF0+PBhi/tPn72KOiKiPXv22CSelMYSkbB/dfetseOvr2PHjrJYuHbtGhEZHmui0uNNJBRUvr6+dOzYMSIiSktLIwD09NNPG2zj7Nmz5OrqapDAfvjhB5PbLy4upoEDB1J6ero0PwBavXq19LfYRv32ifEoxpKTk1OliKfyUlrUWTMGiYgCAwMpPDyciIhKSkoUL3fmzBlKTExUVNRZymkiUzmLyHgcifRzjrFzQqPR0M8//2y0CBBfS69evQxej9JzICoqivr06VOmfUhk36JO1KFDB6sVdYGBgUQkxFJycrKiZTQaDe3du9fk8THG3Pz68UZEBvFmLn6zsrJkx3vw4MGyY/7ee+8ZxBcA2r9/v0E7jeVEIjLIOb1796Z69epZfN1Ej17UVenLr9Z08eJFAICLiwsAoEGDBvDy8rK4nLe3N+bPnw8nJ2FXdunSBQEBAVCpVEbnv3nzJgDg5MmTAID8/Hxpu0SEw4cPY+3atXB2dsagQYPQr18/AMC5c+ekdSQlJUGlUoGI4O/vD39/f3h5ecmWd3d3l9ahu/yuXbvg6emJZs2aAQA6deoEAPj3v/8ta2dkZCTu3LmDoUOHStOUrB8AXn/9dZw9e1Z2SbgqEI+9NejGk9JYEvev7r41dvz1NWnSBFevXoWvry/8/f3RsGFDAIbHGig93uLzs2fPRo8ePQAAjRo1AgD8+eefBtt45ZVXDGIEAFasWGFy+zdv3sSSJUvwxBNPGLRXtw2enp4G7RO3JcbSwoULq1w8lYc1Y7CwsBB//fWXtE5nZ2fFy7Zr1w4tWrRQNK+5nCY6ePCg0ZwlMhZHgPGcY+yccHJywpNPPmn2tYjxrUvJOZCcnIxJkyZhy5YtZdqHlYW1YkqMJ0CIpaCgIEXLOTk5oX///iaPjzHm5tePN5Hu6zQXv7t27ZId74ULFwIQjrlGo8H69evh5+eHAQMGyJb75ptvZH8XFBQYzYlEhPj4eOk8AIRzQqwRbM16GaSSWLhwIaKjo/HKK69g2rRpCAwMRFxcHDw8PNCvXz/8/PPPuHz5Mrp06YJBg3SYv9sAABIHSURBVAYhMjISeXl5OHXqFAAhUOrXr48nnngCW7ZsMbmdkJAQPPfcc9KBJyLs2LEDhYWF2Ldvn8nl3nrrLaSkpGD+/Pn44IMP8OyzzyI9PV1649O/tt+0aVO4urpizJgx0rTJkydj1apVUKvV0Gq1qFWrFhISEtCoUSOj/UHE5YkIt2/fxv379xEbG4sOHTqgTZs2aNGiBS5duoRz586hdevW6Nq1K95++20cPHgQI0aMkNajUqnMrh8AZs2ahcjISKhUKri7u6OoqAgdOnTAtm3bFCeBykKMpaVLl+LixYtSPCUmJqJfv35ISUnB+fPnpXjKzs5GXl4eEhMTZfEUGhpqNpaA0nhScvx1paSkoLi4GMOGDcOtW7cAALNnz8bKlSsNjjUA6XifO3cOEydONFifSqXC+PHjZdP27NmDM2fO4PLlywbzDxkyBCdPnjS6/YYNG0oFXnp6OhYvXozNmzejf//+ACCLR5F+PIqxtHbtWnz88cdSPIn793GQmpqKAwcOoHXr1liwYAGOHj2KDRs2YOLEiXB2dsZ3332HUaNGYfbs2fjmm2+kfotnz56FVqvFX3/9hdjYWLi5ueHAgQNmtzVt2jRUr169TO2zlNMA4N69eyZzVkpKitE4IiKjOcfSOVEWls6BnJwctGjRAlqtFq1atcKdO3fg6uqKf/3rX5gzZ84jb7+ipaamIiwsDHPmzJHiaezYsdJ+0M1rderUkd4jk5KSpHgSYyk4OBhffPGF2e2Jec2a9OOtefPmBvFmjv4xDwoKko75zp07cfv2bYwdO1ZWhDVr1gy7d+9Geno6GjVqhD179uDDDz9EZGSkwfpVKhVcXV3h6emJ5s2bIysrCxEREZg0adKjvXCllH6lZ+OH5FEuv8bFxZGzszOlpaUREVHfvn0JAK1du5by8/PJzc1NWi40NJSaNm0q/T18+HACQDdu3CAioR8H9PqA6D7efPNNWTs++OADatCgAQEgf39/ysrKMtvuIUOGEAAaPHiwyUtz2dnZVLduXZozZ47R548ePUpPPPEEAaCXXnrJ5Dp0l3/xxRcJAM2fP5+IiO7du0c+Pj7k5OREeXl5REQ0ceJEg/0iXn61tP7g4GACQB999BHl5eXRpEmTCAB17NjRbL8wkT0vv/7666+yeNKNJaLSeBJjqU+fPtJzoaGhsnXpxpOlWDIWT0SWj7+u/Px8mjJlCgEglUpFarXa4FgTlR5v8Vjrq1mzJl2/fl36OyMjgwICAoiI6Pz580YvNRjbvq6kpCTq1KkTubm5GVzqF9uo3z4xHsVYIiJZPCmJJaKqd/lVPwaJhFwhOnnyJAEgPz8/IhLy3+nTp4mI6JdffiEAtHnzZiIqPV5TpkyRljcXgyqVim7fvi1rDxRcftVtp6WcZiln6caRWq02ug5T58SVK1fMXt4bO3asotejew4cPnyYAFDjxo2JiOjUqVNSH7VffvnF7HqIKsfl1y5dukjn2JAhQ2jSpEnScydPnpRiST+v6b9HivEkKmtes3R89FmaX4w3ACbjTcnxjoiIkI75l19+aXDOEBG1adNGikkxJ168eNFkToyPj5fa5u3tTXv27FH0mom4T50sAb799tsEgNq3b0/t27en4OBgCgoKouXLlxMRUXJyMmVnZ9NXX31FNWvWlBV5+kUdkRDkph6m+lYEBgYSAFq6dKnJNkdGRlKNGjXogw8+IADUtWtXun//vsF8M2bMoD59+pgsqIiIMjMzqXr16uTp6Wn0+RkzZsiWT0xMJF9fX1KpVDRt2jQKCAggANSsWTMiIjpx4oTsBLFU1Omvv3r16gRASooFBQVUp04dAkB//vmnydchqkxFnW4s6cYTkRBLf//9tyyedOnHk7lYMhVPSo6/vrlz5xIA+vjjj2XHeuzYsbLjbczVq1cNOoi/8MIL9PXXXxOR+aJOf/vGFBcXEwDy8PCQXpPYRv32ifGoG0tEpfGkJJaIqn5RJ75xijEYEhJCQUFB1KxZMyooKKDY2FgiIvr9999p4MCBBIAiIiKIyHhRZy4Gjd0spbSoU5rTiOQ5y1RxPnfuXPr444+NPmfqnLBGUad/DqxevZoA0OTJk6VpK1asIAA0bNgwk+sRVaaiToylWrVqyeJJjCUieV7Tf4/UL+qIypbXrFnU6cZbrVq1TMabkuPt4+MjHXOxL/Rrr70mm69ly5YECH2kleTEHj160PPPP0+DBg0iQLgRY+fOnYpeN/ep03Hp0iXUqFEDarUaarUaFy9eRHJysvQ1eb169dCjRw94eXkhODjY4vo8PDxMPkz1rfj6668BmO4DdfPmTUyYMAEfffQRFi9ejODgYMTExGD79u2y+TZt2oSDBw9i586dZvtE1KlTB88//7zsEpb+OnSXb9GiBeLi4vDpp58iODgY77zzDgDg2WefBSBc2l2yZAnmzZuHefPmISEhAQDw7rvvKlq/eLlNvITj7u6OkJAQAJD6Y1QVurGkG0+AEEsrVqxQHE/mYslYPCk9/vpGjx4NALhy5YrsWHt7e8uOt7779+/jpZdewvPPPy9N27VrF3799VekpKRg3rx5WLlyJQBh6Il58+YZvU1f3L4xLi4u6NKlCwoKCnDlyhUApfGo3z4xHgHILgeK8VTVYqm8kpKSAECKwbi4OCQnJyMpKQnu7u64d+8eOnTogG+//Va6rG2OuRh0d3cvVxuV5jSRbs7KyMgwOs/o0aOlGNG1adOmMp8TShk7B2rVqgVAHoNdunQBUPXymRhLCxculMWTGEuAPK896nukuffJR6Efb2fOnDEbb6aIxzsiIkI65mJfzry8PNm8ubm5qF27NmJiYhTlxJSUFOzatQu7d+/GsmXLoNVqMXv27Ed96Yo4VJ+66tWrIycnB5cuXZJ1kkxNTYW/vz9GjBiBI0eOwMfHBxs2bDC7roKCAowaNcrk8/3790d4eLjB9O7duwMoDQ5927Ztg0ajkfribdmyBe3bt0dUVJTsWn9ERAQOHTokJRVzfH194ePjI5t24sQJaR36/Pz8MHv2bBQVFaFjx46oUaMGFi9eDEB409Tt/3X37l0AwPbt2/Hpp59aXH/z5s1x+fJlnDp1SjpR/Pz8AEDRa6lMjMUSAGg0GowYMQJpaWmIjo62GE+WYgmQx5Puvi3rPqtfvz4AoFWrVgBKjzUA2fHWfz1jxozBP//5T4N2169fH1u3bgUgjI8ICG8OGRkZaNmyJUJDQ41u35Tu3bvj1KlTUvEvtnH16tVG4xGALJbE+ataLJWXqf5t+fn5yM3NRb9+/bB69WqEh4fjyJEjFtc3ePBgs8//97//Rc2aNcvURqU5TZeYs8TcoK9+/fpSDIvE82Ls2LFlap8Sps6B5s2bA4CsD2eDBg0AVL18JsaSfh9FMZbq1Kkjy2uWYqWsec1a9ONNvPHKXLzp0z3eU6ZMkaY3b94cTk5OSElJkc2fmZmJDh06KMqJbdu2Rc+ePaUbwN577z2cPHkSP//886O9cIUc6ps68RuhBQsWSNNiYmIwb9487Nq1C/v375eKn/v37wvXnx8S/y/+q1KpcOPGDaSnpxt9iMWOvlOnTkGlUuGll14y+vz169cBlH7Ke/rpp/GPf/wDWVlZ0jzx8fHYs2ePlDwAYO/evfj+++8N1qfRaLB7927Z9uLj4xEeHi5bh7HlZ82ahYSEBHz44YfSm/Hp06dx7do16SGe2LqfSs2tf8KECQCAP/74Q5r/3Llz8Pb2Rps2bYzuk8pswYIFsjiJiYmRYiksLEwWT7p048lSLOnHk/6+BYT9K3rw4IHJ9kZFRcHV1dXotza6x1u3nVOmTMGTTz4pfRtbXFyM8PBwjBkzRhYLx48fByB0VL527ZrBDRW62zfl1KlT6N27t9G7gY3FIyCPJUCIp6oYS+URFBQELy8vxMXFSdO0Wi3GjRuHtLQ0aDQajBw5EkBpDOrnMt34NReD6enpigeT1Wq10huakpymSzdnmRolICoqShbDujlHZConlpW5c+Dpp59GmzZtcObMGWl+8erFM88888jbrkhiLEVFRcniSYwl/bxm6j1SVNa8Zom5vKZLP94AmI03Y/SPNyAccz8/PwwePBjx8fGyb5GLioowYsQIRTlR9y5hke6HUptTep3Wxg9JWfpAif0datWqJXUGj4mJkW5YaNy4MR05coSIhLFyBgwYQA0bNqRly5bR559/Tk5OTtSmTRs6ceIEOTk5EQAKCQmhL774QtH2iYQxbVQqFQGgoKAg+vHHHw3mWb58uaxj79atW6lBgwYUEBBANWvWpIULF5JWq6UHDx5IfYr0H6NHjyYiov3790udL9u3b09Tp06le/fuEREpWv7KlSv0ySefUJMmTWQ3AZgyatQoqU+dkvUTCf2eJk6cSEOHDqUmTZrQkiVLTI7bp89efepWr14tdeAW40k3lnTjSYwlJycnWTyJsTRkyBBZPCmlZP8uX76cAEjxJI6x1LJlS+rcubNsLCXdYz1gwACD471+/XqTHZwvXbpk0L5Lly4Z9B8R5ze2/fT0dGmwWFdXV+rWrRtlZmbK1im20Vj7iEpjyd/fXxZPSlW1PnX6MUgkxIUYTyqViqZMmUL5+flEROTr60vVqlWjsLAwOn36NAUEBJCHhwd9+eWX1KRJE2nfT5o0yegxNeXkyZNSR/RWrVrRBx98IHvex8eH3N3d6c6dO0RkOqcRCTnLw8PDaM4iKo1hY3GkNOcQETVr1kzaR+PGjaPff/9d9lrEgWZ1X4/Sc+Do0aPUo0cP6tOnD4WEhMjGfjTHnn3qrl27Jt2IhIed/B88eECTJk2SxZMYS/p5Tfc9cvHixVI8lTWWiISxBvWPjy79vGZsfvF4EsnjrX///rJ4IzIfv+aOuSg6OpqaNm1KISEh1LdvX5PH21hOJCIaNmwYubi4UL9+/SgoKIhCQ0Pp7NmzivbVY3+jhDFarZaSk5MtFhK6I9s/iqysLMrIyDB7R55+WzQaDaWmppZpdHxRfHw8paamKi6UdP3vf/+jH374wWC0d1u4cOFCmTr5E9n3RgljtFotXb582Wg8WSt+yury5ctSW7RaLcXHxxu9m7UijvWVK1dMbp9IuDkiIyPD5L4S22hJfn5+pYin8lJa1JmSm5tLcXFxlJOTI5v+4MEDWQfx4uJi6U3alu7evWtQoJvLadnZ2SZzllartRhHlUFqaiplZGSUaZnKcKOEMabiiUie1yoyx+nmNSXEeEtMTLRZm65fv17u8yknJ4cSEhL0izOLHrWoc6g+dSKVSqVoTDT9vkXlpaRvhf5lBicnJwQEBJRre49y6albt27lXrasWrZsWWHbshWVSoWmTZsafc5a8VNWuu1RqVQm46EijnVgYKDZ511cXODr62vyeaVt9PDwcIh4Ki8vLy+pe4muatWqoVq1atLfLi4uNrmJQJ+x3xI1l9Nq1Khh8nxRqVQW46gyKG++roxMxRMgz2sVmeNM5VlTHuU9VCndLjBl5e3tjdatW1uxNco4VJ86xhhjjLHHFRd1jDHGGGMOgIs6xhhjjDEHwEUdY4wxxpgD4KKOMcYYY8wBcFHHGGOMMeYAuKhjjDHGGHMAXNQxxhhjjDkALuoYY4wxxhwAF3WMMcYYYw6AizrGGGOMMQfARR1jjDHGmAPgoo4xxhhjzAG42LsBxhw9ehQqlcrezWB20rNnT6uuj+Pp8WbteCqvmTNnYubMmfZuBnMQnNOYMZWuqHvttdfQq1cvezeD2VHdunWtti6OJ2bNeCqvdevWoaCgwN7NYHbi5eVl1fV9++23KCoqsuo6WeVSo0aNci2nIiIrN6VcKkUjGGOMMcYqmUUAFiuZkfvUMcYYY4w5AC7qGGOMMcYcQGXpU8c9PhljjDHGHgF/U8cYY4wx5gC4qGOMMcYYcwBc1DHGGGOMOQAu6hhjjDHGHAAXdYwxxhhjDoCLOsYYY4wxB8BFHWOMMcaYA+CijjHGGGPMAXBRxxhjjDHmALioY4wxxhhzAFzUMcYYY4w5AC7qGGOMMcYcABd1jDHGGGMOgIs6xhhjjDEHwEUdY4wxxpgD4KKOMcYYY8wBcFHHGGOMMeYAuKhjjDHGGHMAXNQxxhhjjDkALuoYY4wxxhwAF3WMMcYYYw6AizrGGGOMMQfARR1jjDHGmAP4f+juG4kepj7mAAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<Figure size 3600x720 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "xgb.plot_tree(xg_reg,num_trees=0)\n",
    "plt.rcParams['figure.figsize'] = [100, 50]\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "These plots provide insight into how the model arrived at its final decisions and what splits it made to arrive at those decisions."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Another way to visualize your XGBoost models is to examine the importance of each feature column in the original dataset within the model.\n",
    "\n",
    "One simple way of doing this, each feature is split on across all boosting rounds (trees) in the model, and then visualizing the result as a bar graph, with the features ordered according to how many times they appear. XGBoost has a <code>plot_importance()</code> function that allows you to do exactly this."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 360x360 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "xgb.plot_importance(xg_reg)\n",
    "plt.rcParams['figure.figsize'] = [10, 10]\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As you can see the feature RM has been given the highest importance score among all the features. Thus XGBoost also gives you a way to do Feature Selection. Isn't this brilliant?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
