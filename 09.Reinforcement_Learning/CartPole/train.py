'''
tensorboard --logdir=runs
localhost:8006
'''
import os
import gym
import time
import argparse
from collections import deque

import numpy as np
import torch
import torch.optim as optim
import torch.nn.functional as F
from torch.utils.tensorboard import SummaryWriter

from model import MLP
from replay_buffer import ReplayBuffer

parser = argparse.ArgumentParser()
parser.add_argument('--training_eps', type=int, default=500)
parser.add_argument('--threshold_return', type=int, default=495)
parser.add_argument('--render', action="store_true", default=False)
parser.add_argument('--gamma', type=float, default=0.99)
parser.add_argument('--epsilon', type=float, default=1.0)
parser.add_argument('--epsilon_decay', type=float, default=0.995)
parser.add_argument('--buffer_size', type=int, default=10000)
parser.add_argument('--batch_size', type=int, default=64)
parser.add_argument('--target_update_period', type=int, default=100)
args = parser.parse_args()
device = torch.device("cuda:0" if torch.cuda.is_available() else "cpu") 

def select_action(obs, act_num, qf):
    '''
    obs: s,a,r,s'
    qf: q function
    '''
    # Decaying epsilon
    args.epsilon *= args.epsilon_decay
    args.epsilon = max(args.epsilon, 0.01)

    if np.random.rand() <= args.epsilon:
        # Choose a random action with probability epsilon
        return np.random.randint(act_num)
    else:
        # Choose the action with highest Q-value at the current state
        action = qf(obs).argmax()
        return action.detach().cpu().numpy() # actionì„ ë½‘ì„ ë•, GPUëŠ” ì‚¬ìš©ì•ˆí•œë‹¤ê³  í•¨.
        # detachëŠ” envì™€ì˜ ìƒí˜¸ì‘ìš©ì„ ì•Œë ¤ì£¼ëŠ” ê²ƒ
        # ì™œ cpuë¥¼ í•´ì¤Œ?

def train_model(qf, qf_target, qf_optimizer, batch, step_count):
    obs1 = batch['obs1'] # s
    obs2 = batch['obs2'] # a
    acts = batch['acts'] # r
    rews = batch['rews'] # s'
    done = batch['done'] # done

    if 0: # Check shape of experiences
        print("obs1", obs1.shape)
        print("obs2", obs2.shape)
        print("acts", acts.shape)
        print("rews", rews.shape)
        print("done", done.shape)

    # Prediction Q(s), Qâ€¾(s'):target network
    q = qf(obs1).gather(1, acts.long()).squeeze(1) # 1: dimenstion / ì—´
    # qf(obs1)ì˜ ê²°ê³¼ë¡œ qê°’ ë‹´ì€ í…ì„œ ë°œìƒ.
    # long: intergerí™”
    q_target = qf_target(obs2)

    # Target for Q regression
    q_backup = rews + args.gamma*(1-done)*q_target.max(1)[0] # done: 0 or 1
    q_backup.to(device) # gpuë¥¼ ì“¸ ë•Œë¥¼ ê°€ì •í•˜ê³  ë“¤ì–´ê°„ to(detach)ì„. gpuë¥¼ ì“°ì§€ ì•ŠëŠ” ìƒí™©ì—ì„œëŠ” í•„ìš”ì¹˜ ì•Šì€ ì½”ë“œ. íŒŒì´í† ì¹˜ì˜ íŠ¹ì„± ë•Œë¬¸.

    if 0: # Check shape of prediction and target
        print("q", q.shape)
        print("q_backup", q_backup.shape)

    # Update perdiction network parameter
    qf_loss = F.mse_loss(q, q_backup.detach()) # detachë¥¼ ì•ˆí•´ì£¼ë©´ í•¨ê»˜ í•™ìŠµë¨. detachë¥¼ í•˜ë©´ í•™ìŠµì„ ë©ˆì¶”ëŠ” íš¨ê³¼ê°€ ìˆìŒ.
    qf_optimizer.zero_grad()
    qf_loss.backward()
    qf_optimizer.step() # ?

    # Synchronize target parameters ğœƒâ€¾ as ğœƒ every N steps
    if step_count % args.target_update_period == 0:
        qf_target.load_state_dict(qf.state_dict())

def main():
    # Initialize environment
    env = gym.make('CartPole-v1')
    obs_dim = env.observation_space.shape[0]
    act_num = env.action_space.n
    print('State dimension:', obs_dim)
    print('Action number:', act_num)

    # Set a random seed
    env.seed(0)
    np.random.seed(0)
    torch.manual_seed(0) # ?

    # Create a SummaryWriter object by TensorBoard
    dir_name = 'runs/' + 'CartPole-v1' + '_' + time.ctime() # tensorboardì— ì°í ê·¸ë˜í”„ì˜ nameì„ í˜„ì¬ì˜ ì‹œê°„ì •ë³´ë¥¼ ë„£ì€ ê²ƒ.
    writer = SummaryWriter(log_dir=dir_name)

    # Main network
    qf = MLP(obs_dim, act_num).to(device) # ?
    # Target network
    qf_target = MLP(obs_dim, act_num).to(device) # ? to deviceëŠ” ì™œ í•´ì£¼ëŠ”ê±°?

    # Initialize target parameters to match main parameters
    qf_target.load_state_dict(qf.state_dict()) # ?

    # Create an optimizer
    qf_optimizer = optim.Adam(qf.parameters(), lr=1e-3)

    # Experience buffer
    replay_buffer = ReplayBuffer(obs_dim, 1, args.buffer_size) # í•œ ë²ˆì— ì—°ì†ëœ buffer_sizeë§Œí¼ì˜ ìš”ì†Œë¥¼ ê°–ê³ ì˜¤ë‚˜?
    
    step_count = 0
    sum_returns = 0.
    num_episodes = 0
    recent_returns = deque(maxlen=10)

    start_time = time.time()
    
    for episode in range(1, args.training_eps+1):
        total_reward = 0.

        obs = env.reset()
        done = False

        # Keep interacting until agent reaches a terminal state.
        while not done:
            if args.render:
                env.render()

            step_count += 1 

            # Collect experience (s, a, r, s') using some policy
            action = select_action(torch.Tensor(obs).to(device), act_num, qf) # type(action): numpy.ndarray
            next_obs, reward, done, _ = env.step(action)

            # Add experience to replay buffer
            replay_buffer.add(obs, action, reward, next_obs, done)

            # Start training when the number of experience is greater than batch size
            if step_count > args.batch_size:
                batch = replay_buffer.sample(args.batch_size)
                train_model(qf, qf_target, qf_optimizer, batch, step_count)
            
            total_reward += reward
            obs = next_obs
        
        recent_returns.append(total_reward)
        sum_returns += total_reward
        num_episodes += 1
        average_return = sum_returns / num_episodes if num_episodes > 0 else 0.0

        # Log experiment result for training episodes
        writer.add_scalar('Train/AverageReturns', average_return, episode)
        writer.add_scalar('Train/EpisodeReturns', sum_returns, episode)
        
        if episode % 10 == 0:
            print('---------------------------------------')
            print('Episodes:', episode)
            print('Steps:', step_count)
            print('AverageReturn:', round(average_return, 2))
            print('RecentReturn:', np.mean(recent_returns))
            print('Time:', int(time.time() - start_time))
            print('---------------------------------------')

        # Save a training model
        if (np.mean(recent_returns)) >= args.threshold_return: 
            # average_returnë„ ê°€ëŠ¥. 
            # threshold_returnë¥¼ ì•ˆ ë„£ì–´ ì¤„ ê²½ìš°, ì„¤ì •í•œ ì—í”¼ì†Œë“œìˆ˜ë§Œí¼ë§Œ ëŒê³  trainì´ ì¢…ë£Œë¨.
            print('Recent returns {} exceed threshold return. So end'.format(np.mean(recent_returns)))
            if not os.path.exists('./save_model'):
                os.mkdir('./save_model')

            ckpt_path = os.path.join('./save_model/' + 'CartPole-v1_dqn' + '_ep_' + str(episode) \
                                                                         + '_rt_' + str(round(average_return, 2)) \
                                                                         + '_t_' + str(int(time.time() - start_time)) + '.pt')
            torch.save(qf.state_dict(), ckpt_path)
            break  

if __name__ == '__main__':
    main()