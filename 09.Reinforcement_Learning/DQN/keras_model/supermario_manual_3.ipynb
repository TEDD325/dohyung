{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 강화학습을 사용하여 나만의 똑똑한(????) 슈퍼마리오 만들기\n",
    "\n",
    "\n",
    "이제부터 DQN을 사용하여 장애물을 피해 Goal 까지 가는 마리오를 만들어 보도록 하겠습니다. \n",
    "\n",
    "이 코드는 example 코드로 봐주시고, 직접 \"파이썬과 케라스로 배우는 강화학습\"을 응용하시거나 직접 만드신 코드로 \n",
    "\n",
    "똑똑한 슈퍼마리오를 만들어주세요 ^^\n",
    "\n",
    "코드를 이제 막 시작한 컴맹인 저의 코드는 함정이 아주 많으니 틀린부분이나 개선해야할 부분 있으면 꼭 꼭 말씀 부탁드려요 ^^\n",
    "\n",
    "DQN이외에도 본인이 원하시는 학습 알고리즘을 사용하셔도 무방합니다!\n",
    "\n",
    "\n",
    "다시한번,\n",
    "\n",
    "## 이 코드는 슈퍼마리오가 어떻게 환경과 상호작용하는지 참고만 하시고 더 좋은 코드를 만들어주세요 ^^ "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.5/dist-packages/h5py/__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n",
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "\n",
    "import gym\n",
    "import ppaquette_gym_super_mario\n",
    "import random\n",
    "import numpy as np \n",
    "from wrapper import action_space\n",
    "from keras.layers.convolutional import Conv2D\n",
    "from keras.layers import Dense, Flatten, Input, Lambda, merge\n",
    "from keras.optimizers import RMSprop\n",
    "from keras.models import Model, Sequential\n",
    "from skimage.color import rgb2gray\n",
    "from collections import deque\n",
    "from keras import backend as K\n",
    "import tensorflow as tf\n",
    "from sum_tree import SumTree\n",
    "\n",
    "\n",
    "\n",
    "load_model = False\n",
    "\n",
    "\n",
    "\n",
    "#DQN의 하이퍼파라미터 입니다. \n",
    "\n",
    "\n",
    "#epsilon 값은 처음에 1로 시작하여 학습이 진행될수록 0.1까지 계쏙 줄어듭니다. \n",
    "epsilon = 1.\n",
    "epsilon_start, epsilon_end = 1, 0.1\n",
    "exploration_steps = 400000.\n",
    "\n",
    "epsilon_decay_step = (epsilon_start - epsilon_end) \\\n",
    "                                  / exploration_steps\n",
    "batch_size = 32\n",
    "\n",
    "discount_factor = 0.99\n",
    "\n",
    "train_start= 10000\n",
    "update_target_rate = 5000\n",
    "#리플레이 메모리에 최대 100000 까지 저장합니다.  \n",
    "memory=deque(maxlen=100000)\n",
    "just_start_train=True\n",
    "state_size = (84,84,4)\n",
    "action_size =6\n",
    "env = gym.make('ppaquette/meta-SuperMarioBros-v0')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Deep learning model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_model():\n",
    "    model = Sequential()\n",
    "    model.add(Conv2D(32,(8,8),strides =(4,4), activation='relu', input_shape=state_size))\n",
    "    model.add(Conv2D(64,(4,4), strides=(2,2), activation='relu'))\n",
    "    model.add(Conv2D(64,(3,3), strides =(1,1), activation = 'relu'))\n",
    "    model.add(Flatten())\n",
    "    model.add(Dense(512,activation='relu'))\n",
    "    model.add(Dense(action_size))\n",
    "    model.summary()\n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Define model and target model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv2d_1 (Conv2D)            (None, 20, 20, 32)        8224      \n",
      "_________________________________________________________________\n",
      "conv2d_2 (Conv2D)            (None, 9, 9, 64)          32832     \n",
      "_________________________________________________________________\n",
      "conv2d_3 (Conv2D)            (None, 7, 7, 64)          36928     \n",
      "_________________________________________________________________\n",
      "flatten_1 (Flatten)          (None, 3136)              0         \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 512)               1606144   \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 6)                 3078      \n",
      "=================================================================\n",
      "Total params: 1,687,206\n",
      "Trainable params: 1,687,206\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv2d_4 (Conv2D)            (None, 20, 20, 32)        8224      \n",
      "_________________________________________________________________\n",
      "conv2d_5 (Conv2D)            (None, 9, 9, 64)          32832     \n",
      "_________________________________________________________________\n",
      "conv2d_6 (Conv2D)            (None, 7, 7, 64)          36928     \n",
      "_________________________________________________________________\n",
      "flatten_2 (Flatten)          (None, 3136)              0         \n",
      "_________________________________________________________________\n",
      "dense_3 (Dense)              (None, 512)               1606144   \n",
      "_________________________________________________________________\n",
      "dense_4 (Dense)              (None, 6)                 3078      \n",
      "=================================================================\n",
      "Total params: 1,687,206\n",
      "Trainable params: 1,687,206\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model = build_model()\n",
    "target_model=build_model()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### append <s,a,r,s'> at replay memory"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Functions "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### change action(number) to the action( action what I think I need )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### append <s,a,r,s'> at replay memory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import random\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Epsilon greedy action을 선택합니다.\n",
    "\n",
    "action_size=6\n",
    "def get_action(history):\n",
    "        history = np.float32(history / 255.0)\n",
    "        if np.random.rand() <= epsilon:\n",
    "            return random.randrange(action_size)\n",
    "        else:\n",
    "            q_value = model.predict(history)\n",
    "            return np.argmax(q_value[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#state, action, reward, next state의 정보를 memory에 append합니다\n",
    "\n",
    "def append_sample(history, action, reward, next_history, dead):\n",
    "    memory.append((history,action,reward,next_history,dead))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Optimizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def optimizer(self):\n",
    "        a = K.placeholder(shape=(None,), dtype='int32')\n",
    "        y = K.placeholder(shape=(None,), dtype='float32')\n",
    "\n",
    "        prediction = model.output\n",
    "\n",
    "        a_one_hot = K.one_hot(a, action_size)\n",
    "        q_value = K.sum(prediction * a_one_hot, axis=1)\n",
    "        error = K.abs(y - q_value)\n",
    "\n",
    "        quadratic_part = K.clip(error, 0.0, 1.0)\n",
    "        linear_part = error - quadratic_part\n",
    "        loss = K.mean(0.5 * K.square(quadratic_part) + linear_part)\n",
    "\n",
    "        optimizer = RMSprop(lr=0.00025, epsilon=0.01)\n",
    "        updates = optimizer.get_updates(model.trainable_weights, [], loss)\n",
    "        train = K.function([model.input, a, y], [loss], updates=updates)\n",
    "\n",
    "        return train"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Update models \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def update_target_model():\n",
    "        target_model.set_weights(model.get_weights())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train model\n",
    "(Batch from the memory)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "epsilon=0.9\n",
    "\n",
    "def train_model():\n",
    "    global epsilon\n",
    "    if epsilon > epsilon_end:\n",
    "        epsilon -= epsilon_decay_step\n",
    "    \n",
    "    mini_batch= random.sample(memory, batch_size)\n",
    "   \n",
    "    \n",
    "  \n",
    "    history = np.zeros((batch_size, state_size[0],\n",
    "                            state_size[1], state_size[2]))\n",
    "    next_history = np.zeros((batch_size, state_size[0],\n",
    "                             state_size[1], state_size[2]))\n",
    "    target = np.zeros((batch_size,))\n",
    "    action, reward, dead = [], [], []\n",
    "\n",
    "    for i in range(batch_size):\n",
    "            history[i] = np.float32(mini_batch[i][0] / 255.)\n",
    "            next_history[i] = np.float32(mini_batch[i][3] / 255.)\n",
    "            action.append(mini_batch[i][1])\n",
    "            reward.append(mini_batch[i][2])\n",
    "            dead.append(mini_batch[i][4])\n",
    "\n",
    "    target_value = target_model.predict(next_history)\n",
    "\n",
    "    for i in range(batch_size):\n",
    "        if dead[i]:\n",
    "            target[i] = reward[i]\n",
    "        else:\n",
    "            target[i] = reward[i] + discount_factor * \\\n",
    "                                        np.amax(target_value[i])\n",
    "\n",
    "    loss = optimizer([history, action, target])\n",
    "\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 그럼 학습을 시작합니다. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-10-a590ad072b72>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     25\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     26\u001b[0m         \u001b[0mreshape_obs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreshape\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mobs\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m84\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m84\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 27\u001b[0;31m         \u001b[0mhistory\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstack\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mobs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mobs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mobs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mobs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     28\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     29\u001b[0m         \u001b[0mhistory\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreshape\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mhistory\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m84\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m84\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m4\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.5/site-packages/numpy/core/shape_base.py\u001b[0m in \u001b[0;36mstack\u001b[0;34m(arrays, axis, out)\u001b[0m\n\u001b[1;32m    358\u001b[0m     \u001b[0msl\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mslice\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0maxis\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0m_nx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnewaxis\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    359\u001b[0m     \u001b[0mexpanded_arrays\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0marr\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0msl\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0marr\u001b[0m \u001b[0;32min\u001b[0m \u001b[0marrays\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 360\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0m_nx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconcatenate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mexpanded_arrays\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0maxis\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mout\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    361\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    362\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "\n",
    "    \n",
    "\n",
    "###### 환경을 불러옵니다.\n",
    "\n",
    "wra_act=action_space\n",
    "scores, episodes, global_step = [], [], 0\n",
    "\n",
    "#reduce actions\n",
    "env=wra_act.mario_action(env)\n",
    "\n",
    "#reduce inputsize to 84,84,1\n",
    "env=wra_act.ProcessFrame84(env)\n",
    "\n",
    "step=0\n",
    "obs=env.reset()\n",
    "\n",
    "\n",
    "for e in range(100000):\n",
    "        done = False\n",
    "        \n",
    "       \n",
    "        \n",
    "        \n",
    "        \n",
    "        reshape_obs=np.reshape([obs],(1,84,84,1))\n",
    "        history=np.stack((obs,obs,obs,obs), axis = 2)\n",
    "        \n",
    "        history = np.reshape([history], (1, 84, 84, 4))\n",
    "\n",
    "\n",
    "        while not done:\n",
    "           \n",
    "            global_step += 1\n",
    "            step += 1\n",
    "\n",
    "            \"\"\"\n",
    "                0: [0, 0, 0, 0, 0, 0],  # NOOP\n",
    "                1: [1, 0, 0, 0, 0, 0],  # Up\n",
    "                2: [0, 0, 1, 0, 0, 0],  # Down\n",
    "                3: [0, 1, 0, 0, 0, 0],  # Left\n",
    "                4: [0, 1, 0, 0, 1, 0],  # Left + A\n",
    "                5: [0, 1, 0, 0, 0, 1],  # Left + B\n",
    "                6: [0, 1, 0, 0, 1, 1],  # Left + A + B\n",
    "                7: [0, 0, 0, 1, 0, 0],  # Right\n",
    "                8: [0, 0, 0, 1, 1, 0],  # Right + A\n",
    "                9: [0, 0, 0, 1, 0, 1],  # Right + B\n",
    "                10: [0, 0, 0, 1, 1, 1],  # Right + A + B\n",
    "                11: [0, 0, 0, 0, 1, 0],  # A\n",
    "                12: [0, 0, 0, 0, 0, 1],  # B\n",
    "                13: [0, 0, 0, 0, 1, 1],  # A + B\n",
    "                6개의 버튼으로 14개의 action을 조합할수 있습니다. \n",
    "                좀 더 빠른 학습을 위해 필요하지 않은 action은 없애주셔도 좋습니다. ( 예를 들어 Up action, 아무것도 하지 않는 action) \n",
    "                \n",
    "                아래의 action 선택은 본인이 생각하셨을때 필요한 action들로 바꿔보세요 ^^ action size를 더 줄이셔도, 늘리셔도 무관합니다. \n",
    "            \"\"\"\n",
    "            #바로 전 4개의 state로 action을 선택합니다.\n",
    "            \n",
    "            action=get_action(history)\n",
    "            if action==0:\n",
    "                actions=[0,0,0,0,0,0]\n",
    "            elif action==1:\n",
    "                actions=[1,0,0,0,0,0]\n",
    "    \n",
    "            elif action==2:\n",
    "                actions=[0,1,0,0,0,0]\n",
    "            elif action==3:\n",
    "                actions=[0,0,1,0,0,0]\n",
    "            elif action==4:\n",
    "                actions=[0,0,0,1,0,0]\n",
    "    \n",
    "            elif action==5:\n",
    "                actions=[0,0,0,0,1,0]\n",
    "            elif action==6:\n",
    "                actions=[0,0,0,0,0,1]\n",
    "    \n",
    "            \n",
    "            # 선택한 행동으로 환경에서 한 타임스텝 진행\n",
    "            observe, reward, done, clear= env.step(actions)\n",
    "            \n",
    "            if clear:\n",
    "                reward += 30\n",
    "                done = True\n",
    "            #끝났지만 clear 하지 못했다면 \n",
    "            if done and not clear:\n",
    "                reward = -30\n",
    "              \n",
    "            reward = np.clip(reward, -1., 1.)\n",
    "               \n",
    "    \n",
    "            next_state = observe\n",
    "            \n",
    "            next_state = np.reshape([next_state], (1, 84, 84, 1))\n",
    "            next_history = np.append(next_state, history[:, :, :, :3], axis=3)\n",
    "            \n",
    "            \n",
    "\n",
    "            \n",
    "            #sample <s,a,r,s'>를 리플레이 메모리에 저장 후 학습\n",
    "            \n",
    "            append_sample(history, action, reward, next_history, done)\n",
    "\n",
    "    \n",
    "\n",
    "            if len(memory)>= train_start:\n",
    "                train_model()\n",
    "                \n",
    "            if global_step % update_target_rate ==0:\n",
    "                update_target_model()\n",
    "            \n",
    "            history = next_history\n",
    "            \n",
    "            #모델의 weight를 저장합니다. \n",
    " \n",
    "            if e % 100 == 0:\n",
    "                model.save_weights(\"./save/manual.h5\")\n",
    "            "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
