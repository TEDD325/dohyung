# Spread Matching with Keras GAN
# July 5, 2019
# Sung Kyu Lim
# Georgia Institute of Technology
# limsk@ece.gatech.edu


# import packages
import numpy as np
import matplotlib.pyplot as plt
import os
from keras import models
from keras.layers import Dense, Conv1D, Reshape, Flatten, Lambda
from keras.optimizers import Adam


# create directories
OUT_DIR = "./output/"
if not os.path.exists(OUT_DIR):
    os.makedirs(OUT_DIR)


# global constants and hyper-parameters
NUM_DATA = 100
MY_BATCH = 1
MY_STAGE = 10
MY_EPOCH = 100
MY_TEST = 20

# number of neurons
D_SIZE = 50
G_SIZE = 50

# number of times trained in each epoch
D_LEARN = 10
G_LEARN = 5

MY_MU = 10
MY_SIGMA = 0.25
MY_ADAM = Adam(lr = 0.0002, beta_1 = 0.9, beta_2 = 0.999)


# we use the same compilation setting for all models
def model_compile(model):
    return model.compile(loss = 'binary_crossentropy', optimizer = "adam", 
            metrics=['accuracy'])


    ####################
    # DATABASE SETTING #
    ####################


# we do NOT use a separate DB
# instead we build random data on-the-fly
# our goal is to learn the distribution itself

# random samples with mean = mu and stddev = sigma
def real_sample():
    return np.random.normal(MY_MU, MY_SIGMA, (MY_BATCH, NUM_DATA))

# random samples from a uniform distribution over [0, 1).
def in_sample():
    return np.random.rand(MY_BATCH, NUM_DATA)



    ##################
    # MODEL BUILDING #
    ##################


# DNN discriminator definition
def build_D():
    D = models.Sequential()

    # first dense layer with 50 neurons
    # input shape is (NUM_DATA,)
    D.add(Dense(D_SIZE, activation = 'relu', input_shape = (NUM_DATA,)))

    # second dense layer 
    D.add(Dense(D_SIZE, activation = 'relu'))
    
    # third dense layer
    D.add(Dense(D_SIZE, activation = 'relu'))

    # we need a single output for classification: real or fake
    D.add(Dense(1, activation = 'sigmoid'))

    # compile the model with common setting
    model_compile(D)

    print('\n== DISCRIMINATOR MODEL DETAILS ==')
    D.summary()
    return D

# build_D()
# exit()

# CNN generator definition
def build_G():

    G = models.Sequential()

    # we use CNN for generator
    # CNN needs channel info, so we reshape the CNN input to (NUM_DATA, 1)
    # we need Reshape layer to enter CNN
    G.add(Reshape((NUM_DATA, 1), input_shape = (NUM_DATA,)))

    # first 1-dimensional CNN layer
    # kernel size is 1
    # total parameter count formula = 
    # (filter_height * filter_width * input_channels + 1) * output_channel
    # (1 * 1 * 1 + 1) * 50
    G.add(Conv1D(G_SIZE, 1, activation = 'relu'))

    # second 1-dimensional CNN layer
    # (1 * 1 * 50 + 1) * 50
    G.add(Conv1D(G_SIZE, 1, activation = 'sigmoid'))

    # third 1-dimensional CNN layer
    # (1 * 1 * 50 + 1) * 1
    # the final output has NUM_DATA numbers
    G.add(Conv1D(1, 1))

    # flatten it to one dimension
    G.add(Flatten())

    # compile the model with common setting
    model_compile(G)

    print('\n== GENERTOR MODEL DETAILS ==')
    G.summary()

    return G

# build_G()
# exit()

# GAN definition
def build_GAN(dis, gen):
    GAN = models.Sequential()
    
    # add the generator
    GAN.add(gen)

    # add the discriminator
    GAN.add(dis)

    # we initially set the discriminator non-trainable 
    dis.trainable = False
    model_compile(GAN)

    print('\n== GAN MODEL DETAILS ==')
    GAN.summary()
    
    # exit()

    return GAN


    ##################
    # MODEL TRAINING #
    ##################


# training discriminator
# input: real and fake data concatenated
# output: ground truth labels
def D_train_on_batch(real, fake, dis):

    # real and fake data merged into a single array
    # two arrays concatenated, so we get (MY_BATCH X 2, NUM_DATA)
    # we concatenate because our discriminator has single input
    X = np.concatenate([real, fake], axis = 0)

    # print(X)
    # exit()

    # build truth labels (in 2-dimensional array)
    # 1 means real, 0 fake
    # if MY_BATCH is 2, we get [1 1 0 0]
    y = np.array([1] * MY_BATCH + [0] * MY_BATCH)

    # print(y)
    # exit()

    # Keras train_on_batch
    # we give (input, label)
    # use train_on_batch() instead of fit() for GAN
    dis.train_on_batch(X, y)


# discriminator training
def train_D(gan, dis, gen):

    # real data for discriminator training
    # this data is generated randomly
    real = real_sample()
    

    # input data for discriminator training
    # this data is generated randomly
    Z = in_sample()


    # fake data generated by the generator
    # we use the input data
    # shape is (batch, NUM_DATA)
    fake = gen.predict(Z)


    # we make discriminator trainable
    dis.trainable = True
    D_train_on_batch(real, fake, dis)


# we train the generator with Z-input data 
# and the labels that say that the data is real
# input shape is (# batch, data size)
# output shape is (# batch,)
def GAN_train_on_batch(Z, gan):

    # build the truth label array
    y = np.array([1] * MY_BATCH)


    # Keras train_on_batch
    # we give (input, label)
    gan.train_on_batch(Z, y)


# generator training
def train_GAN(gan, dis, gen):
    # Z input data for generator training
    # this data is generated randomly again
    Z = in_sample()
    


    # we do not train discriminator during generator training
    dis.trainable = False
    GAN_train_on_batch(Z, gan)


# run epochs
def train_epochs(gan, dis, gen):
    for e in range(MY_EPOCH):

        # number of times we train discriminator in each epoc (1 currently)
        for i in range(D_LEARN):
            train_D(gan, dis, gen)

        # number of times we train generator in each epoch (5 currently)
        for i in range(G_LEARN):
            train_GAN(gan, dis, gen)


    ####################
    # MODEL EVALUATION #
    ####################


# we generate test data and use them for prediction
# test data input shape is (# test data, data size)
# fake data shape is (# test data, data size)
def test_data():

    # generate test data input (random)
    # n_test is the number of test data
    Z = np.random.rand(MY_TEST, NUM_DATA)


    # our generator produces fake data using the test input
    fake = gen.predict(Z)

    return fake, Z


# show histograms
# real data, fake data, input data
def show_hist(Real, Gen, Z):
    plt.hist(Real.reshape(-1), histtype = 'step', label = 'Real')
    plt.hist(Gen.reshape(-1), histtype = 'step', label = 'Generated')
    plt.hist(Z.reshape(-1), histtype = 'step', label = 'Input')
    plt.legend(loc = 0)


# main routine to orchestrate the test process
# we do not use discriminator
# we just compare real and fake and want them to be close
def test_and_show():

    # generate random test input and their fake data
    # fake is generated using the Z input
    fake, Z = test_data()


    # obtain random real data
    real = np.random.normal(MY_MU, MY_SIGMA, (MY_TEST, NUM_DATA))
    show_hist(real, fake, Z)


    # print the mean and deviation comparison
    # between real and fake
    print('        Real: mean = {:.2f}'.format(np.mean(real)), 
            ', std-dev = {:.2f}'.format(np.std(real)))
    
    print('        Fake: mean = {:.2f}'.format(np.mean(fake)), 
            ', std-dev = {:.2f}'.format(np.std(fake)))


# train and test GAN
def run(gan, dis, gen):

    for i in range(MY_STAGE):
        print('\nStage', i, '(Epoch: {})'.format(i * MY_EPOCH))

        train_epochs(gan, dis, gen)
        test_and_show()

        path = "output/chap4-img-{}".format(i)
        plt.savefig(path)
        plt.close()


# build and run GAN 
dis = build_D()
gen = build_G()
GAN = build_GAN(dis, gen)
build_GAN(dis, gen)
# Non-trainable params: 10,201 -> discriminator의 파라미터 수. non-trainable임.
run(GAN, dis, gen)

