{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "http://nbviewer.jupyter.org/github/link-kut/deeplink/blob/master/0.Common/1.DeepLearning/02.VanillaNN/mnist_one_hidden_layer.ipynb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "((55000, 784), (55000, 10))\n",
      "\n",
      "((5000, 784), (5000, 10))\n",
      "\n",
      "((10000, 784), (10000, 10))\n",
      "[ 0.          0.          0.          0.          0.          0.          0.\n",
      "  0.          0.          0.          0.          0.          0.          0.\n",
      "  0.          0.          0.          0.43137255  0.47450981  0.47843137\n",
      "  0.47450981  0.79215688  0.98823529  0.76078433  0.01176471  0.          0.\n",
      "  0.          0.          0.          0.          0.          0.          0.\n",
      "  0.          0.          0.          0.          0.          0.          0.\n",
      "  0.          0.03921569  0.20784314  0.7019608   0.99215686  0.99215686\n",
      "  1.          0.99215686  0.99215686  0.89411765  0.13725491  0.          0.\n",
      "  0.          0.          0.          0.          0.          0.          0.\n",
      "  0.          0.          0.          0.          0.          0.          0.\n",
      "  0.01960784  0.21176471  0.89019608  0.98823529  0.95294118  0.89411765\n",
      "  0.66666669  0.94901961  0.98823529  0.98823529  0.90588236  0.45882353\n",
      "  0.02352941  0.          0.          0.          0.          0.          0.\n",
      "  0.          0.          0.          0.          0.          0.          0.\n",
      "  0.          0.02352941  0.30588236  0.98823529  0.98823529  0.49019608\n",
      "  0.23137255  0.          0.07058824  0.81568629  0.98823529  0.98823529\n",
      "  0.98823529  0.98823529  0.34117648  0.02745098]\n",
      "[[ 0.  0.  0. ...,  0.  0.  0.]\n",
      " [ 1.  0.  0. ...,  0.  0.  0.]\n",
      " [ 0.  0.  0. ...,  0.  0.  0.]\n",
      " ..., \n",
      " [ 1.  0.  0. ...,  0.  0.  0.]\n",
      " [ 0.  0.  0. ...,  0.  0.  0.]\n",
      " [ 1.  0.  0. ...,  0.  0.  0.]]\n"
     ]
    }
   ],
   "source": [
    "# coding: utf-8\n",
    "import urllib.request\n",
    "import os.path\n",
    "import gzip\n",
    "import pickle\n",
    "import os\n",
    "import numpy as np\n",
    "    \n",
    "# url_base = 'http://yann.lecun.com/exdb/mnist/'\n",
    "# key_file = {\n",
    "#     'train_img':'train-images-idx3-ubyte.gz',\n",
    "#     'train_label':'train-labels-idx1-ubyte.gz',\n",
    "#     'test_img':'t10k-images-idx3-ubyte.gz',\n",
    "#     'test_label':'t10k-labels-idx1-ubyte.gz'\n",
    "# }\n",
    "\n",
    "dataset_dir = os.path.dirname(\"/Users/do-hyungkwon/GoogleDrive/jupyter_notebook/MNIST_data.\")\n",
    "save_file = dataset_dir + \"/mnist.pkl\"\n",
    "\n",
    "train_num = 60000\n",
    "test_num = 10000\n",
    "img_dim = (1, 28, 28)\n",
    "img_size = 784\n",
    "\n",
    "def _download(file_name):\n",
    "    file_path = dataset_dir + \"/\" + file_name\n",
    "    print(file_path)\n",
    "    if os.path.exists(file_path):\n",
    "        return\n",
    "\n",
    "    print(\"Downloading \" + file_name + \" ... \")\n",
    "    urllib.request.urlretrieve(url_base + file_name, file_path)\n",
    "    print(\"Done\")\n",
    "    \n",
    "def download_mnist():\n",
    "    for v in key_file.values():\n",
    "        _download(v)\n",
    "        \n",
    "def _load_label(file_name):\n",
    "    file_path = dataset_dir + \"/\" + file_name\n",
    "    \n",
    "    print(\"Converting \" + file_name + \" to NumPy Array ...\")\n",
    "    with gzip.open(file_path, 'rb') as f:\n",
    "            labels = np.frombuffer(f.read(), np.uint8, offset=8)\n",
    "    print(\"Done\")\n",
    "    \n",
    "    return labels\n",
    "\n",
    "def _load_img(file_name):\n",
    "    file_path = dataset_dir + \"/\" + file_name\n",
    "    \n",
    "    print(\"Converting \" + file_name + \" to NumPy Array ...\")    \n",
    "    with gzip.open(file_path, 'rb') as f:\n",
    "            data = np.frombuffer(f.read(), np.uint8, offset=16)\n",
    "    data = data.reshape(-1, img_size)\n",
    "    print(\"Done\")\n",
    "    \n",
    "    return data\n",
    "    \n",
    "def _convert_numpy():\n",
    "    dataset = {}\n",
    "    dataset['train_img'] =  _load_img(key_file['train_img'])\n",
    "    dataset['train_label'] = _load_label(key_file['train_label'])\n",
    "    dataset['test_img'] = _load_img(key_file['test_img'])\n",
    "    dataset['test_label'] = _load_label(key_file['test_label'])\n",
    "\n",
    "    dataset['validation_img'] = dataset['train_img'][55000:]\n",
    "    dataset['validation_label'] = dataset['train_label'][55000:]\n",
    "    dataset['train_img'] =  dataset['train_img'][:55000]\n",
    "    dataset['train_label'] = dataset['train_label'][:55000]\n",
    "    return dataset\n",
    "\n",
    "def init_mnist():\n",
    "    download_mnist()\n",
    "    dataset = _convert_numpy()\n",
    "    print(\"Creating pickle file ...\")\n",
    "    with open(save_file, 'wb') as f:\n",
    "        pickle.dump(dataset, f, -1)\n",
    "    print(\"Done!\")\n",
    "\n",
    "def _change_one_hot_label(X):\n",
    "    T = np.zeros((X.size, 10))\n",
    "#     print(\"X : \",X)\n",
    "    for idx, row in enumerate(T):\n",
    "        row[X[idx]] = 1\n",
    "        \n",
    "    return T\n",
    "    \n",
    "\n",
    "def load_mnist(normalize=True, flatten=True, one_hot_label=True):\n",
    "    if not os.path.exists(save_file):\n",
    "        init_mnist()\n",
    "        \n",
    "    with open(save_file, 'rb') as f:\n",
    "        dataset = pickle.load(f)\n",
    "    \n",
    "    if normalize:\n",
    "        for key in ('train_img', 'validation_img', 'test_img'):\n",
    "            dataset[key] = dataset[key].astype(np.float32)\n",
    "            dataset[key] /= 255.0\n",
    "            \n",
    "    \n",
    "    \n",
    "    if one_hot_label:\n",
    "        dataset['train_label'] = _change_one_hot_label(dataset['train_label'])\n",
    "#         print(dataset['train_label'])\n",
    "#         print(type(dataset['train_label']))\n",
    "#         print(dataset['train_label'].shape)\n",
    "        dataset['validation_label'] = _change_one_hot_label(dataset['validation_label'])\n",
    "        dataset['test_label'] = _change_one_hot_label(dataset['test_label'])\n",
    "    \n",
    "    if not flatten:\n",
    "         for key in ('train_img', 'validation_img', 'test_img'):\n",
    "            dataset[key] = dataset[key].reshape(-1, 1, 28, 28) #풀어해치기\n",
    "\n",
    "    return (dataset['train_img'], dataset['train_label']), (dataset['validation_img'], dataset['validation_label']), (dataset['test_img'], dataset['test_label']) \n",
    "\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    (img_train, label_train), (img_val, label_val), (img_test, label_test) = load_mnist()\n",
    "    print((img_train.shape, label_train.shape))\n",
    "    print()\n",
    "    print((img_val.shape, label_val.shape))\n",
    "    print()\n",
    "    print((img_test.shape, label_test.shape))\n",
    "    \n",
    "    print(img_train[5][360:470])\n",
    "    # 내부의 값은 0~1사이의 값으로 정규화 되어 있음\n",
    "#     print(type(img_train))%%!\n",
    "    print(label_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
