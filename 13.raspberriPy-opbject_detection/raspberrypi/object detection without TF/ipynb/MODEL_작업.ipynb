{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.image as mpimg\n",
    "from scipy.misc import imread\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import os\n",
    "import sys\n",
    "import math\n",
    "import gzip\n",
    "import glob\n",
    "import random\n",
    "import shutil\n",
    "import pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# functions.py\n",
    "def identity_function(x):\n",
    "    return x\n",
    "\n",
    "\n",
    "def step_function(x):\n",
    "    return np.array(x > 0, dtype=np.int)\n",
    "\n",
    "\n",
    "def sigmoid(x):\n",
    "    return 1 / (1 + np.exp(-x))    \n",
    "\n",
    "\n",
    "def sigmoid_grad(x):\n",
    "    return (1.0 - sigmoid(x)) * sigmoid(x)\n",
    "    \n",
    "\n",
    "def relu(x):\n",
    "    return np.maximum(0, x)\n",
    "\n",
    "\n",
    "def relu_grad(x):\n",
    "    grad = np.zeros(x)\n",
    "    grad[x>=0] = 1\n",
    "    return grad\n",
    "    \n",
    "\n",
    "def softmax(x):\n",
    "    if x.ndim == 2:\n",
    "        x = x.T\n",
    "        x = x - np.max(x, axis=0)\n",
    "        y = np.exp(x) / np.sum(np.exp(x), axis=0)\n",
    "        return y.T \n",
    "\n",
    "    x = x - np.max(x)\n",
    "    return np.exp(x) / np.sum(np.exp(x))\n",
    "\n",
    "\n",
    "def mean_squared_error(y, t):\n",
    "    return 0.5 * np.sum((y-t)**2)\n",
    "\n",
    "\n",
    "def cross_entropy_error(y, t):\n",
    "    if y.ndim == 1:\n",
    "        t = t.reshape(1, t.size)\n",
    "        y = y.reshape(1, y.size)\n",
    "\n",
    "    if t.size == y.size:\n",
    "        t = t.argmax(axis=1)\n",
    "             \n",
    "    batch_size = y.shape[0]\n",
    "    #print(\"y: \" + str(y[np.arange(batch_size), t]))\n",
    "    return -np.sum(np.log(y[np.arange(batch_size), t])) / batch_size\n",
    "\n",
    "\n",
    "def softmax_loss(X, t):\n",
    "    y = softmax(X)\n",
    "    return cross_entropy_error(y, t)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# util.py\n",
    "def smooth_curve(x):\n",
    "    \"\"\"\n",
    "    http://glowingpython.blogspot.jp/2012/02/convolution-with-numpy.html\n",
    "    \"\"\"\n",
    "    window_len = 11\n",
    "    s = np.r_[x[window_len-1:0:-1], x, x[-1:-window_len:-1]]\n",
    "    w = np.kaiser(window_len, 2)\n",
    "    y = np.convolve(w/w.sum(), s, mode='valid')\n",
    "    return y[5:len(y)-5]\n",
    "\n",
    "\n",
    "def shuffle_dataset(x, t):\n",
    "    \"\"\"\n",
    "    \"\"\"\n",
    "    permutation = np.random.permutation(x.shape[0])\n",
    "    x = x[permutation,:] if x.ndim == 2 else x[permutation,:,:,:]\n",
    "    t = t[permutation]\n",
    "\n",
    "    return x, t\n",
    "\n",
    "def conv_output_size(input_size, filter_size, stride=1, pad=0):\n",
    "    return (input_size + 2*pad - filter_size) / stride + 1\n",
    "\n",
    "\n",
    "def im2col(input_data, filter_h, filter_w, stride=1, pad=0):\n",
    "    \"\"\"\n",
    "    Parameters\n",
    "    ----------\n",
    "    input_data : 이미지 데이터\n",
    "    filter_h : 필터 높이\n",
    "    filter_w : 필터 폭\n",
    "    stride : 스트라이드\n",
    "    pad : 패드\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    col : 2차원행렬\n",
    "    \"\"\"\n",
    "    N, C, H, W = input_data.shape\n",
    "    out_h = (H + 2 * pad - filter_h) // stride + 1\n",
    "    out_w = (W + 2 * pad - filter_w) // stride + 1\n",
    "\n",
    "    img = np.pad(input_data, [(0,0), (0,0), (pad, pad), (pad, pad)], 'constant')\n",
    "    col = np.zeros((N, C, filter_h, filter_w, out_h, out_w))\n",
    "\n",
    "    for y in range(filter_h):\n",
    "        y_max = y + stride * out_h\n",
    "        for x in range(filter_w):\n",
    "            x_max = x + stride * out_w\n",
    "            col[:, :, y, x, :, :] = img[:, :, y:y_max:stride, x:x_max:stride]\n",
    "\n",
    "    col = col.transpose(0, 4, 5, 1, 2, 3).reshape(N * out_h * out_w, -1)\n",
    "    return col\n",
    "\n",
    "\n",
    "def col2im(col, input_shape, filter_h, filter_w, stride=1, pad=0):\n",
    "    \"\"\"\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    col :\n",
    "    input_shape : 이미지 데이터 Shape（例：(10, 1, 28, 28)）\n",
    "    filter_h\n",
    "    filter_w\n",
    "    stride\n",
    "    pad\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "\n",
    "    \"\"\"\n",
    "    N, C, H, W = input_shape\n",
    "    out_h = (H + 2*pad - filter_h)//stride + 1\n",
    "    out_w = (W + 2*pad - filter_w)//stride + 1\n",
    "    col = col.reshape(N, out_h, out_w, C, filter_h, filter_w).transpose(0, 3, 4, 5, 1, 2)\n",
    "\n",
    "    img = np.zeros((N, C, H + 2*pad + stride - 1, W + 2*pad + stride - 1))\n",
    "    for y in range(filter_h):\n",
    "        y_max = y + stride*out_h\n",
    "        for x in range(filter_w):\n",
    "            x_max = x + stride*out_w\n",
    "            img[:, :, y:y_max:stride, x:x_max:stride] += col[:, :, y, x, :, :]\n",
    "\n",
    "    return img[:, :, pad:H + pad, pad:W + pad]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# layers.py\n",
    "class ReLU:\n",
    "    def __init__(self):\n",
    "        self.mask = None\n",
    "\n",
    "    def forward(self, x):\n",
    "        self.mask = (x <= 0)\n",
    "        out = x.copy()\n",
    "        out[self.mask] = 0\n",
    "        return out\n",
    "\n",
    "    def backward(self, din):\n",
    "        din[self.mask] = 0\n",
    "        dx = din\n",
    "        return dx\n",
    "\n",
    "\n",
    "class Sigmoid:\n",
    "    def __init__(self):\n",
    "        self.out = None\n",
    "\n",
    "    def forward(self, x):\n",
    "        out = sigmoid(x)\n",
    "        self.out = out\n",
    "        return out\n",
    "\n",
    "    def backward(self, din):\n",
    "        dx = din * self.out * (1.0 - self.out)\n",
    "        return dx\n",
    "\n",
    "\n",
    "class Affine:\n",
    "    def __init__(self, W, b):\n",
    "        self.W = W\n",
    "        self.b = b\n",
    "        self.x = None\n",
    "        self.dW = None\n",
    "        self.db = None\n",
    "\n",
    "    def forward(self, x):\n",
    "        self.x = x\n",
    "        out = np.dot(self.x, self.W) + self.b\n",
    "        return out\n",
    "\n",
    "    def backward(self, din):\n",
    "        dx = np.dot(din, self.W.T)\n",
    "        self.dW = np.dot(self.x.T, din)\n",
    "        self.db = np.sum(din, axis=0)\n",
    "        return dx\n",
    "\n",
    "\n",
    "class SoftmaxWithCrossEntropyLoss:\n",
    "    def __init__(self):\n",
    "        self.loss = None\n",
    "        self.y = None\n",
    "        self.t = None\n",
    "\n",
    "    def forward(self, x, t):\n",
    "        self.t = t\n",
    "        self.y = softmax(x)\n",
    "        self.loss = cross_entropy_error(self.y, self.t)\n",
    "        return self.loss\n",
    "\n",
    "    def backward(self, din=1):\n",
    "        batch_size = self.t.shape[0]\n",
    "        dx = (self.y - self.t) / float(batch_size)\n",
    "        return dx\n",
    "\n",
    "\n",
    "class Dropout:\n",
    "    \"\"\"\n",
    "    http://arxiv.org/abs/1207.0580\n",
    "    \"\"\"\n",
    "    def __init__(self, dropout_ratio=0.5):\n",
    "        self.dropout_ratio = dropout_ratio\n",
    "        self.mask = None\n",
    "\n",
    "    def forward(self, x, is_train=True):\n",
    "        if is_train:\n",
    "            self.mask = np.random.rand(*x.shape) >= self.dropout_ratio\n",
    "            return x * self.mask\n",
    "        else:\n",
    "            return x\n",
    "\n",
    "    def backward(self, dout):\n",
    "        return dout * self.mask\n",
    "\n",
    "\n",
    "class BatchNormalization:\n",
    "    \"\"\"\n",
    "    http://arxiv.org/abs/1502.03167\n",
    "    \"\"\"\n",
    "    def __init__(self, gamma, beta, momentum=0.9, running_mean=None, running_var=None):\n",
    "        self.gamma = gamma\n",
    "        self.beta = beta\n",
    "        self.momentum = momentum\n",
    "        self.input_shape = None\n",
    "\n",
    "        self.running_mean = running_mean\n",
    "        self.running_var = running_var  \n",
    "        \n",
    "        self.batch_size = None\n",
    "        self.xc = None\n",
    "        self.std = None\n",
    "        self.dgamma = None\n",
    "        self.dbeta = None\n",
    "\n",
    "    def forward(self, x, is_train=True):\n",
    "        self.input_shape = x.shape\n",
    "        if x.ndim != 2:\n",
    "            N, C, H, W = x.shape\n",
    "            x = x.reshape(N, -1)\n",
    "\n",
    "        out = self.__forward(x, is_train)\n",
    "        \n",
    "        return out.reshape(*self.input_shape)\n",
    "            \n",
    "    def __forward(self, x, is_train):\n",
    "        if self.running_mean is None:\n",
    "            N, D = x.shape\n",
    "            self.running_mean = np.zeros(D)\n",
    "            self.running_var = np.zeros(D)\n",
    "                        \n",
    "        if is_train:\n",
    "            mu = x.mean(axis=0)\n",
    "            xc = x - mu\n",
    "            var = np.mean(xc**2, axis=0)\n",
    "            std = np.sqrt(var + 10e-7)\n",
    "            xn = xc / std\n",
    "            \n",
    "            self.batch_size = x.shape[0]\n",
    "            self.xc = xc\n",
    "            self.xn = xn\n",
    "            self.std = std\n",
    "            self.running_mean = self.momentum * self.running_mean + (1-self.momentum) * mu\n",
    "            self.running_var = self.momentum * self.running_var + (1-self.momentum) * var            \n",
    "        else:\n",
    "            xc = x - self.running_mean\n",
    "            xn = xc / ((np.sqrt(self.running_var + 10e-7)))\n",
    "            \n",
    "        out = self.gamma * xn + self.beta \n",
    "        return out\n",
    "\n",
    "    def backward(self, dout):\n",
    "        if dout.ndim != 2:\n",
    "            N, C, H, W = dout.shape\n",
    "            dout = dout.reshape(N, -1)\n",
    "\n",
    "        dx = self.__backward(dout)\n",
    "\n",
    "        dx = dx.reshape(*self.input_shape)\n",
    "        return dx\n",
    "\n",
    "    def __backward(self, dout):\n",
    "        dbeta = dout.sum(axis=0)\n",
    "        dgamma = np.sum(self.xn * dout, axis=0)\n",
    "        dxn = self.gamma * dout\n",
    "        dxc = dxn / self.std\n",
    "        dstd = -np.sum((dxn * self.xc) / (self.std * self.std), axis=0)\n",
    "        dvar = 0.5 * dstd / self.std\n",
    "        dxc += (2.0 / self.batch_size) * self.xc * dvar\n",
    "        dmu = np.sum(dxc, axis=0)\n",
    "        dx = dxc - dmu / self.batch_size\n",
    "        \n",
    "        self.dgamma = dgamma\n",
    "        self.dbeta = dbeta\n",
    "        \n",
    "        return dx\n",
    "\n",
    "\n",
    "class Convolution:\n",
    "    def __init__(self, W, b, stride=1, pad=0):\n",
    "        self.W = W\n",
    "        self.b = b\n",
    "        self.stride = stride\n",
    "        self.pad = pad\n",
    "\n",
    "        self.x = None   \n",
    "        self.col = None\n",
    "        self.col_W = None\n",
    "        \n",
    "        self.dW = None\n",
    "        self.db = None\n",
    "\n",
    "    def forward(self, x):\n",
    "        FN, C, FH, FW = self.W.shape\n",
    "        N, C, H, W = x.shape\n",
    "        out_h = 1 + int((H + 2*self.pad - FH) / self.stride)\n",
    "        out_w = 1 + int((W + 2*self.pad - FW) / self.stride)\n",
    "\n",
    "        col = im2col(x, FH, FW, self.stride, self.pad)\n",
    "        col_W = self.W.reshape(FN, -1).T\n",
    "\n",
    "        out = np.dot(col, col_W) + self.b\n",
    "        out = out.reshape(N, out_h, out_w, -1).transpose(0, 3, 1, 2)\n",
    "\n",
    "        self.x = x\n",
    "        self.col = col\n",
    "        self.col_W = col_W\n",
    "\n",
    "        return out\n",
    "\n",
    "    def backward(self, dout):\n",
    "        FN, C, FH, FW = self.W.shape\n",
    "        dout = dout.transpose(0,2,3,1).reshape(-1, FN)\n",
    "\n",
    "        self.db = np.sum(dout, axis=0)\n",
    "        self.dW = np.dot(self.col.T, dout)\n",
    "        self.dW = self.dW.transpose(1, 0).reshape(FN, C, FH, FW)\n",
    "\n",
    "        dcol = np.dot(dout, self.col_W.T)\n",
    "        dx = col2im(dcol, self.x.shape, FH, FW, self.stride, self.pad)\n",
    "\n",
    "        return dx\n",
    "\n",
    "\n",
    "class Pooling:\n",
    "    def __init__(self, pool_h, pool_w, stride=1, pad=0):\n",
    "        self.pool_h = pool_h\n",
    "        self.pool_w = pool_w\n",
    "        self.stride = stride\n",
    "        self.pad = pad\n",
    "        \n",
    "        self.x = None\n",
    "        self.arg_max = None\n",
    "\n",
    "    def forward(self, x):\n",
    "        N, C, H, W = x.shape\n",
    "        out_h = int(1 + (H - self.pool_h) / self.stride)\n",
    "        out_w = int(1 + (W - self.pool_w) / self.stride)\n",
    "\n",
    "        col = im2col(x, self.pool_h, self.pool_w, self.stride, self.pad)\n",
    "        col = col.reshape(-1, self.pool_h*self.pool_w)\n",
    "\n",
    "        arg_max = np.argmax(col, axis=1)\n",
    "        out = np.max(col, axis=1)\n",
    "        out = out.reshape(N, out_h, out_w, C).transpose(0, 3, 1, 2)\n",
    "\n",
    "        self.x = x\n",
    "        self.arg_max = arg_max\n",
    "\n",
    "        return out\n",
    "\n",
    "    def backward(self, dout):\n",
    "        dout = dout.transpose(0, 2, 3, 1)\n",
    "        \n",
    "        pool_size = self.pool_h * self.pool_w\n",
    "        dmax = np.zeros((dout.size, pool_size))\n",
    "        dmax[np.arange(self.arg_max.size), self.arg_max.flatten()] = dout.flatten()\n",
    "        dmax = dmax.reshape(dout.shape + (pool_size,)) \n",
    "        \n",
    "        dcol = dmax.reshape(dmax.shape[0] * dmax.shape[1] * dmax.shape[2], -1)\n",
    "        dx = col2im(dcol, self.x.shape, self.pool_h, self.pool_w, self.stride, self.pad)\n",
    "        \n",
    "        return dx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# optimizers.py\n",
    "class SGD:\n",
    "    def __init__(self, lr=0.01):\n",
    "        self.lr = lr\n",
    "        \n",
    "    def update(self, params, grads):\n",
    "        for key in params.keys():\n",
    "            params[key] -= self.lr * grads[key] \n",
    "\n",
    "\n",
    "class Momentum:\n",
    "    def __init__(self, lr=0.01, momentum=0.9):\n",
    "        self.lr = lr\n",
    "        self.momentum = momentum\n",
    "        self.v = None\n",
    "        \n",
    "    def update(self, params, grads):\n",
    "        if self.v is None:\n",
    "            self.v = {}\n",
    "            for key, val in params.items():                                \n",
    "                self.v[key] = np.zeros_like(val)\n",
    "                \n",
    "        for key in params.keys():\n",
    "            self.v[key] = self.momentum*self.v[key] - self.lr*grads[key] \n",
    "            params[key] += self.v[key]\n",
    "\n",
    "\n",
    "class Nesterov:\n",
    "    def __init__(self, lr=0.01, momentum=0.9):\n",
    "        self.lr = lr\n",
    "        self.momentum = momentum\n",
    "        self.v = None\n",
    "        \n",
    "    def update(self, params, grads):\n",
    "        if self.v is None:\n",
    "            self.v = {}\n",
    "            for key, val in params.items():\n",
    "                self.v[key] = np.zeros_like(val)\n",
    "            \n",
    "        for key in params.keys():\n",
    "            self.v[key] *= self.momentum\n",
    "            self.v[key] -= self.lr * grads[key]\n",
    "            params[key] += self.momentum * self.momentum * self.v[key]\n",
    "            params[key] -= (1 + self.momentum) * self.lr * grads[key]\n",
    "\n",
    "\n",
    "class AdaGrad:\n",
    "    def __init__(self, lr=0.01):\n",
    "        self.lr = lr\n",
    "        self.h = None\n",
    "        \n",
    "    def update(self, params, grads):\n",
    "        if self.h is None:\n",
    "            self.h = {}\n",
    "            for key, val in params.items():\n",
    "                self.h[key] = np.zeros_like(val)\n",
    "            \n",
    "        for key in params.keys():\n",
    "            self.h[key] += grads[key] * grads[key]\n",
    "            params[key] -= self.lr * grads[key] / (np.sqrt(self.h[key]) + 1e-7)\n",
    "\n",
    "\n",
    "class RMSprop:\n",
    "    def __init__(self, lr=0.01, decay_rate = 0.99):\n",
    "        self.lr = lr\n",
    "        self.decay_rate = decay_rate\n",
    "        self.h = None\n",
    "        \n",
    "    def update(self, params, grads):\n",
    "        if self.h is None:\n",
    "            self.h = {}\n",
    "            for key, val in params.items():\n",
    "                self.h[key] = np.zeros_like(val)\n",
    "            \n",
    "        for key in params.keys():\n",
    "            self.h[key] *= self.decay_rate\n",
    "            self.h[key] += (1 - self.decay_rate) * grads[key] * grads[key]\n",
    "            params[key] -= self.lr * grads[key] / (np.sqrt(self.h[key]) + 1e-7)\n",
    "\n",
    "\n",
    "class Adam:\n",
    "    def __init__(self, lr=0.001, beta1=0.9, beta2=0.999):\n",
    "        self.lr = lr\n",
    "        self.beta1 = beta1\n",
    "        self.beta2 = beta2\n",
    "        self.iter = 0\n",
    "        self.m = None\n",
    "        self.v = None\n",
    "        \n",
    "    def update(self, params, grads):\n",
    "        if self.m is None:\n",
    "            self.m, self.v = {}, {}\n",
    "            for key, val in params.items():\n",
    "                self.m[key] = np.zeros_like(val)\n",
    "                self.v[key] = np.zeros_like(val)\n",
    "        \n",
    "        self.iter += 1\n",
    "        lr_t  = self.lr * np.sqrt(1.0 - self.beta2**self.iter) / (1.0 - self.beta1**self.iter)         \n",
    "        \n",
    "        for key in params.keys():\n",
    "            self.m[key] += (1 - self.beta1) * (grads[key] - self.m[key])\n",
    "            self.v[key] += (1 - self.beta2) * (grads[key]**2 - self.v[key])\n",
    "            params[key] -= lr_t * self.m[key] / (np.sqrt(self.v[key]) + 1e-7)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# initializers.py\n",
    "\n",
    "\n",
    "class Initializer:\n",
    "    def __init__(self, params, params_size_list, use_batch_normalization=False):\n",
    "        self.params = params\n",
    "        self.params_size_list = params_size_list\n",
    "        self.use_batch_normalization = use_batch_normalization\n",
    "\n",
    "    def initialize_params(self):\n",
    "        pass\n",
    "\n",
    "    def get_params(self):\n",
    "        return self.params\n",
    "\n",
    "\n",
    "class Zero_Initializer(Initializer):\n",
    "    def initialize_params(self, use_batch_normalization):\n",
    "        for idx in range(1, len(self.params_size_list)):\n",
    "            self.params['W' + str(idx)] = np.zeros(self.params_size_list[idx - 1], self.params_size_list[idx])\n",
    "            self.params['b' + str(idx)] = np.zeros(self.params_size_list[idx])\n",
    "            if self.use_batch_normalization and idx < len(self.params_size_list) - 1:\n",
    "                self.params['gamma' + str(idx)] = np.zeros(self.params_size_list[idx])\n",
    "                self.params['beta' + str(idx)] = np.zeros(self.params_size_list[idx])\n",
    "\n",
    "class N1_Initializer(Initializer):\n",
    "    def initialize_params(self):\n",
    "        for idx in range(1, len(self.params_size_list)):\n",
    "            self.params['W' + str(idx)] = np.random.randn(self.params_size_list[idx - 1], self.params_size_list[idx])\n",
    "            self.params['b' + str(idx)] = np.random.randn(self.params_size_list[idx])\n",
    "            if self.use_batch_normalization and idx < len(self.params_size_list) - 1:\n",
    "                self.params['gamma' + str(idx)] = np.random.randn(self.params_size_list[idx])\n",
    "                self.params['beta' + str(idx)] = np.random.randn(self.params_size_list[idx])\n",
    "\n",
    "class N2_Initializer(Initializer):\n",
    "    def initialize_params(self):\n",
    "        for idx in range(1, len(self.params_size_list)):\n",
    "            self.params['W' + str(idx)] = np.random.randn(self.params_size_list[idx - 1], self.params_size_list[idx]) * 0.01\n",
    "            self.params['b' + str(idx)] = np.random.randn(self.params_size_list[idx]) * 0.01\n",
    "            if self.use_batch_normalization and idx < len(self.params_size_list) - 1:\n",
    "                self.params['gamma' + str(idx)] = np.random.randn(self.params_size_list[idx]) * 0.01\n",
    "                self.params['beta' + str(idx)] = np.random.randn(self.params_size_list[idx]) * 0.01\n",
    "\n",
    "class Xavier_Initializer(Initializer):\n",
    "    def initialize_params(self):\n",
    "        for idx in range(1, len(self.params_size_list)):\n",
    "            self.params['W' + str(idx)] = np.random.randn(self.params_size_list[idx - 1], self.params_size_list[idx]) / np.sqrt(self.params_size_list[idx - 1])\n",
    "            self.params['b' + str(idx)] = np.random.randn(self.params_size_list[idx]) / np.sqrt(self.params_size_list[idx - 1])\n",
    "            if self.use_batch_normalization and idx < len(self.params_size_list) - 1:\n",
    "                self.params['gamma' + str(idx)] = np.random.randn(self.params_size_list[idx]) / np.sqrt(self.params_size_list[idx - 1])\n",
    "                self.params['beta' + str(idx)] = np.random.randn(self.params_size_list[idx]) / np.sqrt(self.params_size_list[idx - 1])\n",
    "\n",
    "\n",
    "class He_Initializer(Initializer):\n",
    "    def initialize_params(self):\n",
    "        for idx in range(1, len(self.params_size_list)):\n",
    "            self.params['W' + str(idx)] = np.random.randn(self.params_size_list[idx - 1], self.params_size_list[idx]) * np.sqrt(2) / np.sqrt(self.params_size_list[idx - 1])\n",
    "            self.params['b' + str(idx)] = np.random.randn(self.params_size_list[idx]) * np.sqrt(2) / np.sqrt(self.params_size_list[idx - 1])\n",
    "            if self.use_batch_normalization and idx < len(self.params_size_list) - 1:\n",
    "                self.params['gamma' + str(idx)] = np.random.randn(self.params_size_list[idx]) * np.sqrt(2) / np.sqrt(self.params_size_list[idx - 1])\n",
    "                self.params['beta' + str(idx)] = np.random.randn(self.params_size_list[idx]) * np.sqrt(2) / np.sqrt(self.params_size_list[idx - 1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# networks.py\n",
    "\n",
    "activation_layers = {\n",
    "    'Sigmoid': Sigmoid,\n",
    "    'ReLU': ReLU\n",
    "}\n",
    "\n",
    "optimizers = {\n",
    "    \"SGD\": SGD,\n",
    "    \"Momentum\": Momentum,\n",
    "    \"Nesterov\": Nesterov,\n",
    "    \"AdaGrad\": AdaGrad,\n",
    "    \"RMSprop\": RMSprop,\n",
    "    \"Adam\": Adam\n",
    "}\n",
    "\n",
    "initializers = {\n",
    "    'Zero': Zero_Initializer,\n",
    "    'N1': N1_Initializer,\n",
    "    'N2': N2_Initializer,\n",
    "    'Xavier': Xavier_Initializer,\n",
    "    'He': He_Initializer\n",
    "}\n",
    "\n",
    "\n",
    "class MultiLayerNet:\n",
    "    def __init__(self, input_size, hidden_size_list, output_size, activation='ReLU', initializer='He',\n",
    "                 optimizer='AdaGrad', learning_rate=0.01):\n",
    "        self.input_size = input_size\n",
    "        self.output_size = output_size\n",
    "        self.hidden_size_list = hidden_size_list\n",
    "        self.hidden_layer_num = len(hidden_size_list)\n",
    "\n",
    "        # Weight Initialization\n",
    "        self.params = {}\n",
    "        self.weight_initialization(initializer)\n",
    "\n",
    "        # Layering\n",
    "        self.layers = OrderedDict()\n",
    "        self.last_layer = None\n",
    "        self.layering(activation)\n",
    "\n",
    "        # Optimizer Initialization\n",
    "        self.optimizer = optimizers[optimizer](lr=learning_rate)\n",
    "\n",
    "    def weight_initialization(self, initializer):\n",
    "        params_size_list = [self.input_size] + self.hidden_size_list + [self.output_size]\n",
    "        initializer_obj = initializers[initializer](self.params, params_size_list)\n",
    "        initializer_obj.initialize_params();\n",
    "\n",
    "    def layering(self, activation):\n",
    "        for idx in range(1, self.hidden_layer_num + 1):\n",
    "            self.layers['Affine' + str(idx)] = Affine(self.params['W' + str(idx)], self.params['b' + str(idx)])\n",
    "            self.layers['Activation' + str(idx)] = activation_layers[activation]()\n",
    "\n",
    "        idx = self.hidden_layer_num + 1\n",
    "        self.layers['Affine' + str(idx)] = Affine(self.params['W' + str(idx)], self.params['b' + str(idx)])\n",
    "\n",
    "        self.last_layer = SoftmaxWithCrossEntropyLoss()\n",
    "\n",
    "    def predict(self, x):\n",
    "        for layer in self.layers.values():\n",
    "            x = layer.forward(x)\n",
    "\n",
    "        return x\n",
    "\n",
    "    def loss(self, x, t):\n",
    "        y = self.predict(x)\n",
    "        return self.last_layer.forward(y, t)\n",
    "\n",
    "    def accuracy(self, x, t):\n",
    "        y = self.predict(x)\n",
    "        y = np.argmax(y, axis=1)\n",
    "        if t.ndim != 1 : t = np.argmax(t, axis=1)\n",
    "\n",
    "        accuracy = np.sum(y == t) / float(x.shape[0])\n",
    "        return accuracy\n",
    "\n",
    "    def backpropagation_gradient(self, x, t):\n",
    "        # forward\n",
    "        self.loss(x, t)\n",
    "\n",
    "        # backward\n",
    "        din = 1\n",
    "        din = self.last_layer.backward(din)\n",
    "\n",
    "        layers = list(self.layers.values())\n",
    "        layers.reverse()\n",
    "        for layer in layers:\n",
    "            din = layer.backward(din)\n",
    "\n",
    "        grads = {}\n",
    "        for idx in range(1, self.hidden_layer_num+2):\n",
    "            grads['W' + str(idx)] = self.layers['Affine' + str(idx)].dW\n",
    "            grads['b' + str(idx)] = self.layers['Affine' + str(idx)].db\n",
    "\n",
    "        return grads\n",
    "\n",
    "    def learning(self, x_batch, t_batch):\n",
    "        grads = self.backpropagation_gradient(x_batch, t_batch)\n",
    "        self.optimizer.update(self.params, grads)\n",
    "\n",
    "\n",
    "class MultiLayerNetExtended:\n",
    "    def __init__(self, input_size, hidden_size_list, output_size, activation='ReLU', initializer='N2',\n",
    "                 optimizer='AdaGrad', learning_rate=0.01,\n",
    "                 use_batch_normalization=False,\n",
    "                 use_weight_decay=False, weight_decay_lambda=0.0,\n",
    "                 use_dropout=False, dropout_ratio_list=None):\n",
    "        self.input_size = input_size\n",
    "        self.output_size = output_size\n",
    "        self.hidden_size_list = hidden_size_list\n",
    "        self.hidden_layer_num = len(hidden_size_list)\n",
    "\n",
    "        self.use_batch_normalization = use_batch_normalization\n",
    "\n",
    "        self.use_weight_decay = use_weight_decay\n",
    "        self.weight_decay_lambda = weight_decay_lambda\n",
    "\n",
    "        self.use_dropout = use_dropout\n",
    "        self.dropout_ratio_list = dropout_ratio_list\n",
    "\n",
    "        # Weight Initialization\n",
    "        self.params = {}\n",
    "        self.weight_initialization(initializer)\n",
    "\n",
    "        # Layering\n",
    "        self.layers = OrderedDict()\n",
    "        self.last_layer = None\n",
    "        self.layering(activation)\n",
    "\n",
    "        # Optimization Method\n",
    "        self.optimizer = optimizers[optimizer](lr=learning_rate)\n",
    "\n",
    "    def weight_initialization(self, initializer):\n",
    "        params_size_list = [self.input_size] + self.hidden_size_list + [self.output_size]\n",
    "        initializer_obj = initializers[initializer](self.params,\n",
    "                                                    params_size_list,\n",
    "                                                    self.use_batch_normalization)\n",
    "        initializer_obj.initialize_params();\n",
    "\n",
    "    def layering(self, activation):\n",
    "        for idx in range(1, self.hidden_layer_num + 1):\n",
    "            self.layers['Affine' + str(idx)] = Affine(self.params['W' + str(idx)], self.params['b' + str(idx)])\n",
    "            if self.use_batch_normalization:\n",
    "                self.layers['Batch_Normalization' + str(idx)] = BatchNormalization(self.params['gamma' + str(idx)],\n",
    "                                                                                   self.params['beta' + str(idx)])\n",
    "            self.layers['Activation' + str(idx)] = activation_layers[activation]()\n",
    "\n",
    "            if self.use_dropout:\n",
    "                self.layers['Dropout' + str(idx)] = Dropout(self.dropout_ratio_list[idx - 1])\n",
    "\n",
    "        idx = self.hidden_layer_num + 1\n",
    "        self.layers['Affine' + str(idx)] = Affine(self.params['W' + str(idx)], self.params['b' + str(idx)])\n",
    "\n",
    "        self.last_layer = SoftmaxWithCrossEntropyLoss()\n",
    "\n",
    "    def predict(self, x, is_train=False):\n",
    "        for key, layer in self.layers.items():\n",
    "            if \"BatchNorm\" in key or \"Dropout\" in key:\n",
    "                x = layer.forward(x, is_train)\n",
    "            else:\n",
    "                x = layer.forward(x)\n",
    "        return x\n",
    "\n",
    "    def loss(self, x, t, is_train=False):\n",
    "        y = self.predict(x, is_train)\n",
    "\n",
    "        if self.use_weight_decay:\n",
    "            weight_decay = 0.0\n",
    "            for idx in range(1, self.hidden_layer_num + 2):\n",
    "                W = self.params['W' + str(idx)]\n",
    "                weight_decay += 0.5 * self.weight_decay_lambda * np.sum(W**2)\n",
    "            return self.last_layer.forward(y, t) + weight_decay\n",
    "        else:\n",
    "            return self.last_layer.forward(y, t)\n",
    "\n",
    "    def accuracy(self, x, t):\n",
    "        y = self.predict(x, is_train=False)\n",
    "        y = np.argmax(y, axis=1)\n",
    "        if t.ndim != 1 : t = np.argmax(t, axis=1)\n",
    "\n",
    "        accuracy = np.sum(y == t) / float(x.shape[0])\n",
    "        return accuracy\n",
    "\n",
    "    def backpropagation_gradient(self, x, t):\n",
    "        # forward\n",
    "        self.loss(x, t, is_train=True)\n",
    "\n",
    "        # backward\n",
    "        din = 1\n",
    "        din = self.last_layer.backward(din)\n",
    "\n",
    "        layers = list(self.layers.values())\n",
    "        layers.reverse()\n",
    "        for layer in layers:\n",
    "            din = layer.backward(din)\n",
    "\n",
    "        grads = {}\n",
    "        for idx in range(1, self.hidden_layer_num + 2):\n",
    "            if self.use_weight_decay:\n",
    "                grads['W' + str(idx)] = self.layers['Affine' + str(idx)].dW + self.weight_decay_lambda * self.params['W' + str(idx)]\n",
    "            else:\n",
    "                grads['W' + str(idx)] = self.layers['Affine' + str(idx)].dW\n",
    "            grads['b' + str(idx)] = self.layers['Affine' + str(idx)].db\n",
    "\n",
    "            if self.use_batch_normalization and idx <= self.hidden_layer_num:\n",
    "                grads['gamma' + str(idx)] = self.layers['Batch_Normalization' + str(idx)].dgamma\n",
    "                grads['beta' + str(idx)] = self.layers['Batch_Normalization' + str(idx)].dbeta\n",
    "\n",
    "        return grads\n",
    "\n",
    "    def learning(self, x_batch, t_batch):\n",
    "        grads = self.backpropagation_gradient(x_batch, t_batch)\n",
    "        self.optimizer.update(self.params, grads)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# deep_convnet.py\n",
    "\n",
    "\n",
    "optimizers = {\n",
    "    \"SGD\": SGD,\n",
    "    \"Momentum\": Momentum,\n",
    "    \"Nesterov\": Nesterov,\n",
    "    \"AdaGrad\": AdaGrad,\n",
    "    \"RMSprop\": RMSprop,\n",
    "    \"Adam\": Adam\n",
    "}\n",
    "\n",
    "class DeepConvNet:\n",
    "    \"\"\"\n",
    "        conv - relu - conv- relu - pool -\n",
    "        conv - relu - conv- relu - pool -\n",
    "        conv - relu - conv- relu - pool -\n",
    "        affine - relu - dropout - affine - dropout - softmax\n",
    "    \"\"\"\n",
    "    def __init__(self, input_dim=(1, 320, 240),\n",
    "                 conv_param_1 = {'filter_num':16, 'filter_size':3, 'pad':1, 'stride':1},\n",
    "                 conv_param_2 = {'filter_num':16, 'filter_size':3, 'pad':1, 'stride':1},\n",
    "                 conv_param_3 = {'filter_num':32, 'filter_size':3, 'pad':1, 'stride':1},\n",
    "                 conv_param_4 = {'filter_num':32, 'filter_size':3, 'pad':2, 'stride':1},\n",
    "                 conv_param_5 = {'filter_num':64, 'filter_size':3, 'pad':1, 'stride':1},\n",
    "                 conv_param_6 = {'filter_num':64, 'filter_size':4, 'pad':2, 'stride':1},\n",
    "                 hidden_size=50, output_size=3, optimizer = 'Adam', learning_rate=0.01):\n",
    "\n",
    "        pre_node_nums = np.array([1*3*3, 16*3*3, 16*3*3, 32*3*3, 32*3*3, 64*3*3, 64*21*16, hidden_size])\n",
    "\n",
    "        weight_init_scales = np.sqrt(2.0 / pre_node_nums)\n",
    "        \n",
    "        self.params = {}\n",
    "\n",
    "        pre_channel_num = input_dim[0]\n",
    "\n",
    "        for idx, conv_param in enumerate([conv_param_1, conv_param_2, conv_param_3, conv_param_4, conv_param_5, conv_param_6]):\n",
    "            self.params['W' + str(idx+1)] = weight_init_scales[idx] * np.random.randn(conv_param['filter_num'], pre_channel_num, conv_param['filter_size'], conv_param['filter_size'])\n",
    "            self.params['b' + str(idx+1)] = np.zeros(conv_param['filter_num'])\n",
    "            pre_channel_num = conv_param['filter_num']\n",
    "\n",
    "        self.params['W7'] = weight_init_scales[6] * np.random.randn(64*16*21, hidden_size)\n",
    "        self.params['b7'] = np.zeros(hidden_size)\n",
    "        self.params['W8'] = weight_init_scales[7] * np.random.randn(hidden_size, output_size)\n",
    "        self.params['b8'] = np.zeros(output_size)\n",
    "\n",
    "        self.layers = []\n",
    "        self.layers.append(Convolution(self.params['W1'], self.params['b1'], \n",
    "                           conv_param_1['stride'], conv_param_1['pad']))\n",
    "        self.layers.append(ReLU())\n",
    "        self.layers.append(Convolution(self.params['W2'], self.params['b2'], \n",
    "                           conv_param_2['stride'], conv_param_2['pad']))\n",
    "        self.layers.append(ReLU())\n",
    "        self.layers.append(Pooling(pool_h=2, pool_w=2, stride=2))\n",
    "        self.layers.append(Convolution(self.params['W3'], self.params['b3'], \n",
    "                           conv_param_3['stride'], conv_param_3['pad']))\n",
    "        self.layers.append(ReLU())\n",
    "        self.layers.append(Convolution(self.params['W4'], self.params['b4'],\n",
    "                           conv_param_4['stride'], conv_param_4['pad']))\n",
    "        self.layers.append(ReLU())\n",
    "        self.layers.append(Pooling(pool_h=2, pool_w=2, stride=2))\n",
    "        self.layers.append(Convolution(self.params['W5'], self.params['b5'],\n",
    "                           conv_param_5['stride'], conv_param_5['pad']))\n",
    "        self.layers.append(ReLU())\n",
    "        self.layers.append(Convolution(self.params['W6'], self.params['b6'],\n",
    "                           conv_param_6['stride'], conv_param_6['pad']))\n",
    "        self.layers.append(ReLU())\n",
    "        self.layers.append(Pooling(pool_h=2, pool_w=2, stride=2))\n",
    "        self.layers.append(Affine(self.params['W7'], self.params['b7']))\n",
    "        self.layers.append(ReLU())\n",
    "        self.layers.append(Dropout(0.5))\n",
    "        self.layers.append(Affine(self.params['W8'], self.params['b8']))\n",
    "        self.layers.append(Dropout(0.5))\n",
    "        \n",
    "        self.last_layer = SoftmaxWithCrossEntropyLoss()\n",
    "\n",
    "        # Optimizer Initialization\n",
    "        self.optimizer = optimizers[optimizer](lr=learning_rate)\n",
    "\n",
    "    def predict(self, x, train_flg=False):\n",
    "        isFirstAffine = False\n",
    "        for layer in self.layers:\n",
    "            if isinstance(layer, Affine) and not isFirstAffine:\n",
    "                isFirstAffine = True\n",
    "                x = x.reshape(-1, 64*21*16)\n",
    "\n",
    "            if isinstance(layer, Dropout):\n",
    "                x = layer.forward(x, train_flg)\n",
    "            else:\n",
    "                x = layer.forward(x)\n",
    "        return x\n",
    "\n",
    "    def loss(self, x, t):\n",
    "        y = self.predict(x, train_flg=True)\n",
    "        return self.last_layer.forward(y, t)\n",
    "\n",
    "    def accuracy(self, x, t, batch_size=10):\n",
    "        if t.ndim != 1 : t = np.argmax(t, axis=1)\n",
    "\n",
    "        acc = 0.0\n",
    "\n",
    "        for i in range(int(x.shape[0] / batch_size)):\n",
    "            tx = x[i*batch_size:(i+1)*batch_size]\n",
    "            tt = t[i*batch_size:(i+1)*batch_size]\n",
    "            y = self.predict(tx, train_flg=False)\n",
    "            y = np.argmax(y, axis=1)\n",
    "            acc += np.sum(y == tt)\n",
    "\n",
    "        return acc / x.shape[0]\n",
    "\n",
    "    def backpropagation_gradient(self, x, t):\n",
    "        # forward\n",
    "        self.loss(x, t)\n",
    "\n",
    "        # backward\n",
    "        dout = 1\n",
    "        dout = self.last_layer.backward(dout)\n",
    "\n",
    "        tmp_layers = self.layers.copy()\n",
    "        tmp_layers.reverse()\n",
    "\n",
    "        isFirstPooling = False\n",
    "        for layer in tmp_layers:\n",
    "            if isinstance(layer, Pooling) and not isFirstPooling:\n",
    "                isFirstPooling = True\n",
    "                dout = dout.reshape(-1, 64, 21, 16)\n",
    "            dout = layer.backward(dout)\n",
    "\n",
    "        # 設定\n",
    "        grads = {}\n",
    "        for i, layer_idx in enumerate((0, 2, 5, 7, 10, 12, 15, 18)):\n",
    "            grads['W' + str(i+1)] = self.layers[layer_idx].dW\n",
    "            grads['b' + str(i+1)] = self.layers[layer_idx].db\n",
    "\n",
    "        return grads\n",
    "\n",
    "    def learning(self, x_batch, t_batch):\n",
    "        grads = self.backpropagation_gradient(x_batch, t_batch)\n",
    "        self.optimizer.update(self.params, grads)\n",
    "\n",
    "    def save_params(self, file_name=\"params.pkl\"):\n",
    "        with open(file_name, 'wb') as f:\n",
    "            pickle.dump(self.params, f)\n",
    "\n",
    "    def load_params(self, file_name=\"params.pkl\"):\n",
    "        with open(file_name, 'rb') as f:\n",
    "            params = pickle.load(f)\n",
    "\n",
    "        for key, val in params.items():\n",
    "            self.params[key] = val\n",
    "\n",
    "        for i, layer_idx in enumerate((0, 2, 5, 7, 10, 12, 15, 18)):\n",
    "            self.layers[layer_idx].W = self.params['W' + str(i+1)]\n",
    "            self.layers[layer_idx].b = self.params['b' + str(i+1)]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "IMG_width = 320\n",
    "IMG_height = 240\n",
    "IMG_channel = 1\n",
    "DATASET_PATH = \"/Users/yhhan/Downloads/trainimage/dataset_jpg_gray\"\n",
    "kinds = [\"train\", \"validation\", \"test\"]\n",
    "train_img = np.array([], dtype=\"float32\")\n",
    "train_label = np.array([], dtype=\"int8\")\n",
    "validation_img = np.array([], dtype=\"float32\")\n",
    "validation_label = np.array([], dtype=\"int8\")\n",
    "test_img = np.array([], dtype=\"float32\")\n",
    "test_label = np.array([], dtype=\"int8\")\n",
    "(label_train, filename_train) = (0, 0)\n",
    "(label_validation, filename_validation) = (0, 0)\n",
    "(label_test, filename_test) = (0, 0)\n",
    "\n",
    "\n",
    "def file_info(category_name, dataset_path=DATASET_PATH):\n",
    "    # 디렉토리 상의 파일경로와 파일의 제일 앞에 매겨진 숫자정보(라벨 정보)를 긁어서 반환\n",
    "    full_path =  dataset_path + '/' + category_name + '/' + '*.jpg'\n",
    "    image_filenames = glob.glob(full_path)\n",
    "    filename = []\n",
    "    label = []\n",
    "    for image_filename in image_filenames:\n",
    "        filename.append(image_filename.split(\"/\")[3])\n",
    "        label.append(image_filename.split(\"/\")[3].split(\"-\")[0])\n",
    "    return (label, filename)\n",
    "\n",
    "\n",
    "def display_image(image, label):\n",
    "    %matplotlib\n",
    "    inline\n",
    "\n",
    "    fig = plt.figure(figsize=(20, 5))\n",
    "    for i in range(5):\n",
    "        print(label[i])\n",
    "        img = image[i]\n",
    "        img = img.reshape(240, 320)\n",
    "        img.shape = (240, 320)\n",
    "        plt.subplot(150 + (i + 1))\n",
    "        plt.imshow(img)\n",
    "\n",
    "\n",
    "def data_processing_about_train(idx, kind):\n",
    "    for i in range(idx):\n",
    "        global train_img\n",
    "        global train_label\n",
    "\n",
    "        (labels, filenames) = file_info(kind)\n",
    "        random_value = random.randrange(0, len(filenames))\n",
    "        filename = filenames[random_value]\n",
    "        label = labels[random_value]\n",
    "        #         label = _change_one_hot_label(train_label, label)\n",
    "        #         label = labels[random_value]\n",
    "        #         image_display(kinds, filename)\n",
    "        full_path = ( DATASET_PATH + '/' + kind + '/' + filename)\n",
    "        image = imread(full_path)\n",
    "        image = image.reshape(IMG_width * IMG_height * IMG_channel) / 255.0\n",
    "        train_img = np.append(train_img, image)\n",
    "        label = _change_one_hot_label(label)\n",
    "        train_label = np.append(train_label, label)\n",
    "        # file move\n",
    "        # 학습한 데이터는 이동시킴\n",
    "        print(\"---------\")\n",
    "        print(len(filenames))\n",
    "        print(filename)\n",
    "        src =  DATASET_PATH + '/' + kind + '/'\n",
    "        dir =  DATASET_PATH + '/' + 'tmp' + '/' + kind + '/'\n",
    "        shutil.move(src + filename, dir + filename)\n",
    "\n",
    "\n",
    "def data_processing_about_validation(idx, kind):\n",
    "    for i in range(idx):\n",
    "        global validation_img\n",
    "        global validation_label\n",
    "\n",
    "        (labels, filenames) = file_info(kind)\n",
    "        random_value = random.randrange(0, len(filenames))\n",
    "        filename = filenames[random_value]\n",
    "        label = labels[random_value]\n",
    "        #         label = _change_one_hot_label(train_label, label)\n",
    "        #         label = labels[random_value]\n",
    "        #         image_display(kinds, filename)\n",
    "        full_path = ( DATASET_PATH + '/' + kind + '/' + filename)\n",
    "        image = imread(full_path)\n",
    "        image = image.reshape(IMG_width * IMG_height * IMG_channel) / 255.0\n",
    "        validation_img = np.append(validation_img, image)\n",
    "        label = _change_one_hot_label(label)\n",
    "        validation_label = np.append(validation_label, label)\n",
    "        # file move\n",
    "        print(\"---------\")\n",
    "        print(len(filenames))\n",
    "        print(filename)\n",
    "        src =  DATASET_PATH + '/' + kind + '/'\n",
    "        dir =  DATASET_PATH + '/' + 'tmp' + '/' + kind + '/'\n",
    "        shutil.move(src + filename, dir + filename)\n",
    "\n",
    "\n",
    "def data_processing_about_test(idx, kind):\n",
    "    for i in range(idx):\n",
    "        global test_img\n",
    "        global test_label\n",
    "\n",
    "        (labels, filenames) = file_info(kind)\n",
    "        random_value = random.randrange(0, len(filenames))\n",
    "        filename = filenames[random_value]\n",
    "        label = labels[random_value]\n",
    "        #         label = _change_one_hot_label(train_label, label)\n",
    "        #         label = labels[random_value]\n",
    "        #         image_display(kinds, filename)\n",
    "        full_path = ( DATASET_PATH + '/' + kind + '/' + filename)\n",
    "        image = imread(full_path)\n",
    "        image = image.reshape(IMG_width * IMG_height * IMG_channel) / 255.0\n",
    "        test_img = np.append(test_img, image)\n",
    "        label = _change_one_hot_label(label)\n",
    "        test_label = np.append(test_label, label)\n",
    "        # file move\n",
    "        print(\"---------\")\n",
    "        print(len(filenames))\n",
    "        print(filename)\n",
    "        src =  DATASET_PATH + '/' + kind + '/'\n",
    "        dir =  DATASET_PATH + '/' + 'tmp' + '/' + kind + '/'\n",
    "        shutil.move(src + filename, dir + filename)\n",
    "\n",
    "\n",
    "def _change_one_hot_label(target_label):\n",
    "    target_label = int(target_label)\n",
    "    T = np.zeros((1, 3))\n",
    "    T[0][target_label] = 1\n",
    "\n",
    "    return T\n",
    "\n",
    "\n",
    "def store_dataset(dataset_path=DATASET_PATH):\n",
    "    global train_img\n",
    "    global train_label\n",
    "    global validation_img\n",
    "    global validation_label\n",
    "    global test_img\n",
    "    global test_label\n",
    "\n",
    "    (_, idx_train) = file_info(\"train\")\n",
    "    (_, idx_validation) = file_info(\"validation\")\n",
    "    (_, idx_test) = file_info(\"test\")\n",
    "    if len(idx_train) != 0 or len(idx_validation) != 0 or len(idx_test) != 0:\n",
    "        data_processing_about_train(len(idx_train), \"train\")\n",
    "        data_processing_about_validation(len(idx_validation), \"validation\")\n",
    "        data_processing_about_test(len(idx_test), \"test\")\n",
    "\n",
    "        # file로 쓰기\n",
    "        train_img.tofile(dataset_path + '/' + 'train_img_dataset')\n",
    "        train_label.tofile(dataset_path + '/' + 'train_label')\n",
    "        validation_img.tofile(dataset_path + '/' + 'validation_img_dataset')\n",
    "        validation_label.tofile(dataset_path + '/' + 'validation_label')\n",
    "        test_img.tofile(dataset_path + '/' + 'test_img_dataset')\n",
    "        test_label.tofile(dataset_path + '/' + 'test_label')\n",
    "    else:\n",
    "        print(\"[!] Already Finished Generateing Dataset. Please check directory.\")\n",
    "\n",
    "def load_dataset(dataset_path, flatten=True):\n",
    "    (img_train, label_train) = (0,0)\n",
    "    (img_validation, label_validation) = (0,0)\n",
    "    (img_test, label_test) = (0,0)\n",
    "    filenames = glob.glob(dataset_path + \"/*\")\n",
    "    filename = []\n",
    "    for filename in filenames:\n",
    "        print(filename, )\n",
    "        img_train = np.fromfile(open(dataset_path + \"/\" + \"train_img_dataset\", 'rb')).reshape(451,IMG_width * IMG_height * IMG_channel)\n",
    "        label_train = np.fromfile(open(dataset_path + \"/\" + \"train_label\", 'rb')).reshape(451, 3)\n",
    "        img_validation = np.fromfile(open(dataset_path + \"/\" + \"validation_img_dataset\", 'rb')).reshape(65, IMG_width * IMG_height * IMG_channel)\n",
    "        label_validation = np.fromfile(open(dataset_path + \"/\" + \"validation_label\", 'rb')).reshape(65,3)\n",
    "        img_test = np.fromfile(open(dataset_path + \"/\" + \"test_img_dataset\", 'rb')).reshape(130,IMG_width * IMG_height * IMG_channel)\n",
    "        label_test = np.fromfile(open(dataset_path + \"/\" + \"test_label\", 'rb')).reshape(130, 3)\n",
    "        \n",
    "    if not flatten:\n",
    "        img_train      = img_train.reshape(-1, 1, 240, 320)\n",
    "        img_validation = img_validation.reshape(-1, 1, 240, 320)\n",
    "        img_test       = img_test.reshape(-1, 1, 240, 320)\n",
    "            \n",
    "    return ((img_train, label_train), (img_validation, label_validation), (img_test, label_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/Users/yhhan/Downloads/trainimage/imdata/test_img_dataset\n",
      "/Users/yhhan/Downloads/trainimage/imdata/test_label\n",
      "/Users/yhhan/Downloads/trainimage/imdata/train_img_dataset\n",
      "/Users/yhhan/Downloads/trainimage/imdata/train_label\n",
      "/Users/yhhan/Downloads/trainimage/imdata/validation_img_dataset\n",
      "/Users/yhhan/Downloads/trainimage/imdata/validation_label\n",
      "(451, 1, 240, 320)\n",
      "(451, 3)\n",
      "(65, 1, 240, 320)\n",
      "(65, 3)\n",
      "(130, 1, 240, 320)\n",
      "(130, 3)\n",
      "0, "
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "cannot reshape array of size 81344 into shape (21504)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-16-4cc9fd00c5f8>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     32\u001b[0m         \u001b[0mx_batch\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mimg_train\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mj\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mj\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mbatch_size\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     33\u001b[0m         \u001b[0mt_batch\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlabel_train\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mj\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mj\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mbatch_size\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 34\u001b[0;31m         \u001b[0mnetwork\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlearning\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx_batch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mt_batch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     35\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     36\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-8-5e2da7717465>\u001b[0m in \u001b[0;36mlearning\u001b[0;34m(self, x_batch, t_batch)\u001b[0m\n\u001b[1;32m    136\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    137\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mlearning\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx_batch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mt_batch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 138\u001b[0;31m         \u001b[0mgrads\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackpropagation_gradient\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx_batch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mt_batch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    139\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparams\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgrads\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    140\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-8-5e2da7717465>\u001b[0m in \u001b[0;36mbackpropagation_gradient\u001b[0;34m(self, x, t)\u001b[0m\n\u001b[1;32m    111\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mbackpropagation_gradient\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mt\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    112\u001b[0m         \u001b[0;31m# forward\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 113\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mloss\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mt\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    114\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    115\u001b[0m         \u001b[0;31m# backward\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-8-5e2da7717465>\u001b[0m in \u001b[0;36mloss\u001b[0;34m(self, x, t)\u001b[0m\n\u001b[1;32m     92\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     93\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mloss\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mt\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 94\u001b[0;31m         \u001b[0my\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_flg\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     95\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlast_layer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mt\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     96\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-8-5e2da7717465>\u001b[0m in \u001b[0;36mpredict\u001b[0;34m(self, x, train_flg)\u001b[0m\n\u001b[1;32m     83\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlayer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mAffine\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0misFirstAffine\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     84\u001b[0m                 \u001b[0misFirstAffine\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 85\u001b[0;31m                 \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreshape\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m64\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0;36m21\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0;36m16\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     86\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     87\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlayer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mDropout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: cannot reshape array of size 81344 into shape (21504)"
     ]
    }
   ],
   "source": [
    "# train_deepnet.py\n",
    "(img_train, label_train), (img_validation, label_validation), (img_test, label_test) = load_dataset(dataset_path=\"/Users/yhhan/Downloads/trainimage/imdata\", flatten=False)\n",
    "\n",
    "print(img_train.shape)\n",
    "print(label_train.shape)\n",
    "print(img_validation.shape)\n",
    "print(label_validation.shape)\n",
    "print(img_test.shape)\n",
    "print(label_test.shape)\n",
    "\n",
    "num_epochs = 50\n",
    "train_size = img_train.shape[0]\n",
    "batch_size = 1\n",
    "learning_rate = 0.01\n",
    "\n",
    "\n",
    "network = DeepConvNet(learning_rate = learning_rate)\n",
    "\n",
    "\n",
    "train_error_list = []\n",
    "validation_error_list = []\n",
    "\n",
    "test_accuracy_list = []\n",
    "epoch_list = []\n",
    "\n",
    "num_batch = math.ceil(train_size / batch_size)\n",
    "\n",
    "for i in range(num_epochs):\n",
    "    for j in range(num_batch):\n",
    "        print(j, end=\", \")\n",
    "        sys.stdout.flush()\n",
    "        x_batch = img_train[j * batch_size: j * batch_size + batch_size]\n",
    "        t_batch = label_train[j * batch_size: j * batch_size + batch_size]\n",
    "        network.learning(x_batch, t_batch)\n",
    "\n",
    "    print()\n",
    "\n",
    "    epoch_list.append(i)\n",
    "\n",
    "    train_loss = network.loss(x_batch, t_batch)\n",
    "    train_error_list.append(train_loss)\n",
    "\n",
    "    validation_loss = network.loss(img_validation, label_validation)\n",
    "    validation_error_list.append(validation_loss)\n",
    "\n",
    "    test_accuracy = network.accuracy(img_test, label_test)\n",
    "    test_accuracy_list.append(test_accuracy)\n",
    "\n",
    "    print(\"Epoch: {0:5d}, Train Error: {1:7.5f}, Validation Error: {2:7.5f} - Test Accuracy: {3:7.5f}\".format(\n",
    "        i,\n",
    "        train_loss,\n",
    "        validation_loss,\n",
    "        test_accuracy\n",
    "    ))\n",
    "\n",
    "# Draw Graph about Error Values & Accuracy Values\n",
    "def draw_error_values_and_accuracy(epoch_list, train_error_list, validation_error_list, test_accuracy_list):\n",
    "    # Draw Error Values and Accuracy\n",
    "    fig = plt.figure(figsize=(20, 5))\n",
    "    plt.subplot(121)\n",
    "    plt.plot(epoch_list[1:], train_error_list[1:], 'r', label='Train')\n",
    "    plt.plot(epoch_list[1:], validation_error_list[1:], 'g', label='Validation')\n",
    "    plt.ylabel('Total Error')\n",
    "    plt.xlabel('Epochs')\n",
    "    plt.grid(True)\n",
    "    plt.legend(loc='upper right')\n",
    "\n",
    "    plt.subplot(122)\n",
    "    plt.plot(epoch_list[1:], test_accuracy_list[1:], 'b', label='Test')\n",
    "    plt.ylabel('Accuracy')\n",
    "    plt.xlabel('Epochs')\n",
    "    plt.yticks(np.arange(0.0, 1.0, 0.05))\n",
    "    plt.grid(True)\n",
    "    plt.legend(loc='lower right')\n",
    "    plt.show()\n",
    "\n",
    "def draw_false_prediction(diff_index_list):\n",
    "    fig = plt.figure(figsize=(20, 5))\n",
    "    for i in range(5):\n",
    "        j = diff_index_list[i]\n",
    "        print(\"False Prediction Index: %s, Prediction: %s, Ground Truth: %s\" % (j, prediction[j], ground_truth[j]))\n",
    "        img = np.array(img_test[j])\n",
    "        img.shape = (240, 320)\n",
    "        plt.subplot(150 + (i + 1))\n",
    "        plt.imshow(img, cmap='gray')\n",
    "\n",
    "draw_error_values_and_accuracy(epoch_list, train_error_list, validation_error_list, test_accuracy_list)\n",
    "\n",
    "prediction = np.argmax(network.predict(img_test), axis=1)\n",
    "ground_truth = np.argmax(label_test, axis=1)\n",
    "\n",
    "print(prediction)\n",
    "print(ground_truth)\n",
    "\n",
    "diff_index_list = []\n",
    "for i in range(len(img_test)):\n",
    "    if (prediction[i] != ground_truth[i]):\n",
    "        diff_index_list.append(i)\n",
    "\n",
    "print(\"Total Test Image: {0}, Number of False Prediction: {1}\".format(len(img_test), len(diff_index_list)))\n",
    "print(\"Test Accuracy:\", float(len(img_test) - len(diff_index_list)) / float(len(img_test)))\n",
    "draw_false_prediction(diff_index_list)\n",
    "\n",
    "network.save_params(\"deep_convnet_params.pkl\")\n",
    "print(\"Saved Network Parameters!\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
