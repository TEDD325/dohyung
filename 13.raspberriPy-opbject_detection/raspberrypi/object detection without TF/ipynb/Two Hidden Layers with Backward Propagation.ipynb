{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.image as mpimg\n",
    "from scipy.misc import imread\n",
    "import numpy as np\n",
    "import os\n",
    "import tensorflow as tf\n",
    "import glob\n",
    "import random\n",
    "import shutil\n",
    "\n",
    "IMG_width = 320\n",
    "IMG_height = 240\n",
    "IMG_channel = 1\n",
    "DATASET_PATH = \"./dataset_jpg_gray\"\n",
    "kinds = [\"train\", \"validation\", \"test\"]\n",
    "train_img = np.array([], dtype=\"float32\")\n",
    "train_label = np.array([], dtype=\"int8\")\n",
    "validation_img = np.array([], dtype=\"float32\")\n",
    "validation_label = np.array([], dtype=\"int8\")\n",
    "test_img = np.array([], dtype=\"float32\")\n",
    "test_label = np.array([], dtype=\"int8\")\n",
    "(label_train, filename_train) = (0, 0)\n",
    "(label_validation, filename_validation) = (0, 0)\n",
    "(label_test, filename_test) = (0, 0)\n",
    "\n",
    "\n",
    "def file_info(category_name, dataset_path=DATASET_PATH):\n",
    "    # 디렉토리 상의 파일경로와 파일의 제일 앞에 매겨진 숫자정보(라벨 정보)를 긁어서 반환\n",
    "    full_path =  dataset_path + '/' + category_name + '/' + '*.jpg'\n",
    "    image_filenames = glob.glob(full_path)\n",
    "    filename = []\n",
    "    label = []\n",
    "    for image_filename in image_filenames:\n",
    "        filename.append(image_filename.split(\"/\")[3])\n",
    "        label.append(image_filename.split(\"/\")[3].split(\"-\")[0])\n",
    "    return (label, filename)\n",
    "\n",
    "\n",
    "def display_image(image, label):\n",
    "    %matplotlib\n",
    "    inline\n",
    "\n",
    "    fig = plt.figure(figsize=(20, 5))\n",
    "    for i in range(5):\n",
    "        print(label[i])\n",
    "        img = image[i]\n",
    "        img = img.reshape(240, 320, 1)\n",
    "        img.shape = (240, 320, 1)\n",
    "        plt.subplot(150 + (i + 1))\n",
    "        plt.imshow(img)\n",
    "\n",
    "\n",
    "def data_processing_about_train(idx, kind):\n",
    "    for i in range(idx):\n",
    "        global train_img\n",
    "        global train_label\n",
    "\n",
    "        (labels, filenames) = file_info(kind)\n",
    "        random_value = random.randrange(0, len(filenames))\n",
    "        filename = filenames[random_value]\n",
    "        label = labels[random_value]\n",
    "        #         label = _change_one_hot_label(train_label, label)\n",
    "        #         label = labels[random_value]\n",
    "        #         image_display(kinds, filename)\n",
    "        full_path = ( DATASET_PATH + '/' + kind + '/' + filename)\n",
    "        image = imread(full_path)\n",
    "        image = image.reshape(IMG_width * IMG_height * IMG_channel) / 255.0\n",
    "        train_img = np.append(train_img, image)\n",
    "        label = _change_one_hot_label(label)\n",
    "        train_label = np.append(train_label, label)\n",
    "        # file move\n",
    "        # 학습한 데이터는 이동시킴\n",
    "        print(\"---------\")\n",
    "        print(len(filenames))\n",
    "        print(filename)\n",
    "        src =  DATASET_PATH + '/' + kind + '/'\n",
    "        dir =  DATASET_PATH + '/' + 'tmp' + '/' + kind + '/'\n",
    "        shutil.move(src + filename, dir + filename)\n",
    "\n",
    "\n",
    "def data_processing_about_validation(idx, kind):\n",
    "    for i in range(idx):\n",
    "        global validation_img\n",
    "        global validation_label\n",
    "\n",
    "        (labels, filenames) = file_info(kind)\n",
    "        random_value = random.randrange(0, len(filenames))\n",
    "        filename = filenames[random_value]\n",
    "        label = labels[random_value]\n",
    "        #         label = _change_one_hot_label(train_label, label)\n",
    "        #         label = labels[random_value]\n",
    "        #         image_display(kinds, filename)\n",
    "        full_path = ( DATASET_PATH + '/' + kind + '/' + filename)\n",
    "        image = imread(full_path)\n",
    "        image = image.reshape(IMG_width * IMG_height * IMG_channel) / 255.0\n",
    "        validation_img = np.append(validation_img, image)\n",
    "        label = _change_one_hot_label(label)\n",
    "        validation_label = np.append(validation_label, label)\n",
    "        # file move\n",
    "        print(\"---------\")\n",
    "        print(len(filenames))\n",
    "        print(filename)\n",
    "        src =  DATASET_PATH + '/' + kind + '/'\n",
    "        dir =  DATASET_PATH + '/' + 'tmp' + '/' + kind + '/'\n",
    "        shutil.move(src + filename, dir + filename)\n",
    "\n",
    "\n",
    "def data_processing_about_test(idx, kind):\n",
    "    for i in range(idx):\n",
    "        global test_img\n",
    "        global test_label\n",
    "\n",
    "        (labels, filenames) = file_info(kind)\n",
    "        random_value = random.randrange(0, len(filenames))\n",
    "        filename = filenames[random_value]\n",
    "        label = labels[random_value]\n",
    "        #         label = _change_one_hot_label(train_label, label)\n",
    "        #         label = labels[random_value]\n",
    "        #         image_display(kinds, filename)\n",
    "        full_path = ( DATASET_PATH + '/' + kind + '/' + filename)\n",
    "        image = imread(full_path)\n",
    "        image = image.reshape(IMG_width * IMG_height * IMG_channel) / 255.0\n",
    "        test_img = np.append(test_img, image)\n",
    "        label = _change_one_hot_label(label)\n",
    "        test_label = np.append(test_label, label)\n",
    "        # file move\n",
    "        print(\"---------\")\n",
    "        print(len(filenames))\n",
    "        print(filename)\n",
    "        src =  DATASET_PATH + '/' + kind + '/'\n",
    "        dir =  DATASET_PATH + '/' + 'tmp' + '/' + kind + '/'\n",
    "        shutil.move(src + filename, dir + filename)\n",
    "\n",
    "\n",
    "def _change_one_hot_label(target_label):\n",
    "    target_label = int(target_label)\n",
    "    T = np.zeros((1, 3))\n",
    "    T[0][target_label] = 1\n",
    "\n",
    "    return T\n",
    "\n",
    "\n",
    "def store_dataset(dataset_path=DATASET_PATH):\n",
    "    global train_img\n",
    "    global train_label\n",
    "    global validation_img\n",
    "    global validation_label\n",
    "    global test_img\n",
    "    global test_label\n",
    "\n",
    "    (_, idx_train) = file_info(\"train\")\n",
    "    (_, idx_validation) = file_info(\"validation\")\n",
    "    (_, idx_test) = file_info(\"test\")\n",
    "    if len(idx_train) != 0 or len(idx_validation) != 0 or len(idx_test) != 0:\n",
    "        data_processing_about_train(len(idx_train), \"train\")\n",
    "        data_processing_about_validation(len(idx_validation), \"validation\")\n",
    "        data_processing_about_test(len(idx_test), \"test\")\n",
    "\n",
    "        # file로 쓰기\n",
    "        train_img.tofile(dataset_path + '/' + 'train_img_dataset.txt')\n",
    "        train_label.tofile(dataset_path + '/' + 'train_label.txt')\n",
    "        validation_img.tofile(dataset_path + '/' + 'validation_img_dataset.txt')\n",
    "        validation_label.tofile(dataset_path + '/' + 'validation_label.txt')\n",
    "        test_img.tofile(dataset_path + '/' + 'test_img_dataset.txt')\n",
    "        test_label.tofile(dataset_path + '/' + 'test_label.txt')\n",
    "\n",
    "    else:\n",
    "        print(\"[!] Already Finished Generateing Dataset. Please check directory.\")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def load_dataset(dataset_path):\n",
    "    (img_train, label_train) = (0,0)\n",
    "    (img_validation, label_validation) = (0,0)\n",
    "    (img_test, label_test) = (0,0)\n",
    "    filenames = glob.glob(dataset_path + \"/\" + \"*.txt\")\n",
    "    filename = []\n",
    "    for filename in filenames:\n",
    "        print(filename, )\n",
    "        if filename.split(\"/\")[6][-4:] == \".txt\": # CAUTION : filename.split(\"/\")[directory level][-4:]\n",
    "            img_train = np.fromfile(open(dataset_path + \"/\" + \"train_img_dataset.txt\", 'rb')).reshape(451,IMG_width * IMG_height * IMG_channel)\n",
    "            label_train = np.fromfile(open(dataset_path + \"/\" + \"train_label.txt\", 'rb')).reshape(451, 3)\n",
    "            img_validation = np.fromfile(open(dataset_path + \"/\" + \"validation_img_dataset.txt\", 'rb')).reshape(65, IMG_width * IMG_height * IMG_channel)\n",
    "            label_validation = np.fromfile(open(dataset_path + \"/\" + \"validation_label.txt\", 'rb')).reshape(65,3)\n",
    "            img_test = np.fromfile(open(dataset_path + \"/\" + \"test_img_dataset.txt\", 'rb')).reshape(130,IMG_width * IMG_height * IMG_channel)\n",
    "            label_test = np.fromfile(open(dataset_path + \"/\" + \"test_label.txt\", 'rb')).reshape(130, 3)\n",
    "    return ((img_train, label_train), (img_validation, label_validation), (img_test, label_test))\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "def softmax(x):\n",
    "    if x.ndim == 2:\n",
    "        x = x.T\n",
    "        x = x - np.max(x, axis=0)\n",
    "        y = np.exp(x) / np.sum(np.exp(x), axis=0)\n",
    "        return y.T \n",
    "\n",
    "    x = x - np.max(x)\n",
    "    return np.exp(x) / np.sum(np.exp(x))\n",
    "\n",
    "def cross_entropy_error(y, t):\n",
    "    #print(y.shape, t.shape)\n",
    "    if y.ndim == 1:\n",
    "        y = y.reshape(1, y.size)\n",
    "        t = t.reshape(1, t.size)\n",
    "\n",
    "    if t.size == y.size:\n",
    "        t = t.argmax(axis=1)\n",
    "\n",
    "    batch_size = y.shape[0]\n",
    "    return -np.sum(np.log(y[np.arange(batch_size), t])) / batch_size\n",
    "\n",
    "class Relu:\n",
    "    def __init__(self):\n",
    "        self.mask = None\n",
    "\n",
    "    def forward(self, x):\n",
    "        self.mask = (x <= 0)\n",
    "        out = x.copy()\n",
    "        out[self.mask] = 0\n",
    "        return out\n",
    "\n",
    "    def backward(self, din):\n",
    "        din[self.mask] = 0\n",
    "        dx = din\n",
    "        return dx\n",
    "\n",
    "class Sigmoid:\n",
    "    def __init__(self):\n",
    "        self.out = None\n",
    "\n",
    "    def forward(self, x):\n",
    "        out = sigmoid(x)\n",
    "        self.out = out\n",
    "        return out\n",
    "\n",
    "    def backward(self, din):\n",
    "        dx = din * self.out * (1.0 - self.out)\n",
    "        return dx\n",
    "\n",
    "class Affine:\n",
    "    def __init__(self, W, b):\n",
    "        self.W = W\n",
    "        self.b = b        \n",
    "        self.x = None\n",
    "        self.dW = None\n",
    "        self.db = None\n",
    "\n",
    "    def forward(self, x):\n",
    "        self.x = x\n",
    "        out = np.dot(self.x, self.W) + self.b\n",
    "        return out\n",
    "\n",
    "    def backward(self, din):\n",
    "        dx = np.dot(din, self.W.T)\n",
    "        self.dW = np.dot(self.x.T, din)\n",
    "        self.db = np.sum(din, axis=0)\n",
    "        return dx\n",
    "\n",
    "class SoftmaxWithCrossEntropyLoss:\n",
    "    def __init__(self):\n",
    "        self.loss = None\n",
    "        self.y = None\n",
    "        self.t = None\n",
    "    \n",
    "    def forward(self, x, t):\n",
    "        self.t = t\n",
    "        self.y = softmax(x)\n",
    "        self.loss = cross_entropy_error(self.y, self.t)\n",
    "        return self.loss\n",
    "\n",
    "    def backward(self, din=1):\n",
    "        batch_size = self.t.shape[0]\n",
    "        dx = (self.y - self.t) / float(batch_size)        \n",
    "        return dx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import sys, os\n",
    "from collections import OrderedDict\n",
    "from scipy import stats\n",
    "from pandas import DataFrame\n",
    "\n",
    "class TwoLayerNet2:\n",
    "    def __init__(self, input_size, hidden_layer1_size, hidden_layer2_size, output_size, weight_init_std = 0.01):\n",
    "        self.params = {}\n",
    "        self.params['W1'] = weight_init_std * np.random.randn(input_size, hidden_layer1_size)\n",
    "        self.params['b1'] = np.zeros(hidden_layer1_size)\n",
    "        self.params['W2'] = weight_init_std * np.random.randn(hidden_layer1_size, hidden_layer2_size)\n",
    "        self.params['b2'] = np.zeros(hidden_layer2_size)\n",
    "        self.params['W3'] = weight_init_std * np.random.randn(hidden_layer2_size, output_size)\n",
    "        self.params['b3'] = np.zeros(output_size)\n",
    "\n",
    "        self.layers = OrderedDict()\n",
    "        self.layers['Affine1'] = Affine(self.params['W1'], self.params['b1'])\n",
    "        self.layers['Relu1'] = Relu()\n",
    "        self.layers['Affine2'] = Affine(self.params['W2'], self.params['b2'])\n",
    "        self.layers['Relu2'] = Relu()\n",
    "        self.layers['Affine3'] = Affine(self.params['W3'], self.params['b3'])\n",
    "\n",
    "        self.lastLayer = SoftmaxWithCrossEntropyLoss()\n",
    "        \n",
    "    def predict(self, x):\n",
    "        for layer in self.layers.values():\n",
    "            x = layer.forward(x)\n",
    "        \n",
    "        return x\n",
    "        \n",
    "    def loss(self, x, t):\n",
    "        y = self.predict(x)\n",
    "        return self.lastLayer.forward(y, t)\n",
    "    \n",
    "    def accuracy(self, x, t):\n",
    "        y = self.predict(x)\n",
    "        y = np.argmax(y, axis=1)\n",
    "        if t.ndim != 1 : t = np.argmax(t, axis=1)\n",
    "        \n",
    "        accuracy = np.sum(y == t) / float(x.shape[0])\n",
    "        return accuracy\n",
    "        \n",
    "    def backpropagation_gradient(self, x, t):\n",
    "        # forward\n",
    "        self.loss(x, t)\n",
    "\n",
    "        # backward\n",
    "        din = 1\n",
    "        din = self.lastLayer.backward(din)\n",
    "        \n",
    "        layers = list(self.layers.values())\n",
    "        layers.reverse()\n",
    "        for layer in layers:\n",
    "            din = layer.backward(din)\n",
    "\n",
    "        grads = {}\n",
    "        grads['W1'], grads['b1'] = self.layers['Affine1'].dW, self.layers['Affine1'].db\n",
    "        grads['W2'], grads['b2'] = self.layers['Affine2'].dW, self.layers['Affine2'].db\n",
    "        grads['W3'], grads['b3'] = self.layers['Affine3'].dW, self.layers['Affine3'].db\n",
    "\n",
    "        return grads\n",
    "    \n",
    "    def learning(self, learning_rate, x_batch, t_batch):\n",
    "        grads = self.backpropagation_gradient(x_batch, t_batch)\n",
    "        for key in ('W1', 'b1', 'W2', 'b2', 'W3', 'b3'):\n",
    "            self.params[key] -= learning_rate * grads[key]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/Users/do-hyungkwon/GoogleDrive/jupyter_notebook/imdata/test_img_dataset.txt\n",
      "/Users/do-hyungkwon/GoogleDrive/jupyter_notebook/imdata/test_label.txt\n",
      "/Users/do-hyungkwon/GoogleDrive/jupyter_notebook/imdata/train_img_dataset.txt\n",
      "/Users/do-hyungkwon/GoogleDrive/jupyter_notebook/imdata/train_label.txt\n",
      "/Users/do-hyungkwon/GoogleDrive/jupyter_notebook/imdata/validation_img_dataset.txt\n",
      "/Users/do-hyungkwon/GoogleDrive/jupyter_notebook/imdata/validation_label.txt\n",
      "Epoch:     0, Train Error: 1.10374, Validation Error: 1.07906 - Test Accuracy: 0.49231\n",
      "Epoch:     1, Train Error: 1.08171, Validation Error: 1.06636 - Test Accuracy: 0.49231\n",
      "Epoch:     2, Train Error: 1.01092, Validation Error: 1.05535 - Test Accuracy: 0.50000\n",
      "Epoch:     3, Train Error: 0.74636, Validation Error: 1.05754 - Test Accuracy: 0.55385\n",
      "Epoch:     4, Train Error: 0.30162, Validation Error: 1.17453 - Test Accuracy: 0.23077\n",
      "Epoch:     5, Train Error: 0.16953, Validation Error: 1.23481 - Test Accuracy: 0.26923\n",
      "Epoch:     6, Train Error: 0.14809, Validation Error: 1.10576 - Test Accuracy: 0.55385\n",
      "Epoch:     7, Train Error: 0.11832, Validation Error: 1.09005 - Test Accuracy: 0.56923\n",
      "Epoch:     8, Train Error: 0.10593, Validation Error: 1.03064 - Test Accuracy: 0.60000\n",
      "Epoch:     9, Train Error: 0.09132, Validation Error: 0.98116 - Test Accuracy: 0.60769\n",
      "Epoch:    10, Train Error: 0.08042, Validation Error: 0.97852 - Test Accuracy: 0.58462\n",
      "Epoch:    11, Train Error: 0.08256, Validation Error: 0.84561 - Test Accuracy: 0.62308\n",
      "Epoch:    12, Train Error: 0.06862, Validation Error: 0.84898 - Test Accuracy: 0.62308\n",
      "Epoch:    13, Train Error: 0.07712, Validation Error: 0.75370 - Test Accuracy: 0.67692\n",
      "Epoch:    14, Train Error: 0.06832, Validation Error: 0.76176 - Test Accuracy: 0.66154\n",
      "Epoch:    15, Train Error: 0.06385, Validation Error: 0.70762 - Test Accuracy: 0.66923\n",
      "Epoch:    16, Train Error: 0.05278, Validation Error: 0.75192 - Test Accuracy: 0.63846\n",
      "Epoch:    17, Train Error: 0.05081, Validation Error: 0.70847 - Test Accuracy: 0.64615\n",
      "Epoch:    18, Train Error: 0.05421, Validation Error: 0.72861 - Test Accuracy: 0.57692\n",
      "Epoch:    19, Train Error: 0.05352, Validation Error: 0.66780 - Test Accuracy: 0.61538\n",
      "Epoch:    20, Train Error: 0.05033, Validation Error: 0.68499 - Test Accuracy: 0.58462\n",
      "Epoch:    21, Train Error: 0.05026, Validation Error: 0.69256 - Test Accuracy: 0.55385\n",
      "Epoch:    22, Train Error: 0.05038, Validation Error: 0.67593 - Test Accuracy: 0.56154\n",
      "Epoch:    23, Train Error: 0.05156, Validation Error: 0.63062 - Test Accuracy: 0.57692\n",
      "Epoch:    24, Train Error: 0.04979, Validation Error: 0.65353 - Test Accuracy: 0.55385\n",
      "Epoch:    25, Train Error: 0.04680, Validation Error: 0.64519 - Test Accuracy: 0.55385\n",
      "Epoch:    26, Train Error: 0.04809, Validation Error: 0.63552 - Test Accuracy: 0.53846\n",
      "Epoch:    27, Train Error: 0.04636, Validation Error: 0.64601 - Test Accuracy: 0.53077\n",
      "Epoch:    28, Train Error: 0.04513, Validation Error: 0.62992 - Test Accuracy: 0.52308\n",
      "Epoch:    29, Train Error: 0.04650, Validation Error: 0.62032 - Test Accuracy: 0.53846\n",
      "Epoch:    30, Train Error: 0.04248, Validation Error: 0.64977 - Test Accuracy: 0.48462\n",
      "Epoch:    31, Train Error: 0.04800, Validation Error: 0.64902 - Test Accuracy: 0.53846\n",
      "Epoch:    32, Train Error: 0.04216, Validation Error: 0.64356 - Test Accuracy: 0.51538\n",
      "Epoch:    33, Train Error: 0.04677, Validation Error: 0.59590 - Test Accuracy: 0.54615\n",
      "Epoch:    34, Train Error: 0.03887, Validation Error: 0.59377 - Test Accuracy: 0.52308\n",
      "Epoch:    35, Train Error: 0.03727, Validation Error: 0.56277 - Test Accuracy: 0.52308\n",
      "Epoch:    36, Train Error: 0.03795, Validation Error: 0.55813 - Test Accuracy: 0.53846\n",
      "Epoch:    37, Train Error: 0.03895, Validation Error: 0.59570 - Test Accuracy: 0.51538\n",
      "Epoch:    38, Train Error: 0.03388, Validation Error: 0.56613 - Test Accuracy: 0.53846\n",
      "Epoch:    39, Train Error: 0.03510, Validation Error: 0.59133 - Test Accuracy: 0.51538\n",
      "Epoch:    40, Train Error: 0.03160, Validation Error: 0.64979 - Test Accuracy: 0.47692\n",
      "Epoch:    41, Train Error: 0.03485, Validation Error: 0.70704 - Test Accuracy: 0.44615\n",
      "Epoch:    42, Train Error: 0.02908, Validation Error: 0.58983 - Test Accuracy: 0.53077\n",
      "Epoch:    43, Train Error: 0.03199, Validation Error: 0.51742 - Test Accuracy: 0.53846\n",
      "Epoch:    44, Train Error: 0.04467, Validation Error: 0.57836 - Test Accuracy: 0.53846\n",
      "Epoch:    45, Train Error: 0.03464, Validation Error: 0.60360 - Test Accuracy: 0.53077\n",
      "Epoch:    46, Train Error: 0.03040, Validation Error: 0.55493 - Test Accuracy: 0.53077\n",
      "Epoch:    47, Train Error: 0.02868, Validation Error: 0.55626 - Test Accuracy: 0.53846\n",
      "Epoch:    48, Train Error: 0.02756, Validation Error: 0.51719 - Test Accuracy: 0.56154\n",
      "Epoch:    49, Train Error: 0.02625, Validation Error: 0.58413 - Test Accuracy: 0.52308\n",
      "consumed time :  326 Second.\n"
     ]
    }
   ],
   "source": [
    "import math\n",
    "import time\n",
    "\n",
    "(img_train, label_train), (img_validation, label_validation), (img_test, label_test) = load_dataset(\"/Users/do-hyungkwon/GoogleDrive/jupyter_notebook/imdata\")\n",
    "\n",
    "\n",
    "network = TwoLayerNet2(input_size=76800, hidden_layer1_size=128, hidden_layer2_size=128, output_size=3)\n",
    "\n",
    "num_epochs = 50\n",
    "train_size = img_train.shape[0]\n",
    "batch_size = 10\n",
    "learning_rate = 0.01\n",
    "\n",
    "train_error_list = []\n",
    "validation_error_list = []\n",
    "\n",
    "test_accuracy_list = []\n",
    "epoch_list = []\n",
    "\n",
    "num_batch = math.ceil(train_size / batch_size)\n",
    "\n",
    "start = time.time()\n",
    "for i in range(num_epochs):\n",
    "#     batch_mask = np.random.choice(train_size, batch_size)\n",
    "#     x_batch = img_train[batch_mask]\n",
    "#     t_batch = label_train[batch_mask]\n",
    "#     network.learning(learning_rate, x_batch, t_batch)\n",
    "    for j in range(num_batch):\n",
    "        x_batch = img_train[j * batch_size : j * batch_size + batch_size]\n",
    "        t_batch = label_train[j * batch_size : j * batch_size + batch_size]\n",
    "        network.learning(learning_rate, x_batch, t_batch)\n",
    "\n",
    "    epoch_list.append(i)\n",
    "    \n",
    "    train_loss = network.loss(x_batch, t_batch)\n",
    "    train_error_list.append(train_loss)\n",
    "    \n",
    "    validation_loss = network.loss(img_validation, label_validation)\n",
    "    validation_error_list.append(validation_loss)    \n",
    "    \n",
    "    test_accuracy = network.accuracy(img_test, label_test)\n",
    "    test_accuracy_list.append(test_accuracy)\n",
    "    \n",
    "    print(\"Epoch: {0:5d}, Train Error: {1:7.5f}, Validation Error: {2:7.5f} - Test Accuracy: {3:7.5f}\".format(\n",
    "        i,\n",
    "        train_loss,\n",
    "        validation_loss,\n",
    "        test_accuracy\n",
    "    ))\n",
    "end = time.time()\n",
    "print(\"consumed time : \", int(end-start), \"Second.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
