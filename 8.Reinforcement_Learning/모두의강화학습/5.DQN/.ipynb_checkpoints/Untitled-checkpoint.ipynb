{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/anaconda3/lib/python3.6/importlib/_bootstrap.py:219: RuntimeWarning: compiletime version 3.5 of module 'tensorflow.python.framework.fast_tensor_util' does not match runtime version 3.6\n",
      "  return f(*args, **kwds)\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "DQN (NIPS 2013)\n",
    "\n",
    "Playing Atari with Deep Reinforcement Learning\n",
    "https://www.cs.toronto.edu/~vmnih/docs/dqn.pdf\n",
    "\"\"\"\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import random\n",
    "import dqn\n",
    "import gym\n",
    "from collections import deque\n",
    "\n",
    "env = gym.make('CartPole-v0')\n",
    "env = gym.wrappers.Monitor(env, 'gym-results/', force=True)\n",
    "INPUT_SIZE = env.observation_space.shape[0]\n",
    "OUTPUT_SIZE = env.action_space.n\n",
    "\n",
    "DISCOUNT_RATE = 0.99\n",
    "REPLAY_MEMORY = 50000\n",
    "MAX_EPISODE = 5000\n",
    "BATCH_SIZE = 64\n",
    "\n",
    "# minimum epsilon for epsilon greedy\n",
    "MIN_E = 0.0\n",
    "# epsilon will be `MIN_E` at `EPSILON_DECAYING_EPISODE`\n",
    "EPSILON_DECAYING_EPISODE = MAX_EPISODE * 0.01\n",
    "\n",
    "\n",
    "def bot_play(mainDQN: dqn.DQN) -> None:\n",
    "    \"\"\"Runs a single episode with rendering and prints a reward\n",
    "\n",
    "    Args:\n",
    "        mainDQN (dqn.DQN): DQN Agent\n",
    "    \"\"\"\n",
    "    state = env.reset()\n",
    "    total_reward = 0\n",
    "\n",
    "    while True:\n",
    "        env.render()\n",
    "        action = np.argmax(mainDQN.predict(state))\n",
    "        state, reward, done, _ = env.step(action)\n",
    "        total_reward += reward\n",
    "        if done:\n",
    "            print(\"Total score: {}\".format(total_reward))\n",
    "            break\n",
    "\n",
    "\n",
    "def train_minibatch(DQN: dqn.DQN, train_batch: list) -> float:\n",
    "    \"\"\"Prepare X_batch, y_batch and train them\n",
    "\n",
    "    Recall our loss function is\n",
    "        target = reward + discount * max Q(s',a)\n",
    "                 or reward if done early\n",
    "\n",
    "        Loss function: [target - Q(s, a)]^2\n",
    "\n",
    "    Hence,\n",
    "\n",
    "        X_batch is a state list\n",
    "        y_batch is reward + discount * max Q\n",
    "                   or reward if terminated early\n",
    "\n",
    "    Args:\n",
    "        DQN (dqn.DQN): DQN Agent to train & run\n",
    "        train_batch (list): Minibatch of Replay memory\n",
    "            Eeach element is a tuple of (s, a, r, s', done)\n",
    "\n",
    "    Returns:\n",
    "        loss: Returns a loss\n",
    "\n",
    "    \"\"\"\n",
    "    state_array = np.vstack([x[0] for x in train_batch])\n",
    "    action_array = np.array([x[1] for x in train_batch])\n",
    "    reward_array = np.array([x[2] for x in train_batch])\n",
    "    next_state_array = np.vstack([x[3] for x in train_batch])\n",
    "    done_array = np.array([x[4] for x in train_batch])\n",
    "\n",
    "    X_batch = state_array\n",
    "    y_batch = DQN.predict(state_array)\n",
    "\n",
    "    Q_target = reward_array + DISCOUNT_RATE * np.max(DQN.predict(next_state_array), axis=1) * ~done_array\n",
    "    y_batch[np.arange(len(X_batch)), action_array] = Q_target\n",
    "\n",
    "    # Train our network using target and predicted Q values on each episode\n",
    "    loss, _ = DQN.update(X_batch, y_batch)\n",
    "\n",
    "    return loss\n",
    "\n",
    "\n",
    "def annealing_epsilon(episode: int, min_e: float, max_e: float, target_episode: int) -> float:\n",
    "    \"\"\"Return an linearly annealed epsilon\n",
    "\n",
    "    Epsilon will decrease over time until it reaches `target_episode`\n",
    "\n",
    "         (epsilon)\n",
    "             |\n",
    "    max_e ---|\\\n",
    "             | \\\n",
    "             |  \\\n",
    "             |   \\\n",
    "    min_e ---|____\\_______________(episode)\n",
    "                  |\n",
    "                 target_episode\n",
    "\n",
    "     slope = (min_e - max_e) / (target_episode)\n",
    "     intercept = max_e\n",
    "\n",
    "     e = slope * episode + intercept\n",
    "\n",
    "    Args:\n",
    "        episode (int): Current episode\n",
    "        min_e (float): Minimum epsilon\n",
    "        max_e (float): Maximum epsilon\n",
    "        target_episode (int): epsilon becomes the `min_e` at `target_episode`\n",
    "\n",
    "    Returns:\n",
    "        float: epsilon between `min_e` and `max_e`\n",
    "    \"\"\"\n",
    "\n",
    "    slope = (min_e - max_e) / (target_episode)\n",
    "    intercept = max_e\n",
    "\n",
    "    return max(min_e, slope * episode + intercept)\n",
    "\n",
    "\n",
    "def main():\n",
    "    # store the previous observations in replay memory\n",
    "    replay_buffer = deque(maxlen=REPLAY_MEMORY)\n",
    "    last_100_game_reward = deque(maxlen=100)\n",
    "\n",
    "    with tf.Session() as sess:\n",
    "        mainDQN = dqn.DQN(sess, INPUT_SIZE, OUTPUT_SIZE)\n",
    "        init = tf.global_variables_initializer()\n",
    "        sess.run(init)\n",
    "\n",
    "        for episode in range(MAX_EPISODE):\n",
    "            e = annealing_epsilon(episode, MIN_E, 1.0, EPSILON_DECAYING_EPISODE)\n",
    "            done = False\n",
    "            state = env.reset()\n",
    "\n",
    "            step_count = 0\n",
    "            while not done:\n",
    "\n",
    "                if np.random.rand() < e:\n",
    "                    action = env.action_space.sample()\n",
    "                else:\n",
    "                    action = np.argmax(mainDQN.predict(state))\n",
    "\n",
    "                next_state, reward, done, _ = env.step(action)\n",
    "\n",
    "                if done:\n",
    "                    reward = -1\n",
    "\n",
    "                replay_buffer.append((state, action, reward, next_state, done))\n",
    "\n",
    "                state = next_state\n",
    "                step_count += 1\n",
    "\n",
    "                if len(replay_buffer) > BATCH_SIZE:\n",
    "                    minibatch = random.sample(replay_buffer, BATCH_SIZE)\n",
    "                    train_minibatch(mainDQN, minibatch)\n",
    "\n",
    "            print(\"[Episode {:>5}]  steps: {:>5} e: {:>5.2f}\".format(episode, step_count, e))\n",
    "\n",
    "            # CartPole-v0 Game Clear Logic\n",
    "            last_100_game_reward.append(step_count)\n",
    "            if len(last_100_game_reward) == last_100_game_reward.maxlen:\n",
    "                avg_reward = np.mean(last_100_game_reward)\n",
    "                if avg_reward > 199.0:\n",
    "                    print(\"Game Cleared within {} episodes with avg reward {}\".format(episode, avg_reward))\n",
    "                    break\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
