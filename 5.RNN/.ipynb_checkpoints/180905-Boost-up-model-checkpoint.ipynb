{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/anaconda3/lib/python3.6/importlib/_bootstrap.py:219: RuntimeWarning: compiletime version 3.5 of module 'tensorflow.python.framework.fast_tensor_util' does not match runtime version 3.6\n",
      "  return f(*args, **kwds)\n",
      "/anaconda3/lib/python3.6/site-packages/h5py/__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n",
      "Using TensorFlow backend.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using Keras version: 2.2.2 backend: tensorflow\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/anaconda3/lib/python3.6/site-packages/sklearn/ensemble/weight_boosting.py:29: DeprecationWarning: numpy.core.umath_tests is an internal NumPy module and should not be imported. It will be removed in a future NumPy release.\n",
      "  from numpy.core.umath_tests import inner1d\n"
     ]
    }
   ],
   "source": [
    "# import library\n",
    "import tensorflow as tf\n",
    "\n",
    "import keras\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Dropout, Flatten, Activation\n",
    "from keras.layers import SimpleRNN, LSTM, GRU\n",
    "from keras import layers, models\n",
    "from keras.wrappers.scikit_learn import KerasClassifier\n",
    "from keras import backend as K\n",
    "from keras.utils import multi_gpu_model\n",
    "from keras import metrics\n",
    "from keras import losses\n",
    "from keras import __version__\n",
    "print('Using Keras version:', __version__, 'backend:', K.backend())\n",
    "# assert(LV(__version__) >= LV(\"2.0.0\"))\n",
    "from keras.backend.tensorflow_backend import set_session\n",
    "# config = tf.ConfigProto()\n",
    "# config.gpu_options.allow_growth = True\n",
    "# sess = tf.Session(config=config)\n",
    "# set_session(sess)\n",
    "# cfg = K.tf.ConfigProto()\n",
    "# cfg.gpu_options.allow_growth = True\n",
    "# K.set_session(K.tf.Session(config=cfg))\n",
    "\n",
    "# import os\n",
    "# os.environ[\"CUDA_DEVICE_ORDER\"]=\"PCI_BUS_ID\"\n",
    "# # The GPU id to use, usually either \"0\" or \"1\"\n",
    "# os.environ[\"CUDA_VISIBLE_DEVICES\"]=\"0\" \n",
    "\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "# from sklearn.pipeline import Pipeline\n",
    "from sklearn.preprocessing import MinMaxScaler, StandardScaler, PolynomialFeatures\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "from sklearn.metrics import precision_recall_fscore_support\n",
    "from sklearn.metrics import confusion_matrix, f1_score, precision_score, recall_score\n",
    "from sklearn.metrics import classification_report # https://stackoverflow.com/questions/50065484/getting-precision-recall-and-f1-score-per-class-in-keras\n",
    "\n",
    "from IPython.display import Javascript\n",
    "import numpy as np\n",
    "from distutils.version import LooseVersion as LV\n",
    "from datetime import datetime\n",
    "import csv\n",
    "import sys\n",
    "from os import listdir\n",
    "from os.path import isfile, join\n",
    "import pprint\n",
    "#import boto3\n",
    "import pickle\n",
    "import time\n",
    "import os.path\n",
    "import pickle\n",
    "sys.path.append(os.getcwd())\n",
    "from matplotlib import pyplot\n",
    "\n",
    "import smtplib\n",
    "from email.mime.text import MIMEText # simple MSG\n",
    "from email.mime.multipart import MIMEMultipart # complex MSG\n",
    "        \n",
    "from link_aws_key import *\n",
    "pp = pprint.PrettyPrinter(indent=4)\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "coins = {\n",
    "    0: 'KRW',\n",
    "    1: 'BTC',\n",
    "    2: 'ETH',\n",
    "    3: 'XRP',\n",
    "    4: 'BCH',\n",
    "    5: 'LTC',\n",
    "    6: 'DASH',\n",
    "    7: 'ETC'\n",
    "}\n",
    "\n",
    "# aws_client = boto3.client(\n",
    "#     's3',\n",
    "#     aws_access_key_id=LINK_AWSAccessKeyId,\n",
    "#     aws_secret_access_key=LINK_AWSSecretKey\n",
    "# )\n",
    "\n",
    "bucket = \"bithumb10\"\n",
    "cleanup_file_name = \"coin_{0}_{1}_cleanup.csv\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "def get_all_raw_data_from_aws(coin_name_list, start_date, end_date):\n",
    "    start_ms_time = datetime.strptime(start_date + \" +0900\", \"%Y-%m-%d %H:%M:%S %z\").timestamp() * 1000\n",
    "    end_ms_time = datetime.strptime(end_date + \" +0900\", \"%Y-%m-%d %H:%M:%S %z\").timestamp() * 1000\n",
    "    \n",
    "    year_temp = start_date[:4]\n",
    "    years = [year_temp]\n",
    "    while year_temp < end_date[:4]:\n",
    "        year_temp = str(int(start_date[:4]) + 1)\n",
    "        years.append(year_temp)\n",
    "    raw_data = {}  # 전체 CSV Raw 데이터\n",
    "    for coin_name in coin_name_list:\n",
    "        raw_data[coin_name] = []\n",
    "\n",
    "    # KRW 제외한 나머지 CSV Raw 데이터 수집\n",
    "    for coin_name in coin_name_list:\n",
    "        if coin_name == 'KRW':\n",
    "            continue\n",
    "        lines = []\n",
    "        for year in years:\n",
    "            obj = aws_client.get_object(\n",
    "                Bucket=bucket,\n",
    "                Key='cleanup/' + year + '/' + cleanup_file_name.format(coin_name, year)\n",
    "            )\n",
    "            if lines != []:\n",
    "                lines += obj.get('Body')._raw_stream.readlines()\n",
    "            else:\n",
    "                lines = obj.get('Body')._raw_stream.readlines()\n",
    "\n",
    "        for line in lines:\n",
    "            line = str(line.strip())[2:-1]\n",
    "            line = line.split(',')\n",
    "            if start_ms_time <= int(line[0]) and int(line[0]) <= end_ms_time:\n",
    "                raw_data[coin_name].append(line)\n",
    "\n",
    "    raw_data['KRW'] = list()\n",
    "    for line in raw_data['BTC']:\n",
    "        raw_data['KRW'].append([line[0], line[1], 1, 1, 1, 1, 1.0, 'normal'])\n",
    "\n",
    "    return raw_data\n",
    "\n",
    "def get_all_raw_data(coin_name_list, start_date, end_date, data_files_dir):\n",
    "    trading_files = []\n",
    "    for coin_name in coin_name_list:\n",
    "        for data_file_name in [f for f in listdir(data_files_dir) if isfile(join(data_files_dir, f))]:\n",
    "            if coin_name in data_file_name:\n",
    "                trading_files.append(data_file_name)\n",
    "\n",
    "    start_ms_time = datetime.strptime(start_date, \"%Y-%m-%d %H:%M:%S\").timestamp() * 1000\n",
    "    end_ms_time = datetime.strptime(end_date, \"%Y-%m-%d %H:%M:%S\").timestamp() * 1000\n",
    "    \n",
    "    raw_data = {} #전체 CSV Raw 데이터\n",
    "    for coin_name in coin_name_list:\n",
    "        raw_data[coin_name] = []\n",
    "    \n",
    "    #KRW 제외한 나머지 CSV Raw 데이터 수집\n",
    "    for coin_name in coin_name_list:\n",
    "        for data_file_name in trading_files:\n",
    "            if coin_name in data_file_name:\n",
    "                file = open(data_files_dir + data_file_name, 'r', encoding='utf-8')\n",
    "                rdr = csv.reader(file)\n",
    "                for line in rdr:\n",
    "                    if start_ms_time <= int(line[0]) and int(line[0]) <= end_ms_time:\n",
    "                        raw_data[coin_name].append(line)\n",
    "                file.close()\n",
    "    \n",
    "    for line in raw_data['BTC']:\n",
    "        raw_data['KRW'].append([line[0], line[1], 1, 1, 1, 1, 1.0, 'normal'])\n",
    "#     print(\"test\")\n",
    "    return raw_data\n",
    "\n",
    "def Make_Dataset(dir_path, coin_list, start_date, end_date, time_unit, window_size, gap, margin_rate):\n",
    "    print(\"Make_Dataset is Started.\")\n",
    "    a = time.time()\n",
    "\n",
    "    X = {}\n",
    "    y = {}\n",
    "    idx = []\n",
    "\n",
    "    key_name_X = \"X_\"\n",
    "    key_name_y = \"y_\"\n",
    "    for idx_time_unit in time_unit:\n",
    "        for idx_window_size in window_size:\n",
    "            for idx_gap in gap:\n",
    "                for idx_margin_rate in margin_rate:\n",
    "                    key_name_X += str(idx_time_unit) + \"_\" + str(idx_window_size) + \"_\" + str(idx_gap) + \"_\" + str(idx_margin_rate)\n",
    "                    key_name_y += str(idx_time_unit) + \"_\" + str(idx_window_size) + \"_\" + str(idx_gap) + \"_\" + str(idx_margin_rate)\n",
    "                    if (os.path.isfile(dir_path+key_name_X+\".pickle\")) is not True:\n",
    "                        print(key_name_X)\n",
    "                        X[key_name_X] = \\\n",
    "                        make_cryptocurrency_dataset(\n",
    "                                                    coin_list, \n",
    "                                                    start_date, \n",
    "                                                    end_date,\n",
    "                                                    idx_time_unit,\n",
    "                                                    idx_window_size,\n",
    "                                                    idx_gap,\n",
    "                                                    idx_margin_rate,\n",
    "                                                    data_files_dir)\n",
    "                        with open(dir_path + key_name_X+\".pickle\", 'wb') as handle:\n",
    "                            pickle.dump(X[key_name_X], handle, protocol=pickle.HIGHEST_PROTOCOL)\n",
    "                        X = {}\n",
    "                        print(\"X_success.\")\n",
    "                    if (os.path.isfile(dir_path + key_name_y+\".pickle\")) is not True:\n",
    "                        print(key_name_y)\n",
    "                        y[key_name_y] = \\\n",
    "                        make_cryptocurrency_dataset(\n",
    "                                                    coin_list, \n",
    "                                                    start_date, \n",
    "                                                    end_date,\n",
    "                                                    idx_time_unit,\n",
    "                                                    idx_window_size,\n",
    "                                                    idx_gap,\n",
    "                                                    idx_margin_rate,\n",
    "                                                    data_files_dir)\n",
    "                        with open(dir_path + key_name_y+\".pickle\", 'wb') as handle:\n",
    "                            pickle.dump(y[key_name_y], handle, protocol=pickle.HIGHEST_PROTOCOL)\n",
    "                        y = {}\n",
    "                        print(\"y_success.\")\n",
    "                    key_name_X = \"X_\"\n",
    "                    key_name_y = \"y_\"\n",
    "    b = time.time()\n",
    "    print(\"Make_Dataset is Done.\")\n",
    "    #print(\"time: \", b-a)\n",
    "\n",
    "def make_cryptocurrency_dataset(coin_name_list, start_date, end_date, time_unit, window_size, gap, margin_rate, data_files_dir):\n",
    "    y_trv = []\n",
    "    y_btv = []\n",
    "    num_coins = len(coin_name_list)\n",
    "    #raw_data = get_all_raw_data_from_aws(coin_name_list, start_date, end_date)\n",
    "    raw_data = get_all_raw_data(coin_name_list, start_date, end_date, data_files_dir)    \n",
    "    num_sticks = len(raw_data['BTC'])\n",
    "    \n",
    "    if time_unit % 10 != 0 or num_sticks < (time_unit / 10) * window_size + gap:\n",
    "        return None, None\n",
    "    \n",
    "    num = int(num_sticks - ((time_unit / 10) * window_size + gap) + 1)\n",
    "        \n",
    "    X = []\n",
    "    y = []\n",
    "    # (윈도우 개수, 코인 개수, 윈도우 사이즈, 3)\n",
    "    for idx in range(num):\n",
    "        X.append([])\n",
    "        y.append([])\n",
    "        idx_coin = 0\n",
    "        for coin_name in coin_name_list:\n",
    "            X[idx].append([])\n",
    "            \n",
    "            last_idx_in_window = int(idx + time_unit / 10 * window_size - 1)\n",
    "            close_price_in_last_idx_in_window = float(raw_data[coin_name][last_idx_in_window][3])\n",
    "            \n",
    "            for idx_in_window in range(window_size):\n",
    "                X[idx][idx_coin].append([])\n",
    "                idx_stick = int(idx + time_unit / 10 * (idx_in_window + 1) - 1)\n",
    "                X[idx][idx_coin][idx_in_window].append(float(raw_data[coin_name][idx_stick][3]) / close_price_in_last_idx_in_window)\n",
    "                X[idx][idx_coin][idx_in_window].append(float(raw_data[coin_name][idx_stick][4]) / close_price_in_last_idx_in_window)\n",
    "                X[idx][idx_coin][idx_in_window].append(float(raw_data[coin_name][idx_stick][5]) / close_price_in_last_idx_in_window)\n",
    "                X[idx][idx_coin][idx_in_window].append(float(raw_data[coin_name][idx_stick][6]))\n",
    "                \n",
    "            target_idx_for_window = int(idx + time_unit / 10 * window_size - 1 + gap)\n",
    "            target_price = float(raw_data[coin_name][target_idx_for_window][3])\n",
    "            \n",
    "            target = 0\n",
    "            if target_price >= close_price_in_last_idx_in_window * (1.0 + float(margin_rate) / 100.0):\n",
    "                target = 1\n",
    "            y[idx].append(target)\n",
    "            \n",
    "            idx_coin += 1\n",
    "           \n",
    "    X = np.asarray(X)\n",
    "    y = np.asarray(y)\n",
    "    \n",
    "    return X, y\n",
    "    \n",
    "def make_cryptocurrency_dataset_X(coin_name_list, start_date, end_date, time_unit, window_size, gap, margin_rate, data_files_dir):\n",
    "    num_coins = len(coin_name_list)\n",
    "    #raw_data = get_all_raw_data_from_aws(coin_name_list, start_date, end_date)\n",
    "    raw_data = get_all_raw_data(coin_name_list, start_date, end_date, data_files_dir)    \n",
    "    num_sticks = len(raw_data['BTC'])\n",
    "    \n",
    "    if time_unit % 10 != 0 or num_sticks < (time_unit / 10) * window_size + gap:\n",
    "        return None, None\n",
    "    \n",
    "    num = int(num_sticks - ((time_unit / 10) * window_size + gap) + 1)\n",
    "        \n",
    "    X = []\n",
    "    # (윈도우 개수, 코인 개수, 윈도우 사이즈, 3)\n",
    "    for idx in range(num):\n",
    "        X.append([])\n",
    "        idx_coin = 0\n",
    "        for coin_name in coin_name_list:\n",
    "            X[idx].append([])\n",
    "            \n",
    "            last_idx_in_window = int(idx + time_unit / 10 * window_size - 1)\n",
    "            close_price_in_last_idx_in_window = float(raw_data[coin_name][last_idx_in_window][3])\n",
    "            \n",
    "            for idx_in_window in range(window_size):\n",
    "                X[idx][idx_coin].append([])\n",
    "                idx_stick = int(idx + time_unit / 10 * (idx_in_window + 1) - 1)\n",
    "                X[idx][idx_coin][idx_in_window].append(float(raw_data[coin_name][idx_stick][3]) / close_price_in_last_idx_in_window)\n",
    "                X[idx][idx_coin][idx_in_window].append(float(raw_data[coin_name][idx_stick][4]) / close_price_in_last_idx_in_window)\n",
    "                X[idx][idx_coin][idx_in_window].append(float(raw_data[coin_name][idx_stick][5]) / close_price_in_last_idx_in_window)\n",
    "                X[idx][idx_coin][idx_in_window].append(float(raw_data[coin_name][idx_stick][6]))\n",
    "                \n",
    "    X = np.array(X)\n",
    "    \n",
    "    return X\n",
    "\n",
    "def make_cryptocurrency_dataset_y(coin_name_list, start_date, end_date, time_unit, window_size, gap, margin_rate, data_files_dir):\n",
    "    y_trv = []\n",
    "    y_btv = []\n",
    "    num_coins = len(coin_name_list)\n",
    "    #raw_data = get_all_raw_data_from_aws(coin_name_list, start_date, end_date)\n",
    "    raw_data = get_all_raw_data(coin_name_list, start_date, end_date, data_files_dir)    \n",
    "    num_sticks = len(raw_data['BTC'])\n",
    "    \n",
    "    if time_unit % 10 != 0 or num_sticks < (time_unit / 10) * window_size + gap:\n",
    "        return None, None\n",
    "    \n",
    "    num = int(num_sticks - ((time_unit / 10) * window_size + gap) + 1)\n",
    "        \n",
    "    \n",
    "    y = []\n",
    "    # (윈도우 개수, 코인 개수, 윈도우 사이즈, 3)\n",
    "    for idx in range(num):\n",
    "        \n",
    "        y.append([])\n",
    "        idx_coin = 0\n",
    "        for coin_name in coin_name_list:            \n",
    "            last_idx_in_window = int(idx + time_unit / 10 * window_size - 1)\n",
    "            close_price_in_last_idx_in_window = float(raw_data[coin_name][last_idx_in_window][3])\n",
    "            target_idx_for_window = int(idx + time_unit / 10 * window_size - 1 + gap)\n",
    "            target_price = float(raw_data[coin_name][target_idx_for_window][3])\n",
    "            target = 0\n",
    "            \n",
    "            if target_price >= close_price_in_last_idx_in_window * (1.0 + float(margin_rate) / 100.0):\n",
    "                target = 1\n",
    "            y[idx].append(target)\n",
    "            \n",
    "            idx_coin += 1\n",
    "           \n",
    "    y = np.asarray(y)\n",
    "    \n",
    "    return y\n",
    "\n",
    "#\n",
    "\n",
    "def Load_Dataset_X(dir_path, time_unit, window_size, gap, margin_rate):\n",
    "    key_name_X = \"X_\" + \\\n",
    "                    str(time_unit) + \"_\" + \\\n",
    "                    str(window_size) + \"_\" + \\\n",
    "                    str(gap) + \"_\" + \\\n",
    "                    str(margin_rate)\n",
    "                    \n",
    "\n",
    "    with open(dir_path + key_name_X + \".pickle\", 'rb') as handle:\n",
    "        b_x = pickle.load(handle)\n",
    "    return b_x\n",
    "    \n",
    "def Load_Dataset_y(dir_path, time_unit, window_size, gap, margin_rate):\n",
    "    key_name_y = \"y_\" + \\\n",
    "                    str(time_unit) + \"_\" + \\\n",
    "                    str(window_size) + \"_\" + \\\n",
    "                    str(gap) + \"_\" + \\\n",
    "                    str(margin_rate)\n",
    "                    \n",
    "\n",
    "    with open(dir_path + key_name_y + \".pickle\", 'rb') as handle:\n",
    "        b_y = pickle.load(handle)\n",
    "    return b_y\n",
    "    \n",
    "def Make_Dataset_numpy(dir_path, coin_list, start_date, end_date, time_unit, window_size, gap, margin_rate, data_files_dir):\n",
    "    print(\"Make_Dataset_numpy is Started.\")\n",
    "    a = time.time()\n",
    "\n",
    "    X = {}\n",
    "    y = {}\n",
    "    idx = []\n",
    "\n",
    "    key_name_X = \"X_\"\n",
    "    key_name_y = \"y_\"\n",
    "    for idx_time_unit in time_unit:\n",
    "        for idx_window_size in window_size:\n",
    "            for idx_gap in gap:\n",
    "                for idx_margin_rate in margin_rate:\n",
    "                    key_name_X += str(idx_time_unit) + \"_\" + str(idx_window_size) + \"_\" + str(idx_gap) + \"_\" + str(idx_margin_rate)\n",
    "                    key_name_y += str(idx_time_unit) + \"_\" + str(idx_window_size) + \"_\" + str(idx_gap) + \"_\" + str(idx_margin_rate)\n",
    "                    if (os.path.isfile(dir_path + key_name_X+\".pickle\")) is not True:\n",
    "                        print(key_name_X)\n",
    "                        X[key_name_X] = \\\n",
    "                        make_cryptocurrency_dataset_X(\n",
    "                                                    coin_list, \n",
    "                                                    start_date, \n",
    "                                                    end_date,\n",
    "                                                    idx_time_unit,\n",
    "                                                    idx_window_size,\n",
    "                                                    idx_gap,\n",
    "                                                    idx_margin_rate,\n",
    "                                                    data_files_dir)\n",
    "                        with open(dir_path + key_name_X+\".pickle\", 'wb') as handle:\n",
    "                            pickle.dump(X[key_name_X], handle, protocol=pickle.HIGHEST_PROTOCOL)\n",
    "                        X = {}\n",
    "                        print(\"X_success.\")\n",
    "                    if (os.path.isfile(dir_path+key_name_y+\".pickle\")) is not True:\n",
    "                        print(key_name_y)\n",
    "                        y[key_name_y] = \\\n",
    "                        make_cryptocurrency_dataset_y(\n",
    "                                                    coin_list, \n",
    "                                                    start_date, \n",
    "                                                    end_date,\n",
    "                                                    idx_time_unit,\n",
    "                                                    idx_window_size,\n",
    "                                                    idx_gap,\n",
    "                                                    idx_margin_rate,\n",
    "                                                    data_files_dir)\n",
    "                        with open(dir_path + key_name_y+\".pickle\", 'wb') as handle:\n",
    "                            pickle.dump(y[key_name_y], handle, protocol=pickle.HIGHEST_PROTOCOL)\n",
    "                        y = {}\n",
    "                        print(\"y_success.\")\n",
    "                    key_name_X = \"X_\"\n",
    "                    key_name_y = \"y_\"\n",
    "    b = time.time()\n",
    "    print(\"Make_Dataset_numpy is Done.\")\n",
    "    print()\n",
    "    #print(\"time: \", b-a)\n",
    "    \n",
    "    \n",
    "def Make_Dataset_tuple(dir_path, coin_list, start_date, end_date, time_unit, window_size, gap, margin_rate, data_files_dir):\n",
    "    print(\"Make_Dataset_tuple is Started.\")\n",
    "    a = time.time()\n",
    "\n",
    "    X = {}\n",
    "    y = {}\n",
    "    idx = []\n",
    "\n",
    "    key_name_X = \"X_\"\n",
    "    key_name_y = \"y_\"\n",
    "    for idx_time_unit in time_unit:\n",
    "        for idx_window_size in window_size:\n",
    "            for idx_gap in gap:\n",
    "                for idx_margin_rate in margin_rate:\n",
    "                    key_name_X += str(idx_time_unit) + \"_\" + str(idx_window_size) + \"_\" + str(idx_gap) + \"_\" + str(idx_margin_rate)\n",
    "                    key_name_y += str(idx_time_unit) + \"_\" + str(idx_window_size) + \"_\" + str(idx_gap) + \"_\" + str(idx_margin_rate)\n",
    "                    if (os.path.isfile(dir_path + key_name_X+\".pickle\")) is not True:\n",
    "                        print(key_name_X)\n",
    "                        X[key_name_X], _ = \\\n",
    "                        make_cryptocurrency_dataset(\n",
    "                                                    coin_list, \n",
    "                                                    start_date, \n",
    "                                                    end_date,\n",
    "                                                    idx_time_unit,\n",
    "                                                    idx_window_size,\n",
    "                                                    idx_gap,\n",
    "                                                    idx_margix_rate,\n",
    "                                                    data_files_dir)\n",
    "                        with open(dir_path + key_name_X+\".pickle\", 'wb') as handle:\n",
    "                            pickle.dump(X[key_name_X], handle, protocol=pickle.HIGHEST_PROTOCOL)\n",
    "                        X = {}\n",
    "                        print(\"X_success.\")\n",
    "                    if (os.path.isfile(dir_path+key_name_y+\".pickle\")) is not True:\n",
    "                        print(key_name_y)\n",
    "                        _, y[key_name_y] = \\\n",
    "                        make_cryptocurrency_dataset(\n",
    "                                                    coin_list, \n",
    "                                                    start_date, \n",
    "                                                    end_date,\n",
    "                                                    idx_time_unit,\n",
    "                                                    idx_window_size,\n",
    "                                                    idx_gap,\n",
    "                                                    idx_margix_rate,\n",
    "                                                    data_files_dir)\n",
    "                        with open(dir_path + key_name_y+\".pickle\", 'wb') as handle:\n",
    "                            pickle.dump(y[key_name_y], handle, protocol=pickle.HIGHEST_PROTOCOL)\n",
    "                        y = {}\n",
    "                        print(\"y_success.\")\n",
    "                    key_name_X = \"X_\"\n",
    "                    key_name_y = \"y_\"\n",
    "    b = time.time()\n",
    "    print(\"Make_Dataset_tuple is Done.\")\n",
    "    print()\n",
    "    #print(\"time: \", b-a)\n",
    "\n",
    "def Check_Dataset(dir_path, coin_list, start_date, end_date, time_unit, window_size, gap, margin_rate, data_files_dir):\n",
    "    print(\"Check_Dataset is Started.\")\n",
    "    a = time.time()\n",
    "\n",
    "    X = {}\n",
    "    y = {}\n",
    "    idx = []\n",
    "\n",
    "    key_name_X = \"X_\"\n",
    "    key_name_y = \"y_\"\n",
    "    for idx_time_unit in time_unit:\n",
    "        for idx_window_size in window_size:\n",
    "            for idx_gap in gap:\n",
    "                for idx_margix_rate in margin_rate:\n",
    "                    key_name_X += str(idx_time_unit) + \"_\" + str(idx_window_size) + \"_\" + str(idx_gap) + \"_\" + str(idx_margix_rate)\n",
    "                    key_name_y += str(idx_time_unit) + \"_\" + str(idx_window_size) + \"_\" + str(idx_gap) + \"_\" + str(idx_margix_rate)\n",
    "                    \n",
    "                    with open(dir_path + key_name_X+\".pickle\", 'rb') as handle:\n",
    "                        data = pickle.load(handle)\n",
    "    \n",
    "                    if type(data) == tuple:\n",
    "#                         print()\n",
    "                        os.system('rm '+dir_path + key_name_X+\".pickle\")\n",
    "                        print(key_name_X,\".pickle is removed.\")\n",
    "                        os.system('rm '+dir_path + key_name_y+\".pickle\")\n",
    "                        print(key_name_y,\".pickle is removed.\")\n",
    "        \n",
    "                    if (os.path.isfile(dir_path+key_name_X+\".pickle\")) is not True:\n",
    "                        print(key_name_X)\n",
    "                        X[key_name_X] = \\\n",
    "                        make_cryptocurrency_dataset_X(\n",
    "                                                    coin_list, \n",
    "                                                    start_date, \n",
    "                                                    end_date,\n",
    "                                                    idx_time_unit,\n",
    "                                                    idx_window_size,\n",
    "                                                    idx_gap,\n",
    "                                                    idx_margix_rate,\n",
    "                                                    data_files_dir)\n",
    "                        with open(dir_path + key_name_X+\".pickle\", 'wb') as handle:\n",
    "                            pickle.dump(X[key_name_X], handle, protocol=pickle.HIGHEST_PROTOCOL)\n",
    "                        X = {}\n",
    "                        print(\"[SUCCESS] \",key_name_X,\".pickle is created.\")\n",
    "                    if (os.path.isfile(dir_path+key_name_y+\".pickle\")) is not True:\n",
    "                        print(key_name_y)\n",
    "                        y[key_name_y] = \\\n",
    "                        make_cryptocurrency_dataset_y(\n",
    "                                                    coin_list, \n",
    "                                                    start_date, \n",
    "                                                    end_date,\n",
    "                                                    idx_time_unit,\n",
    "                                                    idx_window_size,\n",
    "                                                    idx_gap,\n",
    "                                                    idx_margix_rate,\n",
    "                                                    data_files_dir)\n",
    "                        with open(dir_path + key_name_y+\".pickle\", 'wb') as handle:\n",
    "                            pickle.dump(y[key_name_y], handle, protocol=pickle.HIGHEST_PROTOCOL)\n",
    "                        y = {}\n",
    "                        print(\"[SUCCESS] \",key_name_y,\".pickle is created.\")\n",
    "                    key_name_X = \"X_\"\n",
    "                    key_name_y = \"y_\"\n",
    "    b = time.time()\n",
    "    print(\"Check_Dataset is Done.\")\n",
    "    #print(\"time: \", b-a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create Dataset\n",
    "data_files_dir = \"/Users/dohyung/OneDrive/2018-RNN/RNN_python/AWS_dataset/\"\n",
    "dataset_dir_path_tuple_type = \"./dataset_pickle_tuple_type/\"\n",
    "dataset_dir_path_numpy_type = \"./dataset_pickle_numpy.ndarray_type)/\"\n",
    "coin_list = [\"KRW\", \"BTC\", \"ETH\", \"XRP\", \"BCH\", \"LTC\", \"DASH\", \"ETC\"]\n",
    "start_date = \"2017-08-04 21:40:00\"\n",
    "end_date = \"2018-08-20 23:50:00\"\n",
    "# time_unit = [10,30,60]     # candle stick minutes\n",
    "# window_size = [10,25,50,75,100]  # Unit: num. of candle sticks\n",
    "# gap = [1,2,3]            # Unit: num. of candle sticks\n",
    "# margin_rate = [0.1,0.25,0.5]  # Unit: percent\n",
    "\n",
    "# for slave04\n",
    "#time_unit = [10,30,60]     # candle stick minutes\n",
    "#window_size = [10,25,50]  # Unit: num. of candle sticks\n",
    "#gap = [1,2,3]            # Unit: num. of candle sticks\n",
    "#margin_rate = [0.1,0.25,0.5]  # Unit: percent\n",
    "\n",
    "# for slave05\n",
    "# time_unit = [10,30,60]     # candle stick minutes\n",
    "# window_size = [75,100]  # Unit: num. of candle sticks\n",
    "# gap = [1,2,3]            # Unit: num. of candle sticks\n",
    "# margin_rate = [0.1,0.25,0.5]  # Unit: percent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#Make_Dataset_tuple(dataset_dir_path_tuple_type, coin_list, start_date, end_date, time_unit, window_size, gap, margin_rate)\n",
    "#Make_Dataset_numpy(dataset_dir_path_numpy_type, coin_list, start_date, end_date, time_unit, window_size, gap, margin_rate)\n",
    "#Check_Dataset(dataset_dir_path, coin_list, start_date, end_date, time_unit, window_size, gap, margin_rate)\n",
    "# Tuple 형태의 데이터셋이 나오지 않도록."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_history(history):\n",
    "    loss_list = [s for s in history.history.keys() if 'loss' in s and 'val' not in s]\n",
    "    val_loss_list = [s for s in history.history.keys() if 'loss' in s and 'val' in s]\n",
    "    acc_list = [s for s in history.history.keys() if 'acc' in s and 'val' not in s]\n",
    "    val_acc_list = [s for s in history.history.keys() if 'acc' in s and 'val' in s]\n",
    "    \n",
    "    if len(loss_list) == 0:\n",
    "        print('Loss is missing in history')\n",
    "        return \n",
    "    \n",
    "    ## As loss always exists\n",
    "    epochs = range(1,len(history.history[loss_list[0]]) + 1)\n",
    "    \n",
    "    ## Loss\n",
    "    plt.figure(1)\n",
    "    for l in loss_list:\n",
    "        plt.plot(epochs, history.history[l], 'b', label='Training loss (' + str(str(format(history.history[l][-1],'.5f'))+')'))\n",
    "    for l in val_loss_list:\n",
    "        plt.plot(epochs, history.history[l], 'g', label='Validation loss (' + str(str(format(history.history[l][-1],'.5f'))+')'))\n",
    "    \n",
    "    plt.title('Loss')\n",
    "    plt.xlabel('Epochs')\n",
    "    plt.ylabel('Loss')\n",
    "    plt.legend()\n",
    "    \n",
    "    ## Accuracy\n",
    "    plt.figure(2)\n",
    "    for l in acc_list:\n",
    "        plt.plot(epochs, history.history[l], 'b', label='Training accuracy (' + str(format(history.history[l][-1],'.5f'))+')')\n",
    "    for l in val_acc_list:    \n",
    "        plt.plot(epochs, history.history[l], 'g', label='Validation accuracy (' + str(format(history.history[l][-1],'.5f'))+')')\n",
    "\n",
    "    plt.title('Accuracy')\n",
    "    plt.xlabel('Epochs')\n",
    "    plt.ylabel('Accuracy')\n",
    "    plt.legend()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def f1_score(y_true, y_pred):\n",
    "    def recall(y_true, y_pred):\n",
    "        \"\"\"Recall metric.\n",
    "\n",
    "        Only computes a batch-wise average of recall.\n",
    "\n",
    "        Computes the recall, a metric for multi-label classification of\n",
    "        how many relevant items are selected.\n",
    "        \"\"\"\n",
    "        true_positives = K.sum(K.round(K.clip(y_true * y_pred, 0, 1)))\n",
    "        possible_positives = K.sum(K.round(K.clip(y_true, 0, 1)))\n",
    "        recall = true_positives / (possible_positives + K.epsilon())\n",
    "        return recall\n",
    "\n",
    "    def precision(y_true, y_pred):\n",
    "        \"\"\"Precision metric.\n",
    "\n",
    "        Only computes a batch-wise average of precision.\n",
    "\n",
    "        Computes the precision, a metric for multi-label classification of\n",
    "        how many selected items are relevant.\n",
    "        \"\"\"\n",
    "        true_positives = K.sum(K.round(K.clip(y_true * y_pred, 0, 1)))\n",
    "        predicted_positives = K.sum(K.round(K.clip(y_pred, 0, 1)))\n",
    "        precision = true_positives / (predicted_positives + K.epsilon())\n",
    "        return precision\n",
    "    precision = precision(y_true, y_pred)\n",
    "    recall = recall(y_true, y_pred)\n",
    "    return 2*((precision*recall)/(precision+recall+K.epsilon()))\n",
    "\n",
    "def f1_score_(y_true, y_pred):\n",
    "\n",
    "    # Count positive samples.\n",
    "    c1 = K.sum(K.round(K.clip(y_true * y_pred, 0, 1)))\n",
    "    c2 = K.sum(K.round(K.clip(y_pred, 0, 1)))\n",
    "    c3 = K.sum(K.round(K.clip(y_true, 0, 1)))\n",
    "\n",
    "    # If there are no true samples, fix the F1 score at 0.\n",
    "    if c3 == 0:\n",
    "        return 0\n",
    "\n",
    "    # How many selected items are relevant?\n",
    "    precision = c1 / (c2 + 1e-7)\n",
    "    \n",
    "    # How many relevant items are selected?\n",
    "    recall = c1 / (c3 +  + 1e-7)\n",
    "\n",
    "    # Calculate f1_score\n",
    "    f1_score = 2 * (precision * recall) / ((precision + recall) + 1e-7)\n",
    "    return f1_score \n",
    "\n",
    "\n",
    "def create_model_RNN(window_size, n_state_units=32, activation='softmax', optimizer='adam', init='glorot_uniform', dropout_rate=0.0, neurons=2):\n",
    "    model = Sequential()\n",
    "    model.add(# if문을 통해 여러 RNN모델 쓸 수 있도록 하기, SimpleRNN외에 다른 RNN모델 찾아보기\n",
    "        RNN(  n_state_units, \n",
    "              input_shape=(window_size, 32),\n",
    "              use_bias=True, \n",
    "              activation='tanh',\n",
    "              kernel_initializer='glorot_uniform', \n",
    "              recurrent_initializer='orthogonal', \n",
    "              bias_initializer='zeros', \n",
    "              dropout=0.0,\n",
    "              recurrent_dropout=0.0))\n",
    "    \n",
    "    model.add(Dense(units=neurons))\n",
    "    model.add(Dropout(dropout_rate))\n",
    "#     model.add(Dense(units=2))\n",
    "    model = multi_gpu_model(model, gpus=2)\n",
    "\n",
    "    model.compile(loss=keras.losses.categorical_crossentropy, \n",
    "                  optimizer=optimizer, \n",
    "                  metrics=[\"accuracy\", f1_score])\n",
    "\n",
    "    return model\n",
    "\n",
    "\n",
    "def create_model_SimpleRNN(window_size, n_state_units=32, activation='softmax', optimizer='adam', init='glorot_uniform', dropout_rate=0.0, neurons=2):\n",
    "    model = Sequential()\n",
    "    model.add(# if문을 통해 여러 RNN모델 쓸 수 있도록 하기, SimpleRNN외에 다른 RNN모델 찾아보기\n",
    "        SimpleRNN(n_state_units, \n",
    "                  input_shape=(window_size, 32),\n",
    "                  use_bias=True, \n",
    "                  activation='tanh',\n",
    "                  kernel_initializer='glorot_uniform', \n",
    "                  recurrent_initializer='orthogonal', \n",
    "                  bias_initializer='zeros', \n",
    "                  dropout=0.0,\n",
    "                  recurrent_dropout=0.0))\n",
    "    \n",
    "    model.add(Dense(units=neurons))\n",
    "    model.add(Dropout(dropout_rate))\n",
    "#     model.add(Dense(units=2))\n",
    "    model = multi_gpu_model(model, gpus=2)\n",
    "\n",
    "    model.compile(loss=keras.losses.categorical_crossentropy, \n",
    "                  optimizer=optimizer, \n",
    "                  metrics=[\"accuracy\", f1_score])\n",
    "        \n",
    "    return model\n",
    "\n",
    "def create_model_LSTM(window_size, n_state_units=32, activation='softmax', optimizer='adam', init='glorot_uniform', dropout_rate=0.0, neurons=2):\n",
    "    model = Sequential()\n",
    "    model.add(# if문을 통해 여러 RNN모델 쓸 수 있도록 하기, SimpleRNN외에 다른 RNN모델 찾아보기\n",
    "        LSTM( n_state_units, \n",
    "              input_shape=(window_size, 32),\n",
    "              use_bias=True, \n",
    "              activation='tanh',\n",
    "              kernel_initializer='glorot_uniform', \n",
    "              recurrent_initializer='orthogonal', \n",
    "              bias_initializer='zeros', \n",
    "              dropout=0.0,\n",
    "              recurrent_dropout=0.0))\n",
    "    \n",
    "    model.add(Dense(units=neurons))\n",
    "    model.add(Dropout(dropout_rate))\n",
    "#     model.add(Dense(units=2))\n",
    "    model = multi_gpu_model(model, gpus=2)\n",
    "\n",
    "    model.compile(loss=keras.losses.categorical_crossentropy, \n",
    "                  optimizer=optimizer, \n",
    "                  metrics=[\"accuracy\", f1_score])\n",
    "    \n",
    "    return model\n",
    "\n",
    "\n",
    "def create_model_GRU(window_size, n_state_units=32, activation='softmax', optimizer='adam', init='glorot_uniform', dropout_rate=0.0, neurons=2):\n",
    "    model = Sequential()\n",
    "    model.add(# if문을 통해 여러 RNN모델 쓸 수 있도록 하기, SimpleRNN외에 다른 RNN모델 찾아보기\n",
    "        GRU(  n_state_units, \n",
    "              input_shape=(window_size, 32),\n",
    "              use_bias=True, \n",
    "              activation='tanh',\n",
    "              kernel_initializer='glorot_uniform', \n",
    "              recurrent_initializer='orthogonal', \n",
    "              bias_initializer='zeros', \n",
    "              dropout=0.0,\n",
    "              recurrent_dropout=0.0))\n",
    "    \n",
    "    model.add(Dense(units=neurons))\n",
    "    model.add(Dropout(dropout_rate))\n",
    "#     model.add(Dense(units=2))\n",
    "    model = multi_gpu_model(model, gpus=2)\n",
    "\n",
    "    model.compile(loss=keras.losses.categorical_crossentropy, \n",
    "                  optimizer=optimizer, \n",
    "                  metrics=[\"accuracy\", f1_score])\n",
    "\n",
    "    return model\n",
    "\n",
    "\n",
    "def create_model_RNN_non_GPU(window_size, n_state_units=32, activation='softmax', optimizer='adam', init='glorot_uniform', dropout_rate=0.0, neurons=2):\n",
    "    model = Sequential()\n",
    "    model.add(# if문을 통해 여러 RNN모델 쓸 수 있도록 하기, SimpleRNN외에 다른 RNN모델 찾아보기\n",
    "        RNN(  n_state_units, \n",
    "              input_shape=(window_size, 32),\n",
    "              use_bias=True, \n",
    "              activation='tanh',\n",
    "              kernel_initializer='glorot_uniform', \n",
    "              recurrent_initializer='orthogonal', \n",
    "              bias_initializer='zeros', \n",
    "              dropout=0.0,\n",
    "              recurrent_dropout=0.0))\n",
    "    \n",
    "    model.add(Dense(units=neurons))\n",
    "    model.add(Dropout(dropout_rate))\n",
    "#     model.add(Dense(units=2))\n",
    "#     model = multi_gpu_model(model, gpus=2)\n",
    "\n",
    "    model.compile(loss=keras.losses.categorical_crossentropy, \n",
    "                  optimizer=optimizer, \n",
    "                  metrics=[\"accuracy\", f1_score])\n",
    "\n",
    "    return model\n",
    "\n",
    "\n",
    "def create_model_SimpleRNN_non_GPU(window_size, n_state_units=32, activation='softmax', optimizer='adam', init='glorot_uniform', dropout_rate=0.0, neurons=2):\n",
    "    model = Sequential()\n",
    "    model.add(# if문을 통해 여러 RNN모델 쓸 수 있도록 하기, SimpleRNN외에 다른 RNN모델 찾아보기\n",
    "        SimpleRNN(n_state_units, \n",
    "                  input_shape=(window_size, 32),\n",
    "                  use_bias=True, \n",
    "                  activation='tanh',\n",
    "                  kernel_initializer='glorot_uniform', \n",
    "                  recurrent_initializer='orthogonal', \n",
    "                  bias_initializer='zeros', \n",
    "                  dropout=0.0,\n",
    "                  recurrent_dropout=0.0))\n",
    "    \n",
    "    model.add(Dense(units=neurons))\n",
    "    model.add(Dropout(dropout_rate))\n",
    "#     model.add(Dense(units=2))\n",
    "#     model = multi_gpu_model(model, gpus=2)\n",
    "\n",
    "    model.compile(loss=keras.losses.categorical_crossentropy, \n",
    "                  optimizer=optimizer, \n",
    "                  metrics=[\"accuracy\", f1_score])\n",
    "\n",
    "    return model\n",
    "\n",
    "def create_model_LSTM_non_GPU(window_size, n_state_units=32, activation='softmax', optimizer='adam', init='glorot_uniform', dropout_rate=0.0, neurons=2):\n",
    "    model = Sequential()\n",
    "    model.add(# if문을 통해 여러 RNN모델 쓸 수 있도록 하기, SimpleRNN외에 다른 RNN모델 찾아보기\n",
    "        LSTM( n_state_units, \n",
    "              input_shape=(window_size, 32),\n",
    "              use_bias=True, \n",
    "              activation='tanh',\n",
    "              kernel_initializer='glorot_uniform', \n",
    "              recurrent_initializer='orthogonal', \n",
    "              bias_initializer='zeros', \n",
    "              dropout=0.0,\n",
    "              recurrent_dropout=0.0))\n",
    "    \n",
    "    model.add(Dense(units=neurons))\n",
    "    model.add(Dropout(dropout_rate))\n",
    "#     model.add(Dense(units=2))\n",
    "#     model = multi_gpu_model(model, gpus=2)\n",
    "\n",
    "    model.compile(loss=keras.losses.categorical_crossentropy, \n",
    "                  optimizer=optimizer, \n",
    "                  metrics=[\"accuracy\", f1_score])\n",
    "\n",
    "    return model\n",
    "\n",
    "\n",
    "def create_model_GRU_non_GPU(window_size, n_state_units=32, activation='softmax', optimizer='adam', init='glorot_uniform', dropout_rate=0.0, neurons=2):\n",
    "    model = Sequential()\n",
    "    model.add(# if문을 통해 여러 RNN모델 쓸 수 있도록 하기, SimpleRNN외에 다른 RNN모델 찾아보기\n",
    "        GRU(  n_state_units, \n",
    "              input_shape=(window_size, 32),\n",
    "              use_bias=True, \n",
    "              activation='tanh',\n",
    "              kernel_initializer='glorot_uniform', \n",
    "              recurrent_initializer='orthogonal', \n",
    "              bias_initializer='zeros', \n",
    "              dropout=0.0,\n",
    "              recurrent_dropout=0.0))\n",
    "    \n",
    "    model.add(Dense(units=neurons))\n",
    "    model.add(Dropout(dropout_rate))\n",
    "#     model.add(Dense(units=2))\n",
    "#     model = multi_gpu_model(model, gpus=2)\n",
    "\n",
    "    model.compile(loss=keras.losses.categorical_crossentropy, \n",
    "                  optimizer=optimizer, \n",
    "                  metrics=[\"accuracy\", f1_score])\n",
    "\n",
    "    return model\n",
    "\n",
    "\n",
    "\n",
    "def input_reshape(X_train_data, X_test_data, n_steps, n_coins, n_price):\n",
    "    X_train_reshape = X_train_data.reshape(\n",
    "        -1, \n",
    "        n_steps, \n",
    "        n_coins * n_price\n",
    "    )\n",
    "    X_test_reshape = X_test_data.reshape(\n",
    "        -1, \n",
    "        n_steps, \n",
    "        n_coins * n_price\n",
    "    )\n",
    "    return X_train_reshape, X_test_reshape\n",
    "\n",
    "def onehottify(x, n=None, dtype=np.int):\n",
    "    \"\"\"1-hot encode x with the max value n (computed from data if n is None).\"\"\"\n",
    "    x = np.asarray(x)\n",
    "    n = np.max(x) + 1 if n is None else n\n",
    "    return np.eye(n, dtype=dtype)[x]\n",
    "\n",
    "def Start_Model(pickle_load_dir_path, data_files_dir, epochs, pickle_result_dir_path, boost_up_result_dir_path, MODEL, idx_time_unit, idx_window_size, idx_gap, idx_margin_rate, _TEST, _ENHANCE, _GPU, n_jobs, machine, Internet_connection, params):\n",
    "    X = {}\n",
    "    y = {}\n",
    "    #if (_TEST == True): \n",
    "        #MODEL_list = [\"SimpleRNN\", \"LSTM\", \"GRU\"]\n",
    "    #    time_unit = [10]     # candle stick minutes\n",
    "    #    window_size = [10]  # Unit: num. of candle sticks\n",
    "    #    gap = [1]            # Unit: num. of candle sticks\n",
    "    #    margin_rate = [0.1]  # Unit: percent\n",
    "    #elif (_TEST == False):\n",
    "        #MODEL_list = [\"SimpleRNN\", \"LSTM\", \"GRU\"]\n",
    "\n",
    "    key_name_X = \"X_\"\n",
    "    key_name_y = \"y_\"\n",
    "\n",
    "    #for MODEL_idx in MODEL_list:\n",
    "        #MODEL = MODEL_idx\n",
    "        #for idx_time_unit in time_unit:\n",
    "                #for idx_window_size in window_size:\n",
    "                    #for idx_gap in gap:\n",
    "                        #for idx_margin_rate in margin_rate:\n",
    "    key_name_X += str(idx_time_unit) + \"_\" + str(idx_window_size) + \"_\" + str(idx_gap) + \"_\" + str(idx_margin_rate)\n",
    "    key_name_y += str(idx_time_unit) + \"_\" + str(idx_window_size) + \"_\" + str(idx_gap) + \"_\" + str(idx_margin_rate)\n",
    "\n",
    "    X = Load_Dataset_X(pickle_load_dir_path, idx_time_unit, idx_window_size, idx_gap, idx_margin_rate)\n",
    "    y = Load_Dataset_y(pickle_load_dir_path, idx_time_unit, idx_window_size, idx_gap, idx_margin_rate)\n",
    "\n",
    "    y_single = {}\n",
    "    y_single['BTC'] = y[:, 1]\n",
    "    y_single['ETH'] = y[:, 2]\n",
    "    y_single['XRP'] = y[:, 3]\n",
    "    y_single['BCH'] = y[:, 4]\n",
    "    y_single['LTC'] = y[:, 5]\n",
    "    y_single['DASH'] = y[:, 6]\n",
    "    y_single['ETC'] = y[:, 7]\n",
    "\n",
    "    coin_list2 = [\"BTC\", \"ETH\", \"XRP\", \"BCH\", \"LTC\", \"DASH\", \"ETC\"]\n",
    "\n",
    "#                         for coin in coin_list2:\n",
    "#                             print(\"y_single[\"+coin+\"]\"+\".shape\")\n",
    "#                             print(y_single[coin].shape)\n",
    "#                             print()\n",
    "\n",
    "\n",
    "    if (_TEST == False and _ENHANCE == False):\n",
    "        for coin in coin_list2:\n",
    "            if (os.path.isfile(pickle_result_dir_path + \\\n",
    "                              MODEL + \"_\" + \\\n",
    "                              coin + \"_\" + \\\n",
    "                              str(idx_time_unit) + \"_\" + \\\n",
    "                              str(idx_window_size) + \"_\" + \\\n",
    "                              str(idx_gap) + \"_\" + \\\n",
    "                              str(idx_margin_rate) + \\\n",
    "                              \"_result.pickle\")) is True:\n",
    "                print(MODEL + \"_\" + \\\n",
    "                      coin + \"_\" + \\\n",
    "                      str(idx_time_unit) + \"_\" + \\\n",
    "                      str(idx_window_size) + \"_\" + \\\n",
    "                      str(idx_gap) + \"_\" + \\\n",
    "                      str(idx_margin_rate) + \\\n",
    "                      \"_result.pickle FILE ALREADY EXIST.\")\n",
    "                continue\n",
    "            else:\n",
    "                y2 = onehottify(y_single[coin], n=2)\n",
    "\n",
    "                Evaluate(pickle_load_dir_path, \n",
    "                         data_files_dir, \n",
    "                         epochs, \n",
    "                         pickle_result_dir_path,\n",
    "                         _TEST, \n",
    "                         _ENHANCE,\n",
    "                         coin,\n",
    "                         X, y2,\n",
    "                         key_name_X,\n",
    "                         key_name_y,\n",
    "                         idx_time_unit,\n",
    "                         idx_window_size,\n",
    "                         idx_gap,\n",
    "                         idx_margin_rate, \n",
    "                         MODEL,\n",
    "                         _GPU,\n",
    "                         n_jobs,\n",
    "                         machine,\n",
    "                         Internet_connection,\n",
    "                         params)\n",
    "            #Javascript('IPython.notebook.kernel.restart()')\n",
    "            #time.sleep(1)\n",
    "            #Javascript('IPython.notebook.execute_all_cells()')\n",
    "\n",
    "\n",
    "    if (_TEST == True and _ENHANCE == False):\n",
    "        # for test                                \n",
    "        for coin in range(1):\n",
    "            if (os.path.isfile(pickle_result_dir_path + \\\n",
    "                              \"_test_\" + \\\n",
    "                              MODEL + \"_\" + \\\n",
    "                              \"BTC\" + \"_\" + \\\n",
    "                              str(idx_time_unit) + \"_\" + \\\n",
    "                              str(idx_window_size) + \"_\" + \\\n",
    "                              str(idx_gap) + \"_\" + \\\n",
    "                              str(idx_margin_rate) + \\\n",
    "                              \"_result.pickle\")) is True:\n",
    "                print(\"_test_\" + \\\n",
    "                      MODEL + \"_\" + \\\n",
    "                      \"BTC\" + \"_\" + \\\n",
    "                      str(idx_time_unit) + \"_\" + \\\n",
    "                      str(idx_window_size) + \"_\" + \\\n",
    "                      str(idx_gap) + \"_\" + \\\n",
    "                      str(idx_margin_rate) + \\\n",
    "                      \"_result.pickle FILE ALREADY EXIST.\")\n",
    "                #Javascript('IPython.notebook.kernel.restart()')\n",
    "                #Javascript('IPython.notebook.execute_all_cells()')\n",
    "                continue\n",
    "            else:\n",
    "                y2 = onehottify(y_single['BTC'], n=2)                          \n",
    "\n",
    "                Evaluate(pickle_load_dir_path, \n",
    "                         data_files_dir, \n",
    "                         epochs, \n",
    "                         pickle_result_dir_path, \n",
    "                         _TEST, \n",
    "                         _ENHANCE,\n",
    "                         coin,\n",
    "                         X, y2,\n",
    "                         key_name_X,\n",
    "                         key_name_y,\n",
    "                         idx_time_unit,\n",
    "                         idx_window_size,\n",
    "                         idx_gap,\n",
    "                         idx_margin_rate, \n",
    "                         MODEL,\n",
    "                         _GPU,\n",
    "                         n_jobs, \n",
    "                         machine,\n",
    "                         Internet_connection,\n",
    "                         params)\n",
    "    #Javascript('IPython.notebook.kernel.restart()')\n",
    "    #time.sleep(1)\n",
    "    #Javascript('IPython.notebook.execute_all_cells()')\n",
    "                #Javascript('IPython.notebook.kernel.restart()')\n",
    "                #Javascript('IPython.notebook.execute_all_cells()')\n",
    "    if (_TEST == False and _ENHANCE == True):\n",
    "        for coin in coin_list2:\n",
    "            if (os.path.isfile(boost_up_result_dir_path + \\\n",
    "                              MODEL + \"_\" + \\\n",
    "                              coin + \"_\" + \\\n",
    "                              str(idx_time_unit) + \"_\" + \\\n",
    "                              str(idx_window_size) + \"_\" + \\\n",
    "                              str(idx_gap) + \"_\" + \\\n",
    "                              str(idx_margin_rate) + \\\n",
    "                              \"_boost_up_result.pickle\")) is True:\n",
    "                print(MODEL + \"_\" + \\\n",
    "                      coin + \"_\" + \\\n",
    "                      str(idx_time_unit) + \"_\" + \\\n",
    "                      str(idx_window_size) + \"_\" + \\\n",
    "                      str(idx_gap) + \"_\" + \\\n",
    "                      str(idx_margin_rate) + \\\n",
    "                      \"_boost_up_result.pickle FILE ALREADY EXIST.\")\n",
    "                continue\n",
    "            else:\n",
    "                print(\"[BOOST UP THE \"+MODEL + \"_\" + \\\n",
    "                                        coin + \"_\" + \\\n",
    "                                        str(idx_time_unit) + \"_\" + \\\n",
    "                                        str(idx_window_size) + \"_\" + \\\n",
    "                                        str(idx_gap) + \"_\" + \\\n",
    "                                        str(idx_margin_rate) + \" STARTED.]\")\n",
    "                y2 = onehottify(y_single[coin], n=2)\n",
    "\n",
    "                Evaluate(pickle_load_dir_path, \n",
    "                         data_files_dir, \n",
    "                         epochs, \n",
    "                         boost_up_result_dir_path,\n",
    "                         _TEST, \n",
    "                         _ENHANCE,\n",
    "                         coin,\n",
    "                         X, y2,\n",
    "                         key_name_X,\n",
    "                         key_name_y,\n",
    "                         idx_time_unit,\n",
    "                         idx_window_size,\n",
    "                         idx_gap,\n",
    "                         idx_margin_rate, \n",
    "                         MODEL,\n",
    "                         _GPU,\n",
    "                         n_jobs,\n",
    "                         machine,\n",
    "                         Internet_connection,\n",
    "                         params)\n",
    "            #Javascript('IPython.notebook.kernel.restart()')\n",
    "            #time.sleep(1)\n",
    "            #Javascript('IPython.notebook.execute_all_cells()')\n",
    "\n",
    "# 저장된 pickle 파일의 데이터 구조\n",
    "# tmp = {}\n",
    "# tmp[\"10_1_1_0.1\"] = {\"grid_result.best_score_\":{}}, {\"grid_result.best_params_\":{}}\n",
    "# type(tmp[\"10_1_1_0.1\"][0])\n",
    "# print(tmp[\"10_1_1_0.1\"])\n",
    "# print(tmp[\"10_1_1_0.1\"])\n",
    "# print(tmp[\"10_1_1_0.1\"][0])\n",
    "# print(tmp[\"10_1_1_0.1\"][0]['grid_result.best_score_'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Evaluate(pickle_load_dir_path, \n",
    "             data_files_dir, \n",
    "             epochs, \n",
    "             pickle_result_dir_path,\n",
    "             _TEST, \n",
    "             _ENHANCE, \n",
    "             coin, \n",
    "             X, y2, \n",
    "             key_name_X,\n",
    "             key_name_y,\n",
    "             idx_time_unit,\n",
    "             idx_window_size,\n",
    "             idx_gap,\n",
    "             idx_margin_rate, \n",
    "             MODEL,\n",
    "             _GPU,\n",
    "             n_jobs, \n",
    "             machine,\n",
    "             Internet_connection,\n",
    "             parameter):\n",
    "    \n",
    "\n",
    "    \n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y2, test_size=0.1, random_state=42)\n",
    "#     print(\"X_train.shape\")\n",
    "#     print(X_train.shape)\n",
    "#     print(\"y_train.shape\")\n",
    "#     print(y_train.shape)\n",
    "#     print()\n",
    "#     print(\"X_test.shape\")\n",
    "#     print(X_test.shape)\n",
    "#     print(\"y_test.shape\")\n",
    "#     print(y_test.shape)\n",
    "#     print()\n",
    "\n",
    "    n_coins = 8\n",
    "    n_price = 4\n",
    "    n_steps = idx_window_size # 원래 100이었음. reshape 문제 때문에 수정함\n",
    "\n",
    "    X_train_2 = X_train.transpose([0, 2, 1, 3])\n",
    "    X_test_2 = X_test.transpose([0, 2, 1, 3])\n",
    "#     print(\"X_train_2.shape\")\n",
    "#     print(X_train_2.shape)\n",
    "#     print(\"X_test_2.shape\")\n",
    "#     print(X_test_2.shape)\n",
    "#     print()\n",
    "\n",
    "    X_train_3 = X_train_2.reshape([X_train.shape[0], n_steps, n_coins * n_price])\n",
    "    X_test_3 = X_test_2.reshape([X_test.shape[0], n_steps, n_coins * n_price])\n",
    "#     print(\"X_train_3.shape\")\n",
    "#     print(X_train_3.shape)\n",
    "#     print(\"X_test_3.shape\")\n",
    "#     print(X_test_3.shape)\n",
    "#     print()\n",
    "\n",
    "    if (_TEST==True and _ENHANCE==False):\n",
    "        param_grid = {'window_size' : [n_steps], \n",
    "                      'n_state_units': [100],\n",
    "                      'activation': ['relu'], \n",
    "                      'optimizer': ['rmsprop'], #sgd 추가\n",
    "                      'init': ['glorot_uniform'], #he 추가\n",
    "                      'batch_size': [2048]}\n",
    "\n",
    "    elif (_TEST==False and _ENHANCE==False):\n",
    "        param_grid = {'window_size' : [n_steps], \n",
    "                      'n_state_units': [40, 80, 160],\n",
    "                      'activation': ['relu', 'softmax'], \n",
    "                      'optimizer': ['rmsprop', 'adam'], #sgd 추가\n",
    "                      'init': ['glorot_uniform', 'uniform', 'he_uniform'], #he 추가\n",
    "                      'batch_size': [64,128,256]}\n",
    "        \n",
    "    elif (_TEST==False and _ENHANCE==True):\n",
    "        param_grid = parameter\n",
    "        \n",
    "        \n",
    "\n",
    "\n",
    "\n",
    "    X_train_reshape = X_train_2.reshape([X_train.shape[0], n_steps*n_coins * n_price])\n",
    "    X_test_reshape = X_test_2.reshape([X_test.shape[0], n_steps*n_coins * n_price])\n",
    "#     print(\"X_train_reshape.shape\")\n",
    "#     print(X_train_reshape.shape)\n",
    "#     print(\"X_test_reshape.shape\")\n",
    "#     print(X_test_reshape.shape)\n",
    "#     print()\n",
    "\n",
    "    scaler = MinMaxScaler()\n",
    "    scaler.fit(X_train_reshape)\n",
    "    X_train_scaled = scaler.transform(X_train_reshape)\n",
    "    X_test_scaled = scaler.transform(X_test_reshape)\n",
    "\n",
    "    X_train_scaled = X_train_scaled.reshape(-1, n_steps, n_coins * n_price)\n",
    "\n",
    "    X_test_scaled = X_test_scaled.reshape(-1, n_steps, n_coins * n_price)\n",
    "\n",
    "    if _GPU == True:\n",
    "        if MODEL == \"SimpleRNN\" :\n",
    "            model = KerasClassifier(build_fn=create_model_SimpleRNN, \n",
    "                                    epochs=epochs, # epochs는 실험을 최종적으로 수행하고자 할 때 높일 것(100~150정도)\n",
    "#                                     batch_size=100, \n",
    "                                    verbose=True)\n",
    "\n",
    "        elif MODEL == \"LSTM\":\n",
    "            model = KerasClassifier(build_fn=create_model_LSTM, \n",
    "                                    epochs=epochs, # epochs는 실험을 최종적으로 수행하고자 할 때 높일 것(100~150정도)\n",
    "#                                     batch_size=100, \n",
    "                                    verbose=True)\n",
    "\n",
    "        elif MODEL == \"RNN\":\n",
    "            model = KerasClassifier(build_fn=create_model_RNN, \n",
    "                                    epochs=epochs, # epochs는 실험을 최종적으로 수행하고자 할 때 높일 것(100~150정도)\n",
    "#                                     batch_size=100, \n",
    "                                    verbose=True)\n",
    "\n",
    "        elif MODEL == \"GRU\":\n",
    "            model = KerasClassifier(build_fn=create_model_GRU, \n",
    "                                    epochs=epochs, # epochs는 실험을 최종적으로 수행하고자 할 때 높일 것(100~150정도)\n",
    "#                                     batch_size=100, \n",
    "                                    verbose=True)\n",
    "    elif _GPU == False:\n",
    "        if MODEL == \"SimpleRNN\" :\n",
    "            model = KerasClassifier(build_fn=create_model_SimpleRNN_non_GPU, \n",
    "                                    epochs=epochs, # epochs는 실험을 최종적으로 수행하고자 할 때 높일 것(100~150정도)\n",
    "                                    batch_size=10, \n",
    "                                    verbose=True)\n",
    "\n",
    "        elif MODEL == \"LSTM\":\n",
    "            model = KerasClassifier(build_fn=create_model_LSTM_non_GPU, \n",
    "                                    epochs=epochs, # epochs는 실험을 최종적으로 수행하고자 할 때 높일 것(100~150정도)\n",
    "                                    batch_size=10, \n",
    "                                    verbose=True)\n",
    "\n",
    "        elif MODEL == \"RNN\":\n",
    "            model = KerasClassifier(build_fn=create_model_RNN_non_GPU, \n",
    "                                    epochs=epochs, # epochs는 실험을 최종적으로 수행하고자 할 때 높일 것(100~150정도)\n",
    "                                    batch_size=10, \n",
    "                                    verbose=True)\n",
    "\n",
    "        elif MODEL == \"GRU\":\n",
    "            model = KerasClassifier(build_fn=create_model_GRU_non_GPU, \n",
    "                                    epochs=epochs, # epochs는 실험을 최종적으로 수행하고자 할 때 높일 것(100~150정도)\n",
    "                                    batch_size=10, \n",
    "                                    verbose=True)\n",
    "    if (_ENHANCE == False):\n",
    "        grid = GridSearchCV(\n",
    "            estimator=model, \n",
    "            cv=5, \n",
    "            n_jobs=n_jobs, # test\n",
    "            param_grid=param_grid,\n",
    "            verbose=1)\n",
    "      \n",
    "        X_train_scaled, X_test_scaled = input_reshape(X_train_scaled, X_test_scaled, n_steps, n_coins, n_price)\n",
    "\n",
    "        if (_TEST == True): \n",
    "            print()\n",
    "            print()\n",
    "            print(\"TEST!\")\n",
    "            print()\n",
    "            print(\"----------------------\")\n",
    "            print(\"<\"+MODEL+\">\")\n",
    "            print(\"----------------------\")\n",
    "            print(\"__\"+\"BTC\"+\"__\" + \\\n",
    "                    \"time unit: \"+str(idx_time_unit) + \"  |  \" + \\\n",
    "                    \"window_size :\"+str(idx_window_size) + \"  |  \" + \\\n",
    "                    \"gap :\"+str(idx_gap) + \"  |  \" + \\\n",
    "                    \"margin_rate :\"+str(idx_margin_rate) + \\\n",
    "                    \"  started.\")\n",
    "        elif (_TEST == False):\n",
    "            print()\n",
    "            print()\n",
    "            print(\"----------------------\")\n",
    "            print(\"<\"+MODEL+\">\")\n",
    "            print(\"----------------------\")\n",
    "            print(\"__\"+coin+\"__\" + \\\n",
    "                    \"time unit: \"+str(idx_time_unit) + \"  |  \" + \\\n",
    "                    \"window_size :\"+str(idx_window_size) + \"  |  \" + \\\n",
    "                    \"gap :\"+str(idx_gap) + \"  |  \" + \\\n",
    "                    \"margin_rate :\"+str(idx_margin_rate) + \\\n",
    "                    \"  started.\")\n",
    "\n",
    "        grid_result = grid.fit(X_train_scaled, \n",
    "                               y_train, \n",
    "                               validation_data=(X_test_scaled,y_test))\n",
    "\n",
    "\n",
    "\n",
    "        print(\"----------------------\")\n",
    "\n",
    "        # \n",
    "    #     means = grid_result.cv_results_['mean_test_score']\n",
    "    #     stds = grid_result.cv_results_['std_test_score']\n",
    "    #     params = grid_result.cv_results_['params']\n",
    "    #     print(\"grid_result.cv_results_\",grid_result.cv_results_)\n",
    "    #     print(\"grid_result.best_estimator_\",grid_result.best_estimator_)\n",
    "        print(\"grid_result.score(X_test_scaled, y_test): \",grid_result.score(X_test_scaled, y_test))\n",
    "\n",
    "\n",
    "\n",
    "        evaluate_result = {}\n",
    "\n",
    "        if (_TEST == True): \n",
    "            test_score = grid_result.score(X_test_scaled, y_test)\n",
    "            evaluate_result[MODEL + \"_\" + \\\n",
    "                          \"BTC\" + \"_\" + \\\n",
    "                          str(idx_time_unit) + \"_\" + \\\n",
    "                          str(idx_window_size) + \"_\" + \\\n",
    "                          str(idx_gap) + \"_\" + \\\n",
    "                          str(idx_margin_rate)] = {\"MODEL\":MODEL,\\\n",
    "                                            \"Cryptocurrency\":\"BTC\",\\\n",
    "    #                                         \"grid_result.cv_results_\":grid_result.cv_results_, \\\n",
    "    #                                         \"grid_result.best_estimator_\":grid_result.best_estimator_, \\\n",
    "                                            \"Score\":grid_result.cv_results_['mean_test_score'], \\\n",
    "                                            \"Params\":grid_result.cv_results_['params'],\\\n",
    "                                            \"test_score\":test_score}  \n",
    "\n",
    "\n",
    "        elif (_TEST == False): \n",
    "            test_score = grid_result.score(X_test_scaled, y_test)\n",
    "            evaluate_result[MODEL + \"_\" + \\\n",
    "                          coin + \"_\" + \\\n",
    "                          str(idx_time_unit) + \"_\" + \\\n",
    "                          str(idx_window_size) + \"_\" + \\\n",
    "                          str(idx_gap) + \"_\" + \\\n",
    "                          str(idx_margin_rate)] = {\"MODEL\":MODEL,\\\n",
    "                                            \"Cryptocurrency\":coin, \\\n",
    "                                            \"Score\":grid_result.cv_results_['mean_test_score'], \\\n",
    "                                            \"Params\":grid_result.cv_results_['params'],\\\n",
    "                                            \"test_score\":test_score}\n",
    "    #     print()\n",
    "    #     print(\"evaluate result dict: \", evaluate_result)\n",
    "    #     print()\n",
    "\n",
    "        # summarize results\n",
    "        print()\n",
    "        print(\"Best: %f using %s\" % (grid_result.best_score_, grid_result.best_params_))\n",
    "        print()\n",
    "        \n",
    "    elif (_ENHANCE == True):\n",
    "        \n",
    "        \n",
    "    # for checking pickle file exist\n",
    "    print(\"---pickle saving..\")\n",
    "    if (_TEST == True):\n",
    "        X = {}\n",
    "        y = {}\n",
    "        key_name_X = \"X_\"\n",
    "        key_name_y = \"y_\"\n",
    "        for idx_time_unit in time_unit:\n",
    "            for idx_window_size in window_size:\n",
    "                for idx_gap in gap:\n",
    "                    for idx_margin_rate in margin_rate:\n",
    "                        key_name_X += str(idx_time_unit) + \"_\" + str(idx_window_size) + \"_\" + str(idx_gap) + \"_\" + str(idx_margin_rate)\n",
    "                        key_name_y += str(idx_time_unit) + \"_\" + str(idx_window_size) + \"_\" + str(idx_gap) + \"_\" + str(idx_margin_rate)\n",
    "                        if (os.path.isfile(pickle_result_dir_path + \\\n",
    "                                          \"_test_\" + \\\n",
    "                                          MODEL + \"_\" + \\\n",
    "                                          \"BTC\" + \"_\" + \\\n",
    "                                          str(idx_time_unit) + \"_\" + \\\n",
    "                                          str(idx_window_size) + \"_\" + \\\n",
    "                                          str(idx_gap) + \"_\" + \\\n",
    "                                          str(idx_margin_rate) + \\\n",
    "                                          \"_result.pickle\")) is not True:\n",
    "                            with open(pickle_result_dir_path + \\\n",
    "                                      \"_test_\" + \\\n",
    "                                      MODEL + \"_\" + \\\n",
    "                                      \"BTC\" + \"_\" + \\\n",
    "                                      str(idx_time_unit) + \"_\" + \\\n",
    "                                      str(idx_window_size) + \"_\" + \\\n",
    "                                      str(idx_gap) + \"_\" + \\\n",
    "                                      str(idx_margin_rate) + \\\n",
    "                                      \"_result.pickle\", 'wb') as handle:\n",
    "                                pickle.dump(evaluate_result, handle, protocol=pickle.HIGHEST_PROTOCOL)\n",
    "                                \n",
    "                                # Sending Email\n",
    "                                if Internet_connection == True:\n",
    "                                    smtp = smtplib.SMTP('smtp.naver.com', 587)\n",
    "                                    smtp.ehlo()      # say Hello\n",
    "                                    smtp.starttls()  # TLS 사용시 필요\n",
    "                                    smtp.login('dhgdohk@naver.com', '30892793@dohk')\n",
    "\n",
    "                                    msg = MIMEText(pickle_result_dir_path + \\\n",
    "                                                   \"_test_\" + \\\n",
    "                                                   MODEL + \"_\" + \\\n",
    "                                                   \"BTC\" + \"_\" + \\\n",
    "                                                   str(idx_time_unit) + \"_\" + \\\n",
    "                                                   str(idx_window_size) + \"_\" + \\\n",
    "                                                   str(idx_gap) + \"_\" + \\\n",
    "                                                   str(idx_margin_rate) + \\\n",
    "                                                   \"_result.pickle\")\n",
    "                                    msg['Subject'] =   pickle_result_dir_path + \\\n",
    "                                                       MODEL + \"_\" + \\\n",
    "                                                       \"BTC\" + \"_\" + \\\n",
    "                                                       str(idx_time_unit) + \"_\" + \\\n",
    "                                                       str(idx_window_size) + \"_\" + \\\n",
    "                                                       str(idx_gap) + \"_\" + \\\n",
    "                                                       str(idx_margin_rate) + \\\n",
    "                                                       \"_result.pickle\"\n",
    "                                    msg['To'] = 'dhgdohk@naver.com'\n",
    "                                    smtp.sendmail('dhgdohk@naver.com', 'dhgdohk@naver.com', msg.as_string())\n",
    "\n",
    "                                    smtp.quit()\n",
    "                        else:\n",
    "                            print(\"Already exist the file: \", pickle_result_dir_path + \\\n",
    "                                                              \"_test_\" + \\\n",
    "                                                              MODEL + \"_\" + \\\n",
    "                                                              \"BTC\" + \"_\" + \\\n",
    "                                                              str(idx_time_unit) + \"_\" + \\\n",
    "                                                              str(idx_window_size) + \"_\" + \\\n",
    "                                                              str(idx_gap) + \"_\" + \\\n",
    "                                                              str(idx_margin_rate) + \\\n",
    "                                                              \"_result.pickle\")\n",
    "\n",
    "    elif (_TEST == False): \n",
    "        X = {}\n",
    "        y = {}\n",
    "        key_name_X = \"X_\"\n",
    "        key_name_y = \"y_\"\n",
    "        \n",
    "        key_name_X += str(idx_time_unit) + \"_\" + str(idx_window_size) + \"_\" + str(idx_gap) + \"_\" + str(idx_margin_rate)\n",
    "        key_name_y += str(idx_time_unit) + \"_\" + str(idx_window_size) + \"_\" + str(idx_gap) + \"_\" + str(idx_margin_rate)\n",
    "        if (os.path.isfile(pickle_result_dir_path + \\\n",
    "                          MODEL + \"_\" + \\\n",
    "                          coin + \"_\" + \\\n",
    "                          str(idx_time_unit) + \"_\" + \\\n",
    "                          str(idx_window_size) + \"_\" + \\\n",
    "                          str(idx_gap) + \"_\" + \\\n",
    "                          str(idx_margin_rate) + \\\n",
    "                          \"_result.pickle\")) is not True:\n",
    "            with open(pickle_result_dir_path + \\\n",
    "                      MODEL + \"_\" + \\\n",
    "                      coin + \"_\" + \\\n",
    "                      str(idx_time_unit) + \"_\" + \\\n",
    "                      str(idx_window_size) + \"_\" + \\\n",
    "                      str(idx_gap) + \"_\" + \\\n",
    "                      str(idx_margin_rate) + \\\n",
    "                      \"_result.pickle\", 'wb') as handle:\n",
    "                pickle.dump(evaluate_result, handle, protocol=pickle.HIGHEST_PROTOCOL)\n",
    "\n",
    "                # Sending Email\n",
    "                if Internet_connection == True:\n",
    "                    smtp = smtplib.SMTP('smtp.naver.com', 587)\n",
    "                    smtp.ehlo()      # say Hello\n",
    "                    smtp.starttls()  # TLS 사용시 필요\n",
    "                    smtp.login('dhgdohk@naver.com', '30892793@dohk')\n",
    "\n",
    "                    msg = MIMEText(machine + \\\n",
    "                                   pickle_result_dir_path + \\\n",
    "                                   MODEL + \"_\" + \\\n",
    "                                   coin + \"_\" + \\\n",
    "                                   str(idx_time_unit) + \"_\" + \\\n",
    "                                   str(idx_window_size) + \"_\" + \\\n",
    "                                   str(idx_gap) + \"_\" + \\\n",
    "                                   str(idx_margin_rate) + \\\n",
    "                                   \"_result.pickle\")\n",
    "                    msg['Subject'] =   machine + \\\n",
    "                                       pickle_result_dir_path + \\\n",
    "                                       MODEL + \"_\" + \\\n",
    "                                       coin + \"_\" + \\\n",
    "                                       str(idx_time_unit) + \"_\" + \\\n",
    "                                       str(idx_window_size) + \"_\" + \\\n",
    "                                       str(idx_gap) + \"_\" + \\\n",
    "                                       str(idx_margin_rate) + \\\n",
    "                                       \"_result.pickle\"\n",
    "                    msg['To'] = 'dhgdohk@naver.com'\n",
    "                    smtp.sendmail('dhgdohk@naver.com', 'dhgdohk@naver.com', msg.as_string())\n",
    "\n",
    "                    smtp.quit()\n",
    "        else:\n",
    "            print(\"Already exist the file: \", pickle_result_dir_path + \\\n",
    "                                              \"_test_\" + \\\n",
    "                                              MODEL + \"_\" + \\\n",
    "                                              \"BTC\" + \"_\" + \\\n",
    "                                              str(idx_time_unit) + \"_\" + \\\n",
    "                                              str(idx_window_size) + \"_\" + \\\n",
    "                                              str(idx_gap) + \"_\" + \\\n",
    "                                              str(idx_margin_rate) + \\\n",
    "                                              \"_result.pickle\")\n",
    "    \n",
    "    \n",
    "    #print(\"!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!\")\n",
    "    #Javascript('Jupyter.notebook.session.delete()')\n",
    "    #print(\"Jupyter notebook kernel restart\")\n",
    "    #time.sleep(1)\n",
    "    #Javascript('Jupyter.notebook.kernel.restart()')\n",
    "    #time.sleep(1)\n",
    "    #print(\"Done\")\n",
    "    #Javascript('Jupyter.notebook.execute_all_cells()')\n",
    "    print()\n",
    "\n",
    "\n",
    "#     for mean, stdev, param in zip(means, stds, params):\n",
    "#         print(\"%f (%f) with: %r\" % (mean, stdev, param))\n",
    "#     print()\n",
    "\n",
    "\n",
    "    key_name_X = \"X_\"\n",
    "    key_name_y = \"y_\"\n",
    "    \n",
    "    \n",
    "#     return eval_result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def start(machine, Internet_connection, pickle_result_dir_path, boost_up_result_dir_path, _TEST, _GPU, _ENHANCED, n_jobs, MODEL, idx_time_unit, idx_window_size, idx_gap, idx_margin_rate, epochs, best_params):\n",
    "    '''\n",
    "        [ATTENTION] In create_model METHOD part, need to set appropriate about GPU\n",
    "        \n",
    "        LINK01 -> GPU OFF\n",
    "        MSI -> GPU OFF\n",
    "        SLAVE04 -> GPU ON\n",
    "        SLAVE05 -> GPU ON\n",
    "    ''' \n",
    "    # \n",
    "    \n",
    "    \n",
    "    if (_ENHANCED == False and machine==\"slave05\"):\n",
    "        #time_unit = [10,30,60]     # candle stick minutes\n",
    "        #window_size = [25]  # Unit: num. of candle sticks\n",
    "        #gap = [1]            # Unit: num. of candle sticks\n",
    "        #margin_rate = [0.1]  # Unit: percent\n",
    "\n",
    "        start_time = time.time()\n",
    "        Start_Model( pickle_load_dir_path = \"./dataset_pickle_tuple_type/\",  \n",
    "                     data_files_dir = dataset_dir_path_tuple_type, \n",
    "                     epochs=epochs, \n",
    "                     pickle_result_dir_path = \"./evaluate_result/\", \n",
    "                     boost_up_result_dir_path = \"./boost_up_result/\",\n",
    "                     MODEL=MODEL, \n",
    "                    idx_time_unit=idx_time_unit,\n",
    "                    idx_window_size=idx_window_size, \n",
    "                    idx_gap=idx_gap, \n",
    "                    idx_margin_rate=idx_margin_rate, \n",
    "                     _TEST=False, \n",
    "                     _ENHANCE=_ENHANCED,\n",
    "                     _GPU=True,\n",
    "                     n_jobs=2,\n",
    "                     machine=machine, \n",
    "                     Internet_connection=Internet_connection, \n",
    "                     params=best_params)\n",
    "        \n",
    "        end_time = time.time()\n",
    "        print()\n",
    "        print(\"TIME: \", end_time-start_time)\n",
    "        Javascript('IPython.notebook.kernel.restart()')\n",
    "        time.sleep(1)\n",
    "        Javascript('IPython.notebook.execute_all_cells()')\n",
    "\n",
    "    elif (_ENHANCED == False and machine==\"link-koreatech\"):\n",
    "        #time_unit = [10]     # candle stick minutes\n",
    "        #window_size = [10,25,50]  # Unit: num. of candle sticks\n",
    "        #gap = [1]            # Unit: num. of candle sticks\n",
    "        #margin_rate = [0.1]  # Unit: percent\n",
    "\n",
    "        Start_Model( pickle_load_dir_path = \"./dataset_pickle_tuple_type/\",  \n",
    "                     data_files_dir = dataset_dir_path_tuple_type, \n",
    "                     epochs=epochs, \n",
    "                     pickle_result_dir_path = \"./evaluate_result/\", \n",
    "                    boost_up_result_dir_path = \"./boost_up_result/\",\n",
    "                     MODEL=MODEL, \n",
    "                    idx_time_unit=idx_time_unit,\n",
    "                    idx_window_size=idx_window_size, \n",
    "                    idx_gap=idx_gap, \n",
    "                    idx_margin_rate=idx_margin_rate,\n",
    "                     _TEST=False, \n",
    "                     _ENHANCE=_ENHANCED,\n",
    "                     _GPU=False,\n",
    "                     n_jobs=1,\n",
    "                     machine=machine, \n",
    "                     Internet_connection=Internet_connection, \n",
    "                     params=best_params)\n",
    "        end_time = time.time()\n",
    "        print()\n",
    "        print(\"TIME: \", end_time-start_time)\n",
    "        Javascript('IPython.notebook.kernel.restart()')\n",
    "        time.sleep(1)\n",
    "        Javascript('IPython.notebook.execute_all_cells()')\n",
    "\n",
    "    elif (_ENHANCED == False and machine==\"slave04\"):\n",
    "        #time_unit = [10]     # candle stick minutes\n",
    "        #window_size = [75]  # Unit: num. of candle sticks\n",
    "        #gap = [1]            # Unit: num. of candle sticks\n",
    "        #margin_rate = [0.1]  # Unit: percent\n",
    "\n",
    "        start_time = time.time()\n",
    "\n",
    "        Start_Model(pickle_load_dir_path = \"./dataset_pickle_tuple_type/\",  \n",
    "                 data_files_dir = dataset_dir_path_tuple_type, \n",
    "                 epochs=epochs, \n",
    "                 pickle_result_dir_path = \"./evaluate_result/\", \n",
    "                    boost_up_result_dir_path = \"./boost_up_result/\",\n",
    "                 MODEL=MODEL, \n",
    "                idx_time_unit=idx_time_unit,\n",
    "                idx_window_size=idx_window_size, \n",
    "                idx_gap=idx_gap, \n",
    "                idx_margin_rate=idx_margin_rate,\n",
    "                 _TEST=False, \n",
    "                 _ENHANCE=_ENHANCED,\n",
    "                 _GPU=True,\n",
    "                 n_jobs=2,\n",
    "                 machine=machine,\n",
    "                 Internet_connection=Internet_connection, \n",
    "                 params=best_params)\n",
    "\n",
    "        end_time = time.time()\n",
    "        print()\n",
    "        print(\"TIME: \", end_time-start_time)\n",
    "        Javascript('IPython.notebook.kernel.restart()')\n",
    "        time.sleep(1)\n",
    "        Javascript('IPython.notebook.execute_all_cells()')\n",
    "\n",
    "    elif (_ENHANCED == False and machine==\"link01\"):\n",
    "        #time_unit = [10]     # candle stick minutes\n",
    "        #window_size = [25]  # Unit: num. of candle sticks\n",
    "        #gap = [1,2,3]            # Unit: num. of candle sticks\n",
    "        #margin_rate = [0.1]  # Unit: percent\n",
    "\n",
    "        start_time = time.time()\n",
    "\n",
    "        Start_Model(pickle_load_dir_path = \"./dataset_pickle_tuple_type/\",  \n",
    "                 data_files_dir = dataset_dir_path_tuple_type, \n",
    "                 epochs=epochs, \n",
    "                 pickle_result_dir_path = \"./evaluate_result/\", \n",
    "                    boost_up_result_dir_path = \"./boost_up_result/\",\n",
    "                 MODEL=MODEL, \n",
    "                idx_time_unit=idx_time_unit,\n",
    "                idx_window_size=idx_window_size, \n",
    "                idx_gap=idx_gap, \n",
    "                idx_margin_rate=idx_margin_rate,\n",
    "                 _TEST=False,\n",
    "                 _ENHANCE=_ENHANCED,\n",
    "                 _GPU=False,\n",
    "                 n_jobs=1,\n",
    "                 machine=machine, \n",
    "                 Internet_connection=Internet_connection,\n",
    "                 params=best_params)\n",
    "\n",
    "        end_time = time.time()\n",
    "        print()\n",
    "        print(\"TIME: \", end_time-start_time)\n",
    "        Javascript('IPython.notebook.kernel.restart()')\n",
    "        time.sleep(1)\n",
    "        Javascript('IPython.notebook.execute_all_cells()')\n",
    "\n",
    "    elif (_ENHANCED == False and machine==\"MSI\"):\n",
    "        #time_unit = [10]     # candle stick minutes\n",
    "        #window_size = [25]  # Unit: num. of candle sticks\n",
    "        #gap = [1]            # Unit: num. of candle sticks\n",
    "        #margin_rate = [0.1,0.25,0.5]  # Unit: percent\n",
    "\n",
    "        start_time = time.time()\n",
    "\n",
    "        Start_Model(pickle_load_dir_path = \"./dataset_pickle_tuple_type/\",  \n",
    "                 data_files_dir = dataset_dir_path_tuple_type, \n",
    "                 epochs=epochs, \n",
    "                 pickle_result_dir_path = \"./evaluate_result/\", \n",
    "                    boost_up_result_dir_path = \"./boost_up_result/\",\n",
    "                 MODEL=MODEL, \n",
    "                idx_time_unit=idx_time_unit,\n",
    "                idx_window_size=idx_window_size, \n",
    "                idx_gap=idx_gap, \n",
    "                idx_margin_rate=idx_margin_rate,\n",
    "                 _TEST=False, \n",
    "                 _ENHANCE=_ENHANCED,\n",
    "                 _GPU=False,\n",
    "                 n_jobs=1,\n",
    "                 machine=machine, \n",
    "                 Internet_connection=Internet_connection,\n",
    "                 params=best_params)\n",
    "\n",
    "        end_time = time.time()\n",
    "        print()\n",
    "        print(\"TIME: \", end_time-start_time)\n",
    "        Javascript('IPython.notebook.kernel.restart()')\n",
    "        time.sleep(1)\n",
    "        Javascript('IPython.notebook.execute_all_cells()')\n",
    "        \n",
    "    elif (_ENHANCED == False and machine==\"test\"):\n",
    "        \n",
    "        #time_unit = [10]     # candle stick minutes\n",
    "        #window_size = [10]  # Unit: num. of candle sticks\n",
    "        #gap = [1]            # Unit: num. of candle sticks\n",
    "        #margin_rate = [0.1]  # Unit: percent\n",
    "\n",
    "        start_time = time.time()\n",
    "\n",
    "        Start_Model(pickle_load_dir_path = \"./dataset_pickle_tuple_type/\",  \n",
    "                 data_files_dir = dataset_dir_path_tuple_type, \n",
    "                 epochs=epochs, \n",
    "                 pickle_result_dir_path = \"./evaluate_result/\", \n",
    "                    boost_up_result_dir_path = \"./boost_up_result/\",\n",
    "                 MODEL=MODEL, \n",
    "                idx_time_unit=idx_time_unit,\n",
    "                idx_window_size=idx_window_size, \n",
    "                idx_gap=idx_gap, \n",
    "                idx_margin_rate=idx_margin_rate,\n",
    "                 _TEST=_TEST, \n",
    "                 _ENHANCE=_ENHANCED,\n",
    "                 _GPU=_GPU,\n",
    "                 n_jobs=1,\n",
    "                 machine=\"test\", \n",
    "                 Internet_connection=Internet_connection,\n",
    "                 params=best_params)\n",
    "\n",
    "        end_time = time.time()\n",
    "        print()\n",
    "        print(\"TIME: \", end_time-start_time)\n",
    "        Javascript('IPython.notebook.kernel.restart()')\n",
    "        time.sleep(1)\n",
    "        Javascript('IPython.notebook.execute_all_cells()')\n",
    "    \n",
    "    elif (_ENHANCED == True):\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # evaluate model using test data\n",
    "# score = "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # load pickle file \n",
    "# import pickle\n",
    "# b_x = pickle.load(open(\"./evaluate_result/_test_SimpleRNN_BTC_10_10_1_0.1_result.pickle\", \"rb\"))\n",
    "# b_x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Boost-up Acc, F1\n",
    "# evaluate_result_dir_path = \"./evaluate_result/acc_f1/\"\n",
    "# dataset_dir_path = dataset_dir_path_tuple_type \n",
    "# # dataset_dir_path = dataset_dir_path_numpy_type\n",
    "# epochs = 100\n",
    "# Evaluate(dataset_dir_path, data_files_dir, epochs, evaluate_result_dir_path, time_unit, window_size, gap, margin_rate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_info = {}\n",
    "model_info[\"test\"] = {#\"MODEL_list\":[\"SimpleRNN\", \"LSTM\", \"GRU\"],\n",
    "                      \"MODEL_list\":[\"SimpleRNN\"],\n",
    "                      \"time_unit\":[10], \n",
    "                      \"window_size\":[10],\n",
    "                      \"gap\":[1], \n",
    "                      \"margin_rate\":[0.1]}\n",
    "\n",
    "model_info[\"slave05\"] = {\"MODEL_list\":[\"SimpleRNN\", \"LSTM\", \"GRU\"],\n",
    "                      \"time_unit\":[10,30,60], \n",
    "                      \"window_size\":[25],\n",
    "                      \"gap\":[1], \n",
    "                      \"margin_rate\":[0.1]}\n",
    "model_info[\"slave04\"] = {\"MODEL_list\":[\"SimpleRNN\", \"LSTM\", \"GRU\"],\n",
    "                      \"time_unit\":[10], \n",
    "                      \"window_size\":[10,50,75],\n",
    "                      \"gap\":[1], \n",
    "                      \"margin_rate\":[0.1]}\n",
    "model_info[\"link01\"] = {\"MODEL_list\":[\"SimpleRNN\", \"LSTM\", \"GRU\"],\n",
    "                      \"time_unit\":[10], \n",
    "                      \"window_size\":[25],\n",
    "                      \"gap\":[2,3], \n",
    "                      \"margin_rate\":[0.1]}\n",
    "model_info[\"link-koreatech\"] = {\"MODEL_list\":[\"SimpleRNN\", \"LSTM\", \"GRU\"],\n",
    "                      \"time_unit\":[10], \n",
    "                      \"window_size\":[25],\n",
    "                      \"gap\":[1], \n",
    "                      \"margin_rate\":[0.25,0.5]}\n",
    "\n",
    "model_info[\"MSI\"] = {\"MODEL_list\":[\"SimpleRNN\", \"LSTM\", \"GRU\"],\n",
    "                      \"time_unit\":[10], \n",
    "                      \"window_size\":[25],\n",
    "                      \"gap\":[1], \n",
    "                      \"margin_rate\":[0.1,0.25,0.5]}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Hyperio test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from __future__ import print_function\n",
    "# import keras\n",
    "# from keras.datasets import mnist\n",
    "# from keras.models import Sequential\n",
    "# from keras.layers import Dense, Dropout, Flatten\n",
    "# from keras.layers import Conv2D, MaxPooling2D\n",
    "# from keras import backend as K\n",
    "\n",
    "# def create_model_SimpleRNN_non_GPU_test(x_train, y_train, x_val, y_val, params):\n",
    "#     model = Sequential()\n",
    "#     model.add(# if문을 통해 여러 RNN모델 쓸 수 있도록 하기, SimpleRNN외에 다른 RNN모델 찾아보기\n",
    "#         SimpleRNN(\n",
    "# #             n_state_units, \n",
    "#                   params['first_neuron'],\n",
    "#                   input_dim=x_train.shape[2], # [dataset 크기, 윈도우 사이즈, 32(코인개수*OLHC)]\n",
    "# #                   input_shape=(window_size, 32),\n",
    "# #                   use_bias=True, \n",
    "#                   #activation='relu'\n",
    "#         ))\n",
    "# #                   kernel_initializer='glorot_uniform', \n",
    "# #                   recurrent_initializer='orthogonal', \n",
    "# #                   bias_initializer='zeros', \n",
    "# #                   dropout=0.0,\n",
    "# #                   recurrent_dropout=0.0))\n",
    "    \n",
    "# #     model.add(Dense(units=neurons))\n",
    "# #     model.add(Dropout(dropout_rate))\n",
    "        \n",
    "#     model.add(Dropout(params['dropout']))\n",
    "#     model.add(Dense(y_train.shape[1],\n",
    "#                     activation=params['last_activation']))\n",
    "        \n",
    "# #     model.add(Dense(units=2))\n",
    "# #     model = multi_gpu_model(model, gpus=2)\n",
    "# #     model.compile(optimizer=params['optimizer'](lr=lr_normalizer(params['lr'], params['optimizer'])),\n",
    "# #                   loss=params['loss'],\n",
    "# #                   metrics=['acc'])\n",
    "#     model = multi_gpu_model(model, gpus=2)\n",
    "#     model.compile(optimizer=params['optimizer'](),\n",
    "#                   loss=params['loss'],\n",
    "#                   metrics=['acc', f1_score])\n",
    "    \n",
    "#     out = model.fit(x_train, y_train,\n",
    "#                     batch_size=params['batch_size'],\n",
    "#                     epochs=params['epochs'],\n",
    "#                     verbose=1,\n",
    "#                     validation_data=[x_val, y_val])\n",
    "# #                     callbacks=early_stopper(params['epochs'], mode='strict'))\n",
    "    \n",
    "#     return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# from keras.optimizers import Adam, Nadam\n",
    "# from keras.activations import softmax\n",
    "# from keras.losses import categorical_crossentropy, logcosh\n",
    "\n",
    "# pickle_load_dir_path = \"./dataset_pickle_tuple_type/\"\n",
    "# X = {}\n",
    "# y = {}\n",
    "\n",
    "\n",
    "# MODEL = [\"SimpleRNN\"]\n",
    "# idx_time_unit = 10     # candle stick minutes\n",
    "# idx_window_size = 25  # Unit: num. of candle sticks\n",
    "# idx_gap = 1            # Unit: num. of candle sticks\n",
    "# idx_margix_rate = 0.1  # Unit: percent\n",
    "\n",
    "# key_name_X = \"X_\"\n",
    "# key_name_y = \"y_\"\n",
    "\n",
    "\n",
    "# key_name_X += str(idx_time_unit) + \"_\" + str(idx_window_size) + \"_\" + str(idx_gap) + \"_\" + str(idx_margix_rate)\n",
    "# key_name_y += str(idx_time_unit) + \"_\" + str(idx_window_size) + \"_\" + str(idx_gap) + \"_\" + str(idx_margix_rate)\n",
    "\n",
    "# X = Load_Dataset_X(pickle_load_dir_path, idx_time_unit, idx_window_size, idx_gap, idx_margix_rate)\n",
    "# y = Load_Dataset_y(pickle_load_dir_path, idx_time_unit, idx_window_size, idx_gap, idx_margix_rate)\n",
    "\n",
    "# y_single = {}\n",
    "# y_single['BTC'] = y[:, 1]\n",
    "# y_single['ETH'] = y[:, 2]\n",
    "# y_single['XRP'] = y[:, 3]\n",
    "# y_single['BCH'] = y[:, 4]\n",
    "# y_single['LTC'] = y[:, 5]\n",
    "# y_single['DASH'] = y[:, 6]\n",
    "# y_single['ETC'] = y[:, 7]\n",
    "\n",
    "# coin_list2 = [\"BTC\", \"ETH\", \"XRP\", \"BCH\", \"LTC\", \"DASH\", \"ETC\"]\n",
    "\n",
    "# y2 = onehottify(y_single['BTC'], n=2) \n",
    "# #                         for coin in coin_list2:\n",
    "# #                             print(\"y_single[\"+coin+\"]\"+\".shape\")\n",
    "# #                             print(y_single[coin].shape)\n",
    "# #                             print()\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# X_train, X_test, y_train, y_test = train_test_split(X, y2, test_size=0.1, random_state=42)\n",
    "# #     print(\"X_train.shape\")\n",
    "# #     print(X_train.shape)\n",
    "# #     print(\"y_train.shape\")\n",
    "# #     print(y_train.shape)\n",
    "# #     print()\n",
    "# #     print(\"X_test.shape\")\n",
    "# #     print(X_test.shape)\n",
    "# #     print(\"y_test.shape\")\n",
    "# #     print(y_test.shape)\n",
    "# #     print()\n",
    "\n",
    "# n_coins = 8\n",
    "# n_price = 4\n",
    "# n_steps = idx_window_size # 원래 100이었음. reshape 문제 때문에 수정함\n",
    "\n",
    "# X_train_2 = X_train.transpose([0, 2, 1, 3])\n",
    "# X_test_2 = X_test.transpose([0, 2, 1, 3])\n",
    "# #     print(\"X_train_2.shape\")\n",
    "# #     print(X_train_2.shape)\n",
    "# #     print(\"X_test_2.shape\")\n",
    "# #     print(X_test_2.shape)\n",
    "# #     print()\n",
    "\n",
    "# X_train_3 = X_train_2.reshape([X_train.shape[0], n_steps, n_coins * n_price])\n",
    "# X_test_3 = X_test_2.reshape([X_test.shape[0], n_steps, n_coins * n_price])\n",
    "# #     print(\"X_train_3.shape\")\n",
    "# #     print(X_train_3.shape)\n",
    "# #     print(\"X_test_3.shape\")\n",
    "# #     print(X_test_3.shape)\n",
    "# #     print()\n",
    "\n",
    "# X_train_reshape = X_train_2.reshape([X_train.shape[0], n_steps*n_coins * n_price])\n",
    "# X_test_reshape = X_test_2.reshape([X_test.shape[0], n_steps*n_coins * n_price])\n",
    "# #     print(\"X_train_reshape.shape\")\n",
    "# #     print(X_train_reshape.shape)\n",
    "# #     print(\"X_test_reshape.shape\")\n",
    "# #     print(X_test_reshape.shape)\n",
    "# #     print()\n",
    "\n",
    "# scaler = MinMaxScaler()\n",
    "# scaler.fit(X_train_reshape)\n",
    "# X_train_scaled = scaler.transform(X_train_reshape)\n",
    "# X_test_scaled = scaler.transform(X_test_reshape)\n",
    "\n",
    "# X_train_scaled = X_train_scaled.reshape(-1, n_steps, n_coins * n_price)\n",
    "\n",
    "# X_test_scaled = X_test_scaled.reshape(-1, n_steps, n_coins * n_price)\n",
    "\n",
    "# X_train_scaled, X_test_scaled = input_reshape(X_train_scaled, X_test_scaled, n_steps, n_coins, n_price)\n",
    "\n",
    "                        \n",
    "# p = {'lr': (0.1, 0.01, 0.001),\n",
    "#      'first_neuron':[4, 8, 16, 32, 64, 128],\n",
    "#      'batch_size': [64,128,256],\n",
    "#      'epochs': [100],\n",
    "#      'activation':['relu', 'softmax'],\n",
    "#      'dropout': (0, 0.40, 10),\n",
    "#      'optimizer': [Adam, Nadam],\n",
    "#      'kernel_initializer':['glorot_uniform', 'uniform', 'he_uniform'],\n",
    "#      'recurrent_initializer':['orthogonal'], \n",
    "#      'bias_initializer':['zeros'],\n",
    "#      'loss': ['categorical_crossentropy', 'logcosh'],\n",
    "#      'last_activation': ['softmax'],\n",
    "#      'weight_regulizer':[None],\n",
    "#      'emb_output_dims': [None]}\n",
    "\n",
    "# import hyperio as hy\n",
    "# h = hy.Hyperio(X_train_scaled, y_train, \n",
    "#                params=p, \n",
    "#                dataset_name='coin', \n",
    "#                experiment_no='1', \n",
    "#                model=create_model_SimpleRNN_non_GPU_test,\n",
    "#                # create_model_SimpleRNN_non_GPU_test(x_train, y_train, x_val, y_val, params)\n",
    "#                grid_downsample=.01)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# h"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# h.result"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Boost-Up"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'activation': ['relu'], 'batch_size': [64], 'init': ['he_uniform'], 'n_state_units': [160], 'optimizer': ['adam'], 'window_size': [10]}\n"
     ]
    }
   ],
   "source": [
    "evaluate_result = pickle.load(open(\"./evaluate_result/SimpleRNN_BTC_10_10_1_0.1_result.pickle\", \"rb\"))\n",
    "key = str(evaluate_result.keys())[12:-3]\n",
    "print(evaluate_result[key]['Best_Params'])\n",
    "best_params = evaluate_result[key]['Best_Params']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Machine = \"test\"\n",
    "# # If Machine is \"test\" then, _TEST argument must be True\n",
    "# test = False\n",
    "# _GPU= False\n",
    "# _ENHANCED = True\n",
    "\n",
    "# Internet_connection=True\n",
    "# n_jobs=-1\n",
    "# epochs=1\n",
    "\n",
    "# pickle_result_dir_path = \"./evaluate_result/\"\n",
    "# boost_up_result_dir_path = \"./boost_up_result/\"\n",
    "\n",
    "# for model in model_info[Machine][\"MODEL_list\"]:\n",
    "#     for idx_time_unit in model_info[Machine][\"time_unit\"]:\n",
    "#         for idx_window_size in model_info[Machine][\"window_size\"]:\n",
    "#             for idx_gap in model_info[Machine][\"gap\"]:\n",
    "#                 for idx_margin_rate in model_info[Machine][\"margin_rate\"]:\n",
    "#                     start(machine=Machine, \n",
    "#                          Internet_connection=Internet_connection, \n",
    "#                          pickle_result_dir_path=pickle_result_dir_path, \n",
    "#                          boost_up_result_dir_path=boost_up_result_dir_path,\n",
    "#                          _TEST=test, \n",
    "#                          _GPU=_GPU, \n",
    "#                          _ENHANCED=_ENHANCED,\n",
    "#                          n_jobs=n_jobs,\n",
    "#                          MODEL = model, \n",
    "#                          idx_time_unit=idx_time_unit, \n",
    "#                          idx_window_size=idx_window_size, \n",
    "#                          idx_gap=idx_gap, \n",
    "#                          idx_margin_rate=idx_margin_rate,\n",
    "#                          epochs=epochs,\n",
    "#                          best_params=best_params)\n",
    "#                     Javascript('IPython.notebook.kernel.restart()')\n",
    "#                     time.sleep(1)\n",
    "#                     Javascript('IPython.notebook.execute_all_cells()')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'activation': ['relu'], 'batch_size': [64], 'init': ['he_uniform'], 'n_state_units': [160], 'optimizer': ['adam'], 'window_size': [10]}\n"
     ]
    }
   ],
   "source": [
    "evaluate_result = pickle.load(open(\"./evaluate_result/SimpleRNN_BTC_10_10_1_0.1_result.pickle\", \"rb\"))\n",
    "key = str(evaluate_result.keys())[12:-3]\n",
    "print(evaluate_result[key]['Best_Params'])\n",
    "best_params = evaluate_result[key]['Best_Params']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['relu']"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "best_params['activation']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Boost_up_SimpleRNN(params, X_train, y_train, X_val, y_val, epochs):\n",
    "    model = Sequential()\n",
    "    model.add(\n",
    "        SimpleRNN('activation': params['activation'][0], \n",
    "                  'batch_size': params['batch_size'][0], \n",
    "                  'init': params['init'][0], \n",
    "                  'n_state_units': params['n_state_units'][0], \n",
    "                  'optimizer': params['optimizer'][0], \n",
    "                  'window_size' : params['window_size'][0], \n",
    "                  kernel_initializer='glorot_uniform', \n",
    "                  recurrent_initializer='orthogonal', \n",
    "                  bias_initializer='zeros', \n",
    "                  dropout=0.0,\n",
    "                  recurrent_dropout=0.0))\n",
    "    \n",
    "    model.add(Dense(units=neurons))\n",
    "    model.add(Dropout(dropout_rate))\n",
    "    \n",
    "#     model.add(Dense(units=2))\n",
    "#     model = multi_gpu_model(model, gpus=2)\n",
    "\n",
    "    model.compile(loss=keras.losses.categorical_crossentropy, \n",
    "                  optimizer=optimizer, \n",
    "                  metrics=[\"accuracy\", f1_score])\n",
    "    model.fit(X_train, y_train, batch_size=params['batch_size'], nb_epoch=epochs,\n",
    "              validation_data=(X_val, y_val))\n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "positional argument follows keyword argument (<ipython-input-35-37ff325e20b2>, line 55)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;36m  File \u001b[0;32m\"<ipython-input-35-37ff325e20b2>\"\u001b[0;36m, line \u001b[0;32m55\u001b[0m\n\u001b[0;31m    model.add(SimpleRNN(32, input_shape=(best_params['window_size'][0], 32),best_params['init'][0]))\u001b[0m\n\u001b[0m                                                                           ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m positional argument follows keyword argument\n"
     ]
    }
   ],
   "source": [
    "pickle_load_dir_path = \"./dataset_pickle_tuple_type/\"\n",
    "idx_time_unit = 10\n",
    "idx_window_size = 10\n",
    "idx_gap = 1\n",
    "idx_margin_rate = 0.1\n",
    "\n",
    "X = Load_Dataset_X(pickle_load_dir_path, idx_time_unit, idx_window_size, idx_gap, idx_margin_rate)\n",
    "y = Load_Dataset_y(pickle_load_dir_path, idx_time_unit, idx_window_size, idx_gap, idx_margin_rate)\n",
    "\n",
    "y_single = {}\n",
    "y_single['BTC'] = y[:, 1]\n",
    "y_single['ETH'] = y[:, 2]\n",
    "y_single['XRP'] = y[:, 3]\n",
    "y_single['BCH'] = y[:, 4]\n",
    "y_single['LTC'] = y[:, 5]\n",
    "y_single['DASH'] = y[:, 6]\n",
    "y_single['ETC'] = y[:, 7]\n",
    "\n",
    "coin = \"BTC\"\n",
    "y2 = onehottify(y_single[coin], n=2)\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y2, test_size=0.1, random_state=42)\n",
    "\n",
    "n_coins = 8\n",
    "n_price = 4\n",
    "n_steps = idx_window_size # 원래 100이었음. reshape 문제 때문에 수정함\n",
    "\n",
    "X_train_2 = X_train.transpose([0, 2, 1, 3])\n",
    "X_test_2 = X_test.transpose([0, 2, 1, 3])\n",
    "\n",
    "X_train_3 = X_train_2.reshape([X_train.shape[0], n_steps, n_coins * n_price])\n",
    "X_test_3 = X_test_2.reshape([X_test.shape[0], n_steps, n_coins * n_price])\n",
    "\n",
    "X_train_reshape = X_train_2.reshape([X_train.shape[0], n_steps*n_coins * n_price])\n",
    "X_test_reshape = X_test_2.reshape([X_test.shape[0], n_steps*n_coins * n_price])\n",
    "\n",
    "scaler = MinMaxScaler()\n",
    "scaler.fit(X_train_reshape)\n",
    "X_train_scaled = scaler.transform(X_train_reshape)\n",
    "X_test_scaled = scaler.transform(X_test_reshape)\n",
    "\n",
    "X_train_scaled = X_train_scaled.reshape(-1, n_steps, n_coins * n_price)\n",
    "\n",
    "X_test_scaled = X_test_scaled.reshape(-1, n_steps, n_coins * n_price)\n",
    "\n",
    "# X_train_scaled, X_test_scaled = input_reshape(X_train_scaled, X_test_scaled, n_steps, n_coins, n_price)\n",
    "\n",
    "# Boost_up_SimpleRNN(best_params, X_train_scaled, y_train, X_test_scaled, y_test, epochs)\n",
    "\n",
    "from keras.models import Sequential\n",
    "from keras.layers import SimpleRNN, Dense\n",
    "\n",
    "np.random.seed(0)\n",
    "model = Sequential()\n",
    "model.add(SimpleRNN(\n",
    "    32, \n",
    "    input_shape=(best_params['window_size'][0], 32),\n",
    "    best_params['init'][0]))\n",
    "model.add(Dense(1, activation=params['activation'][0]))\n",
    "model.compile(loss=keras.losses.categorical_crossentropy, optimizer=params['optimizer'][0])\n",
    "\n",
    "plt.plot(y_train, 'ro-', label=\"target\")\n",
    "plt.plot(model.predict(X_train_scaled[:,:,:]), 'bs-', label=\"output\")\n",
    "# plt.xlim(-0.5, 20.5)\n",
    "# plt.ylim(-1.1, 1.1)\n",
    "# plt.legend()\n",
    "# plt.title(\"Before training\")\n",
    "plt.show()\n",
    "\n",
    "history = model.fit(X_train_scaled, Y_train, epochs=100, verbose=0)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
