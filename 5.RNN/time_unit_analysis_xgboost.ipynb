{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/anaconda3/lib/python3.6/importlib/_bootstrap.py:219: RuntimeWarning: compiletime version 3.5 of module 'tensorflow.python.framework.fast_tensor_util' does not match runtime version 3.6\n",
      "  return f(*args, **kwds)\n",
      "/anaconda3/lib/python3.6/site-packages/h5py/__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n",
      "Using TensorFlow backend.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using Keras version: 2.2.2 backend: tensorflow\n"
     ]
    }
   ],
   "source": [
    "# import library\n",
    "import tensorflow as tf\n",
    "import keras\n",
    "from keras import optimizers\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Dropout, Flatten, Activation\n",
    "from keras.layers import SimpleRNN, LSTM, GRU\n",
    "from keras.wrappers.scikit_learn import KerasClassifier\n",
    "from keras import backend as K\n",
    "from keras.utils import multi_gpu_model\n",
    "from keras import metrics\n",
    "from keras import losses\n",
    "from keras import __version__\n",
    "print('Using Keras version:', __version__, 'backend:', K.backend())\n",
    "from keras.backend.tensorflow_backend import set_session\n",
    "\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "# from sklearn.pipeline import Pipeline\n",
    "from sklearn.preprocessing import MinMaxScaler, StandardScaler, PolynomialFeatures\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "from sklearn.metrics import precision_recall_fscore_support\n",
    "from sklearn.metrics import confusion_matrix, f1_score, precision_score, recall_score\n",
    "from sklearn.metrics import classification_report # https://stackoverflow.com/questions/50065484/getting-precision-recall-and-f1-score-per-class-in-keras\n",
    "\n",
    "from IPython.display import Javascript\n",
    "import numpy as np\n",
    "from distutils.version import LooseVersion as LV\n",
    "from datetime import datetime\n",
    "import csv\n",
    "import sys\n",
    "from os import listdir\n",
    "from os.path import isfile, join\n",
    "import pprint\n",
    "import boto3\n",
    "import pickle\n",
    "import time\n",
    "import os.path\n",
    "import pickle\n",
    "sys.path.append(os.getcwd())\n",
    "from matplotlib import pyplot\n",
    "\n",
    "import smtplib\n",
    "from email.mime.text import MIMEText # simple MSG\n",
    "from email.mime.multipart import MIMEMultipart # complex MSG\n",
    "        \n",
    "from link_aws_key import *\n",
    "pp = pprint.PrettyPrinter(indent=4)\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from datetime import datetime\n",
    "from sklearn.model_selection import RandomizedSearchCV, GridSearchCV\n",
    "from sklearn.metrics import roc_auc_score\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from xgboost import XGBClassifier\n",
    "\n",
    "\n",
    "coins = {\n",
    "    0: 'KRW',\n",
    "    1: 'BTC',\n",
    "    2: 'ETH',\n",
    "    3: 'XRP',\n",
    "    4: 'BCH',\n",
    "    5: 'LTC',\n",
    "    6: 'DASH',\n",
    "    7: 'ETC'\n",
    "}\n",
    "\n",
    "# aws_client = boto3.client(\n",
    "#     's3',\n",
    "#     aws_access_key_id=LINK_AWSAccessKeyId,\n",
    "#     aws_secret_access_key=LINK_AWSSecretKey\n",
    "# )\n",
    "\n",
    "bucket = \"bithumb10\"\n",
    "cleanup_file_name = \"coin_{0}_{1}_cleanup.csv\"\n",
    "\n",
    "\n",
    "#######################################################\n",
    "\n",
    "def Load_Dataset_X(dir_path, time_unit, window_size, gap, margin_rate):\n",
    "    key_name_X = \"X_\" + \\\n",
    "                    str(time_unit) + \"_\" + \\\n",
    "                    str(window_size) + \"_\" + \\\n",
    "                    str(gap) + \"_\" + \\\n",
    "                    str(margin_rate)\n",
    "                    \n",
    "\n",
    "    with open(dir_path + key_name_X + \".pickle\", 'rb') as handle:\n",
    "        b_x = pickle.load(handle)\n",
    "    return b_x\n",
    "\n",
    "def Load_Dataset_y(dir_path, time_unit, window_size, gap, margin_rate):\n",
    "    key_name_y = \"y_\" + \\\n",
    "                    str(time_unit) + \"_\" + \\\n",
    "                    str(window_size) + \"_\" + \\\n",
    "                    str(gap) + \"_\" + \\\n",
    "                    str(margin_rate)\n",
    "                    \n",
    "\n",
    "    with open(dir_path + key_name_y + \".pickle\", 'rb') as handle:\n",
    "        b_y = pickle.load(handle)\n",
    "    return b_y\n",
    "\n",
    "def f1_score(y_true, y_pred):\n",
    "    def recall(y_true, y_pred):\n",
    "        \"\"\"Recall metric.\n",
    "\n",
    "        Only computes a batch-wise average of recall.\n",
    "\n",
    "        Computes the recall, a metric for multi-label classification of\n",
    "        how many relevant items are selected.\n",
    "        \"\"\"\n",
    "        true_positives = K.sum(K.round(K.clip(y_true * y_pred, 0, 1)))\n",
    "        possible_positives = K.sum(K.round(K.clip(y_true, 0, 1)))\n",
    "        recall = true_positives / (possible_positives + K.epsilon())\n",
    "        return recall\n",
    "    \n",
    "    def precision(y_true, y_pred):\n",
    "        \"\"\"Precision metric.\n",
    "\n",
    "        Only computes a batch-wise average of precision.\n",
    "\n",
    "        Computes the precision, a metric for multi-label classification of\n",
    "        how many selected items are relevant.\n",
    "        \"\"\"\n",
    "        true_positives = K.sum(K.round(K.clip(y_true * y_pred, 0, 1)))\n",
    "        predicted_positives = K.sum(K.round(K.clip(y_pred, 0, 1)))\n",
    "        precision = true_positives / (predicted_positives + K.epsilon())\n",
    "        return precision\n",
    "    precision = precision(y_true, y_pred)\n",
    "    recall = recall(y_true, y_pred)\n",
    "    return 2*((precision*recall)/(precision+recall+K.epsilon()))\n",
    "\n",
    "def input_reshape(X_train_data, X_test_data, n_steps, n_coins, n_price):\n",
    "    X_train_reshape = X_train_data.reshape(\n",
    "        -1, \n",
    "        n_steps, \n",
    "        n_coins * n_price\n",
    "    )\n",
    "    X_test_reshape = X_test_data.reshape(\n",
    "        -1, \n",
    "        n_steps, \n",
    "        n_coins * n_price\n",
    "    )\n",
    "    return X_train_reshape, X_test_reshape\n",
    "\n",
    "def onehottify(x, n=None, dtype=np.int):\n",
    "    \"\"\"1-hot encode x with the max value n (computed from data if n is None).\"\"\"\n",
    "    x = np.asarray(x)\n",
    "    n = np.max(x) + 1 if n is None else n\n",
    "    return np.eye(n, dtype=dtype)[x]\n",
    "\n",
    "def timer(start_time=None):\n",
    "    if not start_time:\n",
    "        start_time = datetime.now()\n",
    "        return start_time\n",
    "    elif start_time:\n",
    "        thour, temp_sec = divmod((datetime.now() - start_time).total_seconds(), 3600)\n",
    "        tmin, tsec = divmod(temp_sec, 60)\n",
    "        print('\\n Time taken: %i hours %i minutes and %s seconds.' % (thour, tmin, round(tsec, 2)))\n",
    "\n",
    "def Start_Model(pickle_load_dir_path, \n",
    "                pickle_result_dir_path, \n",
    "                idx_time_unit, \n",
    "                idx_window_size, \n",
    "                idx_gap, \n",
    "                idx_margin_rate, \n",
    "                epochs, \n",
    "                MODEL, \n",
    "                _GPU, \n",
    "                n_jobs,\n",
    "                cv,\n",
    "                dataset_scale,\n",
    "                machine):\n",
    "    X = {}\n",
    "    y = {}\n",
    "\n",
    "    key_name_X = \"X_\"\n",
    "    key_name_y = \"y_\"\n",
    "\n",
    "    key_name_X += str(idx_time_unit) + \"_\" + str(idx_window_size) + \"_\" + str(idx_gap) + \"_\" + str(idx_margin_rate)\n",
    "    key_name_y += str(idx_time_unit) + \"_\" + str(idx_window_size) + \"_\" + str(idx_gap) + \"_\" + str(idx_margin_rate)\n",
    "\n",
    "    # remove [:10000], when real training\n",
    "    X = Load_Dataset_X(pickle_load_dir_path, idx_time_unit, idx_window_size, idx_gap, idx_margin_rate)[0][:dataset_scale]\n",
    "    y = Load_Dataset_y(pickle_load_dir_path, idx_time_unit, idx_window_size, idx_gap, idx_margin_rate)[1][:dataset_scale]\n",
    "    \n",
    "    y_single = {}\n",
    "#     print(\"[INFO] y : {}\".format(y))\n",
    "#     y = np.asarray(y[0])\n",
    "#     print(\"[INFO] y.shape : {}\".format(y.shape))\n",
    "#     print(\"[INFO] y : {}\".format(y))\n",
    "    y_single['BTC'] = y[:, 1]\n",
    "    y_single['ETH'] = y[:, 2]\n",
    "    y_single['XRP'] = y[:, 3]\n",
    "    y_single['BCH'] = y[:, 4]\n",
    "    y_single['LTC'] = y[:, 5]\n",
    "    y_single['DASH'] = y[:, 6]\n",
    "    y_single['ETC'] = y[:, 7]\n",
    "\n",
    "    coin_list2 = [\"BTC\", \"ETH\", \"XRP\", \"BCH\", \"LTC\", \"DASH\", \"ETC\"]\n",
    "\n",
    "\n",
    "    for coin in coin_list2:\n",
    "        if (os.path.isfile(pickle_result_dir_path + \\\n",
    "                          MODEL + \"_\" + \\\n",
    "                          coin + \"_\" + \\\n",
    "                          str(idx_time_unit) + \"_\" + \\\n",
    "                          str(idx_window_size) + \"_\" + \\\n",
    "                          str(idx_gap) + \"_\" + \\\n",
    "                          str(idx_margin_rate) + \\\n",
    "                          \"_result.pickle\")) is True:\n",
    "            print(MODEL + \"_\" + \\\n",
    "                  coin + \"_\" + \\\n",
    "                  str(idx_time_unit) + \"_\" + \\\n",
    "                  str(idx_window_size) + \"_\" + \\\n",
    "                  str(idx_gap) + \"_\" + \\\n",
    "                  str(idx_margin_rate) + \\\n",
    "                  \"_result.pickle FILE ALREADY EXIST.\")\n",
    "            continue\n",
    "        else:\n",
    "            y2 = onehottify(y_single[coin], n=2)\n",
    "\n",
    "            X_train, X_test, y_train, y_test = train_test_split(X, \n",
    "                                                                y2, \n",
    "                                                                test_size=0.1, \n",
    "                                                                random_state=42)\n",
    "            print(\"[INFO] X_train.shape : {}\".format(X_train.shape))\n",
    "            print(\"[INFO] y_train.shape : {}\".format(y_train.shape))\n",
    "            print(\"[INFO] X_test.shape : {}\".format(X_test.shape))\n",
    "            print(\"[INFO] y_test.shape : {}\".format(y_test.shape))\n",
    "            print()\n",
    "\n",
    "            n_coins = 8\n",
    "            n_price = 4\n",
    "            n_steps = idx_window_size \n",
    "\n",
    "            X_train_2 = X_train.transpose([0, 2, 1, 3])\n",
    "            X_test_2 = X_test.transpose([0, 2, 1, 3])\n",
    "            print(\"[INFO] X_train_2.shape: {}\".format(X_train_2.shape))\n",
    "            print(\"[INFO] X_test_2.shape: {}\".format(X_test_2.shape))\n",
    "            print()\n",
    "            \n",
    "            X_train_3 = X_train_2.reshape([X_train.shape[0], n_steps, n_coins * n_price])\n",
    "            X_test_3 = X_test_2.reshape([X_test.shape[0], n_steps, n_coins * n_price])\n",
    "            print(\"[INFO] X_train_3.shape: {}\".format(X_train_3.shape))\n",
    "            print(\"[INFO] X_test_3.shape: {}\".format(X_test_3.shape))\n",
    "            print()\n",
    "\n",
    "            X_train_reshape = X_train_2.reshape([X_train.shape[0], n_steps*n_coins * n_price])\n",
    "            X_test_reshape = X_test_2.reshape([X_test.shape[0], n_steps*n_coins * n_price])\n",
    "            print(\"[INFO] X_train_reshape.shape: {}\".format(X_train_reshape.shape))\n",
    "            print(\"[INFO] X_test_reshape.shape: {}\".format(X_test_reshape.shape))\n",
    "            print()\n",
    "            \n",
    "            param_grid = {'window_size' : [idx_window_size], \n",
    "                          'n_state_units': [16, 32, 64],\n",
    "                          'activation': ['tanh', 'sigmoid', 'relu'], \n",
    "                          'optimizer': ['rmsprop', 'Adam', 'Adagrad', 'SGD']}\n",
    "\n",
    "            scaler = MinMaxScaler()\n",
    "            scaler.fit(X_train_reshape)\n",
    "            \n",
    "            X_train_scaled = scaler.transform(X_train_reshape)\n",
    "            X_test_scaled = scaler.transform(X_test_reshape)\n",
    "\n",
    "            X_train_scaled = X_train_scaled.reshape(-1, \n",
    "                                                    n_steps, \n",
    "                                                    n_coins * n_price)\n",
    "            X_test_scaled = X_test_scaled.reshape(-1, \n",
    "                                                  n_steps, \n",
    "                                                  n_coins * n_price)\n",
    "\n",
    "            if _GPU == True:\n",
    "                model = KerasClassifier(build_fn=create_model_LSTM, \n",
    "                                        epochs=epochs, \n",
    "    #                                     batch_size=100, \n",
    "                                        verbose=True)\n",
    "                \n",
    "            elif _GPU == False:\n",
    "                model = KerasClassifier(build_fn=create_model_LSTM_non_GPU, \n",
    "                                        epochs=epochs, \n",
    "                                        batch_size=10, \n",
    "                                        verbose=True)\n",
    "\n",
    "            grid = GridSearchCV(estimator=model, \n",
    "                                cv=cv, \n",
    "                                n_jobs=n_jobs, # test\n",
    "                                param_grid=param_grid,\n",
    "                                verbose=1)\n",
    "\n",
    "            \n",
    "            X_train_scaled, X_test_scaled = input_reshape(X_train_scaled,  \n",
    "                                                          X_test_scaled, \n",
    "                                                          n_steps, \n",
    "                                                          n_coins, \n",
    "                                                          n_price)\n",
    "                      \n",
    "            \n",
    "            print()\n",
    "            print()\n",
    "            print(\"----------------------\")\n",
    "            print(\"<\"+MODEL+\">\")\n",
    "            print(\"----------------------\")\n",
    "            print(\"__\"+coin+\"__\" + \\\n",
    "                    \"time unit: \"+str(idx_time_unit) + \"  |  \" + \\\n",
    "                    \"window_size :\"+str(idx_window_size) + \"  |  \" + \\\n",
    "                    \"gap :\"+str(idx_gap) + \"  |  \" + \\\n",
    "                    \"margin_rate :\"+str(idx_margin_rate) + \\\n",
    "                    \"  started.\")\n",
    "\n",
    "            grid_result = grid.fit(X_train_scaled, \n",
    "                                   y_train, \n",
    "                                   validation_data=(X_test_scaled,\n",
    "                                                    y_test))\n",
    "\n",
    "            print(\"----------------------\")\n",
    "            print(\"grid_result.score(X_test_scaled, y_test): \",grid_result.score(X_test_scaled, y_test))\n",
    "\n",
    "            evaluate_result = {}\n",
    "\n",
    "            test_score = grid_result.score(X_test_scaled, y_test)\n",
    "            evaluate_result[MODEL + \"_\" + \\\n",
    "                          coin + \"_\" + \\\n",
    "                          str(idx_time_unit) + \"_\" + \\\n",
    "                          str(idx_window_size) + \"_\" + \\\n",
    "                          str(idx_gap) + \"_\" + \\\n",
    "                          str(idx_margin_rate)] = {\"MODEL\":MODEL,\\\n",
    "                                            \"Cryptocurrency\":coin, \\\n",
    "                                            \"Score\":grid_result.cv_results_['mean_test_score'], \\\n",
    "                                            \"Params\":grid_result.cv_results_['params'],\\\n",
    "                                            \"test_score\":test_score} \n",
    "        #     print()\n",
    "        #     print(\"evaluate result dict: \", evaluate_result)\n",
    "        #     print()\n",
    "\n",
    "            # summarize results\n",
    "            print()\n",
    "            print(\"Best: %f using %s\" % (grid_result.best_score_, grid_result.best_params_))\n",
    "            print()\n",
    "            # for checking pickle file exist\n",
    "            print(\"---pickle saving..\")\n",
    "            \n",
    "            \n",
    "            X = {}\n",
    "            y = {}\n",
    "            key_name_X = \"X_\"\n",
    "            key_name_y = \"y_\"\n",
    "\n",
    "            key_name_X += str(idx_time_unit) + \"_\" + str(idx_window_size) + \"_\" + str(idx_gap) + \"_\" + str(idx_margin_rate)\n",
    "            key_name_y += str(idx_time_unit) + \"_\" + str(idx_window_size) + \"_\" + str(idx_gap) + \"_\" + str(idx_margin_rate)\n",
    "            if (os.path.isfile(pickle_result_dir_path + \\\n",
    "                              MODEL + \"_\" + \\\n",
    "                              coin + \"_\" + \\\n",
    "                              str(idx_time_unit) + \"_\" + \\\n",
    "                              str(idx_window_size) + \"_\" + \\\n",
    "                              str(idx_gap) + \"_\" + \\\n",
    "                              str(idx_margin_rate) + \\\n",
    "                              \"_result.pickle\")) is not True:\n",
    "                with open(pickle_result_dir_path + \\\n",
    "                          MODEL + \"_\" + \\\n",
    "                          coin + \"_\" + \\\n",
    "                          str(idx_time_unit) + \"_\" + \\\n",
    "                          str(idx_window_size) + \"_\" + \\\n",
    "                          str(idx_gap) + \"_\" + \\\n",
    "                          str(idx_margin_rate) + \\\n",
    "                          \"_result.pickle\", 'wb') as handle:\n",
    "                    pickle.dump(evaluate_result, handle, protocol=pickle.HIGHEST_PROTOCOL)\n",
    "\n",
    "                    \n",
    "            else:\n",
    "                print(\"Already exist the file: \", pickle_result_dir_path + \\\n",
    "                                                  \"_test_\" + \\\n",
    "                                                  MODEL + \"_\" + \\\n",
    "                                                  \"BTC\" + \"_\" + \\\n",
    "                                                  str(idx_time_unit) + \"_\" + \\\n",
    "                                                  str(idx_window_size) + \"_\" + \\\n",
    "                                                  str(idx_gap) + \"_\" + \\\n",
    "                                                  str(idx_margin_rate) + \\\n",
    "                                                  \"_result.pickle\")\n",
    "\n",
    "            \n",
    "        #     for mean, stdev, param in zip(means, stds, params):\n",
    "        #         print(\"%f (%f) with: %r\" % (mean, stdev, param))\n",
    "        #     print()\n",
    "            key_name_X = \"X_\"\n",
    "            key_name_y = \"y_\"\n",
    "\n",
    "\n",
    "            return evaluate_result\n",
    "        \n",
    "def create_model_LSTM(window_size, n_state_units=32, activation='softmax', optimizer='adam', init='glorot_uniform', dropout_rate=0.0, neurons=2):\n",
    "    model = Sequential()\n",
    "    model.add(# if문을 통해 여러 RNN모델 쓸 수 있도록 하기, SimpleRNN외에 다른 RNN모델 찾아보기\n",
    "        LSTM( n_state_units, \n",
    "              input_shape=(window_size, 32),\n",
    "              use_bias=True, \n",
    "              activation='tanh',\n",
    "              kernel_initializer='glorot_uniform', \n",
    "              recurrent_initializer='orthogonal', \n",
    "              bias_initializer='zeros', \n",
    "              dropout=0.0,\n",
    "              recurrent_dropout=0.0))\n",
    "    \n",
    "    model.add(Dense(units=neurons))\n",
    "    model.add(Dropout(dropout_rate))\n",
    "#     model.add(Dense(units=2))\n",
    "    model = multi_gpu_model(model, gpus=2)\n",
    "\n",
    "    model.compile(loss=keras.losses.categorical_crossentropy, \n",
    "                  optimizer=optimizer, \n",
    "                  metrics=[\"accuracy\", f1_score])\n",
    "\n",
    "    return model\n",
    "\n",
    "\n",
    "def create_model_LSTM_non_GPU(window_size, \n",
    "                              n_state_units=32, \n",
    "                              activation='softmax', \n",
    "                              optimizer='adam'):\n",
    "    K.clear_session()\n",
    "    model = Sequential()\n",
    "    model.add(# if문을 통해 여러 RNN모델 쓸 수 있도록 하기, SimpleRNN외에 다른 RNN모델 찾아보기\n",
    "        LSTM(units=n_state_units, \n",
    "             input_shape=(window_size, 32),\n",
    "             use_bias=True))\n",
    "    \n",
    "    model.add(Dense(units=2))\n",
    "#     model.add(Dropout(dropout_rate))\n",
    "\n",
    "    model.compile(loss=keras.losses.categorical_crossentropy, \n",
    "                  optimizer=optimizer, \n",
    "                  metrics=[\"accuracy\", f1_score])\n",
    "\n",
    "    return model\n",
    "\n",
    "def start(machine, \n",
    "          _GPU,\n",
    "          n_jobs, \n",
    "          MODEL, \n",
    "          idx_time_unit, \n",
    "          idx_window_size, \n",
    "          idx_gap, \n",
    "          idx_margin_rate, \n",
    "          cv,\n",
    "          dataset_scale,\n",
    "          epochs):\n",
    "    \n",
    "    '''\n",
    "        [ATTENTION] In create_model METHOD part, need to set appropriate about GPU\n",
    "        \n",
    "        LINK01 -> GPU OFF\n",
    "        MSI -> GPU OFF\n",
    "        SLAVE04 -> GPU ON\n",
    "        SLAVE05 -> GPU ON\n",
    "    ''' \n",
    "    \n",
    "\n",
    "    start_time = time.time()\n",
    "    evaluate_result = Start_Model(pickle_load_dir_path = \"../_dataset/RNN_coin/\",  \n",
    "                pickle_result_dir_path = \"./evaluate_result/\", \n",
    "                idx_time_unit=idx_time_unit,\n",
    "                idx_window_size=idx_window_size, \n",
    "                idx_gap=idx_gap, \n",
    "                idx_margin_rate=idx_margin_rate, \n",
    "                epochs=epochs, \n",
    "                MODEL=MODEL, \n",
    "                _GPU=_GPU,\n",
    "                n_jobs=n_jobs,\n",
    "                cv=cv,\n",
    "                dataset_scale=dataset_scale,\n",
    "                machine=machine)\n",
    "    print(\"DEBUG\")\n",
    "    end_time = time.time()\n",
    "    print()\n",
    "    print(\"TIME: \", end_time-start_time)\n",
    "    return evaluate_result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_info = {}\n",
    "model_info[\"window_size_standard\"] = {\"MODEL_list\":[\"LSTM\"],\n",
    "                                      \"time_unit\":[10], \n",
    "                                      \"window_size\":[10, 25, 50, 75, 100],\n",
    "                                      \"gap\":[1], \n",
    "                                      \"margin_rate\":[0.1],\n",
    "                                      \"lr\":[0.01, 0.001]}\n",
    "\n",
    "Machine = \"window_size_standard\"\n",
    "# If Machine is \"test\" then, _TEST argument must be True\n",
    "test = True\n",
    "_GPU = False\n",
    "n_jobs = -1\n",
    "epochs = 1\n",
    "cv = 2 # at least 2\n",
    "dataset_scale = 1000\n",
    "evaluate_result_dict = {}\n",
    "coin_list = [\"BTC\", \"ETH\", \"XRP\", \"BCH\", \"LTC\", \"DASH\", \"ETC\"]\n",
    "\n",
    "for model in model_info[Machine][\"MODEL_list\"]:\n",
    "    for coin in coin_list:\n",
    "        for idx_time_unit in model_info[Machine][\"time_unit\"]:\n",
    "            for idx_window_size in model_info[Machine][\"window_size\"]:\n",
    "                for idx_gap in model_info[Machine][\"gap\"]:\n",
    "                    for idx_margin_rate in model_info[Machine][\"margin_rate\"]:\n",
    "                        key_pickleFileName = model + '_' + coin + '_' + str(idx_time_unit) + '_' + str(idx_window_size) + '_' + str(idx_gap) + '_' + str(idx_margin_rate)\n",
    "#                         print(key_pickleFileName)\n",
    "                        try: \n",
    "                            evaluate_result_dict[key_pickleFileName] = start(machine=Machine, \n",
    "                                                                          _GPU=_GPU, \n",
    "                                                                          n_jobs=1,\n",
    "                                                                          MODEL = model, \n",
    "                                                                          idx_time_unit=idx_time_unit, \n",
    "                                                                          idx_window_size=idx_window_size, \n",
    "                                                                          idx_gap=idx_gap, \n",
    "                                                                          idx_margin_rate=idx_margin_rate,\n",
    "                                                                          cv=cv,\n",
    "                                                                          dataset_scale=dataset_scale,\n",
    "                                                                          epochs=epochs)\n",
    "\n",
    "                            print(\"{:.2f}\".format(evaluate_result_dict[key_pickleFileName]['test_score']))\n",
    "                            \n",
    "                        except KeyError:\n",
    "                            print(\"[INFO] Appropriate value of {:4s} is not exist.\".format(coin))\n",
    "                            continue"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
